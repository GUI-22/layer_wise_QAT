model:
  name: tiny-llama-1.1b
  path: /data/gyy/TinyLlama
  root: /dataset/models
  local_path: /dataset/models/tiny/tiny-llama-1.1b
  local_root: /dataset/models
eval:
  num_gpus: 2
  batch_size: 8
  output_root: runs
  output_dirname: skip.y.[q]-krnl-rot-rodr.Seq.[fc2]-smth.xw.a0p3.b0p7.[AbsMax].[out+fc2].yx.a0p5.b0.[AbsMax].[qk]-pileval.128x1024.[0-0]-250116.093931
  attach_timestamp: true
  tasks:
  - wikitext
  max_seq_length: -4096
  evaluator: gptq
calib:
  data: pileval
  num_samples: 128
  cache_root: runs
  dataset_path: mit-han-lab/pile-val-backup
  seq_length: 1024
  min_seq_length: 0
  max_seq_length: 0
  local_dataset_path: /dataset/pile
quant:
  wgts:
    dtype: zint4
    group_shapes:
    - - 1
      - -1
      - -1
    - - 1
      - 128
      - -1
    group_scale_dtypes:
    - torch.float16
    - sint8
    compute_dtype: sint8
    compute_group_level: 0
    saturate_compute_dtype: false
    skips:
    - embed
    - head
    - router
    enable_calib_kernel: true
    calib_kernel:
      enable_gptq: true
      gptq:
        includes:
        - proj_1st
        - proj_2nd
        - proj_out
        - proj_qkv
        damp_percentage: 0.01
        block_size: 128
        num_inv_tries: 250
        hessian_block_size: 512
    enable_calib_range: true
    calib_range:
      degree: 2
      skips: []
      objective: OutputsError
      strategy: Manual
      granularity: Group
      element_batch_size: 64
      sample_batch_size: -1
      element_size: 512
      sample_size: -1
      pre_reshape: true
      outputs_device: cpu
      allow_kernel_calib: false
      ratio: 1.0
      max_shrink: 0.2
      max_expand: 1.0
      num_grids: 80
  ipts:
    dtype: sint8
    group_shapes:
    - - 1
      - -1
      - -1
    group_scale_dtypes:
    - torch.float16
    compute_dtype: null
    compute_group_level: -1
    saturate_compute_dtype: false
    skips:
    - embed
    - head
    - router
    static: false
    enable_calib_range: false
  opts:
    dtype: zint4
    group_shapes:
    - - 1
      - 128
      - -1
    group_scale_dtypes:
    - torch.float16
    compute_dtype: null
    compute_group_level: -1
    saturate_compute_dtype: false
    skips:
    - attn_q
    static: false
    enable_calib_range: false
  enable_rotation: true
  rotation:
    random: false
    transforms: []
  enable_reorder: true
  reorder:
    degree: 2
    skips:
    - proj_out
    - proj_qkv
    - residual
    strategy: Manual
    element_batch_size: -1
    sample_batch_size: -1
    element_size: -1
    sample_size: -1
    pre_reshape: true
    outputs_device: cpu
    allow_kernel_calib: false
    channel_metric: InputsAbsMax
    channel_index: Sequential
    dynamic: false
  enable_smooth: true
  smooth:
    enable_xw: true
    xw:
      degree: 2
      skips:
      - proj_1st
      - proj_qkv
      objective: OutputsError
      strategy: Manual
      granularity: Layer
      element_batch_size: -1
      sample_batch_size: -1
      element_size: -1
      sample_size: -1
      pre_reshape: true
      outputs_device: cpu
      allow_kernel_calib: false
      ranges:
      - - AbsMax
        - AbsMax
      alpha: 0.3
      beta: 0.7
      num_grids: 20
    enable_yx: true
    yx:
      degree: 2
      skips: []
      objective: OutputsError
      strategy: Manual
      granularity: Layer
      element_batch_size: -1
      sample_batch_size: -1
      element_size: -1
      sample_size: -1
      pre_reshape: true
      outputs_device: cpu
      allow_kernel_calib: false
      ranges:
      - - AbsMax
        - AbsMax
      alpha: 0.5
      beta: 0.0
      num_grids: 20
  bias_correction: false
  post_rotary: true
  develop_dtype: torch.float32
  enable_select_wgts: false
  enable_select_ipts: false
  enable_select_opts: false
seed: 12345
save_model: true
fairseq_args: /data/gyy/lmquant-main/projects/llm/configs/fairseq_args_1.1b_without_preprocess.json
gen_teacher_opts: false
enable_cache: true
with_preprocess: false
