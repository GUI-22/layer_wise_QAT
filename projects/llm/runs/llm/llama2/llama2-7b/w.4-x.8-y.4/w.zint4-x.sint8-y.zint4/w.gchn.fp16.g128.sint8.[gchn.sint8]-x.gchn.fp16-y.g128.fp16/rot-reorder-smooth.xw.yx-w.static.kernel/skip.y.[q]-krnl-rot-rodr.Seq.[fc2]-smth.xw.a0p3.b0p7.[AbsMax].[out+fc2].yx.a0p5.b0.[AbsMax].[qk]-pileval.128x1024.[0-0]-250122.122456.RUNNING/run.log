25-01-22 12:24:56 | I | === Configurations ===
25-01-22 12:24:56 | I | LlmRunConfig(
25-01-22 12:24:56 | I |   model=LlmModelConfig(
25-01-22 12:24:56 | I |     name=llama2-7b,
25-01-22 12:24:56 | I |     path=/data/gyy/llama2-7b,
25-01-22 12:24:56 | I |     root=/dataset/models,
25-01-22 12:24:56 | I |     local_path=/dataset/models/llama2/llama2-7b,
25-01-22 12:24:56 | I |     local_root=/dataset/models,
25-01-22 12:24:56 | I |     family=llama2,
25-01-22 12:24:56 | I |     size=7.0),
25-01-22 12:24:56 | I |   eval=LlmEvalConfig(
25-01-22 12:24:56 | I |     num_gpus=4,
25-01-22 12:24:56 | I |     batch_size=8,
25-01-22 12:24:56 | I |     output_root=runs,
25-01-22 12:24:56 | I |     output_dirname=skip.y.[q]-krnl-rot-rodr.Seq.[fc2]-smth.xw.a0p3.b0p7.[AbsMax].[out+fc2].yx.a0p5.b0.[AbsMax].[qk]-pileval.128x1024.[0-0]-250122.122456,
25-01-22 12:24:56 | I |     attach_timestamp=True,
25-01-22 12:24:56 | I |     timestamp=250122.122456,
25-01-22 12:24:56 | I |     output_dirname_without_timestamp=skip.y.[q]-krnl-rot-rodr.Seq.[fc2]-smth.xw.a0p3.b0p7.[AbsMax].[out+fc2].yx.a0p5.b0.[AbsMax].[qk]-pileval.128x1024.[0-0],
25-01-22 12:24:56 | I |     tasks=['wikitext'],
25-01-22 12:24:56 | I |     max_seq_length=-4096,
25-01-22 12:24:56 | I |     evaluator=gptq),
25-01-22 12:24:56 | I |   calib=LlmCalibConfig(
25-01-22 12:24:56 | I |     data=pileval,
25-01-22 12:24:56 | I |     num_samples=128,
25-01-22 12:24:56 | I |     cache_root=runs,
25-01-22 12:24:56 | I |     cache_dirpath=runs/llm/cache/pileval.128x1024.[0-0],
25-01-22 12:24:56 | I |     dataset_path=mit-han-lab/pile-val-backup,
25-01-22 12:24:56 | I |     seq_length=1024,
25-01-22 12:24:56 | I |     min_seq_length=0,
25-01-22 12:24:56 | I |     max_seq_length=0,
25-01-22 12:24:56 | I |     local_dataset_path=/dataset/pile),
25-01-22 12:24:56 | I |   quant=LlmQuantConfig(
25-01-22 12:24:56 | I |     wgts=WeightQuantizerConfig(
25-01-22 12:24:56 | I |       dtype=zint4,
25-01-22 12:24:56 | I |       group_shapes=((1, -1, -1), (1, 128, -1)),
25-01-22 12:24:56 | I |       group_scale_dtypes=(torch.float16, sint8),
25-01-22 12:24:56 | I |       compute_dtype=sint8,
25-01-22 12:24:56 | I |       compute_group_level=0,
25-01-22 12:24:56 | I |       saturate_compute_dtype=False,
25-01-22 12:24:56 | I |       exponent_scaling_level=2,
25-01-22 12:24:56 | I |       skips=['embed', 'head', 'router'],
25-01-22 12:24:56 | I |       static=True,
25-01-22 12:24:56 | I |       calib_kernel=QuantizerKernelConfig(
25-01-22 12:24:56 | I |         _kernels={'proj_1st': QuantGPTQConfig(includes=['proj_1st', 'proj_2nd', 'proj_out', 'proj_qkv'], damp_percentage=0.01, block_size=128, num_inv_tries=250, hessian_block_size=512), 'proj_2nd': QuantGPTQConfig(includes=['proj_1st', 'proj_2nd', 'proj_out', 'proj_qkv'], damp_percentage=0.01, block_size=128, num_inv_tries=250, hessian_block_size=512), 'proj_out': QuantGPTQConfig(includes=['proj_1st', 'proj_2nd', 'proj_out', 'proj_qkv'], damp_percentage=0.01, block_size=128, num_inv_tries=250, hessian_block_size=512), 'proj_qkv': QuantGPTQConfig(includes=['proj_1st', 'proj_2nd', 'proj_out', 'proj_qkv'], damp_percentage=0.01, block_size=128, num_inv_tries=250, hessian_block_size=512)},
25-01-22 12:24:56 | I |         gptq=QuantGPTQConfig(
25-01-22 12:24:56 | I |           includes=['proj_1st', 'proj_2nd', 'proj_out', 'proj_qkv'],
25-01-22 12:24:56 | I |           damp_percentage=0.01,
25-01-22 12:24:56 | I |           block_size=128,
25-01-22 12:24:56 | I |           num_inv_tries=250,
25-01-22 12:24:56 | I |           hessian_block_size=512)),
25-01-22 12:24:56 | I |       calib_range=DynamicRangeCalibConfig(
25-01-22 12:24:56 | I |         degree=2,
25-01-22 12:24:56 | I |         skips=[],
25-01-22 12:24:56 | I |         objective=SearchBasedCalibObjective.OutputsError,
25-01-22 12:24:56 | I |         strategy=SearchBasedCalibStrategy.Manual,
25-01-22 12:24:56 | I |         granularity=SearchBasedCalibGranularity.Group,
25-01-22 12:24:56 | I |         element_batch_size=64,
25-01-22 12:24:56 | I |         sample_batch_size=-1,
25-01-22 12:24:56 | I |         element_size=512,
25-01-22 12:24:56 | I |         sample_size=-1,
25-01-22 12:24:56 | I |         pre_reshape=True,
25-01-22 12:24:56 | I |         outputs_device=cpu,
25-01-22 12:24:56 | I |         allow_kernel_calib=False,
25-01-22 12:24:56 | I |         ratio=1.0,
25-01-22 12:24:56 | I |         max_shrink=0.2,
25-01-22 12:24:56 | I |         max_expand=1.0,
25-01-22 12:24:56 | I |         num_grids=80)),
25-01-22 12:24:56 | I |     ipts=ActivationQuantizerConfig(
25-01-22 12:24:56 | I |       dtype=sint8,
25-01-22 12:24:56 | I |       group_shapes=((1, -1, -1),),
25-01-22 12:24:56 | I |       group_scale_dtypes=(torch.float16,),
25-01-22 12:24:56 | I |       compute_dtype=None,
25-01-22 12:24:56 | I |       compute_group_level=-1,
25-01-22 12:24:56 | I |       saturate_compute_dtype=False,
25-01-22 12:24:56 | I |       exponent_scaling_level=1,
25-01-22 12:24:56 | I |       skips=['embed', 'head', 'router'],
25-01-22 12:24:56 | I |       static=False,
25-01-22 12:24:56 | I |       calib_kernel=None,
25-01-22 12:24:56 | I |       calib_range=None),
25-01-22 12:24:56 | I |     opts=ActivationQuantizerConfig(
25-01-22 12:24:56 | I |       dtype=zint4,
25-01-22 12:24:56 | I |       group_shapes=((1, 128, -1),),
25-01-22 12:24:56 | I |       group_scale_dtypes=(torch.float16,),
25-01-22 12:24:56 | I |       compute_dtype=None,
25-01-22 12:24:56 | I |       compute_group_level=-1,
25-01-22 12:24:56 | I |       saturate_compute_dtype=False,
25-01-22 12:24:56 | I |       exponent_scaling_level=1,
25-01-22 12:24:56 | I |       skips=['attn_q'],
25-01-22 12:24:56 | I |       static=False,
25-01-22 12:24:56 | I |       calib_kernel=None,
25-01-22 12:24:56 | I |       calib_range=None),
25-01-22 12:24:56 | I |     rotation=QuantRotationConfig(
25-01-22 12:24:56 | I |       random=False,
25-01-22 12:24:56 | I |       transforms=[]),
25-01-22 12:24:56 | I |     reorder=QuantReorderConfig(
25-01-22 12:24:56 | I |       degree=2,
25-01-22 12:24:56 | I |       skips=['proj_out', 'proj_qkv', 'residual'],
25-01-22 12:24:56 | I |       objective=SearchBasedCalibObjective.OutputsError,
25-01-22 12:24:56 | I |       strategy=SearchBasedCalibStrategy.Manual,
25-01-22 12:24:56 | I |       granularity=SearchBasedCalibGranularity.Layer,
25-01-22 12:24:56 | I |       element_batch_size=-1,
25-01-22 12:24:56 | I |       sample_batch_size=-1,
25-01-22 12:24:56 | I |       element_size=-1,
25-01-22 12:24:56 | I |       sample_size=-1,
25-01-22 12:24:56 | I |       pre_reshape=True,
25-01-22 12:24:56 | I |       outputs_device=cpu,
25-01-22 12:24:56 | I |       allow_kernel_calib=False,
25-01-22 12:24:56 | I |       channel_metric=ChannelMetric.InputsAbsMax,
25-01-22 12:24:56 | I |       channel_index=ChannelIndex.Sequential,
25-01-22 12:24:56 | I |       dynamic=False),
25-01-22 12:24:56 | I |     smooth=QuantSmoothConfig(
25-01-22 12:24:56 | I |       xw=QuantSmoothCalibConfig(
25-01-22 12:24:56 | I |         degree=2,
25-01-22 12:24:56 | I |         skips=['proj_1st', 'proj_qkv'],
25-01-22 12:24:56 | I |         objective=SearchBasedCalibObjective.OutputsError,
25-01-22 12:24:56 | I |         strategy=SearchBasedCalibStrategy.Manual,
25-01-22 12:24:56 | I |         granularity=SearchBasedCalibGranularity.Layer,
25-01-22 12:24:56 | I |         element_batch_size=-1,
25-01-22 12:24:56 | I |         sample_batch_size=-1,
25-01-22 12:24:56 | I |         element_size=-1,
25-01-22 12:24:56 | I |         sample_size=-1,
25-01-22 12:24:56 | I |         pre_reshape=True,
25-01-22 12:24:56 | I |         outputs_device=cpu,
25-01-22 12:24:56 | I |         allow_kernel_calib=False,
25-01-22 12:24:56 | I |         ranges=[(<RangeMode.AbsMax: 1>, <RangeMode.AbsMax: 1>)],
25-01-22 12:24:56 | I |         x_ranges=[<RangeMode.AbsMax: 1>],
25-01-22 12:24:56 | I |         w_ranges=[<RangeMode.AbsMax: 1>],
25-01-22 12:24:56 | I |         alpha=0.3,
25-01-22 12:24:56 | I |         beta=0.7,
25-01-22 12:24:56 | I |         num_grids=20),
25-01-22 12:24:56 | I |       yx=QuantSmoothCalibConfig(
25-01-22 12:24:56 | I |         degree=2,
25-01-22 12:24:56 | I |         skips=[],
25-01-22 12:24:56 | I |         objective=SearchBasedCalibObjective.OutputsError,
25-01-22 12:24:56 | I |         strategy=SearchBasedCalibStrategy.Manual,
25-01-22 12:24:56 | I |         granularity=SearchBasedCalibGranularity.Layer,
25-01-22 12:24:56 | I |         element_batch_size=-1,
25-01-22 12:24:56 | I |         sample_batch_size=-1,
25-01-22 12:24:56 | I |         element_size=-1,
25-01-22 12:24:56 | I |         sample_size=-1,
25-01-22 12:24:56 | I |         pre_reshape=True,
25-01-22 12:24:56 | I |         outputs_device=cpu,
25-01-22 12:24:56 | I |         allow_kernel_calib=False,
25-01-22 12:24:56 | I |         ranges=[(<RangeMode.AbsMax: 1>, <RangeMode.AbsMax: 1>)],
25-01-22 12:24:56 | I |         x_ranges=[<RangeMode.AbsMax: 1>],
25-01-22 12:24:56 | I |         w_ranges=[<RangeMode.AbsMax: 1>],
25-01-22 12:24:56 | I |         alpha=0.5,
25-01-22 12:24:56 | I |         beta=0.0,
25-01-22 12:24:56 | I |         num_grids=20)),
25-01-22 12:24:56 | I |     bias_correction=False,
25-01-22 12:24:56 | I |     post_rotary=True,
25-01-22 12:24:56 | I |     develop_dtype=torch.float32,
25-01-22 12:24:56 | I |     select_wgts=None,
25-01-22 12:24:56 | I |     select_ipts=None,
25-01-22 12:24:56 | I |     select_opts=None,
25-01-22 12:24:56 | I |     keywords_i={'proj_qkv': ['q_proj', 'k_proj', 'v_proj'], 'proj_out': ['out_proj', 'o_proj'], 'proj_1st': ['fc1', 'up_proj', 'gate_proj', 'w1', 'w3'], 'proj_2nd': ['fc2', 'down_proj', 'w2'], 'head': ['output', 'score', 'qa_outputs'], 'embed': ['embed', 'lm_head', 'embed_out'], 'router': ['block_sparse_moe']},
25-01-22 12:24:56 | I |     keywords_w={'proj_qkv': ['q_proj', 'k_proj', 'v_proj'], 'proj_out': ['out_proj', 'o_proj'], 'proj_1st': ['fc1', 'up_proj', 'gate_proj', 'w1', 'w3'], 'proj_2nd': ['fc2', 'down_proj', 'w2'], 'head': ['output', 'score', 'qa_outputs'], 'embed': ['embed', 'lm_head', 'embed_out'], 'router': ['block_sparse_moe.gate']},
25-01-22 12:24:56 | I |     keywords_o={'attn_q': ['q_rotary_emb'], 'attn_k': ['k_rotary_emb'], 'attn_v': ['v_proj']},
25-01-22 12:24:56 | I |     module_types_i=(<class 'torch.nn.modules.linear.Linear'>, <class 'transformers.models.mixtral.modeling_mixtral.MixtralSparseMoeBlock'>),
25-01-22 12:24:56 | I |     module_types_w=(<class 'torch.nn.modules.linear.Linear'>,),
25-01-22 12:24:56 | I |     module_types_o=(<class 'torch.nn.modules.linear.Linear'>, <class 'lmquant.llm.nn.attention.RotaryEmbedding'>),
25-01-22 12:24:56 | I |     num_hidden_layers=-1),
25-01-22 12:24:56 | I |   seed=12345,
25-01-22 12:24:56 | I |   save_model=True,
25-01-22 12:24:56 | I |   output_dirpath=runs/llm/llama2/llama2-7b/w.4-x.8-y.4/w.zint4-x.sint8-y.zint4/w.gchn.fp16.g128.sint8.[gchn.sint8]-x.gchn.fp16-y.g128.fp16/rot-reorder-smooth.xw.yx-w.static.kernel/skip.y.[q]-krnl-rot-rodr.Seq.[fc2]-smth.xw.a0p3.b0p7.[AbsMax].[out+fc2].yx.a0p5.b0.[AbsMax].[qk]-pileval.128x1024.[0-0]-250122.122456,
25-01-22 12:24:56 | I |   cache_dirpath=LlmQuantCachePath(rotation='runs/llm/cache/rotation/hadamard', reorder='runs/llm/cache/pileval.128x1024.[0-0]/reorder/w.4-x.8-y.4/w.zint4-x.sint8-y.zint4/w.gchn.fp16.g128.sint8.[gchn.sint8]-x.gchn.fp16-y.g128.fp16/w.skip.[embed+head+router]-x.skip.[embed+head+router]-y.skip.[attn_q]/rotate.hadamard/reorder.OutputsError.Manual.Layer.d2.en1.sn1/reorder.InputsAbsMax.Sequential/reorder.skip.[proj_out+proj_qkv+residual]', smooth='runs/llm/cache/pileval.128x1024.[0-0]/smooth/w.4-x.8-y.4/w.zint4-x.sint8-y.zint4/w.gchn.fp16.g128.sint8.[gchn.sint8]-x.gchn.fp16-y.g128.fp16/w.skip.[embed+head+router]-x.skip.[embed+head+router]-y.skip.[attn_q]/rotate.hadamard/reorder.OutputsError.Manual.Layer.d2.en1.sn1/reorder.InputsAbsMax.Sequential/reorder.skip.[proj_out+proj_qkv+residual]/smooth.xw.OutputsError.Manual.Layer.d2.en1.sn1-yx.OutputsError.Manual.Layer.d2.en1.sn1/smooth.xw.[x.AbsMax.w.AbsMax]-yx.[x.AbsMax.w.AbsMax]/smooth.xw.a0p3.b0p7-yx.a0p5.b0/smooth.xw.skip.[proj_1st+proj_qkv]-yx.skip.[]', wgts='runs/llm/cache/pileval.128x1024.[0-0]/wgts/w.4-x.8-y.4/w.zint4-x.sint8-y.zint4/w.gchn.fp16.g128.sint8.[gchn.sint8]-x.gchn.fp16-y.g128.fp16/w.skip.[embed+head+router]-x.skip.[embed+head+router]-y.skip.[attn_q]/rotate.hadamard/reorder.OutputsError.Manual.Layer.d2.en1.sn1/reorder.InputsAbsMax.Sequential/reorder.skip.[proj_out+proj_qkv+residual]/smooth.xw.OutputsError.Manual.Layer.d2.en1.sn1-yx.OutputsError.Manual.Layer.d2.en1.sn1/smooth.xw.[x.AbsMax.w.AbsMax]-yx.[x.AbsMax.w.AbsMax]/smooth.xw.a0p3.b0p7-yx.a0p5.b0/smooth.xw.skip.[proj_1st+proj_qkv]-yx.skip.[]/w.kernel.gptq.d0p01.b128/w.kernel.gptq.include.[proj_1st+proj_2nd+proj_out+proj_qkv]/w.range.OutputsError.Manual.Group.d2.e512.sn1/w.range.r.[1].static/w.range.skip.[]', acts=''),
25-01-22 12:24:56 | I |   cache_path=LlmQuantCachePath(rotation='runs/llm/cache/rotation/hadamard/llama2-7b.pt', reorder='runs/llm/cache/pileval.128x1024.[0-0]/reorder/w.4-x.8-y.4/w.zint4-x.sint8-y.zint4/w.gchn.fp16.g128.sint8.[gchn.sint8]-x.gchn.fp16-y.g128.fp16/w.skip.[embed+head+router]-x.skip.[embed+head+router]-y.skip.[attn_q]/rotate.hadamard/reorder.OutputsError.Manual.Layer.d2.en1.sn1/reorder.InputsAbsMax.Sequential/reorder.skip.[proj_out+proj_qkv+residual]/llama2-7b.pt', smooth='runs/llm/cache/pileval.128x1024.[0-0]/smooth/w.4-x.8-y.4/w.zint4-x.sint8-y.zint4/w.gchn.fp16.g128.sint8.[gchn.sint8]-x.gchn.fp16-y.g128.fp16/w.skip.[embed+head+router]-x.skip.[embed+head+router]-y.skip.[attn_q]/rotate.hadamard/reorder.OutputsError.Manual.Layer.d2.en1.sn1/reorder.InputsAbsMax.Sequential/reorder.skip.[proj_out+proj_qkv+residual]/smooth.xw.OutputsError.Manual.Layer.d2.en1.sn1-yx.OutputsError.Manual.Layer.d2.en1.sn1/smooth.xw.[x.AbsMax.w.AbsMax]-yx.[x.AbsMax.w.AbsMax]/smooth.xw.a0p3.b0p7-yx.a0p5.b0/smooth.xw.skip.[proj_1st+proj_qkv]-yx.skip.[]/llama2-7b.pt', wgts='runs/llm/cache/pileval.128x1024.[0-0]/wgts/w.4-x.8-y.4/w.zint4-x.sint8-y.zint4/w.gchn.fp16.g128.sint8.[gchn.sint8]-x.gchn.fp16-y.g128.fp16/w.skip.[embed+head+router]-x.skip.[embed+head+router]-y.skip.[attn_q]/rotate.hadamard/reorder.OutputsError.Manual.Layer.d2.en1.sn1/reorder.InputsAbsMax.Sequential/reorder.skip.[proj_out+proj_qkv+residual]/smooth.xw.OutputsError.Manual.Layer.d2.en1.sn1-yx.OutputsError.Manual.Layer.d2.en1.sn1/smooth.xw.[x.AbsMax.w.AbsMax]-yx.[x.AbsMax.w.AbsMax]/smooth.xw.a0p3.b0p7-yx.a0p5.b0/smooth.xw.skip.[proj_1st+proj_qkv]-yx.skip.[]/w.kernel.gptq.d0p01.b128/w.kernel.gptq.include.[proj_1st+proj_2nd+proj_out+proj_qkv]/w.range.OutputsError.Manual.Group.d2.e512.sn1/w.range.r.[1].static/w.range.skip.[]/llama2-7b.pt', acts=''),
25-01-22 12:24:56 | I |   fairseq_args=/data/gyy/lmquant-main/projects/llm/configs/fairseq_args_7b_without_preprocess.json,
25-01-22 12:24:56 | I |   gen_teacher_opts=False,
25-01-22 12:24:56 | I |   enable_cache=True,
25-01-22 12:24:56 | I |   with_preprocess=False)
25-01-22 12:24:56 | I | === Dumped Configurations ===
25-01-22 12:24:56 | I | { 'calib': { 'cache_root': 'runs',
25-01-22 12:24:56 | I |              'data': 'pileval',
25-01-22 12:24:56 | I |              'dataset_path': 'mit-han-lab/pile-val-backup',
25-01-22 12:24:56 | I |              'local_dataset_path': '/dataset/pile',
25-01-22 12:24:56 | I |              'max_seq_length': 0,
25-01-22 12:24:56 | I |              'min_seq_length': 0,
25-01-22 12:24:56 | I |              'num_samples': 128,
25-01-22 12:24:56 | I |              'seq_length': 1024},
25-01-22 12:24:56 | I |   'enable_cache': True,
25-01-22 12:24:56 | I |   'eval': { 'attach_timestamp': True,
25-01-22 12:24:56 | I |             'batch_size': 8,
25-01-22 12:24:56 | I |             'evaluator': 'gptq',
25-01-22 12:24:56 | I |             'max_seq_length': -4096,
25-01-22 12:24:56 | I |             'num_gpus': 4,
25-01-22 12:24:56 | I |             'output_dirname': 'skip.y.[q]-krnl-rot-rodr.Seq.[fc2]-smth.xw.a0p3.b0p7.[AbsMax].[out+fc2].yx.a0p5.b0.[AbsMax].[qk]-pileval.128x1024.[0-0]-250122.122456',
25-01-22 12:24:56 | I |             'output_root': 'runs',
25-01-22 12:24:56 | I |             'tasks': ['wikitext']},
25-01-22 12:24:56 | I |   'fairseq_args': '/data/gyy/lmquant-main/projects/llm/configs/fairseq_args_7b_without_preprocess.json',
25-01-22 12:24:56 | I |   'gen_teacher_opts': False,
25-01-22 12:24:56 | I |   'model': { 'local_path': '/dataset/models/llama2/llama2-7b',
25-01-22 12:24:56 | I |              'local_root': '/dataset/models',
25-01-22 12:24:56 | I |              'name': 'llama2-7b',
25-01-22 12:24:56 | I |              'path': '/data/gyy/llama2-7b',
25-01-22 12:24:56 | I |              'root': '/dataset/models'},
25-01-22 12:24:56 | I |   'quant': { 'bias_correction': False,
25-01-22 12:24:56 | I |              'develop_dtype': 'torch.float32',
25-01-22 12:24:56 | I |              'enable_reorder': True,
25-01-22 12:24:56 | I |              'enable_rotation': True,
25-01-22 12:24:56 | I |              'enable_select_ipts': False,
25-01-22 12:24:56 | I |              'enable_select_opts': False,
25-01-22 12:24:56 | I |              'enable_select_wgts': False,
25-01-22 12:24:56 | I |              'enable_smooth': True,
25-01-22 12:24:56 | I |              'ipts': { 'compute_dtype': None,
25-01-22 12:24:56 | I |                        'compute_group_level': -1,
25-01-22 12:24:56 | I |                        'dtype': 'sint8',
25-01-22 12:24:56 | I |                        'enable_calib_range': False,
25-01-22 12:24:56 | I |                        'group_scale_dtypes': ['torch.float16'],
25-01-22 12:24:56 | I |                        'group_shapes': [[1, -1, -1]],
25-01-22 12:24:56 | I |                        'saturate_compute_dtype': False,
25-01-22 12:24:56 | I |                        'skips': ['embed', 'head', 'router'],
25-01-22 12:24:56 | I |                        'static': False},
25-01-22 12:24:56 | I |              'opts': { 'compute_dtype': None,
25-01-22 12:24:56 | I |                        'compute_group_level': -1,
25-01-22 12:24:56 | I |                        'dtype': 'zint4',
25-01-22 12:24:56 | I |                        'enable_calib_range': False,
25-01-22 12:24:56 | I |                        'group_scale_dtypes': ['torch.float16'],
25-01-22 12:24:56 | I |                        'group_shapes': [[1, 128, -1]],
25-01-22 12:24:56 | I |                        'saturate_compute_dtype': False,
25-01-22 12:24:56 | I |                        'skips': ['attn_q'],
25-01-22 12:24:56 | I |                        'static': False},
25-01-22 12:24:56 | I |              'post_rotary': True,
25-01-22 12:24:56 | I |              'reorder': { 'allow_kernel_calib': False,
25-01-22 12:24:56 | I |                           'channel_index': 'Sequential',
25-01-22 12:24:56 | I |                           'channel_metric': 'InputsAbsMax',
25-01-22 12:24:56 | I |                           'degree': 2,
25-01-22 12:24:56 | I |                           'dynamic': False,
25-01-22 12:24:56 | I |                           'element_batch_size': -1,
25-01-22 12:24:56 | I |                           'element_size': -1,
25-01-22 12:24:56 | I |                           'outputs_device': 'cpu',
25-01-22 12:24:56 | I |                           'pre_reshape': True,
25-01-22 12:24:56 | I |                           'sample_batch_size': -1,
25-01-22 12:24:56 | I |                           'sample_size': -1,
25-01-22 12:24:56 | I |                           'skips': ['proj_out', 'proj_qkv', 'residual'],
25-01-22 12:24:56 | I |                           'strategy': 'Manual'},
25-01-22 12:24:56 | I |              'rotation': {'random': False, 'transforms': []},
25-01-22 12:24:56 | I |              'smooth': { 'enable_xw': True,
25-01-22 12:24:56 | I |                          'enable_yx': True,
25-01-22 12:24:56 | I |                          'xw': { 'allow_kernel_calib': False,
25-01-22 12:24:56 | I |                                  'alpha': 0.3,
25-01-22 12:24:56 | I |                                  'beta': 0.7,
25-01-22 12:24:56 | I |                                  'degree': 2,
25-01-22 12:24:56 | I |                                  'element_batch_size': -1,
25-01-22 12:24:56 | I |                                  'element_size': -1,
25-01-22 12:24:56 | I |                                  'granularity': 'Layer',
25-01-22 12:24:56 | I |                                  'num_grids': 20,
25-01-22 12:24:56 | I |                                  'objective': 'OutputsError',
25-01-22 12:24:56 | I |                                  'outputs_device': 'cpu',
25-01-22 12:24:56 | I |                                  'pre_reshape': True,
25-01-22 12:24:56 | I |                                  'ranges': [['AbsMax', 'AbsMax']],
25-01-22 12:24:56 | I |                                  'sample_batch_size': -1,
25-01-22 12:24:56 | I |                                  'sample_size': -1,
25-01-22 12:24:56 | I |                                  'skips': ['proj_1st', 'proj_qkv'],
25-01-22 12:24:56 | I |                                  'strategy': 'Manual'},
25-01-22 12:24:56 | I |                          'yx': { 'allow_kernel_calib': False,
25-01-22 12:24:56 | I |                                  'alpha': 0.5,
25-01-22 12:24:56 | I |                                  'beta': 0.0,
25-01-22 12:24:56 | I |                                  'degree': 2,
25-01-22 12:24:56 | I |                                  'element_batch_size': -1,
25-01-22 12:24:56 | I |                                  'element_size': -1,
25-01-22 12:24:56 | I |                                  'granularity': 'Layer',
25-01-22 12:24:56 | I |                                  'num_grids': 20,
25-01-22 12:24:56 | I |                                  'objective': 'OutputsError',
25-01-22 12:24:56 | I |                                  'outputs_device': 'cpu',
25-01-22 12:24:56 | I |                                  'pre_reshape': True,
25-01-22 12:24:56 | I |                                  'ranges': [['AbsMax', 'AbsMax']],
25-01-22 12:24:56 | I |                                  'sample_batch_size': -1,
25-01-22 12:24:56 | I |                                  'sample_size': -1,
25-01-22 12:24:56 | I |                                  'skips': [],
25-01-22 12:24:56 | I |                                  'strategy': 'Manual'}},
25-01-22 12:24:56 | I |              'wgts': { 'calib_kernel': { 'enable_gptq': True,
25-01-22 12:24:56 | I |                                          'gptq': { 'block_size': 128,
25-01-22 12:24:56 | I |                                                    'damp_percentage': 0.01,
25-01-22 12:24:56 | I |                                                    'hessian_block_size': 512,
25-01-22 12:24:56 | I |                                                    'includes': ['proj_1st', 'proj_2nd', 'proj_out', 'proj_qkv'],
25-01-22 12:24:56 | I |                                                    'num_inv_tries': 250}},
25-01-22 12:24:56 | I |                        'calib_range': { 'allow_kernel_calib': False,
25-01-22 12:24:56 | I |                                         'degree': 2,
25-01-22 12:24:56 | I |                                         'element_batch_size': 64,
25-01-22 12:24:56 | I |                                         'element_size': 512,
25-01-22 12:24:56 | I |                                         'granularity': 'Group',
25-01-22 12:24:56 | I |                                         'max_expand': 1.0,
25-01-22 12:24:56 | I |                                         'max_shrink': 0.2,
25-01-22 12:24:56 | I |                                         'num_grids': 80,
25-01-22 12:24:56 | I |                                         'objective': 'OutputsError',
25-01-22 12:24:56 | I |                                         'outputs_device': 'cpu',
25-01-22 12:24:56 | I |                                         'pre_reshape': True,
25-01-22 12:24:56 | I |                                         'ratio': 1.0,
25-01-22 12:24:56 | I |                                         'sample_batch_size': -1,
25-01-22 12:24:56 | I |                                         'sample_size': -1,
25-01-22 12:24:56 | I |                                         'skips': [],
25-01-22 12:24:56 | I |                                         'strategy': 'Manual'},
25-01-22 12:24:56 | I |                        'compute_dtype': 'sint8',
25-01-22 12:24:56 | I |                        'compute_group_level': 0,
25-01-22 12:24:56 | I |                        'dtype': 'zint4',
25-01-22 12:24:56 | I |                        'enable_calib_kernel': True,
25-01-22 12:24:56 | I |                        'enable_calib_range': True,
25-01-22 12:24:56 | I |                        'group_scale_dtypes': ['torch.float16', 'sint8'],
25-01-22 12:24:56 | I |                        'group_shapes': [[1, -1, -1], [1, 128, -1]],
25-01-22 12:24:56 | I |                        'saturate_compute_dtype': False,
25-01-22 12:24:56 | I |                        'skips': ['embed', 'head', 'router']}},
25-01-22 12:24:56 | I |   'save_model': True,
25-01-22 12:24:56 | I |   'seed': 12345,
25-01-22 12:24:56 | I |   'with_preprocess': False}
25-01-22 12:24:56 | I | === Output Directory ===
25-01-22 12:24:56 | I | runs/llm/llama2/llama2-7b/w.4-x.8-y.4/w.zint4-x.sint8-y.zint4/w.gchn.fp16.g128.sint8.[gchn.sint8]-x.gchn.fp16-y.g128.fp16/rot-reorder-smooth.xw.yx-w.static.kernel/skip.y.[q]-krnl-rot-rodr.Seq.[fc2]-smth.xw.a0p3.b0p7.[AbsMax].[out+fc2].yx.a0p5.b0.[AbsMax].[qk]-pileval.128x1024.[0-0]-250122.122456
25-01-22 12:24:56 | I | === Start Evaluating ===
25-01-22 12:24:56 | I | * Building model llama2-7b from /data/gyy/llama2-7b
25-01-22 12:25:02 | I | - Patching LlamaSdpaAttention._old_forward in model.layers.0.self_attn
25-01-22 12:25:02 | I | - Patching LlamaSdpaAttention._old_forward in model.layers.1.self_attn
25-01-22 12:25:02 | I | - Patching LlamaSdpaAttention._old_forward in model.layers.2.self_attn
25-01-22 12:25:02 | I | - Patching LlamaSdpaAttention._old_forward in model.layers.3.self_attn
25-01-22 12:25:02 | I | - Patching LlamaSdpaAttention._old_forward in model.layers.4.self_attn
25-01-22 12:25:02 | I | - Patching LlamaSdpaAttention._old_forward in model.layers.5.self_attn
25-01-22 12:25:02 | I | - Patching LlamaSdpaAttention._old_forward in model.layers.6.self_attn
25-01-22 12:25:02 | I | - Patching LlamaSdpaAttention._old_forward in model.layers.7.self_attn
25-01-22 12:25:02 | I | - Patching LlamaSdpaAttention._old_forward in model.layers.8.self_attn
25-01-22 12:25:02 | I | - Patching LlamaSdpaAttention._old_forward in model.layers.9.self_attn
25-01-22 12:25:02 | I | - Patching LlamaSdpaAttention._old_forward in model.layers.10.self_attn
25-01-22 12:25:02 | I | - Patching LlamaSdpaAttention._old_forward in model.layers.11.self_attn
25-01-22 12:25:02 | I | - Patching LlamaSdpaAttention._old_forward in model.layers.12.self_attn
25-01-22 12:25:02 | I | - Patching LlamaSdpaAttention._old_forward in model.layers.13.self_attn
25-01-22 12:25:02 | I | - Patching LlamaSdpaAttention._old_forward in model.layers.14.self_attn
25-01-22 12:25:02 | I | - Patching LlamaSdpaAttention._old_forward in model.layers.15.self_attn
25-01-22 12:25:02 | I | - Patching LlamaSdpaAttention._old_forward in model.layers.16.self_attn
25-01-22 12:25:02 | I | - Patching LlamaSdpaAttention._old_forward in model.layers.17.self_attn
25-01-22 12:25:02 | I | - Patching LlamaSdpaAttention._old_forward in model.layers.18.self_attn
25-01-22 12:25:02 | I | - Patching LlamaSdpaAttention._old_forward in model.layers.19.self_attn
25-01-22 12:25:02 | I | - Patching LlamaSdpaAttention._old_forward in model.layers.20.self_attn
25-01-22 12:25:02 | I | - Patching LlamaSdpaAttention._old_forward in model.layers.21.self_attn
25-01-22 12:25:02 | I | - Patching LlamaSdpaAttention._old_forward in model.layers.22.self_attn
25-01-22 12:25:02 | I | - Patching LlamaSdpaAttention._old_forward in model.layers.23.self_attn
25-01-22 12:25:02 | I | - Patching LlamaSdpaAttention._old_forward in model.layers.24.self_attn
25-01-22 12:25:02 | I | - Patching LlamaSdpaAttention._old_forward in model.layers.25.self_attn
25-01-22 12:25:02 | I | - Patching LlamaSdpaAttention._old_forward in model.layers.26.self_attn
25-01-22 12:25:02 | I | - Patching LlamaSdpaAttention._old_forward in model.layers.27.self_attn
25-01-22 12:25:02 | I | - Patching LlamaSdpaAttention._old_forward in model.layers.28.self_attn
25-01-22 12:25:02 | I | - Patching LlamaSdpaAttention._old_forward in model.layers.29.self_attn
25-01-22 12:25:02 | I | - Patching LlamaSdpaAttention._old_forward in model.layers.30.self_attn
25-01-22 12:25:02 | I | - Patching LlamaSdpaAttention._old_forward in model.layers.31.self_attn
25-01-22 12:25:02 | I | * Development dtype is torch.float32
25-01-22 12:25:02 | I | * Begin to QAT
25-01-22 12:25:25 | D | Setting JobRuntime:name=UNKNOWN_NAME
25-01-22 12:25:25 | D | Setting JobRuntime:name=utils
25-01-22 12:29:19 | I | distributed init (rank 0): env://
25-01-22 12:29:19 | I | initialized host 3fb0cc53fbf9 as rank 0
25-01-22 12:29:20 | I | nvidia-smi stats: {'gpu_0_mem_used_gb': 0.0, 'gpu_1_mem_used_gb': 0.0, 'gpu_2_mem_used_gb': 0.0, 'gpu_3_mem_used_gb': 0.0, 'gpu_4_mem_used_gb': 6.150390625, 'gpu_5_mem_used_gb': 7.0859375, 'gpu_6_mem_used_gb': 7.0859375, 'gpu_7_mem_used_gb': 6.06640625}
25-01-22 12:29:20 | I | {'_name': None, 'common': {'_name': None, 'no_progress_bar': False, 'log_interval': 1, 'log_format': 'simple', 'log_file': None, 'tensorboard_logdir': None, 'wandb_project': 'layer_wise_quant', 'azureml_logging': False, 'seed': 1, 'cpu': False, 'tpu': False, 'bf16': False, 'memory_efficient_bf16': False, 'fp16': True, 'memory_efficient_fp16': True, 'fp16_no_flatten_grads': False, 'fp16_init_scale': 128, 'fp16_scale_window': None, 'fp16_scale_tolerance': 0.0, 'min_loss_scale': 0.0001, 'threshold_loss_scale': None, 'user_dir': None, 'empty_cache_freq': 0, 'all_gather_list_size': 16384, 'model_parallel_size': 1, 'quantization_config_path': None, 'profile': False, 'reset_logging': False, 'suppress_crashes': False, 'use_plasma_view': False, 'plasma_path': '/tmp/plasma', 'log_nvidia_smi': False}, 'common_eval': {'_name': None, 'path': None, 'post_process': None, 'quiet': False, 'model_overrides': '{}', 'results_path': None, 'is_moe': False, 'is_model_parallel': False}, 'distributed_training': {'_name': None, 'distributed_world_size': 1, 'distributed_rank': 0, 'distributed_backend': 'nccl', 'distributed_init_method': 'env://', 'distributed_port': -1, 'device_id': 0, 'distributed_no_spawn': True, 'ddp_backend': 'pytorch_ddp', 'bucket_cap_mb': 25, 'fix_batches_to_gpus': False, 'find_unused_parameters': False, 'fast_stat_sync': False, 'heartbeat_timeout': -1, 'broadcast_buffers': False, 'slowmo_momentum': None, 'slowmo_algorithm': 'LocalSGD', 'localsgd_frequency': 3, 'nprocs_per_node': 4, 'pipeline_model_parallel': False, 'pipeline_balance': None, 'pipeline_devices': None, 'pipeline_chunks': 0, 'pipeline_encoder_balance': None, 'pipeline_encoder_devices': None, 'pipeline_decoder_balance': None, 'pipeline_decoder_devices': None, 'pipeline_checkpoint': 'never', 'zero_sharding': 'none', 'zero_group_size': -1, 'save_zero_ckpt_fast': False, 'fp16': True, 'memory_efficient_fp16': True, 'tpu': False, 'no_reshard_after_forward': False, 'fp32_reduce_scatter': False, 'cpu_offload': False, 'use_sharded_state': False}, 'dataset': {'_name': None, 'num_workers': 1, 'num_workers_valid': 0, 'skip_invalid_size_inputs_valid_test': False, 'max_tokens': None, 'batch_size': 16, 'required_batch_size_multiple': 8, 'required_seq_len_multiple': 1, 'dataset_impl': None, 'data_buffer_size': 10, 'train_subset': 'train', 'valid_subset': 'valid', 'combine_valid_subsets': None, 'ignore_unused_valid_subsets': False, 'validate_interval': 1, 'validate_interval_updates': 1, 'validate_after_updates': 0, 'fixed_validation_seed': None, 'disable_validation': False, 'max_tokens_valid': None, 'batch_size_valid': 16, 'max_valid_steps': None, 'curriculum': 0, 'gen_subset': 'test', 'num_shards': 1, 'shard_id': 0}, 'optimization': {'_name': None, 'max_epoch': 0, 'max_update': 100, 'stop_time_hours': 0.0, 'clip_norm': 2.0, 'sentence_avg': False, 'update_freq': [2], 'lr': [0.005], 'stop_min_lr': -1.0, 'use_bmuf': False}, 'checkpoint': {'_name': None, 'save_dir': 'checkpoints', 'restore_file': 'checkpoint_last.pt', 'finetune_from_model': None, 'reset_dataloader': False, 'reset_lr_scheduler': False, 'reset_meters': False, 'reset_optimizer': False, 'optimizer_overrides': '{}', 'save_interval': 1, 'save_interval_updates': 1000, 'keep_interval_updates': -1, 'keep_last_epochs': -1, 'keep_best_checkpoints': -1, 'no_save': True, 'no_epoch_checkpoints': True, 'no_last_checkpoints': False, 'no_best_checkpoints': False, 'no_save_optimizer_state': False, 'no_save_optimizer_state_on_training_finished': False, 'symlink_best_and_last_checkpoints': False, 'best_checkpoint_metric': 'loss_valid', 'maximize_best_checkpoint_metric': False, 'patience': -1, 'checkpoint_suffix': '', 'checkpoint_shard_count': 1, 'load_checkpoint_on_all_dp_ranks': False, 'write_checkpoints_asynchronously': False, 's3_upload_path': None, 'model_parallel_size': 1}, 'bmuf': {'_name': None, 'block_lr': 1.0, 'block_momentum': 0.875, 'global_sync_iter': 50, 'warmup_iterations': 500, 'use_nbm': False, 'average_sync': False, 'distributed_world_size': 4}, 'generation': {'_name': None, 'beam': 5, 'nbest': 1, 'max_len_a': 0.0, 'max_len_b': 200, 'min_len': 1, 'match_source_len': False, 'unnormalized': False, 'no_early_stop': False, 'no_beamable_mm': False, 'lenpen': 1.0, 'unkpen': 0.0, 'replace_unk': None, 'sacrebleu': False, 'score_reference': False, 'prefix_size': 0, 'no_repeat_ngram_size': 0, 'sampling': False, 'sampling_topk': -1, 'sampling_topp': -1.0, 'constraints': None, 'temperature': 1.0, 'diverse_beam_groups': -1, 'diverse_beam_strength': 0.5, 'diversity_rate': -1.0, 'print_alignment': None, 'print_step': False, 'lm_path': None, 'lm_weight': 0.0, 'iter_decode_eos_penalty': 0.0, 'iter_decode_max_iter': 10, 'iter_decode_force_max_iter': False, 'iter_decode_with_beam': 1, 'iter_decode_with_external_reranker': False, 'retain_iter_history': False, 'retain_dropout': False, 'retain_dropout_modules': None, 'decoding_format': None, 'no_seed_provided': False}, 'eval_lm': {'_name': None, 'input_quant_method': '', 'input_bits': -1, 'weight_quant_method': '', 'weight_bits': -1, 'smoothquant': False, 'smoothquant_alpha': 0.5, 'smoothquant_bitnet': False, 'input_bits_post': 8, 'hadamard_group': -1, 'cal_input_stat': 'none', 'output_word_probs': False, 'output_word_stats': False, 'context_window': 0, 'softmax_batch': 9223372036854775807, 'stats_path': None, 'max_valid_steps': None}, 'interactive': {'_name': None, 'buffer_size': 0, 'input': '-'}, 'model': {'_name': 'llama_for_layer_wise_qat', 'load_ckpt': None, 'batch_size': 16, 'share_input_output_embed': False, 'sliding_window': None, 'rope_theta': 10000.0, 'checkpoint_activations': False, 'tokens_per_sample': 512, 'model_parallel_size': 1}, 'task': {'_name': 'kd', 'path_to_labels': None, 'data': '/data/gyy/lmquant-main/lmquant/data/data_without_preprocess_llama_7b', 'tokens_per_sample': 512, 'batch_size_in_quant': 8, 'max_target_positions': None, 'llama_model': None, 'quant_acts_when_training': True, 'tiktoken_model': 'cl100k_base', 'batch_read_ahead': 1, 'pad_to_max_len': True, 'absolute_path': False, 'tokenizer_pad_to_multiple': 1, 'seed': 1, 'batch_size': 16}, 'criterion': {'_name': 'mse', 'sentence_avg': False}, 'optimizer': {'_name': 'adam', 'adam_betas': '(0.9, 0.95)', 'adam_eps': 1e-06, 'weight_decay': 0.05, 'use_old_adam': False, 'fp16_adam_stats': False, 'tpu': False, 'bf16': False, 'lr': [0.005], 'block_wise': False}, 'lr_scheduler': {'_name': 'polynomial_decay', 'warmup_updates': 50, 'force_anneal': None, 'end_learning_rate': 0.0, 'power': 1.0, 'total_num_update': 300000.0, 'lr': [0.005]}, 'scoring': {'_name': 'bleu', 'pad': 1, 'eos': 2, 'unk': 3}, 'bpe': None, 'tokenizer': None}
25-01-22 12:29:20 | I | LlamaModelDecoderLayer(
  (decoder): LlamaDecoderLayerInFairseq(
    (model): LlamaDecoderLayer(
      (self_attn): LlamaSdpaAttention(
        (q_proj): Linear(in_features=4096, out_features=4096, bias=False)
        (k_proj): Linear(in_features=4096, out_features=4096, bias=False)
        (v_proj): Linear(in_features=4096, out_features=4096, bias=False)
        (o_proj): Linear(in_features=4096, out_features=4096, bias=False)
        (rotary_emb): LlamaRotaryEmbedding()
        (q_rotary_emb): RotaryEmbedding()
        (k_rotary_emb): RotaryEmbedding()
      )
      (mlp): LlamaMLP(
        (gate_proj): Linear(in_features=4096, out_features=11008, bias=False)
        (up_proj): Linear(in_features=4096, out_features=11008, bias=False)
        (down_proj): Linear(in_features=11008, out_features=4096, bias=False)
        (act_fn): SiLU()
      )
      (input_layernorm): LlamaRMSNorm((4096,), eps=1e-05)
      (post_attention_layernorm): LlamaRMSNorm((4096,), eps=1e-05)
    )
  )
  (model): LlamaDecoderLayerInFairseq(
    (model): LlamaDecoderLayer(
      (self_attn): LlamaSdpaAttention(
        (q_proj): Linear(in_features=4096, out_features=4096, bias=False)
        (k_proj): Linear(in_features=4096, out_features=4096, bias=False)
        (v_proj): Linear(in_features=4096, out_features=4096, bias=False)
        (o_proj): Linear(in_features=4096, out_features=4096, bias=False)
        (rotary_emb): LlamaRotaryEmbedding()
        (q_rotary_emb): RotaryEmbedding()
        (k_rotary_emb): RotaryEmbedding()
      )
      (mlp): LlamaMLP(
        (gate_proj): Linear(in_features=4096, out_features=11008, bias=False)
        (up_proj): Linear(in_features=4096, out_features=11008, bias=False)
        (down_proj): Linear(in_features=11008, out_features=4096, bias=False)
        (act_fn): SiLU()
      )
      (input_layernorm): LlamaRMSNorm((4096,), eps=1e-05)
      (post_attention_layernorm): LlamaRMSNorm((4096,), eps=1e-05)
    )
  )
)
25-01-22 12:29:20 | I | task: KDTask
25-01-22 12:29:20 | I | model: LlamaModelDecoderLayer
25-01-22 12:29:20 | I | criterion: MSECriterion
25-01-22 12:29:20 | I | num. non-expert model params: 202,383,360 (num. trained: 202,383,360)
25-01-22 12:29:20 | I | num. expert model params: 0 (num. trained: 0)
25-01-22 12:29:20 | I | nvidia-smi stats: {'gpu_0_mem_used_gb': 0.0, 'gpu_1_mem_used_gb': 0.0, 'gpu_2_mem_used_gb': 0.0, 'gpu_3_mem_used_gb': 0.0, 'gpu_4_mem_used_gb': 6.150390625, 'gpu_5_mem_used_gb': 7.0859375, 'gpu_6_mem_used_gb': 7.0859375, 'gpu_7_mem_used_gb': 6.06640625}
25-01-22 12:29:20 | I | detected shared parameter: decoder.model.self_attn.q_proj.weight <- model.model.self_attn.q_proj.weight
25-01-22 12:29:20 | I | detected shared parameter: decoder.model.self_attn.q_proj.bias <- decoder.model.self_attn.k_proj.bias
25-01-22 12:29:20 | I | detected shared parameter: decoder.model.self_attn.q_proj.bias <- decoder.model.self_attn.v_proj.bias
25-01-22 12:29:20 | I | detected shared parameter: decoder.model.self_attn.q_proj.bias <- decoder.model.self_attn.o_proj.bias
25-01-22 12:29:20 | I | detected shared parameter: decoder.model.self_attn.q_proj.bias <- decoder.model.mlp.gate_proj.bias
25-01-22 12:29:20 | I | detected shared parameter: decoder.model.self_attn.q_proj.bias <- decoder.model.mlp.up_proj.bias
25-01-22 12:29:20 | I | detected shared parameter: decoder.model.self_attn.q_proj.bias <- decoder.model.mlp.down_proj.bias
25-01-22 12:29:20 | I | detected shared parameter: decoder.model.self_attn.q_proj.bias <- model.model.self_attn.q_proj.bias
25-01-22 12:29:20 | I | detected shared parameter: decoder.model.self_attn.q_proj.bias <- model.model.self_attn.k_proj.bias
25-01-22 12:29:20 | I | detected shared parameter: decoder.model.self_attn.q_proj.bias <- model.model.self_attn.v_proj.bias
25-01-22 12:29:20 | I | detected shared parameter: decoder.model.self_attn.q_proj.bias <- model.model.self_attn.o_proj.bias
25-01-22 12:29:20 | I | detected shared parameter: decoder.model.self_attn.q_proj.bias <- model.model.mlp.gate_proj.bias
25-01-22 12:29:20 | I | detected shared parameter: decoder.model.self_attn.q_proj.bias <- model.model.mlp.up_proj.bias
25-01-22 12:29:20 | I | detected shared parameter: decoder.model.self_attn.q_proj.bias <- model.model.mlp.down_proj.bias
25-01-22 12:29:20 | I | detected shared parameter: decoder.model.self_attn.k_proj.weight <- model.model.self_attn.k_proj.weight
25-01-22 12:29:20 | I | detected shared parameter: decoder.model.self_attn.v_proj.weight <- model.model.self_attn.v_proj.weight
25-01-22 12:29:20 | I | detected shared parameter: decoder.model.self_attn.o_proj.weight <- model.model.self_attn.o_proj.weight
25-01-22 12:29:20 | I | detected shared parameter: decoder.model.mlp.gate_proj.weight <- model.model.mlp.gate_proj.weight
25-01-22 12:29:20 | I | detected shared parameter: decoder.model.mlp.up_proj.weight <- model.model.mlp.up_proj.weight
25-01-22 12:29:20 | I | detected shared parameter: decoder.model.mlp.down_proj.weight <- model.model.mlp.down_proj.weight
25-01-22 12:29:20 | I | detected shared parameter: decoder.model.input_layernorm.weight <- model.model.input_layernorm.weight
25-01-22 12:29:20 | I | detected shared parameter: decoder.model.post_attention_layernorm.weight <- model.model.post_attention_layernorm.weight
25-01-22 12:29:20 | I | nvidia-smi stats: {'gpu_0_mem_used_gb': 0.0, 'gpu_1_mem_used_gb': 0.0, 'gpu_2_mem_used_gb': 0.0, 'gpu_3_mem_used_gb': 0.0, 'gpu_4_mem_used_gb': 6.19140625, 'gpu_5_mem_used_gb': 7.126953125, 'gpu_6_mem_used_gb': 7.126953125, 'gpu_7_mem_used_gb': 6.107421875}
25-01-22 12:29:20 | I | ***********************CUDA enviroments for all 1 workers***********************
25-01-22 12:29:20 | I | rank   0: capabilities =  8.6  ; total memory = 23.691 GB ; name = NVIDIA GeForce RTX 3090                 
25-01-22 12:29:20 | I | ***********************CUDA enviroments for all 1 workers***********************
25-01-22 12:29:20 | I | training on 1 devices (GPUs/TPUs)
25-01-22 12:29:20 | I | max tokens per GPU = None and batch size per GPU = 16
25-01-22 12:29:20 | I | nvidia-smi stats: {'gpu_0_mem_used_gb': 0.0, 'gpu_1_mem_used_gb': 0.0, 'gpu_2_mem_used_gb': 0.0, 'gpu_3_mem_used_gb': 0.0, 'gpu_4_mem_used_gb': 6.19140625, 'gpu_5_mem_used_gb': 7.126953125, 'gpu_6_mem_used_gb': 7.126953125, 'gpu_7_mem_used_gb': 6.107421875}
25-01-22 12:29:20 | I | loading train data for epoch 1
25-01-22 12:29:25 | I | *** 
 now in /data/gyy/lmquant-main/lmquant/data/data_without_preprocess_llama_1.1b/shard/val/0.jsonl 
 ***
25-01-22 12:29:25 | I | *** 
 total_rows = 700 
 ***
25-01-22 12:29:25 | I | current row num = 0
25-01-22 12:29:25 | I | in forward_and_gen_teacher_outputs, Start iterating over samples
25-01-22 12:29:25 | I | loading train data for epoch 1
25-01-22 12:29:28 | I | *** 
 now in /data/gyy/lmquant-main/lmquant/data/data_without_preprocess_llama_1.1b/shard/val/0.jsonl 
 ***
25-01-22 12:29:28 | I | *** 
 total_rows = 700 
 ***
25-01-22 12:29:28 | I | current row num = 0
25-01-22 12:29:28 | I | in forward_and_gen_args_and_kwargs, Start iterating over samples
25-01-22 12:29:28 | I | No existing checkpoint found checkpoints/checkpoint_last.pt
25-01-22 12:29:28 | I | loading train data for epoch 1
25-01-22 12:29:28 | I | loading valid data for epoch 1
25-01-22 12:29:35 | I | begin training epoch 1
25-01-22 12:29:35 | I | Start iterating over samples
25-01-22 12:29:38 | I | TRAIN CURRENT LAYER_IDX = 4
25-01-22 12:29:38 | I | in layer model.layers.4
25-01-22 12:29:38 | I | quantizing weights for layer model.layers.4
25-01-22 12:29:39 | I | collecting calibration activations in model.layers.4
25-01-22 12:29:39 | I | collecting calibration activations in model.layers.4
25-01-22 12:29:39 | I | collecting calibration activations in model.layers.4
25-01-22 12:29:39 | I | collecting calibration activations in model.layers.4
25-01-22 12:29:40 | I | - Quantizing decoder layer model.layers.4
25-01-22 12:29:40 | I |   - Calibrating model.layers.4.self_attn.q_proj.weight
25-01-22 12:29:41 | I |       - range scale = [    1.0000]
25-01-22 12:29:41 | I |         sum  error  = [    3.3359]
25-01-22 12:29:41 | I |         best error  = [    3.3359]
25-01-22 12:29:41 | I |     + error = [3.3359]
25-01-22 12:29:42 | I |       - range scale = [    1.0000]
25-01-22 12:29:42 | I |         sum  error  = [   31.5180]
25-01-22 12:29:42 | I |         best error  = [   31.5180]
25-01-22 12:29:42 | I |     + error = [31.5180]
25-01-22 12:29:42 | I |   - Calibrating model.layers.4.self_attn.k_proj.weight
25-01-22 12:29:43 | I |       - range scale = [    1.0000]
25-01-22 12:29:43 | I |         sum  error  = [    1.1938]
25-01-22 12:29:43 | I |         best error  = [    1.1938]
25-01-22 12:29:43 | I |     + error = [1.1938]
25-01-22 12:29:44 | I |       - range scale = [    1.0000]
25-01-22 12:29:44 | I |         sum  error  = [   12.1243]
25-01-22 12:29:44 | I |         best error  = [   12.1243]
25-01-22 12:29:44 | I |     + error = [12.1243]
25-01-22 12:29:44 | I |   - Calibrating model.layers.4.self_attn.v_proj.weight
25-01-22 12:29:45 | I |       - range scale = [    1.0000]
25-01-22 12:29:45 | I |         sum  error  = [    3.6374]
25-01-22 12:29:45 | I |         best error  = [    3.6374]
25-01-22 12:29:45 | I |     + error = [3.6374]
25-01-22 12:29:45 | I |       - range scale = [    1.0000]
25-01-22 12:29:46 | I |         sum  error  = [   38.8105]
25-01-22 12:29:46 | I |         best error  = [   38.8105]
25-01-22 12:29:46 | I |     + error = [38.8105]
25-01-22 12:29:46 | I |   - Calibrating model.layers.4.self_attn.o_proj.weight
25-01-22 12:29:46 | I |       - range scale = [    1.0000]
25-01-22 12:29:46 | I |         sum  error  = [    0.4288]
25-01-22 12:29:46 | I |         best error  = [    0.4288]
25-01-22 12:29:46 | I |     + error = [0.4288]
25-01-22 12:29:47 | I |       - range scale = [    1.0000]
25-01-22 12:29:47 | I |         sum  error  = [    4.3502]
25-01-22 12:29:47 | I |         best error  = [    4.3502]
25-01-22 12:29:47 | I |     + error = [4.3502]
25-01-22 12:29:48 | I |   - Calibrating model.layers.4.mlp.up_proj.weight
25-01-22 12:29:48 | I |       - range scale = [    1.0000]
25-01-22 12:29:48 | I |         sum  error  = [    3.9654]
25-01-22 12:29:48 | I |         best error  = [    3.9654]
25-01-22 12:29:48 | I |     + error = [3.9654]
25-01-22 12:29:49 | I |       - range scale = [    1.0000]
25-01-22 12:29:49 | I |         sum  error  = [   41.9417]
25-01-22 12:29:49 | I |         best error  = [   41.9417]
25-01-22 12:29:49 | I |     + error = [41.9417]
25-01-22 12:29:50 | I |   - Calibrating model.layers.4.mlp.gate_proj.weight
25-01-22 12:29:50 | I |       - range scale = [    1.0000]
25-01-22 12:29:50 | I |         sum  error  = [    4.4603]
25-01-22 12:29:50 | I |         best error  = [    4.4603]
25-01-22 12:29:50 | I |     + error = [4.4603]
25-01-22 12:29:51 | I |       - range scale = [    1.0000]
25-01-22 12:29:51 | I |         sum  error  = [   46.6500]
25-01-22 12:29:51 | I |         best error  = [   46.6500]
25-01-22 12:29:51 | I |     + error = [46.6500]
25-01-22 12:29:52 | I |   - Calibrating model.layers.4.mlp.down_proj.weight
25-01-22 12:29:52 | I |       - range scale = [    1.0000]
25-01-22 12:29:52 | I |         sum  error  = [    0.9377]
25-01-22 12:29:52 | I |         best error  = [    0.9377]
25-01-22 12:29:52 | I |     + error = [0.9377]
25-01-22 12:29:54 | I |       - range scale = [    1.0000]
25-01-22 12:29:54 | I |         sum  error  = [    9.0722]
25-01-22 12:29:54 | I |         best error  = [    9.0722]
25-01-22 12:29:54 | I |     + error = [9.0722]
25-01-22 12:29:54 | I |   - Quantizing model.layers.4.self_attn.q_proj.weight
25-01-22 12:29:58 | I |   - Quantizing model.layers.4.self_attn.k_proj.weight
25-01-22 12:30:01 | I |   - Quantizing model.layers.4.self_attn.v_proj.weight
25-01-22 12:30:05 | I |   - Quantizing model.layers.4.self_attn.o_proj.weight
25-01-22 12:30:08 | I |   - Quantizing model.layers.4.mlp.up_proj.weight
25-01-22 12:30:12 | I |   - Quantizing model.layers.4.mlp.gate_proj.weight
25-01-22 12:30:15 | I |   - Quantizing model.layers.4.mlp.down_proj.weight
25-01-22 12:30:25 | I | quantizing activations for layer model.layers.4
25-01-22 12:30:25 | I | collecting calibration activations in model.layers.4
25-01-22 12:30:26 | I | collecting calibration activations in model.layers.4
25-01-22 12:30:26 | I | collecting calibration activations in model.layers.4
25-01-22 12:30:26 | I | collecting calibration activations in model.layers.4
25-01-22 12:30:28 | I | forward this layer
25-01-22 12:30:28 | I | input_file: /data/gyy/lmquant-main/lmquant/data/data_without_preprocess_llama_7b/shard/val_input_args_layer_4/00.pt
25-01-22 12:30:28 | I | output_file: /data/gyy/lmquant-main/lmquant/data/data_without_preprocess_llama_7b/shard/val_teacher_output_layer_4/00.pt
25-01-22 12:30:29 | W | OOM: Ran out of memory with exception: CUDA out of memory. Tried to allocate 688.00 MiB. GPU 0 has a total capacity of 23.69 GiB of which 608.94 MiB is free. Process 45966 has 23.09 GiB memory in use. Of the allocated memory 22.18 GiB is allocated by PyTorch, and 442.62 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
25-01-22 12:30:29 | W | |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 0                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 1            |        cudaMalloc retries: 1         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |  21213 MiB |  23011 MiB | 542207 MiB | 520994 MiB |
|       from large pool |  21210 MiB |  23008 MiB | 532676 MiB | 511466 MiB |
|       from small pool |      3 MiB |      5 MiB |   9531 MiB |   9528 MiB |
|---------------------------------------------------------------------------|
| Active memory         |  21213 MiB |  23011 MiB | 542207 MiB | 520994 MiB |
|       from large pool |  21210 MiB |  23008 MiB | 532676 MiB | 511466 MiB |
|       from small pool |      3 MiB |      5 MiB |   9531 MiB |   9528 MiB |
|---------------------------------------------------------------------------|
| Requested memory      |  21213 MiB |  23011 MiB | 538693 MiB | 517480 MiB |
|       from large pool |  21210 MiB |  23008 MiB | 529188 MiB | 507978 MiB |
|       from small pool |      3 MiB |      5 MiB |   9505 MiB |   9502 MiB |
|---------------------------------------------------------------------------|
| GPU reserved memory   |  23160 MiB |  23672 MiB |  89044 MiB |  65884 MiB |
|       from large pool |  23156 MiB |  23668 MiB |  88992 MiB |  65836 MiB |
|       from small pool |      4 MiB |      6 MiB |     52 MiB |     48 MiB |
|---------------------------------------------------------------------------|
| Non-releasable memory | 584318 KiB |   1312 MiB | 108307 MiB | 107736 MiB |
|       from large pool | 583424 KiB |   1311 MiB |  97336 MiB |  96767 MiB |
|       from small pool |    894 KiB |      3 MiB |  10970 MiB |  10969 MiB |
|---------------------------------------------------------------------------|
| Allocations           |     268    |     272    |  183170    |  182902    |
|       from large pool |     183    |     186    |   23569    |   23386    |
|       from small pool |      85    |      87    |  159601    |  159516    |
|---------------------------------------------------------------------------|
| Active allocs         |     268    |     272    |  183170    |  182902    |
|       from large pool |     183    |     186    |   23569    |   23386    |
|       from small pool |      85    |      87    |  159601    |  159516    |
|---------------------------------------------------------------------------|
| GPU reserved segments |     166    |     168    |     412    |     246    |
|       from large pool |     164    |     166    |     386    |     222    |
|       from small pool |       2    |       3    |      26    |      24    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |      17    |      21    |   86986    |   86969    |
|       from large pool |      10    |      13    |   11746    |   11736    |
|       from small pool |       7    |       9    |   75240    |   75233    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|

25-01-22 12:30:29 | W | |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 1                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |   6948 MiB |   7036 MiB |   7284 MiB | 344453 KiB |
|       from large pool |   6948 MiB |   7036 MiB |   7281 MiB | 341376 KiB |
|       from small pool |      0 MiB |      1 MiB |      3 MiB |   3077 KiB |
|---------------------------------------------------------------------------|
| Active memory         |   6948 MiB |   7036 MiB |   7284 MiB | 344453 KiB |
|       from large pool |   6948 MiB |   7036 MiB |   7281 MiB | 341376 KiB |
|       from small pool |      0 MiB |      1 MiB |      3 MiB |   3077 KiB |
|---------------------------------------------------------------------------|
| Requested memory      |   6948 MiB |   7034 MiB |   7278 MiB | 338304 KiB |
|       from large pool |   6948 MiB |   7034 MiB |   7275 MiB | 335232 KiB |
|       from small pool |      0 MiB |      1 MiB |      3 MiB |   3072 KiB |
|---------------------------------------------------------------------------|
| GPU reserved memory   |   6950 MiB |   7038 MiB |   7320 MiB | 378880 KiB |
|       from large pool |   6948 MiB |   7036 MiB |   7316 MiB | 376832 KiB |
|       from small pool |      2 MiB |      2 MiB |      4 MiB |   2048 KiB |
|---------------------------------------------------------------------------|
| Non-releasable memory |   1751 KiB |  20855 KiB |  48124 KiB |  46373 KiB |
|       from large pool |      0 KiB |  19104 KiB |  40960 KiB |  40960 KiB |
|       from small pool |   1751 KiB |   2047 KiB |   7164 KiB |   5413 KiB |
|---------------------------------------------------------------------------|
| Allocations           |     180    |     182    |     210    |      30    |
|       from large pool |     126    |     128    |     140    |      14    |
|       from small pool |      54    |      56    |      70    |      16    |
|---------------------------------------------------------------------------|
| Active allocs         |     180    |     182    |     210    |      30    |
|       from large pool |     126    |     128    |     140    |      14    |
|       from small pool |      54    |      56    |      70    |      16    |
|---------------------------------------------------------------------------|
| GPU reserved segments |     127    |     129    |     140    |      13    |
|       from large pool |     126    |     128    |     138    |      12    |
|       from small pool |       1    |       1    |       2    |       1    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       1    |       2    |       4    |       3    |
|       from large pool |       0    |       1    |       2    |       2    |
|       from small pool |       1    |       1    |       2    |       1    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|

25-01-22 12:30:29 | W | |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 2                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |   6948 MiB |   7036 MiB |   7284 MiB | 344453 KiB |
|       from large pool |   6948 MiB |   7036 MiB |   7281 MiB | 341376 KiB |
|       from small pool |      0 MiB |      1 MiB |      3 MiB |   3077 KiB |
|---------------------------------------------------------------------------|
| Active memory         |   6948 MiB |   7036 MiB |   7284 MiB | 344453 KiB |
|       from large pool |   6948 MiB |   7036 MiB |   7281 MiB | 341376 KiB |
|       from small pool |      0 MiB |      1 MiB |      3 MiB |   3077 KiB |
|---------------------------------------------------------------------------|
| Requested memory      |   6948 MiB |   7034 MiB |   7278 MiB | 338304 KiB |
|       from large pool |   6948 MiB |   7034 MiB |   7275 MiB | 335232 KiB |
|       from small pool |      0 MiB |      1 MiB |      3 MiB |   3072 KiB |
|---------------------------------------------------------------------------|
| GPU reserved memory   |   6950 MiB |   7038 MiB |   7320 MiB | 378880 KiB |
|       from large pool |   6948 MiB |   7036 MiB |   7316 MiB | 376832 KiB |
|       from small pool |      2 MiB |      2 MiB |      4 MiB |   2048 KiB |
|---------------------------------------------------------------------------|
| Non-releasable memory |   1751 KiB |  20855 KiB |  48124 KiB |  46373 KiB |
|       from large pool |      0 KiB |  19104 KiB |  40960 KiB |  40960 KiB |
|       from small pool |   1751 KiB |   2047 KiB |   7164 KiB |   5413 KiB |
|---------------------------------------------------------------------------|
| Allocations           |     180    |     182    |     210    |      30    |
|       from large pool |     126    |     128    |     140    |      14    |
|       from small pool |      54    |      56    |      70    |      16    |
|---------------------------------------------------------------------------|
| Active allocs         |     180    |     182    |     210    |      30    |
|       from large pool |     126    |     128    |     140    |      14    |
|       from small pool |      54    |      56    |      70    |      16    |
|---------------------------------------------------------------------------|
| GPU reserved segments |     127    |     129    |     140    |      13    |
|       from large pool |     126    |     128    |     138    |      12    |
|       from small pool |       1    |       1    |       2    |       1    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       1    |       2    |       4    |       3    |
|       from large pool |       0    |       1    |       2    |       2    |
|       from small pool |       1    |       1    |       2    |       1    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|

25-01-22 12:30:29 | W | |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 3                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |   5904 MiB |   5992 MiB |   6240 MiB | 344452 KiB |
|       from large pool |   5904 MiB |   5992 MiB |   6237 MiB | 341376 KiB |
|       from small pool |      0 MiB |      1 MiB |      3 MiB |   3076 KiB |
|---------------------------------------------------------------------------|
| Active memory         |   5904 MiB |   5992 MiB |   6240 MiB | 344452 KiB |
|       from large pool |   5904 MiB |   5992 MiB |   6237 MiB | 341376 KiB |
|       from small pool |      0 MiB |      1 MiB |      3 MiB |   3076 KiB |
|---------------------------------------------------------------------------|
| Requested memory      |   5904 MiB |   5990 MiB |   6234 MiB | 338304 KiB |
|       from large pool |   5904 MiB |   5990 MiB |   6231 MiB | 335232 KiB |
|       from small pool |      0 MiB |      1 MiB |      3 MiB |   3072 KiB |
|---------------------------------------------------------------------------|
| GPU reserved memory   |   5906 MiB |   5994 MiB |   6276 MiB | 378880 KiB |
|       from large pool |   5904 MiB |   5992 MiB |   6272 MiB | 376832 KiB |
|       from small pool |      2 MiB |      2 MiB |      4 MiB |   2048 KiB |
|---------------------------------------------------------------------------|
| Non-releasable memory |   1544 KiB |  20648 KiB |  48123 KiB |  46579 KiB |
|       from large pool |      0 KiB |  19104 KiB |  40960 KiB |  40960 KiB |
|       from small pool |   1544 KiB |   2047 KiB |   7163 KiB |   5619 KiB |
|---------------------------------------------------------------------------|
| Allocations           |     148    |     150    |     177    |      29    |
|       from large pool |     100    |     102    |     114    |      14    |
|       from small pool |      48    |      50    |      63    |      15    |
|---------------------------------------------------------------------------|
| Active allocs         |     148    |     150    |     177    |      29    |
|       from large pool |     100    |     102    |     114    |      14    |
|       from small pool |      48    |      50    |      63    |      15    |
|---------------------------------------------------------------------------|
| GPU reserved segments |     101    |     103    |     114    |      13    |
|       from large pool |     100    |     102    |     112    |      12    |
|       from small pool |       1    |       1    |       2    |       1    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       1    |       2    |       4    |       3    |
|       from large pool |       0    |       1    |       2    |       2    |
|       from small pool |       1    |       1    |       2    |       1    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|

25-01-22 12:30:29 | W | attempting to recover from OOM in forward/backward pass
25-01-22 12:30:31 | I | TRAIN CURRENT LAYER_IDX = 4
25-01-22 12:30:31 | I | in layer model.layers.4
25-01-22 12:30:31 | I | quantizing weights for layer model.layers.4
25-01-22 12:30:32 | I | collecting calibration activations in model.layers.4
25-01-22 12:30:32 | I | collecting calibration activations in model.layers.4
25-01-22 12:30:33 | I | collecting calibration activations in model.layers.4
25-01-22 12:30:33 | I | collecting calibration activations in model.layers.4
25-01-22 12:30:33 | I | - Quantizing decoder layer model.layers.4
25-01-22 12:30:33 | I |   - Calibrating model.layers.4.self_attn.q_proj.weight
25-01-22 12:30:34 | I |       - range scale = [    1.0000]
25-01-22 12:30:34 | I |         sum  error  = [   10.4925]
25-01-22 12:30:34 | I |         best error  = [   10.4925]
25-01-22 12:30:34 | I |     + error = [10.4925]
25-01-22 12:30:35 | I |       - range scale = [    1.0000]
25-01-22 12:30:35 | I |         sum  error  = [   17.7490]
25-01-22 12:30:35 | I |         best error  = [   17.7490]
25-01-22 12:30:35 | I |     + error = [17.7490]
25-01-22 12:30:35 | I |   - Calibrating model.layers.4.self_attn.k_proj.weight
25-01-22 12:30:36 | I |       - range scale = [    1.0000]
25-01-22 12:30:36 | I |         sum  error  = [   19.8104]
25-01-22 12:30:36 | I |         best error  = [   19.8104]
25-01-22 12:30:36 | I |     + error = [19.8104]
25-01-22 12:30:37 | I |       - range scale = [    1.0000]
25-01-22 12:30:37 | I |         sum  error  = [   39.3523]
25-01-22 12:30:37 | I |         best error  = [   39.3523]
25-01-22 12:30:37 | I |     + error = [39.3523]
25-01-22 12:30:37 | I |   - Calibrating model.layers.4.self_attn.v_proj.weight
25-01-22 12:30:38 | I |       - range scale = [    1.0000]
25-01-22 12:30:38 | I |         sum  error  = [    2.8102]
25-01-22 12:30:38 | I |         best error  = [    2.8102]
25-01-22 12:30:38 | I |     + error = [2.8102]
25-01-22 12:30:39 | I |       - range scale = [    1.0000]
25-01-22 12:30:39 | I |         sum  error  = [   12.8397]
25-01-22 12:30:39 | I |         best error  = [   12.8397]
25-01-22 12:30:39 | I |     + error = [12.8397]
25-01-22 12:30:39 | I |   - Calibrating model.layers.4.self_attn.o_proj.weight
25-01-22 12:30:40 | I |       - range scale = [    1.0000]
25-01-22 12:30:40 | I |         sum  error  = [    0.4262]
25-01-22 12:30:40 | I |         best error  = [    0.4262]
25-01-22 12:30:40 | I |     + error = [0.4262]
25-01-22 12:30:41 | I |       - range scale = [    1.0000]
25-01-22 12:30:41 | I |         sum  error  = [    2.0683]
25-01-22 12:30:41 | I |         best error  = [    2.0683]
25-01-22 12:30:41 | I |     + error = [2.0683]
25-01-22 12:30:41 | I |   - Calibrating model.layers.4.mlp.up_proj.weight
25-01-22 12:30:42 | I |       - range scale = [    1.0000]
25-01-22 12:30:42 | I |         sum  error  = [    3.5943]
25-01-22 12:30:42 | I |         best error  = [    3.5943]
25-01-22 12:30:42 | I |     + error = [3.5943]
25-01-22 12:30:43 | I |       - range scale = [    1.0000]
25-01-22 12:30:43 | I |         sum  error  = [   16.8564]
25-01-22 12:30:43 | I |         best error  = [   16.8564]
25-01-22 12:30:43 | I |     + error = [16.8564]
25-01-22 12:30:43 | I |   - Calibrating model.layers.4.mlp.gate_proj.weight
25-01-22 12:30:44 | I |       - range scale = [    1.0000]
25-01-22 12:30:44 | I |         sum  error  = [    4.2662]
25-01-22 12:30:44 | I |         best error  = [    4.2662]
25-01-22 12:30:44 | I |     + error = [4.2662]
25-01-22 12:30:45 | I |       - range scale = [    1.0000]
25-01-22 12:30:45 | I |         sum  error  = [   19.0767]
25-01-22 12:30:45 | I |         best error  = [   19.0767]
25-01-22 12:30:45 | I |     + error = [19.0767]
25-01-22 12:30:45 | I |   - Calibrating model.layers.4.mlp.down_proj.weight
25-01-22 12:30:46 | I |       - range scale = [    1.0000]
25-01-22 12:30:46 | I |         sum  error  = [    0.7896]
25-01-22 12:30:46 | I |         best error  = [    0.7896]
25-01-22 12:30:46 | I |     + error = [0.7896]
25-01-22 12:30:47 | I |       - range scale = [    1.0000]
25-01-22 12:30:47 | I |         sum  error  = [    3.7654]
25-01-22 12:30:47 | I |         best error  = [    3.7654]
25-01-22 12:30:47 | I |     + error = [3.7654]
25-01-22 12:30:47 | I |   - Quantizing model.layers.4.self_attn.q_proj.weight
25-01-22 12:30:50 | I |   - Quantizing model.layers.4.self_attn.k_proj.weight
25-01-22 12:30:53 | I |   - Quantizing model.layers.4.self_attn.v_proj.weight
25-01-22 12:30:56 | I |   - Quantizing model.layers.4.self_attn.o_proj.weight
25-01-22 12:30:59 | I |   - Quantizing model.layers.4.mlp.up_proj.weight
25-01-22 12:31:02 | I |   - Quantizing model.layers.4.mlp.gate_proj.weight
25-01-22 12:31:05 | I |   - Quantizing model.layers.4.mlp.down_proj.weight
25-01-22 12:31:13 | I | quantizing activations for layer model.layers.4
25-01-22 12:31:13 | I | collecting calibration activations in model.layers.4
25-01-22 12:31:14 | I | collecting calibration activations in model.layers.4
25-01-22 12:31:14 | I | collecting calibration activations in model.layers.4
25-01-22 12:31:14 | I | collecting calibration activations in model.layers.4
25-01-22 12:31:16 | I | forward this layer
25-01-22 12:31:16 | I | input_file: /data/gyy/lmquant-main/lmquant/data/data_without_preprocess_llama_7b/shard/val_input_args_layer_4/02.pt
25-01-22 12:31:16 | I | output_file: /data/gyy/lmquant-main/lmquant/data/data_without_preprocess_llama_7b/shard/val_teacher_output_layer_4/02.pt
25-01-22 12:31:17 | W | OOM: Ran out of memory with exception: CUDA out of memory. Tried to allocate 256.00 MiB. GPU 0 has a total capacity of 23.69 GiB of which 98.94 MiB is free. Process 45966 has 23.59 GiB memory in use. Of the allocated memory 22.54 GiB is allocated by PyTorch, and 585.01 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
25-01-22 12:31:17 | W | |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 0                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 2            |        cudaMalloc retries: 2         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |  23084 MiB |  23084 MiB |   1141 GiB |   1119 GiB |
|       from large pool |  23080 MiB |  23080 MiB |   1122 GiB |   1100 GiB |
|       from small pool |      4 MiB |      5 MiB |     18 GiB |     18 GiB |
|---------------------------------------------------------------------------|
| Active memory         |  23084 MiB |  23084 MiB |   1141 GiB |   1119 GiB |
|       from large pool |  23080 MiB |  23080 MiB |   1122 GiB |   1100 GiB |
|       from small pool |      4 MiB |      5 MiB |     18 GiB |     18 GiB |
|---------------------------------------------------------------------------|
| Requested memory      |  23084 MiB |  23084 MiB |   1131 GiB |   1109 GiB |
|       from large pool |  23080 MiB |  23080 MiB |   1113 GiB |   1090 GiB |
|       from small pool |      4 MiB |      5 MiB |     18 GiB |     18 GiB |
|---------------------------------------------------------------------------|
| GPU reserved memory   |  23670 MiB |  23672 MiB | 160740 MiB | 137070 MiB |
|       from large pool |  23664 MiB |  23668 MiB | 160616 MiB | 136952 MiB |
|       from small pool |      6 MiB |      6 MiB |    124 MiB |    118 MiB |
|---------------------------------------------------------------------------|
| Non-releasable memory | 599054 KiB |   1312 MiB | 330449 MiB | 329864 MiB |
|       from large pool | 597760 KiB |   1311 MiB | 308120 MiB | 307536 MiB |
|       from small pool |   1294 KiB |      4 MiB |  22329 MiB |  22327 MiB |
|---------------------------------------------------------------------------|
| Allocations           |     312    |     313    |  373579    |  373267    |
|       from large pool |     214    |     214    |   48367    |   48153    |
|       from small pool |      98    |     100    |  325212    |  325114    |
|---------------------------------------------------------------------------|
| Active allocs         |     312    |     313    |  373579    |  373267    |
|       from large pool |     214    |     214    |   48367    |   48153    |
|       from small pool |      98    |     100    |  325212    |  325114    |
|---------------------------------------------------------------------------|
| GPU reserved segments |     176    |     176    |     583    |     407    |
|       from large pool |     173    |     173    |     521    |     348    |
|       from small pool |       3    |       3    |      62    |      59    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |      21    |      29    |  182075    |  182054    |
|       from large pool |      13    |      20    |   29433    |   29420    |
|       from small pool |       8    |      10    |  152642    |  152634    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|

25-01-22 12:31:17 | W | |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 1                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |   6948 MiB |   7036 MiB |   7621 MiB | 688901 KiB |
|       from large pool |   6948 MiB |   7036 MiB |   7614 MiB | 682752 KiB |
|       from small pool |      0 MiB |      1 MiB |      6 MiB |   6149 KiB |
|---------------------------------------------------------------------------|
| Active memory         |   6948 MiB |   7036 MiB |   7621 MiB | 688901 KiB |
|       from large pool |   6948 MiB |   7036 MiB |   7614 MiB | 682752 KiB |
|       from small pool |      0 MiB |      1 MiB |      6 MiB |   6149 KiB |
|---------------------------------------------------------------------------|
| Requested memory      |   6948 MiB |   7034 MiB |   7609 MiB | 676608 KiB |
|       from large pool |   6948 MiB |   7034 MiB |   7602 MiB | 670464 KiB |
|       from small pool |      0 MiB |      1 MiB |      6 MiB |   6144 KiB |
|---------------------------------------------------------------------------|
| GPU reserved memory   |   6950 MiB |   7038 MiB |   7688 MiB | 755712 KiB |
|       from large pool |   6948 MiB |   7036 MiB |   7684 MiB | 753664 KiB |
|       from small pool |      2 MiB |      2 MiB |      4 MiB |   2048 KiB |
|---------------------------------------------------------------------------|
| Non-releasable memory |   1751 KiB |  20855 KiB |  92156 KiB |  90405 KiB |
|       from large pool |      0 KiB |  19104 KiB |  81920 KiB |  81920 KiB |
|       from small pool |   1751 KiB |   2047 KiB |  10236 KiB |   8485 KiB |
|---------------------------------------------------------------------------|
| Allocations           |     180    |     182    |     230    |      50    |
|       from large pool |     126    |     128    |     154    |      28    |
|       from small pool |      54    |      56    |      76    |      22    |
|---------------------------------------------------------------------------|
| Active allocs         |     180    |     182    |     230    |      50    |
|       from large pool |     126    |     128    |     154    |      28    |
|       from small pool |      54    |      56    |      76    |      22    |
|---------------------------------------------------------------------------|
| GPU reserved segments |     127    |     129    |     152    |      25    |
|       from large pool |     126    |     128    |     150    |      24    |
|       from small pool |       1    |       1    |       2    |       1    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       1    |       2    |       6    |       5    |
|       from large pool |       0    |       1    |       4    |       4    |
|       from small pool |       1    |       1    |       2    |       1    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|

25-01-22 12:31:17 | W | |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 2                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |   6948 MiB |   7036 MiB |   7621 MiB | 688901 KiB |
|       from large pool |   6948 MiB |   7036 MiB |   7614 MiB | 682752 KiB |
|       from small pool |      0 MiB |      1 MiB |      6 MiB |   6149 KiB |
|---------------------------------------------------------------------------|
| Active memory         |   6948 MiB |   7036 MiB |   7621 MiB | 688901 KiB |
|       from large pool |   6948 MiB |   7036 MiB |   7614 MiB | 682752 KiB |
|       from small pool |      0 MiB |      1 MiB |      6 MiB |   6149 KiB |
|---------------------------------------------------------------------------|
| Requested memory      |   6948 MiB |   7034 MiB |   7609 MiB | 676608 KiB |
|       from large pool |   6948 MiB |   7034 MiB |   7602 MiB | 670464 KiB |
|       from small pool |      0 MiB |      1 MiB |      6 MiB |   6144 KiB |
|---------------------------------------------------------------------------|
| GPU reserved memory   |   6950 MiB |   7038 MiB |   7688 MiB | 755712 KiB |
|       from large pool |   6948 MiB |   7036 MiB |   7684 MiB | 753664 KiB |
|       from small pool |      2 MiB |      2 MiB |      4 MiB |   2048 KiB |
|---------------------------------------------------------------------------|
| Non-releasable memory |   1751 KiB |  20855 KiB |  92156 KiB |  90405 KiB |
|       from large pool |      0 KiB |  19104 KiB |  81920 KiB |  81920 KiB |
|       from small pool |   1751 KiB |   2047 KiB |  10236 KiB |   8485 KiB |
|---------------------------------------------------------------------------|
| Allocations           |     180    |     182    |     230    |      50    |
|       from large pool |     126    |     128    |     154    |      28    |
|       from small pool |      54    |      56    |      76    |      22    |
|---------------------------------------------------------------------------|
| Active allocs         |     180    |     182    |     230    |      50    |
|       from large pool |     126    |     128    |     154    |      28    |
|       from small pool |      54    |      56    |      76    |      22    |
|---------------------------------------------------------------------------|
| GPU reserved segments |     127    |     129    |     152    |      25    |
|       from large pool |     126    |     128    |     150    |      24    |
|       from small pool |       1    |       1    |       2    |       1    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       1    |       2    |       6    |       5    |
|       from large pool |       0    |       1    |       4    |       4    |
|       from small pool |       1    |       1    |       2    |       1    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|

25-01-22 12:31:17 | W | |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 3                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |   5904 MiB |   5992 MiB |   6577 MiB | 688900 KiB |
|       from large pool |   5904 MiB |   5992 MiB |   6570 MiB | 682752 KiB |
|       from small pool |      0 MiB |      1 MiB |      6 MiB |   6148 KiB |
|---------------------------------------------------------------------------|
| Active memory         |   5904 MiB |   5992 MiB |   6577 MiB | 688900 KiB |
|       from large pool |   5904 MiB |   5992 MiB |   6570 MiB | 682752 KiB |
|       from small pool |      0 MiB |      1 MiB |      6 MiB |   6148 KiB |
|---------------------------------------------------------------------------|
| Requested memory      |   5904 MiB |   5990 MiB |   6565 MiB | 676608 KiB |
|       from large pool |   5904 MiB |   5990 MiB |   6558 MiB | 670464 KiB |
|       from small pool |      0 MiB |      1 MiB |      6 MiB |   6144 KiB |
|---------------------------------------------------------------------------|
| GPU reserved memory   |   5906 MiB |   5994 MiB |   6644 MiB | 755712 KiB |
|       from large pool |   5904 MiB |   5992 MiB |   6640 MiB | 753664 KiB |
|       from small pool |      2 MiB |      2 MiB |      4 MiB |   2048 KiB |
|---------------------------------------------------------------------------|
| Non-releasable memory |   1544 KiB |  20648 KiB |  92155 KiB |  90611 KiB |
|       from large pool |      0 KiB |  19104 KiB |  81920 KiB |  81920 KiB |
|       from small pool |   1544 KiB |   2047 KiB |  10235 KiB |   8691 KiB |
|---------------------------------------------------------------------------|
| Allocations           |     148    |     150    |     197    |      49    |
|       from large pool |     100    |     102    |     128    |      28    |
|       from small pool |      48    |      50    |      69    |      21    |
|---------------------------------------------------------------------------|
| Active allocs         |     148    |     150    |     197    |      49    |
|       from large pool |     100    |     102    |     128    |      28    |
|       from small pool |      48    |      50    |      69    |      21    |
|---------------------------------------------------------------------------|
| GPU reserved segments |     101    |     103    |     126    |      25    |
|       from large pool |     100    |     102    |     124    |      24    |
|       from small pool |       1    |       1    |       2    |       1    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       1    |       2    |       6    |       5    |
|       from large pool |       0    |       1    |       4    |       4    |
|       from small pool |       1    |       1    |       2    |       1    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|

25-01-22 12:31:17 | W | attempting to recover from OOM in forward/backward pass
25-01-22 12:31:20 | I | TRAIN CURRENT LAYER_IDX = 4
25-01-22 12:31:20 | I | in layer model.layers.4
25-01-22 12:31:20 | I | quantizing weights for layer model.layers.4
25-01-22 12:31:21 | I | collecting calibration activations in model.layers.4
25-01-22 12:31:21 | I | collecting calibration activations in model.layers.4
25-01-22 12:31:21 | I | collecting calibration activations in model.layers.4
25-01-22 12:31:21 | I | collecting calibration activations in model.layers.4
25-01-22 12:31:22 | I | - Quantizing decoder layer model.layers.4
25-01-22 12:31:22 | I |   - Calibrating model.layers.4.self_attn.q_proj.weight
25-01-22 12:31:23 | I |       - range scale = [    1.0000]
25-01-22 12:31:23 | I |         sum  error  = [   10.3037]
25-01-22 12:31:23 | I |         best error  = [   10.3037]
25-01-22 12:31:23 | I |     + error = [10.3037]
25-01-22 12:31:24 | I |       - range scale = [    1.0000]
25-01-22 12:31:24 | I |         sum  error  = [   21.1434]
25-01-22 12:31:24 | I |         best error  = [   21.1434]
25-01-22 12:31:24 | I |     + error = [21.1434]
25-01-22 12:31:24 | I |   - Calibrating model.layers.4.self_attn.k_proj.weight
25-01-22 12:31:25 | I |       - range scale = [    1.0000]
25-01-22 12:31:25 | I |         sum  error  = [   19.8313]
25-01-22 12:31:25 | I |         best error  = [   19.8313]
25-01-22 12:31:25 | I |     + error = [19.8313]
25-01-22 12:31:26 | I |       - range scale = [    1.0000]
25-01-22 12:31:26 | I |         sum  error  = [   34.9766]
25-01-22 12:31:26 | I |         best error  = [   34.9766]
25-01-22 12:31:26 | I |     + error = [34.9766]
25-01-22 12:31:26 | I |   - Calibrating model.layers.4.self_attn.v_proj.weight
25-01-22 12:31:27 | I |       - range scale = [    1.0000]
25-01-22 12:31:27 | I |         sum  error  = [    2.6800]
25-01-22 12:31:27 | I |         best error  = [    2.6800]
25-01-22 12:31:27 | I |     + error = [2.6800]
25-01-22 12:31:28 | I |       - range scale = [    1.0000]
25-01-22 12:31:28 | I |         sum  error  = [    6.0538]
25-01-22 12:31:28 | I |         best error  = [    6.0538]
25-01-22 12:31:28 | I |     + error = [6.0538]
25-01-22 12:31:28 | I |   - Calibrating model.layers.4.self_attn.o_proj.weight
25-01-22 12:31:29 | I |       - range scale = [    1.0000]
25-01-22 12:31:29 | I |         sum  error  = [    0.4284]
25-01-22 12:31:29 | I |         best error  = [    0.4284]
25-01-22 12:31:29 | I |     + error = [0.4284]
25-01-22 12:31:29 | I |       - range scale = [    1.0000]
25-01-22 12:31:29 | I |         sum  error  = [    1.1596]
25-01-22 12:31:29 | I |         best error  = [    1.1596]
25-01-22 12:31:29 | I |     + error = [1.1596]
25-01-22 12:31:30 | I |   - Calibrating model.layers.4.mlp.up_proj.weight
25-01-22 12:31:30 | I |       - range scale = [    1.0000]
25-01-22 12:31:30 | I |         sum  error  = [    3.4476]
25-01-22 12:31:30 | I |         best error  = [    3.4476]
25-01-22 12:31:30 | I |     + error = [3.4476]
25-01-22 12:31:31 | I |       - range scale = [    1.0000]
25-01-22 12:31:32 | I |         sum  error  = [    8.0227]
25-01-22 12:31:32 | I |         best error  = [    8.0227]
25-01-22 12:31:32 | I |     + error = [8.0227]
25-01-22 12:31:32 | I |   - Calibrating model.layers.4.mlp.gate_proj.weight
25-01-22 12:31:32 | I |       - range scale = [    1.0000]
25-01-22 12:31:32 | I |         sum  error  = [    4.1888]
25-01-22 12:31:32 | I |         best error  = [    4.1888]
25-01-22 12:31:32 | I |     + error = [4.1888]
25-01-22 12:31:34 | I |       - range scale = [    1.0000]
25-01-22 12:31:34 | I |         sum  error  = [    9.1162]
25-01-22 12:31:34 | I |         best error  = [    9.1162]
25-01-22 12:31:34 | I |     + error = [9.1162]
25-01-22 12:31:34 | I |   - Calibrating model.layers.4.mlp.down_proj.weight
25-01-22 12:31:35 | I |       - range scale = [    1.0000]
25-01-22 12:31:35 | I |         sum  error  = [    0.7673]
25-01-22 12:31:35 | I |         best error  = [    0.7673]
25-01-22 12:31:35 | I |     + error = [0.7673]
25-01-22 12:31:36 | I |       - range scale = [    1.0000]
25-01-22 12:31:36 | I |         sum  error  = [    1.8566]
25-01-22 12:31:36 | I |         best error  = [    1.8566]
25-01-22 12:31:36 | I |     + error = [1.8566]
25-01-22 12:31:36 | I |   - Quantizing model.layers.4.self_attn.q_proj.weight
25-01-22 12:31:39 | I |   - Quantizing model.layers.4.self_attn.k_proj.weight
25-01-22 12:31:42 | I |   - Quantizing model.layers.4.self_attn.v_proj.weight
25-01-22 12:31:45 | I |   - Quantizing model.layers.4.self_attn.o_proj.weight
25-01-22 12:31:48 | I |   - Quantizing model.layers.4.mlp.up_proj.weight
25-01-22 12:31:51 | I |   - Quantizing model.layers.4.mlp.gate_proj.weight
25-01-22 12:31:54 | I |   - Quantizing model.layers.4.mlp.down_proj.weight
