25-01-23 02:09:07 | I | === Configurations ===
25-01-23 02:09:07 | I | LlmRunConfig(
25-01-23 02:09:07 | I |   model=LlmModelConfig(
25-01-23 02:09:07 | I |     name=llama2-7b,
25-01-23 02:09:07 | I |     path=/data/gyy/llama2-7b,
25-01-23 02:09:07 | I |     root=/dataset/models,
25-01-23 02:09:07 | I |     local_path=/dataset/models/llama2/llama2-7b,
25-01-23 02:09:07 | I |     local_root=/dataset/models,
25-01-23 02:09:07 | I |     family=llama2,
25-01-23 02:09:07 | I |     size=7.0),
25-01-23 02:09:07 | I |   eval=LlmEvalConfig(
25-01-23 02:09:07 | I |     num_gpus=4,
25-01-23 02:09:07 | I |     batch_size=8,
25-01-23 02:09:07 | I |     output_root=runs,
25-01-23 02:09:07 | I |     output_dirname=skip.y.[q]-krnl-rot-rodr.Seq.[fc2]-smth.xw.a0p3.b0p7.[AbsMax].[out+fc2].yx.a0p5.b0.[AbsMax].[qk]-pileval.128x1024.[0-0]-250123.020907,
25-01-23 02:09:07 | I |     attach_timestamp=True,
25-01-23 02:09:07 | I |     timestamp=250123.020907,
25-01-23 02:09:07 | I |     output_dirname_without_timestamp=skip.y.[q]-krnl-rot-rodr.Seq.[fc2]-smth.xw.a0p3.b0p7.[AbsMax].[out+fc2].yx.a0p5.b0.[AbsMax].[qk]-pileval.128x1024.[0-0],
25-01-23 02:09:07 | I |     tasks=['wikitext'],
25-01-23 02:09:07 | I |     max_seq_length=-4096,
25-01-23 02:09:07 | I |     evaluator=gptq),
25-01-23 02:09:07 | I |   calib=LlmCalibConfig(
25-01-23 02:09:07 | I |     data=pileval,
25-01-23 02:09:07 | I |     num_samples=128,
25-01-23 02:09:07 | I |     cache_root=runs,
25-01-23 02:09:07 | I |     cache_dirpath=runs/llm/cache/pileval.128x1024.[0-0],
25-01-23 02:09:07 | I |     dataset_path=mit-han-lab/pile-val-backup,
25-01-23 02:09:07 | I |     seq_length=1024,
25-01-23 02:09:07 | I |     min_seq_length=0,
25-01-23 02:09:07 | I |     max_seq_length=0,
25-01-23 02:09:07 | I |     local_dataset_path=/dataset/pile),
25-01-23 02:09:07 | I |   quant=LlmQuantConfig(
25-01-23 02:09:07 | I |     wgts=WeightQuantizerConfig(
25-01-23 02:09:07 | I |       dtype=zint4,
25-01-23 02:09:07 | I |       group_shapes=((1, -1, -1), (1, 128, -1)),
25-01-23 02:09:07 | I |       group_scale_dtypes=(torch.float16, sint8),
25-01-23 02:09:07 | I |       compute_dtype=sint8,
25-01-23 02:09:07 | I |       compute_group_level=0,
25-01-23 02:09:07 | I |       saturate_compute_dtype=False,
25-01-23 02:09:07 | I |       exponent_scaling_level=2,
25-01-23 02:09:07 | I |       skips=['embed', 'head', 'router'],
25-01-23 02:09:07 | I |       static=True,
25-01-23 02:09:07 | I |       calib_kernel=QuantizerKernelConfig(
25-01-23 02:09:07 | I |         _kernels={'proj_1st': QuantGPTQConfig(includes=['proj_1st', 'proj_2nd', 'proj_out', 'proj_qkv'], damp_percentage=0.01, block_size=128, num_inv_tries=250, hessian_block_size=512), 'proj_2nd': QuantGPTQConfig(includes=['proj_1st', 'proj_2nd', 'proj_out', 'proj_qkv'], damp_percentage=0.01, block_size=128, num_inv_tries=250, hessian_block_size=512), 'proj_out': QuantGPTQConfig(includes=['proj_1st', 'proj_2nd', 'proj_out', 'proj_qkv'], damp_percentage=0.01, block_size=128, num_inv_tries=250, hessian_block_size=512), 'proj_qkv': QuantGPTQConfig(includes=['proj_1st', 'proj_2nd', 'proj_out', 'proj_qkv'], damp_percentage=0.01, block_size=128, num_inv_tries=250, hessian_block_size=512)},
25-01-23 02:09:07 | I |         gptq=QuantGPTQConfig(
25-01-23 02:09:07 | I |           includes=['proj_1st', 'proj_2nd', 'proj_out', 'proj_qkv'],
25-01-23 02:09:07 | I |           damp_percentage=0.01,
25-01-23 02:09:07 | I |           block_size=128,
25-01-23 02:09:07 | I |           num_inv_tries=250,
25-01-23 02:09:07 | I |           hessian_block_size=512)),
25-01-23 02:09:07 | I |       calib_range=DynamicRangeCalibConfig(
25-01-23 02:09:07 | I |         degree=2,
25-01-23 02:09:07 | I |         skips=[],
25-01-23 02:09:07 | I |         objective=SearchBasedCalibObjective.OutputsError,
25-01-23 02:09:07 | I |         strategy=SearchBasedCalibStrategy.Manual,
25-01-23 02:09:07 | I |         granularity=SearchBasedCalibGranularity.Group,
25-01-23 02:09:07 | I |         element_batch_size=64,
25-01-23 02:09:07 | I |         sample_batch_size=-1,
25-01-23 02:09:07 | I |         element_size=512,
25-01-23 02:09:07 | I |         sample_size=-1,
25-01-23 02:09:07 | I |         pre_reshape=True,
25-01-23 02:09:07 | I |         outputs_device=cpu,
25-01-23 02:09:07 | I |         allow_kernel_calib=False,
25-01-23 02:09:07 | I |         ratio=1.0,
25-01-23 02:09:07 | I |         max_shrink=0.2,
25-01-23 02:09:07 | I |         max_expand=1.0,
25-01-23 02:09:07 | I |         num_grids=80)),
25-01-23 02:09:07 | I |     ipts=ActivationQuantizerConfig(
25-01-23 02:09:07 | I |       dtype=sint8,
25-01-23 02:09:07 | I |       group_shapes=((1, -1, -1),),
25-01-23 02:09:07 | I |       group_scale_dtypes=(torch.float16,),
25-01-23 02:09:07 | I |       compute_dtype=None,
25-01-23 02:09:07 | I |       compute_group_level=-1,
25-01-23 02:09:07 | I |       saturate_compute_dtype=False,
25-01-23 02:09:07 | I |       exponent_scaling_level=1,
25-01-23 02:09:07 | I |       skips=['embed', 'head', 'router'],
25-01-23 02:09:07 | I |       static=False,
25-01-23 02:09:07 | I |       calib_kernel=None,
25-01-23 02:09:07 | I |       calib_range=None),
25-01-23 02:09:07 | I |     opts=ActivationQuantizerConfig(
25-01-23 02:09:07 | I |       dtype=zint4,
25-01-23 02:09:07 | I |       group_shapes=((1, 128, -1),),
25-01-23 02:09:07 | I |       group_scale_dtypes=(torch.float16,),
25-01-23 02:09:07 | I |       compute_dtype=None,
25-01-23 02:09:07 | I |       compute_group_level=-1,
25-01-23 02:09:07 | I |       saturate_compute_dtype=False,
25-01-23 02:09:07 | I |       exponent_scaling_level=1,
25-01-23 02:09:07 | I |       skips=['attn_q'],
25-01-23 02:09:07 | I |       static=False,
25-01-23 02:09:07 | I |       calib_kernel=None,
25-01-23 02:09:07 | I |       calib_range=None),
25-01-23 02:09:07 | I |     rotation=QuantRotationConfig(
25-01-23 02:09:07 | I |       random=False,
25-01-23 02:09:07 | I |       transforms=[]),
25-01-23 02:09:07 | I |     reorder=QuantReorderConfig(
25-01-23 02:09:07 | I |       degree=2,
25-01-23 02:09:07 | I |       skips=['proj_out', 'proj_qkv', 'residual'],
25-01-23 02:09:07 | I |       objective=SearchBasedCalibObjective.OutputsError,
25-01-23 02:09:07 | I |       strategy=SearchBasedCalibStrategy.Manual,
25-01-23 02:09:07 | I |       granularity=SearchBasedCalibGranularity.Layer,
25-01-23 02:09:07 | I |       element_batch_size=-1,
25-01-23 02:09:07 | I |       sample_batch_size=-1,
25-01-23 02:09:07 | I |       element_size=-1,
25-01-23 02:09:07 | I |       sample_size=-1,
25-01-23 02:09:07 | I |       pre_reshape=True,
25-01-23 02:09:07 | I |       outputs_device=cpu,
25-01-23 02:09:07 | I |       allow_kernel_calib=False,
25-01-23 02:09:07 | I |       channel_metric=ChannelMetric.InputsAbsMax,
25-01-23 02:09:07 | I |       channel_index=ChannelIndex.Sequential,
25-01-23 02:09:07 | I |       dynamic=False),
25-01-23 02:09:07 | I |     smooth=QuantSmoothConfig(
25-01-23 02:09:07 | I |       xw=QuantSmoothCalibConfig(
25-01-23 02:09:07 | I |         degree=2,
25-01-23 02:09:07 | I |         skips=['proj_1st', 'proj_qkv'],
25-01-23 02:09:07 | I |         objective=SearchBasedCalibObjective.OutputsError,
25-01-23 02:09:07 | I |         strategy=SearchBasedCalibStrategy.Manual,
25-01-23 02:09:07 | I |         granularity=SearchBasedCalibGranularity.Layer,
25-01-23 02:09:07 | I |         element_batch_size=-1,
25-01-23 02:09:07 | I |         sample_batch_size=-1,
25-01-23 02:09:07 | I |         element_size=-1,
25-01-23 02:09:07 | I |         sample_size=-1,
25-01-23 02:09:07 | I |         pre_reshape=True,
25-01-23 02:09:07 | I |         outputs_device=cpu,
25-01-23 02:09:07 | I |         allow_kernel_calib=False,
25-01-23 02:09:07 | I |         ranges=[(<RangeMode.AbsMax: 1>, <RangeMode.AbsMax: 1>)],
25-01-23 02:09:07 | I |         x_ranges=[<RangeMode.AbsMax: 1>],
25-01-23 02:09:07 | I |         w_ranges=[<RangeMode.AbsMax: 1>],
25-01-23 02:09:07 | I |         alpha=0.3,
25-01-23 02:09:07 | I |         beta=0.7,
25-01-23 02:09:07 | I |         num_grids=20),
25-01-23 02:09:07 | I |       yx=QuantSmoothCalibConfig(
25-01-23 02:09:07 | I |         degree=2,
25-01-23 02:09:07 | I |         skips=[],
25-01-23 02:09:07 | I |         objective=SearchBasedCalibObjective.OutputsError,
25-01-23 02:09:07 | I |         strategy=SearchBasedCalibStrategy.Manual,
25-01-23 02:09:07 | I |         granularity=SearchBasedCalibGranularity.Layer,
25-01-23 02:09:07 | I |         element_batch_size=-1,
25-01-23 02:09:07 | I |         sample_batch_size=-1,
25-01-23 02:09:07 | I |         element_size=-1,
25-01-23 02:09:07 | I |         sample_size=-1,
25-01-23 02:09:07 | I |         pre_reshape=True,
25-01-23 02:09:07 | I |         outputs_device=cpu,
25-01-23 02:09:07 | I |         allow_kernel_calib=False,
25-01-23 02:09:07 | I |         ranges=[(<RangeMode.AbsMax: 1>, <RangeMode.AbsMax: 1>)],
25-01-23 02:09:07 | I |         x_ranges=[<RangeMode.AbsMax: 1>],
25-01-23 02:09:07 | I |         w_ranges=[<RangeMode.AbsMax: 1>],
25-01-23 02:09:07 | I |         alpha=0.5,
25-01-23 02:09:07 | I |         beta=0.0,
25-01-23 02:09:07 | I |         num_grids=20)),
25-01-23 02:09:07 | I |     bias_correction=False,
25-01-23 02:09:07 | I |     post_rotary=True,
25-01-23 02:09:07 | I |     develop_dtype=torch.float32,
25-01-23 02:09:07 | I |     select_wgts=None,
25-01-23 02:09:07 | I |     select_ipts=None,
25-01-23 02:09:07 | I |     select_opts=None,
25-01-23 02:09:07 | I |     keywords_i={'proj_qkv': ['q_proj', 'k_proj', 'v_proj'], 'proj_out': ['out_proj', 'o_proj'], 'proj_1st': ['fc1', 'up_proj', 'gate_proj', 'w1', 'w3'], 'proj_2nd': ['fc2', 'down_proj', 'w2'], 'head': ['output', 'score', 'qa_outputs'], 'embed': ['embed', 'lm_head', 'embed_out'], 'router': ['block_sparse_moe']},
25-01-23 02:09:07 | I |     keywords_w={'proj_qkv': ['q_proj', 'k_proj', 'v_proj'], 'proj_out': ['out_proj', 'o_proj'], 'proj_1st': ['fc1', 'up_proj', 'gate_proj', 'w1', 'w3'], 'proj_2nd': ['fc2', 'down_proj', 'w2'], 'head': ['output', 'score', 'qa_outputs'], 'embed': ['embed', 'lm_head', 'embed_out'], 'router': ['block_sparse_moe.gate']},
25-01-23 02:09:07 | I |     keywords_o={'attn_q': ['q_rotary_emb'], 'attn_k': ['k_rotary_emb'], 'attn_v': ['v_proj']},
25-01-23 02:09:07 | I |     module_types_i=(<class 'torch.nn.modules.linear.Linear'>, <class 'transformers.models.mixtral.modeling_mixtral.MixtralSparseMoeBlock'>),
25-01-23 02:09:07 | I |     module_types_w=(<class 'torch.nn.modules.linear.Linear'>,),
25-01-23 02:09:07 | I |     module_types_o=(<class 'torch.nn.modules.linear.Linear'>, <class 'lmquant.llm.nn.attention.RotaryEmbedding'>),
25-01-23 02:09:07 | I |     num_hidden_layers=-1),
25-01-23 02:09:07 | I |   seed=12345,
25-01-23 02:09:07 | I |   save_model=True,
25-01-23 02:09:07 | I |   output_dirpath=runs/llm/llama2/llama2-7b/w.4-x.8-y.4/w.zint4-x.sint8-y.zint4/w.gchn.fp16.g128.sint8.[gchn.sint8]-x.gchn.fp16-y.g128.fp16/rot-reorder-smooth.xw.yx-w.static.kernel/skip.y.[q]-krnl-rot-rodr.Seq.[fc2]-smth.xw.a0p3.b0p7.[AbsMax].[out+fc2].yx.a0p5.b0.[AbsMax].[qk]-pileval.128x1024.[0-0]-250123.020907,
25-01-23 02:09:07 | I |   cache_dirpath=LlmQuantCachePath(rotation='runs/llm/cache/rotation/hadamard', reorder='runs/llm/cache/pileval.128x1024.[0-0]/reorder/w.4-x.8-y.4/w.zint4-x.sint8-y.zint4/w.gchn.fp16.g128.sint8.[gchn.sint8]-x.gchn.fp16-y.g128.fp16/w.skip.[embed+head+router]-x.skip.[embed+head+router]-y.skip.[attn_q]/rotate.hadamard/reorder.OutputsError.Manual.Layer.d2.en1.sn1/reorder.InputsAbsMax.Sequential/reorder.skip.[proj_out+proj_qkv+residual]', smooth='runs/llm/cache/pileval.128x1024.[0-0]/smooth/w.4-x.8-y.4/w.zint4-x.sint8-y.zint4/w.gchn.fp16.g128.sint8.[gchn.sint8]-x.gchn.fp16-y.g128.fp16/w.skip.[embed+head+router]-x.skip.[embed+head+router]-y.skip.[attn_q]/rotate.hadamard/reorder.OutputsError.Manual.Layer.d2.en1.sn1/reorder.InputsAbsMax.Sequential/reorder.skip.[proj_out+proj_qkv+residual]/smooth.xw.OutputsError.Manual.Layer.d2.en1.sn1-yx.OutputsError.Manual.Layer.d2.en1.sn1/smooth.xw.[x.AbsMax.w.AbsMax]-yx.[x.AbsMax.w.AbsMax]/smooth.xw.a0p3.b0p7-yx.a0p5.b0/smooth.xw.skip.[proj_1st+proj_qkv]-yx.skip.[]', wgts='runs/llm/cache/pileval.128x1024.[0-0]/wgts/w.4-x.8-y.4/w.zint4-x.sint8-y.zint4/w.gchn.fp16.g128.sint8.[gchn.sint8]-x.gchn.fp16-y.g128.fp16/w.skip.[embed+head+router]-x.skip.[embed+head+router]-y.skip.[attn_q]/rotate.hadamard/reorder.OutputsError.Manual.Layer.d2.en1.sn1/reorder.InputsAbsMax.Sequential/reorder.skip.[proj_out+proj_qkv+residual]/smooth.xw.OutputsError.Manual.Layer.d2.en1.sn1-yx.OutputsError.Manual.Layer.d2.en1.sn1/smooth.xw.[x.AbsMax.w.AbsMax]-yx.[x.AbsMax.w.AbsMax]/smooth.xw.a0p3.b0p7-yx.a0p5.b0/smooth.xw.skip.[proj_1st+proj_qkv]-yx.skip.[]/w.kernel.gptq.d0p01.b128/w.kernel.gptq.include.[proj_1st+proj_2nd+proj_out+proj_qkv]/w.range.OutputsError.Manual.Group.d2.e512.sn1/w.range.r.[1].static/w.range.skip.[]', acts=''),
25-01-23 02:09:07 | I |   cache_path=LlmQuantCachePath(rotation='runs/llm/cache/rotation/hadamard/llama2-7b.pt', reorder='runs/llm/cache/pileval.128x1024.[0-0]/reorder/w.4-x.8-y.4/w.zint4-x.sint8-y.zint4/w.gchn.fp16.g128.sint8.[gchn.sint8]-x.gchn.fp16-y.g128.fp16/w.skip.[embed+head+router]-x.skip.[embed+head+router]-y.skip.[attn_q]/rotate.hadamard/reorder.OutputsError.Manual.Layer.d2.en1.sn1/reorder.InputsAbsMax.Sequential/reorder.skip.[proj_out+proj_qkv+residual]/llama2-7b.pt', smooth='runs/llm/cache/pileval.128x1024.[0-0]/smooth/w.4-x.8-y.4/w.zint4-x.sint8-y.zint4/w.gchn.fp16.g128.sint8.[gchn.sint8]-x.gchn.fp16-y.g128.fp16/w.skip.[embed+head+router]-x.skip.[embed+head+router]-y.skip.[attn_q]/rotate.hadamard/reorder.OutputsError.Manual.Layer.d2.en1.sn1/reorder.InputsAbsMax.Sequential/reorder.skip.[proj_out+proj_qkv+residual]/smooth.xw.OutputsError.Manual.Layer.d2.en1.sn1-yx.OutputsError.Manual.Layer.d2.en1.sn1/smooth.xw.[x.AbsMax.w.AbsMax]-yx.[x.AbsMax.w.AbsMax]/smooth.xw.a0p3.b0p7-yx.a0p5.b0/smooth.xw.skip.[proj_1st+proj_qkv]-yx.skip.[]/llama2-7b.pt', wgts='runs/llm/cache/pileval.128x1024.[0-0]/wgts/w.4-x.8-y.4/w.zint4-x.sint8-y.zint4/w.gchn.fp16.g128.sint8.[gchn.sint8]-x.gchn.fp16-y.g128.fp16/w.skip.[embed+head+router]-x.skip.[embed+head+router]-y.skip.[attn_q]/rotate.hadamard/reorder.OutputsError.Manual.Layer.d2.en1.sn1/reorder.InputsAbsMax.Sequential/reorder.skip.[proj_out+proj_qkv+residual]/smooth.xw.OutputsError.Manual.Layer.d2.en1.sn1-yx.OutputsError.Manual.Layer.d2.en1.sn1/smooth.xw.[x.AbsMax.w.AbsMax]-yx.[x.AbsMax.w.AbsMax]/smooth.xw.a0p3.b0p7-yx.a0p5.b0/smooth.xw.skip.[proj_1st+proj_qkv]-yx.skip.[]/w.kernel.gptq.d0p01.b128/w.kernel.gptq.include.[proj_1st+proj_2nd+proj_out+proj_qkv]/w.range.OutputsError.Manual.Group.d2.e512.sn1/w.range.r.[1].static/w.range.skip.[]/llama2-7b.pt', acts=''),
25-01-23 02:09:07 | I |   fairseq_args=/data/gyy/lmquant-main/projects/llm/configs/fairseq_args_7b_without_preprocess.json,
25-01-23 02:09:07 | I |   gen_teacher_opts=False,
25-01-23 02:09:07 | I |   enable_cache=True,
25-01-23 02:09:07 | I |   with_preprocess=False)
25-01-23 02:09:07 | I | === Dumped Configurations ===
25-01-23 02:09:07 | I | { 'calib': { 'cache_root': 'runs',
25-01-23 02:09:07 | I |              'data': 'pileval',
25-01-23 02:09:07 | I |              'dataset_path': 'mit-han-lab/pile-val-backup',
25-01-23 02:09:07 | I |              'local_dataset_path': '/dataset/pile',
25-01-23 02:09:07 | I |              'max_seq_length': 0,
25-01-23 02:09:07 | I |              'min_seq_length': 0,
25-01-23 02:09:07 | I |              'num_samples': 128,
25-01-23 02:09:07 | I |              'seq_length': 1024},
25-01-23 02:09:07 | I |   'enable_cache': True,
25-01-23 02:09:07 | I |   'eval': { 'attach_timestamp': True,
25-01-23 02:09:07 | I |             'batch_size': 8,
25-01-23 02:09:07 | I |             'evaluator': 'gptq',
25-01-23 02:09:07 | I |             'max_seq_length': -4096,
25-01-23 02:09:07 | I |             'num_gpus': 4,
25-01-23 02:09:07 | I |             'output_dirname': 'skip.y.[q]-krnl-rot-rodr.Seq.[fc2]-smth.xw.a0p3.b0p7.[AbsMax].[out+fc2].yx.a0p5.b0.[AbsMax].[qk]-pileval.128x1024.[0-0]-250123.020907',
25-01-23 02:09:07 | I |             'output_root': 'runs',
25-01-23 02:09:07 | I |             'tasks': ['wikitext']},
25-01-23 02:09:07 | I |   'fairseq_args': '/data/gyy/lmquant-main/projects/llm/configs/fairseq_args_7b_without_preprocess.json',
25-01-23 02:09:07 | I |   'gen_teacher_opts': False,
25-01-23 02:09:07 | I |   'model': { 'local_path': '/dataset/models/llama2/llama2-7b',
25-01-23 02:09:07 | I |              'local_root': '/dataset/models',
25-01-23 02:09:07 | I |              'name': 'llama2-7b',
25-01-23 02:09:07 | I |              'path': '/data/gyy/llama2-7b',
25-01-23 02:09:07 | I |              'root': '/dataset/models'},
25-01-23 02:09:07 | I |   'quant': { 'bias_correction': False,
25-01-23 02:09:07 | I |              'develop_dtype': 'torch.float32',
25-01-23 02:09:07 | I |              'enable_reorder': True,
25-01-23 02:09:07 | I |              'enable_rotation': True,
25-01-23 02:09:07 | I |              'enable_select_ipts': False,
25-01-23 02:09:07 | I |              'enable_select_opts': False,
25-01-23 02:09:07 | I |              'enable_select_wgts': False,
25-01-23 02:09:07 | I |              'enable_smooth': True,
25-01-23 02:09:07 | I |              'ipts': { 'compute_dtype': None,
25-01-23 02:09:07 | I |                        'compute_group_level': -1,
25-01-23 02:09:07 | I |                        'dtype': 'sint8',
25-01-23 02:09:07 | I |                        'enable_calib_range': False,
25-01-23 02:09:07 | I |                        'group_scale_dtypes': ['torch.float16'],
25-01-23 02:09:07 | I |                        'group_shapes': [[1, -1, -1]],
25-01-23 02:09:07 | I |                        'saturate_compute_dtype': False,
25-01-23 02:09:07 | I |                        'skips': ['embed', 'head', 'router'],
25-01-23 02:09:07 | I |                        'static': False},
25-01-23 02:09:07 | I |              'opts': { 'compute_dtype': None,
25-01-23 02:09:07 | I |                        'compute_group_level': -1,
25-01-23 02:09:07 | I |                        'dtype': 'zint4',
25-01-23 02:09:07 | I |                        'enable_calib_range': False,
25-01-23 02:09:07 | I |                        'group_scale_dtypes': ['torch.float16'],
25-01-23 02:09:07 | I |                        'group_shapes': [[1, 128, -1]],
25-01-23 02:09:07 | I |                        'saturate_compute_dtype': False,
25-01-23 02:09:07 | I |                        'skips': ['attn_q'],
25-01-23 02:09:07 | I |                        'static': False},
25-01-23 02:09:07 | I |              'post_rotary': True,
25-01-23 02:09:07 | I |              'reorder': { 'allow_kernel_calib': False,
25-01-23 02:09:07 | I |                           'channel_index': 'Sequential',
25-01-23 02:09:07 | I |                           'channel_metric': 'InputsAbsMax',
25-01-23 02:09:07 | I |                           'degree': 2,
25-01-23 02:09:07 | I |                           'dynamic': False,
25-01-23 02:09:07 | I |                           'element_batch_size': -1,
25-01-23 02:09:07 | I |                           'element_size': -1,
25-01-23 02:09:07 | I |                           'outputs_device': 'cpu',
25-01-23 02:09:07 | I |                           'pre_reshape': True,
25-01-23 02:09:07 | I |                           'sample_batch_size': -1,
25-01-23 02:09:07 | I |                           'sample_size': -1,
25-01-23 02:09:07 | I |                           'skips': ['proj_out', 'proj_qkv', 'residual'],
25-01-23 02:09:07 | I |                           'strategy': 'Manual'},
25-01-23 02:09:07 | I |              'rotation': {'random': False, 'transforms': []},
25-01-23 02:09:07 | I |              'smooth': { 'enable_xw': True,
25-01-23 02:09:07 | I |                          'enable_yx': True,
25-01-23 02:09:07 | I |                          'xw': { 'allow_kernel_calib': False,
25-01-23 02:09:07 | I |                                  'alpha': 0.3,
25-01-23 02:09:07 | I |                                  'beta': 0.7,
25-01-23 02:09:07 | I |                                  'degree': 2,
25-01-23 02:09:07 | I |                                  'element_batch_size': -1,
25-01-23 02:09:07 | I |                                  'element_size': -1,
25-01-23 02:09:07 | I |                                  'granularity': 'Layer',
25-01-23 02:09:07 | I |                                  'num_grids': 20,
25-01-23 02:09:07 | I |                                  'objective': 'OutputsError',
25-01-23 02:09:07 | I |                                  'outputs_device': 'cpu',
25-01-23 02:09:07 | I |                                  'pre_reshape': True,
25-01-23 02:09:07 | I |                                  'ranges': [['AbsMax', 'AbsMax']],
25-01-23 02:09:07 | I |                                  'sample_batch_size': -1,
25-01-23 02:09:07 | I |                                  'sample_size': -1,
25-01-23 02:09:07 | I |                                  'skips': ['proj_1st', 'proj_qkv'],
25-01-23 02:09:07 | I |                                  'strategy': 'Manual'},
25-01-23 02:09:07 | I |                          'yx': { 'allow_kernel_calib': False,
25-01-23 02:09:07 | I |                                  'alpha': 0.5,
25-01-23 02:09:07 | I |                                  'beta': 0.0,
25-01-23 02:09:07 | I |                                  'degree': 2,
25-01-23 02:09:07 | I |                                  'element_batch_size': -1,
25-01-23 02:09:07 | I |                                  'element_size': -1,
25-01-23 02:09:07 | I |                                  'granularity': 'Layer',
25-01-23 02:09:07 | I |                                  'num_grids': 20,
25-01-23 02:09:07 | I |                                  'objective': 'OutputsError',
25-01-23 02:09:07 | I |                                  'outputs_device': 'cpu',
25-01-23 02:09:07 | I |                                  'pre_reshape': True,
25-01-23 02:09:07 | I |                                  'ranges': [['AbsMax', 'AbsMax']],
25-01-23 02:09:07 | I |                                  'sample_batch_size': -1,
25-01-23 02:09:07 | I |                                  'sample_size': -1,
25-01-23 02:09:07 | I |                                  'skips': [],
25-01-23 02:09:07 | I |                                  'strategy': 'Manual'}},
25-01-23 02:09:07 | I |              'wgts': { 'calib_kernel': { 'enable_gptq': True,
25-01-23 02:09:07 | I |                                          'gptq': { 'block_size': 128,
25-01-23 02:09:07 | I |                                                    'damp_percentage': 0.01,
25-01-23 02:09:07 | I |                                                    'hessian_block_size': 512,
25-01-23 02:09:07 | I |                                                    'includes': ['proj_1st', 'proj_2nd', 'proj_out', 'proj_qkv'],
25-01-23 02:09:07 | I |                                                    'num_inv_tries': 250}},
25-01-23 02:09:07 | I |                        'calib_range': { 'allow_kernel_calib': False,
25-01-23 02:09:07 | I |                                         'degree': 2,
25-01-23 02:09:07 | I |                                         'element_batch_size': 64,
25-01-23 02:09:07 | I |                                         'element_size': 512,
25-01-23 02:09:07 | I |                                         'granularity': 'Group',
25-01-23 02:09:07 | I |                                         'max_expand': 1.0,
25-01-23 02:09:07 | I |                                         'max_shrink': 0.2,
25-01-23 02:09:07 | I |                                         'num_grids': 80,
25-01-23 02:09:07 | I |                                         'objective': 'OutputsError',
25-01-23 02:09:07 | I |                                         'outputs_device': 'cpu',
25-01-23 02:09:07 | I |                                         'pre_reshape': True,
25-01-23 02:09:07 | I |                                         'ratio': 1.0,
25-01-23 02:09:07 | I |                                         'sample_batch_size': -1,
25-01-23 02:09:07 | I |                                         'sample_size': -1,
25-01-23 02:09:07 | I |                                         'skips': [],
25-01-23 02:09:07 | I |                                         'strategy': 'Manual'},
25-01-23 02:09:07 | I |                        'compute_dtype': 'sint8',
25-01-23 02:09:07 | I |                        'compute_group_level': 0,
25-01-23 02:09:07 | I |                        'dtype': 'zint4',
25-01-23 02:09:07 | I |                        'enable_calib_kernel': True,
25-01-23 02:09:07 | I |                        'enable_calib_range': True,
25-01-23 02:09:07 | I |                        'group_scale_dtypes': ['torch.float16', 'sint8'],
25-01-23 02:09:07 | I |                        'group_shapes': [[1, -1, -1], [1, 128, -1]],
25-01-23 02:09:07 | I |                        'saturate_compute_dtype': False,
25-01-23 02:09:07 | I |                        'skips': ['embed', 'head', 'router']}},
25-01-23 02:09:07 | I |   'save_model': True,
25-01-23 02:09:07 | I |   'seed': 12345,
25-01-23 02:09:07 | I |   'with_preprocess': False}
25-01-23 02:09:07 | I | === Output Directory ===
25-01-23 02:09:07 | I | runs/llm/llama2/llama2-7b/w.4-x.8-y.4/w.zint4-x.sint8-y.zint4/w.gchn.fp16.g128.sint8.[gchn.sint8]-x.gchn.fp16-y.g128.fp16/rot-reorder-smooth.xw.yx-w.static.kernel/skip.y.[q]-krnl-rot-rodr.Seq.[fc2]-smth.xw.a0p3.b0p7.[AbsMax].[out+fc2].yx.a0p5.b0.[AbsMax].[qk]-pileval.128x1024.[0-0]-250123.020907
25-01-23 02:09:07 | I | === Start Evaluating ===
25-01-23 02:09:07 | I | * Building model llama2-7b from /data/gyy/llama2-7b
25-01-23 02:10:50 | I | - Patching LlamaSdpaAttention._old_forward in model.layers.0.self_attn
25-01-23 02:10:50 | I | - Patching LlamaSdpaAttention._old_forward in model.layers.1.self_attn
25-01-23 02:10:50 | I | - Patching LlamaSdpaAttention._old_forward in model.layers.2.self_attn
25-01-23 02:10:50 | I | - Patching LlamaSdpaAttention._old_forward in model.layers.3.self_attn
25-01-23 02:10:50 | I | - Patching LlamaSdpaAttention._old_forward in model.layers.4.self_attn
25-01-23 02:10:50 | I | - Patching LlamaSdpaAttention._old_forward in model.layers.5.self_attn
25-01-23 02:10:50 | I | - Patching LlamaSdpaAttention._old_forward in model.layers.6.self_attn
25-01-23 02:10:51 | I | - Patching LlamaSdpaAttention._old_forward in model.layers.7.self_attn
25-01-23 02:10:51 | I | - Patching LlamaSdpaAttention._old_forward in model.layers.8.self_attn
25-01-23 02:10:51 | I | - Patching LlamaSdpaAttention._old_forward in model.layers.9.self_attn
25-01-23 02:10:51 | I | - Patching LlamaSdpaAttention._old_forward in model.layers.10.self_attn
25-01-23 02:10:51 | I | - Patching LlamaSdpaAttention._old_forward in model.layers.11.self_attn
25-01-23 02:10:51 | I | - Patching LlamaSdpaAttention._old_forward in model.layers.12.self_attn
25-01-23 02:10:51 | I | - Patching LlamaSdpaAttention._old_forward in model.layers.13.self_attn
25-01-23 02:10:51 | I | - Patching LlamaSdpaAttention._old_forward in model.layers.14.self_attn
25-01-23 02:10:51 | I | - Patching LlamaSdpaAttention._old_forward in model.layers.15.self_attn
25-01-23 02:10:51 | I | - Patching LlamaSdpaAttention._old_forward in model.layers.16.self_attn
25-01-23 02:10:51 | I | - Patching LlamaSdpaAttention._old_forward in model.layers.17.self_attn
25-01-23 02:10:51 | I | - Patching LlamaSdpaAttention._old_forward in model.layers.18.self_attn
25-01-23 02:10:51 | I | - Patching LlamaSdpaAttention._old_forward in model.layers.19.self_attn
25-01-23 02:10:51 | I | - Patching LlamaSdpaAttention._old_forward in model.layers.20.self_attn
25-01-23 02:10:51 | I | - Patching LlamaSdpaAttention._old_forward in model.layers.21.self_attn
25-01-23 02:10:51 | I | - Patching LlamaSdpaAttention._old_forward in model.layers.22.self_attn
25-01-23 02:10:51 | I | - Patching LlamaSdpaAttention._old_forward in model.layers.23.self_attn
25-01-23 02:10:51 | I | - Patching LlamaSdpaAttention._old_forward in model.layers.24.self_attn
25-01-23 02:10:51 | I | - Patching LlamaSdpaAttention._old_forward in model.layers.25.self_attn
25-01-23 02:10:51 | I | - Patching LlamaSdpaAttention._old_forward in model.layers.26.self_attn
25-01-23 02:10:51 | I | - Patching LlamaSdpaAttention._old_forward in model.layers.27.self_attn
25-01-23 02:10:51 | I | - Patching LlamaSdpaAttention._old_forward in model.layers.28.self_attn
25-01-23 02:10:51 | I | - Patching LlamaSdpaAttention._old_forward in model.layers.29.self_attn
25-01-23 02:10:51 | I | - Patching LlamaSdpaAttention._old_forward in model.layers.30.self_attn
25-01-23 02:10:51 | I | - Patching LlamaSdpaAttention._old_forward in model.layers.31.self_attn
25-01-23 02:10:51 | I | * Development dtype is torch.float32
25-01-23 02:10:51 | I | * Begin to QAT
25-01-23 02:10:51 | D | Setting JobRuntime:name=UNKNOWN_NAME
25-01-23 02:10:51 | D | Setting JobRuntime:name=utils
25-01-23 02:10:53 | I | distributed init (rank 0): env://
25-01-23 02:10:53 | I | initialized host 3fb0cc53fbf9 as rank 0
25-01-23 02:10:54 | I | nvidia-smi stats: {'gpu_0_mem_used_gb': 2.080078125, 'gpu_1_mem_used_gb': 6.150390625, 'gpu_2_mem_used_gb': 7.0859375, 'gpu_3_mem_used_gb': 7.0859375, 'gpu_4_mem_used_gb': 19.76953125, 'gpu_5_mem_used_gb': 7.173828125, 'gpu_6_mem_used_gb': 7.173828125, 'gpu_7_mem_used_gb': 6.142578125}
25-01-23 02:10:54 | I | {'_name': None, 'common': {'_name': None, 'no_progress_bar': False, 'log_interval': 1, 'log_format': 'simple', 'log_file': None, 'tensorboard_logdir': None, 'wandb_project': 'layer_wise_quant', 'azureml_logging': False, 'seed': 1, 'cpu': False, 'tpu': False, 'bf16': False, 'memory_efficient_bf16': False, 'fp16': True, 'memory_efficient_fp16': True, 'fp16_no_flatten_grads': False, 'fp16_init_scale': 128, 'fp16_scale_window': None, 'fp16_scale_tolerance': 0.0, 'min_loss_scale': 0.0001, 'threshold_loss_scale': None, 'user_dir': None, 'empty_cache_freq': 0, 'all_gather_list_size': 16384, 'model_parallel_size': 1, 'quantization_config_path': None, 'profile': False, 'reset_logging': False, 'suppress_crashes': False, 'use_plasma_view': False, 'plasma_path': '/tmp/plasma', 'log_nvidia_smi': False}, 'common_eval': {'_name': None, 'path': None, 'post_process': None, 'quiet': False, 'model_overrides': '{}', 'results_path': None, 'is_moe': False, 'is_model_parallel': False}, 'distributed_training': {'_name': None, 'distributed_world_size': 1, 'distributed_rank': 0, 'distributed_backend': 'nccl', 'distributed_init_method': 'env://', 'distributed_port': -1, 'device_id': 0, 'distributed_no_spawn': True, 'ddp_backend': 'pytorch_ddp', 'bucket_cap_mb': 25, 'fix_batches_to_gpus': False, 'find_unused_parameters': False, 'fast_stat_sync': False, 'heartbeat_timeout': -1, 'broadcast_buffers': False, 'slowmo_momentum': None, 'slowmo_algorithm': 'LocalSGD', 'localsgd_frequency': 3, 'nprocs_per_node': 4, 'pipeline_model_parallel': False, 'pipeline_balance': None, 'pipeline_devices': None, 'pipeline_chunks': 0, 'pipeline_encoder_balance': None, 'pipeline_encoder_devices': None, 'pipeline_decoder_balance': None, 'pipeline_decoder_devices': None, 'pipeline_checkpoint': 'never', 'zero_sharding': 'none', 'zero_group_size': -1, 'save_zero_ckpt_fast': False, 'fp16': True, 'memory_efficient_fp16': True, 'tpu': False, 'no_reshard_after_forward': False, 'fp32_reduce_scatter': False, 'cpu_offload': False, 'use_sharded_state': False}, 'dataset': {'_name': None, 'num_workers': 1, 'num_workers_valid': 0, 'skip_invalid_size_inputs_valid_test': False, 'max_tokens': None, 'batch_size': 16, 'required_batch_size_multiple': 8, 'required_seq_len_multiple': 1, 'dataset_impl': None, 'data_buffer_size': 10, 'train_subset': 'train', 'valid_subset': 'valid', 'combine_valid_subsets': None, 'ignore_unused_valid_subsets': False, 'validate_interval': 1, 'validate_interval_updates': 4, 'validate_after_updates': 0, 'fixed_validation_seed': None, 'disable_validation': False, 'max_tokens_valid': None, 'batch_size_valid': 16, 'max_valid_steps': None, 'curriculum': 0, 'gen_subset': 'test', 'num_shards': 1, 'shard_id': 0}, 'optimization': {'_name': None, 'max_epoch': 0, 'max_update': 100, 'stop_time_hours': 0.0, 'clip_norm': 2.0, 'sentence_avg': False, 'update_freq': [1], 'lr': [0.005], 'stop_min_lr': -1.0, 'use_bmuf': False}, 'checkpoint': {'_name': None, 'save_dir': 'checkpoints', 'restore_file': 'checkpoint_last.pt', 'finetune_from_model': None, 'reset_dataloader': False, 'reset_lr_scheduler': False, 'reset_meters': False, 'reset_optimizer': False, 'optimizer_overrides': '{}', 'save_interval': 1, 'save_interval_updates': 1000, 'keep_interval_updates': -1, 'keep_last_epochs': -1, 'keep_best_checkpoints': -1, 'no_save': True, 'no_epoch_checkpoints': True, 'no_last_checkpoints': False, 'no_best_checkpoints': False, 'no_save_optimizer_state': False, 'no_save_optimizer_state_on_training_finished': False, 'symlink_best_and_last_checkpoints': False, 'best_checkpoint_metric': 'loss_valid', 'maximize_best_checkpoint_metric': False, 'patience': -1, 'checkpoint_suffix': '', 'checkpoint_shard_count': 1, 'load_checkpoint_on_all_dp_ranks': False, 'write_checkpoints_asynchronously': False, 's3_upload_path': None, 'model_parallel_size': 1}, 'bmuf': {'_name': None, 'block_lr': 1.0, 'block_momentum': 0.875, 'global_sync_iter': 50, 'warmup_iterations': 500, 'use_nbm': False, 'average_sync': False, 'distributed_world_size': 4}, 'generation': {'_name': None, 'beam': 5, 'nbest': 1, 'max_len_a': 0.0, 'max_len_b': 200, 'min_len': 1, 'match_source_len': False, 'unnormalized': False, 'no_early_stop': False, 'no_beamable_mm': False, 'lenpen': 1.0, 'unkpen': 0.0, 'replace_unk': None, 'sacrebleu': False, 'score_reference': False, 'prefix_size': 0, 'no_repeat_ngram_size': 0, 'sampling': False, 'sampling_topk': -1, 'sampling_topp': -1.0, 'constraints': None, 'temperature': 1.0, 'diverse_beam_groups': -1, 'diverse_beam_strength': 0.5, 'diversity_rate': -1.0, 'print_alignment': None, 'print_step': False, 'lm_path': None, 'lm_weight': 0.0, 'iter_decode_eos_penalty': 0.0, 'iter_decode_max_iter': 10, 'iter_decode_force_max_iter': False, 'iter_decode_with_beam': 1, 'iter_decode_with_external_reranker': False, 'retain_iter_history': False, 'retain_dropout': False, 'retain_dropout_modules': None, 'decoding_format': None, 'no_seed_provided': False}, 'eval_lm': {'_name': None, 'input_quant_method': '', 'input_bits': -1, 'weight_quant_method': '', 'weight_bits': -1, 'smoothquant': False, 'smoothquant_alpha': 0.5, 'smoothquant_bitnet': False, 'input_bits_post': 8, 'hadamard_group': -1, 'cal_input_stat': 'none', 'output_word_probs': False, 'output_word_stats': False, 'context_window': 0, 'softmax_batch': 9223372036854775807, 'stats_path': None, 'max_valid_steps': None}, 'interactive': {'_name': None, 'buffer_size': 0, 'input': '-'}, 'model': {'_name': 'llama_for_layer_wise_qat', 'load_ckpt': None, 'batch_size': 16, 'share_input_output_embed': False, 'sliding_window': None, 'rope_theta': 10000.0, 'checkpoint_activations': False, 'tokens_per_sample': 512, 'model_parallel_size': 1}, 'task': {'_name': 'kd', 'path_to_labels': None, 'data': '/data/gyy/lmquant-main/lmquant/data/data_without_preprocess_llama_7b_bs16', 'tokens_per_sample': 512, 'batch_size_in_quant': 8, 'max_target_positions': None, 'llama_model': None, 'quant_acts_when_training': True, 'tiktoken_model': 'cl100k_base', 'batch_read_ahead': 1, 'pad_to_max_len': True, 'absolute_path': False, 'tokenizer_pad_to_multiple': 1, 'seed': 1, 'batch_size': 16}, 'criterion': {'_name': 'mse', 'sentence_avg': False}, 'optimizer': {'_name': 'adam', 'adam_betas': '(0.9, 0.95)', 'adam_eps': 1e-06, 'weight_decay': 0.05, 'use_old_adam': False, 'fp16_adam_stats': False, 'tpu': False, 'bf16': False, 'lr': [0.005], 'block_wise': False}, 'lr_scheduler': {'_name': 'polynomial_decay', 'warmup_updates': 50, 'force_anneal': None, 'end_learning_rate': 0.0, 'power': 1.0, 'total_num_update': 300000.0, 'lr': [0.005]}, 'scoring': {'_name': 'bleu', 'pad': 1, 'eos': 2, 'unk': 3}, 'bpe': None, 'tokenizer': None}
25-01-23 02:10:54 | I | LlamaModelDecoderLayer(
  (decoder): LlamaDecoderLayerInFairseq(
    (model): LlamaDecoderLayer(
      (self_attn): LlamaSdpaAttention(
        (q_proj): Linear(in_features=4096, out_features=4096, bias=False)
        (k_proj): Linear(in_features=4096, out_features=4096, bias=False)
        (v_proj): Linear(in_features=4096, out_features=4096, bias=False)
        (o_proj): Linear(in_features=4096, out_features=4096, bias=False)
        (rotary_emb): LlamaRotaryEmbedding()
        (q_rotary_emb): RotaryEmbedding()
        (k_rotary_emb): RotaryEmbedding()
      )
      (mlp): LlamaMLP(
        (gate_proj): Linear(in_features=4096, out_features=11008, bias=False)
        (up_proj): Linear(in_features=4096, out_features=11008, bias=False)
        (down_proj): Linear(in_features=11008, out_features=4096, bias=False)
        (act_fn): SiLU()
      )
      (input_layernorm): LlamaRMSNorm((4096,), eps=1e-05)
      (post_attention_layernorm): LlamaRMSNorm((4096,), eps=1e-05)
    )
  )
  (model): LlamaDecoderLayerInFairseq(
    (model): LlamaDecoderLayer(
      (self_attn): LlamaSdpaAttention(
        (q_proj): Linear(in_features=4096, out_features=4096, bias=False)
        (k_proj): Linear(in_features=4096, out_features=4096, bias=False)
        (v_proj): Linear(in_features=4096, out_features=4096, bias=False)
        (o_proj): Linear(in_features=4096, out_features=4096, bias=False)
        (rotary_emb): LlamaRotaryEmbedding()
        (q_rotary_emb): RotaryEmbedding()
        (k_rotary_emb): RotaryEmbedding()
      )
      (mlp): LlamaMLP(
        (gate_proj): Linear(in_features=4096, out_features=11008, bias=False)
        (up_proj): Linear(in_features=4096, out_features=11008, bias=False)
        (down_proj): Linear(in_features=11008, out_features=4096, bias=False)
        (act_fn): SiLU()
      )
      (input_layernorm): LlamaRMSNorm((4096,), eps=1e-05)
      (post_attention_layernorm): LlamaRMSNorm((4096,), eps=1e-05)
    )
  )
)
25-01-23 02:10:54 | I | task: KDTask
25-01-23 02:10:54 | I | model: LlamaModelDecoderLayer
25-01-23 02:10:54 | I | criterion: MSECriterion
25-01-23 02:10:54 | I | num. non-expert model params: 202,383,360 (num. trained: 202,383,360)
25-01-23 02:10:54 | I | num. expert model params: 0 (num. trained: 0)
25-01-23 02:10:54 | I | nvidia-smi stats: {'gpu_0_mem_used_gb': 2.080078125, 'gpu_1_mem_used_gb': 6.150390625, 'gpu_2_mem_used_gb': 7.0859375, 'gpu_3_mem_used_gb': 7.0859375, 'gpu_4_mem_used_gb': 19.76953125, 'gpu_5_mem_used_gb': 7.173828125, 'gpu_6_mem_used_gb': 7.173828125, 'gpu_7_mem_used_gb': 6.142578125}
25-01-23 02:10:55 | I | detected shared parameter: decoder.model.self_attn.q_proj.weight <- model.model.self_attn.q_proj.weight
25-01-23 02:10:55 | I | detected shared parameter: decoder.model.self_attn.q_proj.bias <- decoder.model.self_attn.k_proj.bias
25-01-23 02:10:55 | I | detected shared parameter: decoder.model.self_attn.q_proj.bias <- decoder.model.self_attn.v_proj.bias
25-01-23 02:10:55 | I | detected shared parameter: decoder.model.self_attn.q_proj.bias <- decoder.model.self_attn.o_proj.bias
25-01-23 02:10:55 | I | detected shared parameter: decoder.model.self_attn.q_proj.bias <- decoder.model.mlp.gate_proj.bias
25-01-23 02:10:55 | I | detected shared parameter: decoder.model.self_attn.q_proj.bias <- decoder.model.mlp.up_proj.bias
25-01-23 02:10:55 | I | detected shared parameter: decoder.model.self_attn.q_proj.bias <- decoder.model.mlp.down_proj.bias
25-01-23 02:10:55 | I | detected shared parameter: decoder.model.self_attn.q_proj.bias <- model.model.self_attn.q_proj.bias
25-01-23 02:10:55 | I | detected shared parameter: decoder.model.self_attn.q_proj.bias <- model.model.self_attn.k_proj.bias
25-01-23 02:10:55 | I | detected shared parameter: decoder.model.self_attn.q_proj.bias <- model.model.self_attn.v_proj.bias
25-01-23 02:10:55 | I | detected shared parameter: decoder.model.self_attn.q_proj.bias <- model.model.self_attn.o_proj.bias
25-01-23 02:10:55 | I | detected shared parameter: decoder.model.self_attn.q_proj.bias <- model.model.mlp.gate_proj.bias
25-01-23 02:10:55 | I | detected shared parameter: decoder.model.self_attn.q_proj.bias <- model.model.mlp.up_proj.bias
25-01-23 02:10:55 | I | detected shared parameter: decoder.model.self_attn.q_proj.bias <- model.model.mlp.down_proj.bias
25-01-23 02:10:55 | I | detected shared parameter: decoder.model.self_attn.k_proj.weight <- model.model.self_attn.k_proj.weight
25-01-23 02:10:55 | I | detected shared parameter: decoder.model.self_attn.v_proj.weight <- model.model.self_attn.v_proj.weight
25-01-23 02:10:55 | I | detected shared parameter: decoder.model.self_attn.o_proj.weight <- model.model.self_attn.o_proj.weight
25-01-23 02:10:55 | I | detected shared parameter: decoder.model.mlp.gate_proj.weight <- model.model.mlp.gate_proj.weight
25-01-23 02:10:55 | I | detected shared parameter: decoder.model.mlp.up_proj.weight <- model.model.mlp.up_proj.weight
25-01-23 02:10:55 | I | detected shared parameter: decoder.model.mlp.down_proj.weight <- model.model.mlp.down_proj.weight
25-01-23 02:10:55 | I | detected shared parameter: decoder.model.input_layernorm.weight <- model.model.input_layernorm.weight
25-01-23 02:10:55 | I | detected shared parameter: decoder.model.post_attention_layernorm.weight <- model.model.post_attention_layernorm.weight
25-01-23 02:10:55 | I | nvidia-smi stats: {'gpu_0_mem_used_gb': 2.080078125, 'gpu_1_mem_used_gb': 6.19140625, 'gpu_2_mem_used_gb': 7.126953125, 'gpu_3_mem_used_gb': 7.126953125, 'gpu_4_mem_used_gb': 19.056640625, 'gpu_5_mem_used_gb': 7.173828125, 'gpu_6_mem_used_gb': 7.173828125, 'gpu_7_mem_used_gb': 6.142578125}
25-01-23 02:10:55 | I | ***********************CUDA enviroments for all 1 workers***********************
25-01-23 02:10:55 | I | rank   0: capabilities =  8.6  ; total memory = 23.691 GB ; name = NVIDIA GeForce RTX 3090                 
25-01-23 02:10:55 | I | ***********************CUDA enviroments for all 1 workers***********************
25-01-23 02:10:55 | I | training on 1 devices (GPUs/TPUs)
25-01-23 02:10:55 | I | max tokens per GPU = None and batch size per GPU = 16
25-01-23 02:10:55 | I | nvidia-smi stats: {'gpu_0_mem_used_gb': 2.080078125, 'gpu_1_mem_used_gb': 6.19140625, 'gpu_2_mem_used_gb': 7.126953125, 'gpu_3_mem_used_gb': 7.126953125, 'gpu_4_mem_used_gb': 19.056640625, 'gpu_5_mem_used_gb': 7.173828125, 'gpu_6_mem_used_gb': 7.173828125, 'gpu_7_mem_used_gb': 6.142578125}
25-01-23 02:10:55 | I | loading train data for epoch 1
25-01-23 02:10:56 | I | *** 
 now in /data/gyy/lmquant-main/lmquant/data/data_without_preprocess_llama_1.1b/shard/val/0.jsonl 
 ***
25-01-23 02:10:56 | I | *** 
 total_rows = 700 
 ***
25-01-23 02:10:56 | I | current row num = 0
25-01-23 02:10:56 | I | in forward_and_gen_teacher_outputs, Start iterating over samples
25-01-23 02:10:56 | I | loading train data for epoch 1
25-01-23 02:10:58 | I | *** 
 now in /data/gyy/lmquant-main/lmquant/data/data_without_preprocess_llama_1.1b/shard/val/0.jsonl 
 ***
25-01-23 02:10:58 | I | *** 
 total_rows = 700 
 ***
25-01-23 02:10:58 | I | current row num = 0
25-01-23 02:10:58 | I | in forward_and_gen_args_and_kwargs, Start iterating over samples
25-01-23 02:10:58 | I | No existing checkpoint found checkpoints/checkpoint_last.pt
25-01-23 02:10:58 | I | loading train data for epoch 1
25-01-23 02:10:58 | I | loading valid data for epoch 1
25-01-23 02:11:03 | I | begin training epoch 1
25-01-23 02:11:03 | I | Start iterating over samples
25-01-23 02:11:03 | I | TRAIN CURRENT LAYER_IDX = 4
25-01-23 02:11:03 | I | in layer model.layers.4
25-01-23 02:11:03 | I | quantizing weights for layer model.layers.4
25-01-23 02:11:04 | I | collecting calibration activations in model.layers.4
25-01-23 02:11:04 | I | collecting calibration activations in model.layers.4
25-01-23 02:11:05 | I | - Quantizing decoder layer model.layers.4
25-01-23 02:11:05 | I |   - Calibrating model.layers.4.self_attn.q_proj.weight
25-01-23 02:11:07 | I |       - range scale = [    1.0000]
25-01-23 02:11:07 | I |         sum  error  = [    2.3792]
25-01-23 02:11:07 | I |         best error  = [    2.3792]
25-01-23 02:11:07 | I |     + error = [2.3792]
25-01-23 02:11:07 | I |       - range scale = [    1.0000]
25-01-23 02:11:07 | I |         sum  error  = [   22.6175]
25-01-23 02:11:07 | I |         best error  = [   22.6175]
25-01-23 02:11:07 | I |     + error = [22.6175]
25-01-23 02:11:08 | I |   - Calibrating model.layers.4.self_attn.k_proj.weight
25-01-23 02:11:08 | I |       - range scale = [    1.0000]
25-01-23 02:11:08 | I |         sum  error  = [    0.8495]
25-01-23 02:11:08 | I |         best error  = [    0.8495]
25-01-23 02:11:08 | I |     + error = [0.8495]
25-01-23 02:11:09 | I |       - range scale = [    1.0000]
25-01-23 02:11:09 | I |         sum  error  = [    8.6524]
25-01-23 02:11:09 | I |         best error  = [    8.6524]
25-01-23 02:11:09 | I |     + error = [8.6524]
25-01-23 02:11:09 | I |   - Calibrating model.layers.4.self_attn.v_proj.weight
25-01-23 02:11:10 | I |       - range scale = [    1.0000]
25-01-23 02:11:10 | I |         sum  error  = [    3.6119]
25-01-23 02:11:10 | I |         best error  = [    3.6119]
25-01-23 02:11:10 | I |     + error = [3.6119]
25-01-23 02:11:11 | I |       - range scale = [    1.0000]
25-01-23 02:11:11 | I |         sum  error  = [   38.5598]
25-01-23 02:11:11 | I |         best error  = [   38.5598]
25-01-23 02:11:11 | I |     + error = [38.5598]
25-01-23 02:11:11 | I |   - Calibrating model.layers.4.self_attn.o_proj.weight
25-01-23 02:11:12 | I |       - range scale = [    1.0000]
25-01-23 02:11:12 | I |         sum  error  = [    0.4228]
25-01-23 02:11:12 | I |         best error  = [    0.4228]
25-01-23 02:11:12 | I |     + error = [0.4228]
25-01-23 02:11:12 | I |       - range scale = [    1.0000]
25-01-23 02:11:12 | I |         sum  error  = [    4.3035]
25-01-23 02:11:12 | I |         best error  = [    4.3035]
25-01-23 02:11:12 | I |     + error = [4.3035]
25-01-23 02:11:13 | I |   - Calibrating model.layers.4.mlp.up_proj.weight
25-01-23 02:11:13 | I |       - range scale = [    1.0000]
25-01-23 02:11:13 | I |         sum  error  = [    3.9203]
25-01-23 02:11:13 | I |         best error  = [    3.9203]
25-01-23 02:11:13 | I |     + error = [3.9203]
25-01-23 02:11:14 | I |       - range scale = [    1.0000]
25-01-23 02:11:14 | I |         sum  error  = [   41.4810]
25-01-23 02:11:14 | I |         best error  = [   41.4810]
25-01-23 02:11:14 | I |     + error = [41.4810]
25-01-23 02:11:15 | I |   - Calibrating model.layers.4.mlp.gate_proj.weight
25-01-23 02:11:15 | I |       - range scale = [    1.0000]
25-01-23 02:11:15 | I |         sum  error  = [    4.4128]
25-01-23 02:11:15 | I |         best error  = [    4.4128]
25-01-23 02:11:15 | I |     + error = [4.4128]
25-01-23 02:11:16 | I |       - range scale = [    1.0000]
25-01-23 02:11:16 | I |         sum  error  = [   46.1335]
25-01-23 02:11:16 | I |         best error  = [   46.1335]
25-01-23 02:11:16 | I |     + error = [46.1335]
25-01-23 02:11:17 | I |   - Calibrating model.layers.4.mlp.down_proj.weight
25-01-23 02:11:17 | I |       - range scale = [    1.0000]
25-01-23 02:11:17 | I |         sum  error  = [    0.9663]
25-01-23 02:11:17 | I |         best error  = [    0.9663]
25-01-23 02:11:17 | I |     + error = [0.9663]
25-01-23 02:11:18 | I |       - range scale = [    1.0000]
25-01-23 02:11:18 | I |         sum  error  = [    9.3359]
25-01-23 02:11:18 | I |         best error  = [    9.3359]
25-01-23 02:11:18 | I |     + error = [9.3359]
25-01-23 02:11:19 | I |   - Quantizing model.layers.4.self_attn.q_proj.weight
25-01-23 02:11:21 | I |   - Quantizing model.layers.4.self_attn.k_proj.weight
25-01-23 02:11:24 | I |   - Quantizing model.layers.4.self_attn.v_proj.weight
25-01-23 02:11:26 | I |   - Quantizing model.layers.4.self_attn.o_proj.weight
25-01-23 02:11:28 | I |   - Quantizing model.layers.4.mlp.up_proj.weight
25-01-23 02:11:31 | I |   - Quantizing model.layers.4.mlp.gate_proj.weight
25-01-23 02:11:33 | I |   - Quantizing model.layers.4.mlp.down_proj.weight
25-01-23 02:11:39 | I | quantizing activations for layer model.layers.4
25-01-23 02:11:39 | I | collecting calibration activations in model.layers.4
25-01-23 02:11:39 | I | collecting calibration activations in model.layers.4
25-01-23 02:11:41 | I | forward this layer
25-01-23 02:11:41 | I | input_file: /data/gyy/lmquant-main/lmquant/data/data_without_preprocess_llama_7b_bs16/shard/val_input_args_layer_4/00.pt
25-01-23 02:11:41 | I | output_file: /data/gyy/lmquant-main/lmquant/data/data_without_preprocess_llama_7b_bs16/shard/val_teacher_output_layer_4/00.pt
25-01-23 02:11:42 | I | In layer model.layers.4, gradient of model.layers.4.self_attn.q_proj has nan: False
25-01-23 02:11:42 | I | In layer model.layers.4, gradient of model.layers.4.self_attn.q_proj has inf: True
25-01-23 02:11:42 | I | inf: first position tensor([  15, 3431], device='cuda:0')
25-01-23 02:11:42 | I | In layer model.layers.4, gradient of model.layers.4.self_attn.k_proj has nan: False
25-01-23 02:11:42 | I | In layer model.layers.4, gradient of model.layers.4.self_attn.k_proj has inf: True
25-01-23 02:11:42 | I | inf: first position tensor([  19, 3431], device='cuda:0')
25-01-23 02:11:42 | I | In layer model.layers.4, gradient of model.layers.4.self_attn.v_proj has nan: False
25-01-23 02:11:42 | I | In layer model.layers.4, gradient of model.layers.4.self_attn.v_proj has inf: True
25-01-23 02:11:42 | I | inf: first position tensor([   4, 2533], device='cuda:0')
25-01-23 02:11:42 | I | In layer model.layers.4, gradient of model.layers.4.self_attn.o_proj has nan: False
25-01-23 02:11:42 | I | In layer model.layers.4, gradient of model.layers.4.self_attn.o_proj has inf: False
25-01-23 02:11:42 | I | In layer model.layers.4, gradient of model.layers.4.mlp.up_proj has nan: False
25-01-23 02:11:42 | I | In layer model.layers.4, gradient of model.layers.4.mlp.up_proj has inf: True
25-01-23 02:11:42 | I | inf: first position tensor([2618, 1415], device='cuda:0')
25-01-23 02:11:42 | I | In layer model.layers.4, gradient of model.layers.4.mlp.gate_proj has nan: False
25-01-23 02:11:42 | I | In layer model.layers.4, gradient of model.layers.4.mlp.gate_proj has inf: True
25-01-23 02:11:42 | I | inf: first position tensor([2618, 1415], device='cuda:0')
25-01-23 02:11:42 | I | In layer model.layers.4, gradient of model.layers.4.mlp.down_proj has nan: False
25-01-23 02:11:42 | I | In layer model.layers.4, gradient of model.layers.4.mlp.down_proj has inf: True
25-01-23 02:11:42 | I | inf: first position tensor([2533, 2618], device='cuda:0')
25-01-23 02:11:42 | I | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 64.0
25-01-23 02:11:42 | I | TRAIN CURRENT LAYER_IDX = 4
25-01-23 02:11:42 | I | in layer model.layers.4
25-01-23 02:11:42 | I | quantizing weights for layer model.layers.4
25-01-23 02:11:43 | I | collecting calibration activations in model.layers.4
25-01-23 02:11:43 | I | collecting calibration activations in model.layers.4
25-01-23 02:11:43 | I | - Quantizing decoder layer model.layers.4
25-01-23 02:11:43 | I |   - Calibrating model.layers.4.self_attn.q_proj.weight
25-01-23 02:11:44 | I |       - range scale = [    1.0000]
25-01-23 02:11:44 | I |         sum  error  = [    2.3737]
25-01-23 02:11:44 | I |         best error  = [    2.3737]
25-01-23 02:11:44 | I |     + error = [2.3737]
25-01-23 02:11:45 | I |       - range scale = [    1.0000]
25-01-23 02:11:45 | I |         sum  error  = [   22.1531]
25-01-23 02:11:45 | I |         best error  = [   22.1531]
25-01-23 02:11:45 | I |     + error = [22.1531]
25-01-23 02:11:45 | I |   - Calibrating model.layers.4.self_attn.k_proj.weight
25-01-23 02:11:46 | I |       - range scale = [    1.0000]
25-01-23 02:11:46 | I |         sum  error  = [    0.8550]
25-01-23 02:11:46 | I |         best error  = [    0.8550]
25-01-23 02:11:46 | I |     + error = [0.8550]
25-01-23 02:11:46 | I |       - range scale = [    1.0000]
25-01-23 02:11:46 | I |         sum  error  = [    8.5231]
25-01-23 02:11:46 | I |         best error  = [    8.5231]
25-01-23 02:11:46 | I |     + error = [8.5231]
25-01-23 02:11:47 | I |   - Calibrating model.layers.4.self_attn.v_proj.weight
25-01-23 02:11:47 | I |       - range scale = [    1.0000]
25-01-23 02:11:47 | I |         sum  error  = [    3.6273]
25-01-23 02:11:47 | I |         best error  = [    3.6273]
25-01-23 02:11:47 | I |     + error = [3.6273]
25-01-23 02:11:48 | I |       - range scale = [    1.0000]
25-01-23 02:11:48 | I |         sum  error  = [   38.6685]
25-01-23 02:11:48 | I |         best error  = [   38.6685]
25-01-23 02:11:48 | I |     + error = [38.6685]
25-01-23 02:11:48 | I |   - Calibrating model.layers.4.self_attn.o_proj.weight
25-01-23 02:11:49 | I |       - range scale = [    1.0000]
25-01-23 02:11:49 | I |         sum  error  = [    0.4129]
25-01-23 02:11:49 | I |         best error  = [    0.4129]
25-01-23 02:11:49 | I |     + error = [0.4129]
25-01-23 02:11:50 | I |       - range scale = [    1.0000]
25-01-23 02:11:50 | I |         sum  error  = [    4.2035]
25-01-23 02:11:50 | I |         best error  = [    4.2035]
25-01-23 02:11:50 | I |     + error = [4.2035]
25-01-23 02:11:50 | I |   - Calibrating model.layers.4.mlp.up_proj.weight
25-01-23 02:11:50 | I |       - range scale = [    1.0000]
25-01-23 02:11:50 | I |         sum  error  = [    3.9567]
25-01-23 02:11:50 | I |         best error  = [    3.9567]
25-01-23 02:11:50 | I |     + error = [3.9567]
25-01-23 02:11:51 | I |       - range scale = [    1.0000]
25-01-23 02:11:51 | I |         sum  error  = [   41.8730]
25-01-23 02:11:51 | I |         best error  = [   41.8730]
25-01-23 02:11:51 | I |     + error = [41.8730]
25-01-23 02:11:52 | I |   - Calibrating model.layers.4.mlp.gate_proj.weight
25-01-23 02:11:53 | I |       - range scale = [    1.0000]
25-01-23 02:11:53 | I |         sum  error  = [    4.4558]
25-01-23 02:11:53 | I |         best error  = [    4.4558]
25-01-23 02:11:53 | I |     + error = [4.4558]
25-01-23 02:11:54 | I |       - range scale = [    1.0000]
25-01-23 02:11:54 | I |         sum  error  = [   46.5740]
25-01-23 02:11:54 | I |         best error  = [   46.5740]
25-01-23 02:11:54 | I |     + error = [46.5740]
25-01-23 02:11:54 | I |   - Calibrating model.layers.4.mlp.down_proj.weight
25-01-23 02:11:55 | I |       - range scale = [    1.0000]
25-01-23 02:11:55 | I |         sum  error  = [    0.9637]
25-01-23 02:11:55 | I |         best error  = [    0.9637]
25-01-23 02:11:55 | I |     + error = [0.9637]
25-01-23 02:11:56 | I |       - range scale = [    1.0000]
25-01-23 02:11:56 | I |         sum  error  = [    9.3153]
25-01-23 02:11:56 | I |         best error  = [    9.3153]
25-01-23 02:11:56 | I |     + error = [9.3153]
25-01-23 02:11:56 | I |   - Quantizing model.layers.4.self_attn.q_proj.weight
25-01-23 02:11:58 | I |   - Quantizing model.layers.4.self_attn.k_proj.weight
25-01-23 02:12:00 | I |   - Quantizing model.layers.4.self_attn.v_proj.weight
25-01-23 02:12:03 | I |   - Quantizing model.layers.4.self_attn.o_proj.weight
25-01-23 02:12:05 | I |   - Quantizing model.layers.4.mlp.up_proj.weight
25-01-23 02:12:07 | I |   - Quantizing model.layers.4.mlp.gate_proj.weight
25-01-23 02:12:09 | I |   - Quantizing model.layers.4.mlp.down_proj.weight
25-01-23 02:12:15 | I | quantizing activations for layer model.layers.4
25-01-23 02:12:16 | I | collecting calibration activations in model.layers.4
25-01-23 02:12:16 | I | collecting calibration activations in model.layers.4
25-01-23 02:12:18 | I | forward this layer
25-01-23 02:12:18 | I | input_file: /data/gyy/lmquant-main/lmquant/data/data_without_preprocess_llama_7b_bs16/shard/val_input_args_layer_4/01.pt
25-01-23 02:12:18 | I | output_file: /data/gyy/lmquant-main/lmquant/data/data_without_preprocess_llama_7b_bs16/shard/val_teacher_output_layer_4/01.pt
25-01-23 02:12:18 | I | In layer model.layers.4, gradient of model.layers.4.self_attn.q_proj has nan: False
25-01-23 02:12:18 | I | In layer model.layers.4, gradient of model.layers.4.self_attn.q_proj has inf: True
25-01-23 02:12:18 | I | inf: first position tensor([115, 339], device='cuda:0')
25-01-23 02:12:18 | I | In layer model.layers.4, gradient of model.layers.4.self_attn.k_proj has nan: False
25-01-23 02:12:18 | I | In layer model.layers.4, gradient of model.layers.4.self_attn.k_proj has inf: True
25-01-23 02:12:18 | I | inf: first position tensor([  42, 3431], device='cuda:0')
25-01-23 02:12:18 | I | In layer model.layers.4, gradient of model.layers.4.self_attn.v_proj has nan: False
25-01-23 02:12:18 | I | In layer model.layers.4, gradient of model.layers.4.self_attn.v_proj has inf: True
25-01-23 02:12:18 | I | inf: first position tensor([ 256, 2533], device='cuda:0')
25-01-23 02:12:18 | I | In layer model.layers.4, gradient of model.layers.4.self_attn.o_proj has nan: False
25-01-23 02:12:18 | I | In layer model.layers.4, gradient of model.layers.4.self_attn.o_proj has inf: False
25-01-23 02:12:18 | I | In layer model.layers.4, gradient of model.layers.4.mlp.up_proj has nan: False
25-01-23 02:12:18 | I | In layer model.layers.4, gradient of model.layers.4.mlp.up_proj has inf: True
25-01-23 02:12:18 | I | inf: first position tensor([2618, 1415], device='cuda:0')
25-01-23 02:12:18 | I | In layer model.layers.4, gradient of model.layers.4.mlp.gate_proj has nan: False
25-01-23 02:12:18 | I | In layer model.layers.4, gradient of model.layers.4.mlp.gate_proj has inf: True
25-01-23 02:12:18 | I | inf: first position tensor([2618, 1415], device='cuda:0')
25-01-23 02:12:18 | I | In layer model.layers.4, gradient of model.layers.4.mlp.down_proj has nan: False
25-01-23 02:12:18 | I | In layer model.layers.4, gradient of model.layers.4.mlp.down_proj has inf: False
25-01-23 02:12:18 | I | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 32.0
25-01-23 02:12:18 | I | TRAIN CURRENT LAYER_IDX = 4
25-01-23 02:12:18 | I | in layer model.layers.4
25-01-23 02:12:18 | I | quantizing weights for layer model.layers.4
25-01-23 02:12:19 | I | collecting calibration activations in model.layers.4
25-01-23 02:12:19 | I | collecting calibration activations in model.layers.4
25-01-23 02:12:19 | I | - Quantizing decoder layer model.layers.4
25-01-23 02:12:19 | I |   - Calibrating model.layers.4.self_attn.q_proj.weight
25-01-23 02:12:20 | I |       - range scale = [    1.0000]
25-01-23 02:12:20 | I |         sum  error  = [    2.3454]
25-01-23 02:12:20 | I |         best error  = [    2.3454]
25-01-23 02:12:20 | I |     + error = [2.3454]
25-01-23 02:12:21 | I |       - range scale = [    1.0000]
25-01-23 02:12:21 | I |         sum  error  = [   22.4200]
25-01-23 02:12:21 | I |         best error  = [   22.4200]
25-01-23 02:12:21 | I |     + error = [22.4200]
25-01-23 02:12:21 | I |   - Calibrating model.layers.4.self_attn.k_proj.weight
25-01-23 02:12:22 | I |       - range scale = [    1.0000]
25-01-23 02:12:22 | I |         sum  error  = [    0.8361]
25-01-23 02:12:22 | I |         best error  = [    0.8361]
25-01-23 02:12:22 | I |     + error = [0.8361]
25-01-23 02:12:22 | I |       - range scale = [    1.0000]
25-01-23 02:12:22 | I |         sum  error  = [    8.6273]
25-01-23 02:12:22 | I |         best error  = [    8.6273]
25-01-23 02:12:22 | I |     + error = [8.6273]
25-01-23 02:12:23 | I |   - Calibrating model.layers.4.self_attn.v_proj.weight
25-01-23 02:12:23 | I |       - range scale = [    1.0000]
25-01-23 02:12:23 | I |         sum  error  = [    3.6393]
25-01-23 02:12:23 | I |         best error  = [    3.6393]
25-01-23 02:12:23 | I |     + error = [3.6393]
25-01-23 02:12:24 | I |       - range scale = [    1.0000]
25-01-23 02:12:24 | I |         sum  error  = [   38.8088]
25-01-23 02:12:24 | I |         best error  = [   38.8088]
25-01-23 02:12:24 | I |     + error = [38.8088]
25-01-23 02:12:24 | I |   - Calibrating model.layers.4.self_attn.o_proj.weight
25-01-23 02:12:25 | I |       - range scale = [    1.0000]
25-01-23 02:12:25 | I |         sum  error  = [    0.4220]
25-01-23 02:12:25 | I |         best error  = [    0.4220]
25-01-23 02:12:25 | I |     + error = [0.4220]
25-01-23 02:12:26 | I |       - range scale = [    1.0000]
25-01-23 02:12:26 | I |         sum  error  = [    4.2878]
25-01-23 02:12:26 | I |         best error  = [    4.2878]
25-01-23 02:12:26 | I |     + error = [4.2878]
25-01-23 02:12:26 | I |   - Calibrating model.layers.4.mlp.up_proj.weight
25-01-23 02:12:27 | I |       - range scale = [    1.0000]
25-01-23 02:12:27 | I |         sum  error  = [    3.9402]
25-01-23 02:12:27 | I |         best error  = [    3.9402]
25-01-23 02:12:27 | I |     + error = [3.9402]
25-01-23 02:12:28 | I |       - range scale = [    1.0000]
25-01-23 02:12:28 | I |         sum  error  = [   41.6812]
25-01-23 02:12:28 | I |         best error  = [   41.6812]
25-01-23 02:12:28 | I |     + error = [41.6812]
25-01-23 02:12:28 | I |   - Calibrating model.layers.4.mlp.gate_proj.weight
25-01-23 02:12:28 | I |       - range scale = [    1.0000]
25-01-23 02:12:28 | I |         sum  error  = [    4.4346]
25-01-23 02:12:28 | I |         best error  = [    4.4346]
25-01-23 02:12:28 | I |     + error = [4.4346]
25-01-23 02:12:29 | I |       - range scale = [    1.0000]
25-01-23 02:12:29 | I |         sum  error  = [   46.3596]
25-01-23 02:12:29 | I |         best error  = [   46.3596]
25-01-23 02:12:29 | I |     + error = [46.3596]
25-01-23 02:12:30 | I |   - Calibrating model.layers.4.mlp.down_proj.weight
25-01-23 02:12:30 | I |       - range scale = [    1.0000]
25-01-23 02:12:30 | I |         sum  error  = [    0.9371]
25-01-23 02:12:30 | I |         best error  = [    0.9371]
25-01-23 02:12:30 | I |     + error = [0.9371]
25-01-23 02:12:31 | I |       - range scale = [    1.0000]
25-01-23 02:12:31 | I |         sum  error  = [    9.0712]
25-01-23 02:12:31 | I |         best error  = [    9.0712]
25-01-23 02:12:31 | I |     + error = [9.0712]
25-01-23 02:12:32 | I |   - Quantizing model.layers.4.self_attn.q_proj.weight
25-01-23 02:12:34 | I |   - Quantizing model.layers.4.self_attn.k_proj.weight
25-01-23 02:12:36 | I |   - Quantizing model.layers.4.self_attn.v_proj.weight
25-01-23 02:12:38 | I |   - Quantizing model.layers.4.self_attn.o_proj.weight
25-01-23 02:12:41 | I |   - Quantizing model.layers.4.mlp.up_proj.weight
25-01-23 02:12:43 | I |   - Quantizing model.layers.4.mlp.gate_proj.weight
25-01-23 02:12:45 | I |   - Quantizing model.layers.4.mlp.down_proj.weight
25-01-23 02:12:51 | I | quantizing activations for layer model.layers.4
25-01-23 02:12:51 | I | collecting calibration activations in model.layers.4
25-01-23 02:12:52 | I | collecting calibration activations in model.layers.4
25-01-23 02:12:53 | I | forward this layer
25-01-23 02:12:53 | I | input_file: /data/gyy/lmquant-main/lmquant/data/data_without_preprocess_llama_7b_bs16/shard/val_input_args_layer_4/02.pt
25-01-23 02:12:53 | I | output_file: /data/gyy/lmquant-main/lmquant/data/data_without_preprocess_llama_7b_bs16/shard/val_teacher_output_layer_4/02.pt
25-01-23 02:12:54 | I | In layer model.layers.4, gradient of model.layers.4.self_attn.q_proj has nan: False
25-01-23 02:12:54 | I | In layer model.layers.4, gradient of model.layers.4.self_attn.q_proj has inf: True
25-01-23 02:12:54 | I | inf: first position tensor([ 115, 1076], device='cuda:0')
25-01-23 02:12:54 | I | In layer model.layers.4, gradient of model.layers.4.self_attn.k_proj has nan: False
25-01-23 02:12:54 | I | In layer model.layers.4, gradient of model.layers.4.self_attn.k_proj has inf: True
25-01-23 02:12:54 | I | inf: first position tensor([  51, 3431], device='cuda:0')
25-01-23 02:12:54 | I | In layer model.layers.4, gradient of model.layers.4.self_attn.v_proj has nan: False
25-01-23 02:12:54 | I | In layer model.layers.4, gradient of model.layers.4.self_attn.v_proj has inf: True
25-01-23 02:12:54 | I | inf: first position tensor([ 720, 2533], device='cuda:0')
25-01-23 02:12:54 | I | In layer model.layers.4, gradient of model.layers.4.self_attn.o_proj has nan: False
25-01-23 02:12:54 | I | In layer model.layers.4, gradient of model.layers.4.self_attn.o_proj has inf: False
25-01-23 02:12:54 | I | In layer model.layers.4, gradient of model.layers.4.mlp.up_proj has nan: False
25-01-23 02:12:54 | I | In layer model.layers.4, gradient of model.layers.4.mlp.up_proj has inf: True
25-01-23 02:12:54 | I | inf: first position tensor([2618, 1415], device='cuda:0')
25-01-23 02:12:54 | I | In layer model.layers.4, gradient of model.layers.4.mlp.gate_proj has nan: False
25-01-23 02:12:54 | I | In layer model.layers.4, gradient of model.layers.4.mlp.gate_proj has inf: True
25-01-23 02:12:54 | I | inf: first position tensor([2618, 1415], device='cuda:0')
25-01-23 02:12:54 | I | In layer model.layers.4, gradient of model.layers.4.mlp.down_proj has nan: False
25-01-23 02:12:54 | I | In layer model.layers.4, gradient of model.layers.4.mlp.down_proj has inf: False
25-01-23 02:12:54 | I | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
25-01-23 02:12:54 | I | TRAIN CURRENT LAYER_IDX = 4
25-01-23 02:12:54 | I | in layer model.layers.4
25-01-23 02:12:54 | I | quantizing weights for layer model.layers.4
25-01-23 02:12:55 | I | collecting calibration activations in model.layers.4
25-01-23 02:12:55 | I | collecting calibration activations in model.layers.4
25-01-23 02:12:55 | I | - Quantizing decoder layer model.layers.4
25-01-23 02:12:55 | I |   - Calibrating model.layers.4.self_attn.q_proj.weight
25-01-23 02:12:56 | I |       - range scale = [    1.0000]
25-01-23 02:12:56 | I |         sum  error  = [    2.3699]
25-01-23 02:12:56 | I |         best error  = [    2.3699]
25-01-23 02:12:56 | I |     + error = [2.3699]
25-01-23 02:12:56 | I |       - range scale = [    1.0000]
25-01-23 02:12:56 | I |         sum  error  = [   22.0781]
25-01-23 02:12:56 | I |         best error  = [   22.0781]
25-01-23 02:12:56 | I |     + error = [22.0781]
25-01-23 02:12:57 | I |   - Calibrating model.layers.4.self_attn.k_proj.weight
25-01-23 02:12:57 | I |       - range scale = [    1.0000]
25-01-23 02:12:57 | I |         sum  error  = [    0.8554]
25-01-23 02:12:57 | I |         best error  = [    0.8554]
25-01-23 02:12:57 | I |     + error = [0.8554]
25-01-23 02:12:58 | I |       - range scale = [    1.0000]
25-01-23 02:12:58 | I |         sum  error  = [    8.6196]
25-01-23 02:12:58 | I |         best error  = [    8.6196]
25-01-23 02:12:58 | I |     + error = [8.6196]
25-01-23 02:12:58 | I |   - Calibrating model.layers.4.self_attn.v_proj.weight
25-01-23 02:12:59 | I |       - range scale = [    1.0000]
25-01-23 02:12:59 | I |         sum  error  = [    3.6125]
25-01-23 02:12:59 | I |         best error  = [    3.6125]
25-01-23 02:12:59 | I |     + error = [3.6125]
25-01-23 02:13:00 | I |       - range scale = [    1.0000]
25-01-23 02:13:00 | I |         sum  error  = [   38.4905]
25-01-23 02:13:00 | I |         best error  = [   38.4905]
25-01-23 02:13:00 | I |     + error = [38.4905]
25-01-23 02:13:00 | I |   - Calibrating model.layers.4.self_attn.o_proj.weight
25-01-23 02:13:00 | I |       - range scale = [    1.0000]
25-01-23 02:13:00 | I |         sum  error  = [    0.4248]
25-01-23 02:13:00 | I |         best error  = [    0.4248]
25-01-23 02:13:00 | I |     + error = [0.4248]
25-01-23 02:13:01 | I |       - range scale = [    1.0000]
25-01-23 02:13:01 | I |         sum  error  = [    4.3240]
25-01-23 02:13:01 | I |         best error  = [    4.3240]
25-01-23 02:13:01 | I |     + error = [4.3240]
25-01-23 02:13:01 | I |   - Calibrating model.layers.4.mlp.up_proj.weight
25-01-23 02:13:02 | I |       - range scale = [    1.0000]
25-01-23 02:13:02 | I |         sum  error  = [    3.9610]
25-01-23 02:13:02 | I |         best error  = [    3.9610]
25-01-23 02:13:02 | I |     + error = [3.9610]
25-01-23 02:13:03 | I |       - range scale = [    1.0000]
25-01-23 02:13:03 | I |         sum  error  = [   41.9448]
25-01-23 02:13:03 | I |         best error  = [   41.9448]
25-01-23 02:13:03 | I |     + error = [41.9448]
25-01-23 02:13:03 | I |   - Calibrating model.layers.4.mlp.gate_proj.weight
25-01-23 02:13:04 | I |       - range scale = [    1.0000]
25-01-23 02:13:04 | I |         sum  error  = [    4.4683]
25-01-23 02:13:04 | I |         best error  = [    4.4683]
25-01-23 02:13:04 | I |     + error = [4.4683]
25-01-23 02:13:05 | W |   OOM: Ran out of memory with exception: CUDA out of memory. Tried to allocate 44.00 MiB. GPU 3 has a total capacity of 23.69 GiB of which 8.94 MiB is free. Process 39318 has 17.57 GiB memory in use. Process 17406 has 6.11 GiB memory in use. Of the allocated memory 5.77 GiB is allocated by PyTorch, and 1.51 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
25-01-23 02:13:05 | W |   |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 0                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |   6094 MiB |  15157 MiB |   1729 GiB |   1723 GiB |
|       from large pool |   6093 MiB |  15144 MiB |   1701 GiB |   1695 GiB |
|       from small pool |      0 MiB |     16 MiB |     28 GiB |     28 GiB |
|---------------------------------------------------------------------------|
| Active memory         |   6094 MiB |  15157 MiB |   1729 GiB |   1723 GiB |
|       from large pool |   6093 MiB |  15144 MiB |   1701 GiB |   1695 GiB |
|       from small pool |      0 MiB |     16 MiB |     28 GiB |     28 GiB |
|---------------------------------------------------------------------------|
| Requested memory      |   6094 MiB |  15157 MiB |   1715 GiB |   1709 GiB |
|       from large pool |   6093 MiB |  15144 MiB |   1687 GiB |   1681 GiB |
|       from small pool |      0 MiB |     16 MiB |     28 GiB |     28 GiB |
|---------------------------------------------------------------------------|
| GPU reserved memory   |  12066 MiB |  16654 MiB | 218954 MiB | 206888 MiB |
|       from large pool |  12064 MiB |  16636 MiB | 218708 MiB | 206644 MiB |
|       from small pool |      2 MiB |     18 MiB |    246 MiB |    244 MiB |
|---------------------------------------------------------------------------|
| Non-releasable memory | 478462 KiB |   1185 MiB | 403339 MiB | 402872 MiB |
|       from large pool | 477248 KiB |   1183 MiB | 370029 MiB | 369563 MiB |
|       from small pool |   1214 KiB |      4 MiB |  33309 MiB |  33308 MiB |
|---------------------------------------------------------------------------|
| Allocations           |     154    |     272    |  550715    |  550561    |
|       from large pool |     107    |     174    |   70250    |   70143    |
|       from small pool |      47    |      99    |  480465    |  480418    |
|---------------------------------------------------------------------------|
| Active allocs         |     154    |     272    |  550715    |  550561    |
|       from large pool |     107    |     174    |   70250    |   70143    |
|       from small pool |      47    |      99    |  480465    |  480418    |
|---------------------------------------------------------------------------|
| GPU reserved segments |     105    |     173    |     754    |     649    |
|       from large pool |     104    |     164    |     631    |     527    |
|       from small pool |       1    |       9    |     123    |     122    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       6    |      23    |  272372    |  272366    |
|       from large pool |       3    |      18    |   45756    |   45753    |
|       from small pool |       3    |      11    |  226616    |  226613    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|

25-01-23 02:13:05 | W |   |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 1                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |   6992 MiB |   7036 MiB |   8160 MiB |   1168 MiB |
|       from large pool |   6992 MiB |   7036 MiB |   8149 MiB |   1157 MiB |
|       from small pool |      0 MiB |      1 MiB |     11 MiB |     11 MiB |
|---------------------------------------------------------------------------|
| Active memory         |   6992 MiB |   7036 MiB |   8160 MiB |   1168 MiB |
|       from large pool |   6992 MiB |   7036 MiB |   8149 MiB |   1157 MiB |
|       from small pool |      0 MiB |      1 MiB |     11 MiB |     11 MiB |
|---------------------------------------------------------------------------|
| Requested memory      |   6991 MiB |   7034 MiB |   8139 MiB |   1148 MiB |
|       from large pool |   6991 MiB |   7034 MiB |   8128 MiB |   1137 MiB |
|       from small pool |      0 MiB |      1 MiB |     11 MiB |     11 MiB |
|---------------------------------------------------------------------------|
| GPU reserved memory   |   6994 MiB |   7038 MiB |   8292 MiB |   1298 MiB |
|       from large pool |   6992 MiB |   7036 MiB |   8288 MiB |   1296 MiB |
|       from small pool |      2 MiB |      2 MiB |      4 MiB |      2 MiB |
|---------------------------------------------------------------------------|
| Non-releasable memory |   1751 KiB |  20855 KiB | 179196 KiB | 177445 KiB |
|       from large pool |      0 KiB |  19104 KiB | 163840 KiB | 163840 KiB |
|       from small pool |   1751 KiB |   2047 KiB |  15356 KiB |  13605 KiB |
|---------------------------------------------------------------------------|
| Allocations           |     181    |     182    |     265    |      84    |
|       from large pool |     127    |     128    |     179    |      52    |
|       from small pool |      54    |      56    |      86    |      32    |
|---------------------------------------------------------------------------|
| Active allocs         |     181    |     182    |     265    |      84    |
|       from large pool |     127    |     128    |     179    |      52    |
|       from small pool |      54    |      56    |      86    |      32    |
|---------------------------------------------------------------------------|
| GPU reserved segments |     128    |     129    |     173    |      45    |
|       from large pool |     127    |     128    |     171    |      44    |
|       from small pool |       1    |       1    |       2    |       1    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       1    |       2    |      10    |       9    |
|       from large pool |       0    |       1    |       8    |       8    |
|       from small pool |       1    |       1    |       2    |       1    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|

25-01-23 02:13:05 | W |   |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 2                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |   6992 MiB |   7036 MiB |   8160 MiB |   1168 MiB |
|       from large pool |   6992 MiB |   7036 MiB |   8149 MiB |   1157 MiB |
|       from small pool |      0 MiB |      1 MiB |     11 MiB |     11 MiB |
|---------------------------------------------------------------------------|
| Active memory         |   6992 MiB |   7036 MiB |   8160 MiB |   1168 MiB |
|       from large pool |   6992 MiB |   7036 MiB |   8149 MiB |   1157 MiB |
|       from small pool |      0 MiB |      1 MiB |     11 MiB |     11 MiB |
|---------------------------------------------------------------------------|
| Requested memory      |   6991 MiB |   7034 MiB |   8139 MiB |   1148 MiB |
|       from large pool |   6991 MiB |   7034 MiB |   8128 MiB |   1137 MiB |
|       from small pool |      0 MiB |      1 MiB |     11 MiB |     11 MiB |
|---------------------------------------------------------------------------|
| GPU reserved memory   |   6994 MiB |   7038 MiB |   8292 MiB |   1298 MiB |
|       from large pool |   6992 MiB |   7036 MiB |   8288 MiB |   1296 MiB |
|       from small pool |      2 MiB |      2 MiB |      4 MiB |      2 MiB |
|---------------------------------------------------------------------------|
| Non-releasable memory |   1751 KiB |  20855 KiB | 179196 KiB | 177445 KiB |
|       from large pool |      0 KiB |  19104 KiB | 163840 KiB | 163840 KiB |
|       from small pool |   1751 KiB |   2047 KiB |  15356 KiB |  13605 KiB |
|---------------------------------------------------------------------------|
| Allocations           |     181    |     182    |     265    |      84    |
|       from large pool |     127    |     128    |     179    |      52    |
|       from small pool |      54    |      56    |      86    |      32    |
|---------------------------------------------------------------------------|
| Active allocs         |     181    |     182    |     265    |      84    |
|       from large pool |     127    |     128    |     179    |      52    |
|       from small pool |      54    |      56    |      86    |      32    |
|---------------------------------------------------------------------------|
| GPU reserved segments |     128    |     129    |     173    |      45    |
|       from large pool |     127    |     128    |     171    |      44    |
|       from small pool |       1    |       1    |       2    |       1    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       1    |       2    |      10    |       9    |
|       from large pool |       0    |       1    |       8    |       8    |
|       from small pool |       1    |       1    |       2    |       1    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|

25-01-23 02:13:05 | W |   |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 3                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 1            |        cudaMalloc retries: 1         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |   5904 MiB |   5992 MiB |   7072 MiB |   1168 MiB |
|       from large pool |   5904 MiB |   5992 MiB |   7061 MiB |   1157 MiB |
|       from small pool |      0 MiB |      1 MiB |     11 MiB |     11 MiB |
|---------------------------------------------------------------------------|
| Active memory         |   5904 MiB |   5992 MiB |   7072 MiB |   1168 MiB |
|       from large pool |   5904 MiB |   5992 MiB |   7061 MiB |   1157 MiB |
|       from small pool |      0 MiB |      1 MiB |     11 MiB |     11 MiB |
|---------------------------------------------------------------------------|
| Requested memory      |   5904 MiB |   5990 MiB |   7052 MiB |   1148 MiB |
|       from large pool |   5904 MiB |   5990 MiB |   7041 MiB |   1137 MiB |
|       from small pool |      0 MiB |      1 MiB |     11 MiB |     11 MiB |
|---------------------------------------------------------------------------|
| GPU reserved memory   |   5906 MiB |   5994 MiB |   7204 MiB |   1298 MiB |
|       from large pool |   5904 MiB |   5992 MiB |   7200 MiB |   1296 MiB |
|       from small pool |      2 MiB |      2 MiB |      4 MiB |      2 MiB |
|---------------------------------------------------------------------------|
| Non-releasable memory |   1544 KiB |  20648 KiB | 179195 KiB | 177651 KiB |
|       from large pool |      0 KiB |  19104 KiB | 163840 KiB | 163840 KiB |
|       from small pool |   1544 KiB |   2047 KiB |  15355 KiB |  13811 KiB |
|---------------------------------------------------------------------------|
| Allocations           |     148    |     150    |     231    |      83    |
|       from large pool |     100    |     102    |     152    |      52    |
|       from small pool |      48    |      50    |      79    |      31    |
|---------------------------------------------------------------------------|
| Active allocs         |     148    |     150    |     231    |      83    |
|       from large pool |     100    |     102    |     152    |      52    |
|       from small pool |      48    |      50    |      79    |      31    |
|---------------------------------------------------------------------------|
| GPU reserved segments |     101    |     103    |     146    |      45    |
|       from large pool |     100    |     102    |     144    |      44    |
|       from small pool |       1    |       1    |       2    |       1    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       1    |       2    |      10    |       9    |
|       from large pool |       0    |       1    |       8    |       8    |
|       from small pool |       1    |       1    |       2    |       1    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|

25-01-23 02:13:05 | W |   attempting to recover from OOM in forward/backward pass
25-01-23 02:13:05 | I |   TRAIN CURRENT LAYER_IDX = 4
25-01-23 02:13:05 | I |   in layer model.layers.4
25-01-23 02:13:05 | I |   quantizing weights for layer model.layers.4
25-01-23 02:13:05 | I |   collecting calibration activations in model.layers.4
25-01-23 02:13:05 | I |   collecting calibration activations in model.layers.4
25-01-23 02:13:06 | I |   - Quantizing decoder layer model.layers.4
25-01-23 02:13:06 | I |     - Calibrating model.layers.4.self_attn.q_proj.weight
25-01-23 02:13:06 | I |         - range scale = [    1.0000]
25-01-23 02:13:06 | I |           sum  error  = [    2.4124]
25-01-23 02:13:06 | I |           best error  = [    2.4124]
25-01-23 02:13:06 | I |       + error = [2.4124]
25-01-23 02:13:07 | I |         - range scale = [    1.0000]
25-01-23 02:13:07 | I |           sum  error  = [   22.5522]
25-01-23 02:13:07 | I |           best error  = [   22.5522]
25-01-23 02:13:07 | I |       + error = [22.5522]
25-01-23 02:13:07 | I |     - Calibrating model.layers.4.self_attn.k_proj.weight
25-01-23 02:13:08 | I |         - range scale = [    1.0000]
25-01-23 02:13:08 | I |           sum  error  = [    0.8595]
25-01-23 02:13:08 | I |           best error  = [    0.8595]
25-01-23 02:13:08 | I |       + error = [0.8595]
25-01-23 02:13:09 | I |         - range scale = [    1.0000]
25-01-23 02:13:09 | I |           sum  error  = [    8.7538]
25-01-23 02:13:09 | I |           best error  = [    8.7538]
25-01-23 02:13:09 | I |       + error = [8.7538]
25-01-23 02:13:09 | I |     - Calibrating model.layers.4.self_attn.v_proj.weight
25-01-23 02:13:09 | I |         - range scale = [    1.0000]
25-01-23 02:13:09 | I |           sum  error  = [    3.6436]
25-01-23 02:13:09 | I |           best error  = [    3.6436]
25-01-23 02:13:09 | I |       + error = [3.6436]
25-01-23 02:13:10 | I |         - range scale = [    1.0000]
25-01-23 02:13:10 | I |           sum  error  = [   38.7940]
25-01-23 02:13:10 | I |           best error  = [   38.7940]
25-01-23 02:13:10 | I |       + error = [38.7940]
25-01-23 02:13:10 | I |     - Calibrating model.layers.4.self_attn.o_proj.weight
25-01-23 02:13:11 | I |         - range scale = [    1.0000]
25-01-23 02:13:11 | I |           sum  error  = [    0.4235]
25-01-23 02:13:11 | I |           best error  = [    0.4235]
25-01-23 02:13:11 | I |       + error = [0.4235]
25-01-23 02:13:12 | I |         - range scale = [    1.0000]
25-01-23 02:13:12 | I |           sum  error  = [    4.2801]
25-01-23 02:13:12 | I |           best error  = [    4.2801]
25-01-23 02:13:12 | I |       + error = [4.2801]
25-01-23 02:13:12 | I |     - Calibrating model.layers.4.mlp.up_proj.weight
25-01-23 02:13:13 | I |         - range scale = [    1.0000]
25-01-23 02:13:13 | I |           sum  error  = [    3.9250]
25-01-23 02:13:13 | I |           best error  = [    3.9250]
25-01-23 02:13:13 | I |       + error = [3.9250]
25-01-23 02:13:14 | I |         - range scale = [    1.0000]
25-01-23 02:13:14 | I |           sum  error  = [   41.5313]
25-01-23 02:13:14 | I |           best error  = [   41.5313]
25-01-23 02:13:14 | I |       + error = [41.5313]
25-01-23 02:13:14 | I |     - Calibrating model.layers.4.mlp.gate_proj.weight
25-01-23 02:13:15 | I |         - range scale = [    1.0000]
25-01-23 02:13:15 | I |           sum  error  = [    4.4192]
25-01-23 02:13:15 | I |           best error  = [    4.4192]
25-01-23 02:13:15 | I |       + error = [4.4192]
25-01-23 02:13:16 | I |         - range scale = [    1.0000]
25-01-23 02:13:16 | I |           sum  error  = [   46.1946]
25-01-23 02:13:16 | I |           best error  = [   46.1946]
25-01-23 02:13:16 | I |       + error = [46.1946]
25-01-23 02:13:16 | I |     - Calibrating model.layers.4.mlp.down_proj.weight
25-01-23 02:13:17 | I |         - range scale = [    1.0000]
25-01-23 02:13:17 | I |           sum  error  = [    0.9283]
25-01-23 02:13:17 | I |           best error  = [    0.9283]
25-01-23 02:13:17 | I |       + error = [0.9283]
25-01-23 02:13:18 | I |         - range scale = [    1.0000]
25-01-23 02:13:18 | I |           sum  error  = [    8.9878]
25-01-23 02:13:18 | I |           best error  = [    8.9878]
25-01-23 02:13:18 | I |       + error = [8.9878]
25-01-23 02:13:18 | I |     - Quantizing model.layers.4.self_attn.q_proj.weight
25-01-23 02:13:20 | I |     - Quantizing model.layers.4.self_attn.k_proj.weight
25-01-23 02:13:23 | I |     - Quantizing model.layers.4.self_attn.v_proj.weight
25-01-23 02:13:25 | I |     - Quantizing model.layers.4.self_attn.o_proj.weight
25-01-23 02:13:27 | I |     - Quantizing model.layers.4.mlp.up_proj.weight
25-01-23 02:13:29 | I |     - Quantizing model.layers.4.mlp.gate_proj.weight
25-01-23 02:13:32 | I |     - Quantizing model.layers.4.mlp.down_proj.weight
25-01-23 02:13:37 | I |   quantizing activations for layer model.layers.4
25-01-23 02:13:38 | I |   collecting calibration activations in model.layers.4
25-01-23 02:13:38 | I |   collecting calibration activations in model.layers.4
25-01-23 02:13:40 | I |   forward this layer
25-01-23 02:13:40 | I |   input_file: /data/gyy/lmquant-main/lmquant/data/data_without_preprocess_llama_7b_bs16/shard/val_input_args_layer_4/04.pt
25-01-23 02:13:40 | I |   output_file: /data/gyy/lmquant-main/lmquant/data/data_without_preprocess_llama_7b_bs16/shard/val_teacher_output_layer_4/04.pt
25-01-23 02:13:40 | I |   In layer model.layers.4, gradient of model.layers.4.self_attn.q_proj has nan: False
25-01-23 02:13:40 | I |   In layer model.layers.4, gradient of model.layers.4.self_attn.q_proj has inf: True
25-01-23 02:13:40 | I |   inf: first position tensor([3443, 3431], device='cuda:0')
25-01-23 02:13:40 | I |   In layer model.layers.4, gradient of model.layers.4.self_attn.k_proj has nan: False
25-01-23 02:13:40 | I |   In layer model.layers.4, gradient of model.layers.4.self_attn.k_proj has inf: False
25-01-23 02:13:40 | I |   In layer model.layers.4, gradient of model.layers.4.self_attn.v_proj has nan: False
25-01-23 02:13:40 | I |   In layer model.layers.4, gradient of model.layers.4.self_attn.v_proj has inf: False
25-01-23 02:13:40 | I |   In layer model.layers.4, gradient of model.layers.4.self_attn.o_proj has nan: False
25-01-23 02:13:40 | I |   In layer model.layers.4, gradient of model.layers.4.self_attn.o_proj has inf: False
25-01-23 02:13:40 | I |   In layer model.layers.4, gradient of model.layers.4.mlp.up_proj has nan: False
25-01-23 02:13:40 | I |   In layer model.layers.4, gradient of model.layers.4.mlp.up_proj has inf: True
25-01-23 02:13:40 | I |   inf: first position tensor([2618, 1415], device='cuda:0')
25-01-23 02:13:40 | I |   In layer model.layers.4, gradient of model.layers.4.mlp.gate_proj has nan: False
25-01-23 02:13:40 | I |   In layer model.layers.4, gradient of model.layers.4.mlp.gate_proj has inf: False
25-01-23 02:13:40 | I |   In layer model.layers.4, gradient of model.layers.4.mlp.down_proj has nan: False
25-01-23 02:13:40 | I |   In layer model.layers.4, gradient of model.layers.4.mlp.down_proj has inf: False
25-01-23 02:13:40 | I |   NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
25-01-23 02:13:40 | I |   TRAIN CURRENT LAYER_IDX = 4
25-01-23 02:13:40 | I |   in layer model.layers.4
25-01-23 02:13:40 | I |   quantizing weights for layer model.layers.4
25-01-23 02:13:41 | I |   collecting calibration activations in model.layers.4
25-01-23 02:13:41 | I |   collecting calibration activations in model.layers.4
25-01-23 02:13:41 | I |   - Quantizing decoder layer model.layers.4
25-01-23 02:13:41 | I |     - Calibrating model.layers.4.self_attn.q_proj.weight
25-01-23 02:13:42 | I |         - range scale = [    1.0000]
25-01-23 02:13:42 | I |           sum  error  = [    2.3826]
25-01-23 02:13:42 | I |           best error  = [    2.3826]
25-01-23 02:13:42 | I |       + error = [2.3826]
25-01-23 02:13:43 | I |         - range scale = [    1.0000]
25-01-23 02:13:43 | I |           sum  error  = [   22.2398]
25-01-23 02:13:43 | I |           best error  = [   22.2398]
25-01-23 02:13:43 | I |       + error = [22.2398]
25-01-23 02:13:43 | I |     - Calibrating model.layers.4.self_attn.k_proj.weight
25-01-23 02:13:44 | I |         - range scale = [    1.0000]
25-01-23 02:13:44 | I |           sum  error  = [    0.8455]
25-01-23 02:13:44 | I |           best error  = [    0.8455]
25-01-23 02:13:44 | I |       + error = [0.8455]
25-01-23 02:13:44 | I |         - range scale = [    1.0000]
25-01-23 02:13:44 | I |           sum  error  = [    8.6571]
25-01-23 02:13:44 | I |           best error  = [    8.6571]
25-01-23 02:13:44 | I |       + error = [8.6571]
25-01-23 02:13:44 | I |     - Calibrating model.layers.4.self_attn.v_proj.weight
25-01-23 02:13:45 | I |         - range scale = [    1.0000]
25-01-23 02:13:45 | I |           sum  error  = [    3.6311]
25-01-23 02:13:45 | I |           best error  = [    3.6311]
25-01-23 02:13:45 | I |       + error = [3.6311]
25-01-23 02:13:46 | I |         - range scale = [    1.0000]
25-01-23 02:13:46 | I |           sum  error  = [   38.6692]
25-01-23 02:13:46 | I |           best error  = [   38.6692]
25-01-23 02:13:46 | I |       + error = [38.6692]
25-01-23 02:13:46 | I |     - Calibrating model.layers.4.self_attn.o_proj.weight
25-01-23 02:13:47 | I |         - range scale = [    1.0000]
25-01-23 02:13:47 | I |           sum  error  = [    0.4319]
25-01-23 02:13:47 | I |           best error  = [    0.4319]
25-01-23 02:13:47 | I |       + error = [0.4319]
25-01-23 02:13:48 | I |         - range scale = [    1.0000]
25-01-23 02:13:48 | I |           sum  error  = [    4.4042]
25-01-23 02:13:48 | I |           best error  = [    4.4042]
25-01-23 02:13:48 | I |       + error = [4.4042]
25-01-23 02:13:48 | I |     - Calibrating model.layers.4.mlp.up_proj.weight
25-01-23 02:13:48 | I |         - range scale = [    1.0000]
25-01-23 02:13:48 | I |           sum  error  = [    3.9556]
25-01-23 02:13:48 | I |           best error  = [    3.9556]
25-01-23 02:13:48 | I |       + error = [3.9556]
25-01-23 02:13:49 | I |         - range scale = [    1.0000]
25-01-23 02:13:49 | I |           sum  error  = [   41.8725]
25-01-23 02:13:49 | I |           best error  = [   41.8725]
25-01-23 02:13:49 | I |       + error = [41.8725]
25-01-23 02:13:50 | I |     - Calibrating model.layers.4.mlp.gate_proj.weight
25-01-23 02:13:50 | I |         - range scale = [    1.0000]
25-01-23 02:13:50 | I |           sum  error  = [    4.4542]
25-01-23 02:13:50 | I |           best error  = [    4.4542]
25-01-23 02:13:50 | I |       + error = [4.4542]
25-01-23 02:13:51 | I |         - range scale = [    1.0000]
25-01-23 02:13:51 | I |           sum  error  = [   46.5705]
25-01-23 02:13:51 | I |           best error  = [   46.5705]
25-01-23 02:13:51 | I |       + error = [46.5705]
25-01-23 02:13:52 | I |     - Calibrating model.layers.4.mlp.down_proj.weight
25-01-23 02:13:52 | I |         - range scale = [    1.0000]
25-01-23 02:13:52 | I |           sum  error  = [    0.9541]
25-01-23 02:13:52 | I |           best error  = [    0.9541]
25-01-23 02:13:52 | I |       + error = [0.9541]
25-01-23 02:13:53 | I |         - range scale = [    1.0000]
25-01-23 02:13:53 | I |           sum  error  = [    9.2105]
25-01-23 02:13:53 | I |           best error  = [    9.2105]
25-01-23 02:13:53 | I |       + error = [9.2105]
25-01-23 02:13:54 | I |     - Quantizing model.layers.4.self_attn.q_proj.weight
25-01-23 02:13:56 | I |     - Quantizing model.layers.4.self_attn.k_proj.weight
25-01-23 02:13:58 | I |     - Quantizing model.layers.4.self_attn.v_proj.weight
25-01-23 02:14:00 | I |     - Quantizing model.layers.4.self_attn.o_proj.weight
25-01-23 02:14:02 | I |     - Quantizing model.layers.4.mlp.up_proj.weight
25-01-23 02:14:05 | I |     - Quantizing model.layers.4.mlp.gate_proj.weight
25-01-23 02:14:07 | I |     - Quantizing model.layers.4.mlp.down_proj.weight
25-01-23 02:14:13 | I |   quantizing activations for layer model.layers.4
25-01-23 02:14:13 | I |   collecting calibration activations in model.layers.4
25-01-23 02:14:13 | I |   collecting calibration activations in model.layers.4
25-01-23 02:14:15 | I |   forward this layer
25-01-23 02:14:15 | I |   input_file: /data/gyy/lmquant-main/lmquant/data/data_without_preprocess_llama_7b_bs16/shard/val_input_args_layer_4/05.pt
25-01-23 02:14:15 | I |   output_file: /data/gyy/lmquant-main/lmquant/data/data_without_preprocess_llama_7b_bs16/shard/val_teacher_output_layer_4/05.pt
25-01-23 02:14:16 | I |   In layer model.layers.4, gradient of model.layers.4.self_attn.q_proj has nan: False
25-01-23 02:14:16 | I |   In layer model.layers.4, gradient of model.layers.4.self_attn.q_proj has inf: False
25-01-23 02:14:16 | I |   In layer model.layers.4, gradient of model.layers.4.self_attn.k_proj has nan: False
25-01-23 02:14:16 | I |   In layer model.layers.4, gradient of model.layers.4.self_attn.k_proj has inf: False
25-01-23 02:14:16 | I |   In layer model.layers.4, gradient of model.layers.4.self_attn.v_proj has nan: False
25-01-23 02:14:16 | I |   In layer model.layers.4, gradient of model.layers.4.self_attn.v_proj has inf: False
25-01-23 02:14:16 | I |   In layer model.layers.4, gradient of model.layers.4.self_attn.o_proj has nan: False
25-01-23 02:14:16 | I |   In layer model.layers.4, gradient of model.layers.4.self_attn.o_proj has inf: False
25-01-23 02:14:16 | I |   In layer model.layers.4, gradient of model.layers.4.mlp.up_proj has nan: False
25-01-23 02:14:16 | I |   In layer model.layers.4, gradient of model.layers.4.mlp.up_proj has inf: False
25-01-23 02:14:16 | I |   In layer model.layers.4, gradient of model.layers.4.mlp.gate_proj has nan: False
25-01-23 02:14:16 | I |   In layer model.layers.4, gradient of model.layers.4.mlp.gate_proj has inf: False
25-01-23 02:14:16 | I |   In layer model.layers.4, gradient of model.layers.4.mlp.down_proj has nan: False
25-01-23 02:14:16 | I |   In layer model.layers.4, gradient of model.layers.4.mlp.down_proj has inf: False
25-01-23 02:14:16 | I |   NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 4.0
25-01-23 02:14:16 | I |   TRAIN CURRENT LAYER_IDX = 4
25-01-23 02:14:16 | I |   in layer model.layers.4
25-01-23 02:14:16 | I |   quantizing weights for layer model.layers.4
25-01-23 02:14:16 | I |   collecting calibration activations in model.layers.4
25-01-23 02:14:16 | I |   collecting calibration activations in model.layers.4
25-01-23 02:14:17 | I |   - Quantizing decoder layer model.layers.4
25-01-23 02:14:17 | I |     - Calibrating model.layers.4.self_attn.q_proj.weight
25-01-23 02:14:17 | I |         - range scale = [    1.0000]
25-01-23 02:14:17 | I |           sum  error  = [    2.4146]
25-01-23 02:14:17 | I |           best error  = [    2.4146]
25-01-23 02:14:17 | I |       + error = [2.4146]
25-01-23 02:14:18 | I |         - range scale = [    1.0000]
25-01-23 02:14:18 | I |           sum  error  = [   22.2497]
25-01-23 02:14:18 | I |           best error  = [   22.2497]
25-01-23 02:14:18 | I |       + error = [22.2497]
25-01-23 02:14:18 | I |     - Calibrating model.layers.4.self_attn.k_proj.weight
25-01-23 02:14:19 | I |         - range scale = [    1.0000]
25-01-23 02:14:19 | I |           sum  error  = [    0.8462]
25-01-23 02:14:19 | I |           best error  = [    0.8462]
25-01-23 02:14:19 | I |       + error = [0.8462]
25-01-23 02:14:20 | I |         - range scale = [    1.0000]
25-01-23 02:14:20 | I |           sum  error  = [    8.8027]
25-01-23 02:14:20 | I |           best error  = [    8.8027]
25-01-23 02:14:20 | I |       + error = [8.8027]
25-01-23 02:14:20 | I |     - Calibrating model.layers.4.self_attn.v_proj.weight
25-01-23 02:14:20 | I |         - range scale = [    1.0000]
25-01-23 02:14:20 | I |           sum  error  = [    3.6173]
25-01-23 02:14:20 | I |           best error  = [    3.6173]
25-01-23 02:14:20 | I |       + error = [3.6173]
25-01-23 02:14:21 | I |         - range scale = [    1.0000]
25-01-23 02:14:21 | I |           sum  error  = [   38.5457]
25-01-23 02:14:21 | I |           best error  = [   38.5457]
25-01-23 02:14:21 | I |       + error = [38.5457]
25-01-23 02:14:21 | I |     - Calibrating model.layers.4.self_attn.o_proj.weight
25-01-23 02:14:22 | I |         - range scale = [    1.0000]
25-01-23 02:14:22 | I |           sum  error  = [    0.4168]
25-01-23 02:14:22 | I |           best error  = [    0.4168]
25-01-23 02:14:22 | I |       + error = [0.4168]
25-01-23 02:14:23 | I |         - range scale = [    1.0000]
25-01-23 02:14:23 | I |           sum  error  = [    4.2095]
25-01-23 02:14:23 | I |           best error  = [    4.2095]
25-01-23 02:14:23 | I |       + error = [4.2095]
25-01-23 02:14:23 | I |     - Calibrating model.layers.4.mlp.up_proj.weight
25-01-23 02:14:24 | I |         - range scale = [    1.0000]
25-01-23 02:14:24 | I |           sum  error  = [    3.9106]
25-01-23 02:14:24 | I |           best error  = [    3.9106]
25-01-23 02:14:24 | I |       + error = [3.9106]
25-01-23 02:14:25 | I |         - range scale = [    1.0000]
25-01-23 02:14:25 | I |           sum  error  = [   41.3976]
25-01-23 02:14:25 | I |           best error  = [   41.3976]
25-01-23 02:14:25 | I |       + error = [41.3976]
25-01-23 02:14:25 | I |     - Calibrating model.layers.4.mlp.gate_proj.weight
25-01-23 02:14:26 | I |         - range scale = [    1.0000]
25-01-23 02:14:26 | I |           sum  error  = [    4.4036]
25-01-23 02:14:26 | I |           best error  = [    4.4036]
25-01-23 02:14:26 | I |       + error = [4.4036]
25-01-23 02:14:27 | I |         - range scale = [    1.0000]
25-01-23 02:14:27 | I |           sum  error  = [   46.0388]
25-01-23 02:14:27 | I |           best error  = [   46.0388]
25-01-23 02:14:27 | I |       + error = [46.0388]
25-01-23 02:14:27 | I |     - Calibrating model.layers.4.mlp.down_proj.weight
25-01-23 02:14:28 | I |         - range scale = [    1.0000]
25-01-23 02:14:28 | I |           sum  error  = [    0.9485]
25-01-23 02:14:28 | I |           best error  = [    0.9485]
25-01-23 02:14:28 | I |       + error = [0.9485]
25-01-23 02:14:29 | I |         - range scale = [    1.0000]
25-01-23 02:14:29 | I |           sum  error  = [    9.1783]
25-01-23 02:14:29 | I |           best error  = [    9.1783]
25-01-23 02:14:29 | I |       + error = [9.1783]
25-01-23 02:14:29 | I |     - Quantizing model.layers.4.self_attn.q_proj.weight
25-01-23 02:14:31 | I |     - Quantizing model.layers.4.self_attn.k_proj.weight
25-01-23 02:14:34 | I |     - Quantizing model.layers.4.self_attn.v_proj.weight
25-01-23 02:14:36 | I |     - Quantizing model.layers.4.self_attn.o_proj.weight
25-01-23 02:14:38 | I |     - Quantizing model.layers.4.mlp.up_proj.weight
25-01-23 02:14:40 | I |     - Quantizing model.layers.4.mlp.gate_proj.weight
25-01-23 02:14:43 | I |     - Quantizing model.layers.4.mlp.down_proj.weight
25-01-23 02:14:48 | I |   quantizing activations for layer model.layers.4
25-01-23 02:14:49 | I |   collecting calibration activations in model.layers.4
25-01-23 02:14:49 | I |   collecting calibration activations in model.layers.4
25-01-23 02:14:51 | I |   forward this layer
25-01-23 02:14:51 | I |   input_file: /data/gyy/lmquant-main/lmquant/data/data_without_preprocess_llama_7b_bs16/shard/val_input_args_layer_4/06.pt
25-01-23 02:14:51 | I |   output_file: /data/gyy/lmquant-main/lmquant/data/data_without_preprocess_llama_7b_bs16/shard/val_teacher_output_layer_4/06.pt
25-01-23 02:14:51 | I |   In layer model.layers.4, gradient of model.layers.4.self_attn.q_proj has nan: False
25-01-23 02:14:51 | I |   In layer model.layers.4, gradient of model.layers.4.self_attn.q_proj has inf: False
25-01-23 02:14:51 | I |   In layer model.layers.4, gradient of model.layers.4.self_attn.k_proj has nan: False
25-01-23 02:14:51 | I |   In layer model.layers.4, gradient of model.layers.4.self_attn.k_proj has inf: False
25-01-23 02:14:51 | I |   In layer model.layers.4, gradient of model.layers.4.self_attn.v_proj has nan: False
25-01-23 02:14:51 | I |   In layer model.layers.4, gradient of model.layers.4.self_attn.v_proj has inf: False
25-01-23 02:14:51 | I |   In layer model.layers.4, gradient of model.layers.4.self_attn.o_proj has nan: False
25-01-23 02:14:51 | I |   In layer model.layers.4, gradient of model.layers.4.self_attn.o_proj has inf: False
25-01-23 02:14:51 | I |   In layer model.layers.4, gradient of model.layers.4.mlp.up_proj has nan: False
25-01-23 02:14:51 | I |   In layer model.layers.4, gradient of model.layers.4.mlp.up_proj has inf: False
25-01-23 02:14:51 | I |   In layer model.layers.4, gradient of model.layers.4.mlp.gate_proj has nan: False
25-01-23 02:14:51 | I |   In layer model.layers.4, gradient of model.layers.4.mlp.gate_proj has inf: False
25-01-23 02:14:51 | I |   In layer model.layers.4, gradient of model.layers.4.mlp.down_proj has nan: False
25-01-23 02:14:51 | I |   In layer model.layers.4, gradient of model.layers.4.mlp.down_proj has inf: False
25-01-23 02:14:51 | I |   NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 2.0
25-01-23 02:14:52 | I |   TRAIN CURRENT LAYER_IDX = 4
25-01-23 02:14:52 | I |   in layer model.layers.4
25-01-23 02:14:52 | I |   quantizing weights for layer model.layers.4
25-01-23 02:14:52 | I |   collecting calibration activations in model.layers.4
25-01-23 02:14:52 | I |   collecting calibration activations in model.layers.4
25-01-23 02:14:53 | I |   - Quantizing decoder layer model.layers.4
25-01-23 02:14:53 | I |     - Calibrating model.layers.4.self_attn.q_proj.weight
25-01-23 02:14:53 | I |         - range scale = [    1.0000]
25-01-23 02:14:53 | I |           sum  error  = [    2.4346]
25-01-23 02:14:53 | I |           best error  = [    2.4346]
25-01-23 02:14:53 | I |       + error = [2.4346]
25-01-23 02:14:54 | I |         - range scale = [    1.0000]
25-01-23 02:14:54 | I |           sum  error  = [   22.9140]
25-01-23 02:14:54 | I |           best error  = [   22.9140]
25-01-23 02:14:54 | I |       + error = [22.9140]
25-01-23 02:14:54 | I |     - Calibrating model.layers.4.self_attn.k_proj.weight
25-01-23 02:14:55 | I |         - range scale = [    1.0000]
25-01-23 02:14:55 | I |           sum  error  = [    0.8497]
25-01-23 02:14:55 | I |           best error  = [    0.8497]
25-01-23 02:14:55 | I |       + error = [0.8497]
25-01-23 02:14:56 | I |         - range scale = [    1.0000]
25-01-23 02:14:56 | I |           sum  error  = [    8.7464]
25-01-23 02:14:56 | I |           best error  = [    8.7464]
25-01-23 02:14:56 | I |       + error = [8.7464]
25-01-23 02:14:56 | I |     - Calibrating model.layers.4.self_attn.v_proj.weight
25-01-23 02:14:56 | I |         - range scale = [    1.0000]
25-01-23 02:14:56 | I |           sum  error  = [    3.6405]
25-01-23 02:14:56 | I |           best error  = [    3.6405]
25-01-23 02:14:56 | I |       + error = [3.6405]
25-01-23 02:14:57 | I |         - range scale = [    1.0000]
25-01-23 02:14:57 | I |           sum  error  = [   38.8025]
25-01-23 02:14:57 | I |           best error  = [   38.8025]
25-01-23 02:14:57 | I |       + error = [38.8025]
25-01-23 02:14:57 | I |     - Calibrating model.layers.4.self_attn.o_proj.weight
25-01-23 02:14:58 | I |         - range scale = [    1.0000]
25-01-23 02:14:58 | I |           sum  error  = [    0.4189]
25-01-23 02:14:58 | I |           best error  = [    0.4189]
25-01-23 02:14:58 | I |       + error = [0.4189]
25-01-23 02:14:59 | I |         - range scale = [    1.0000]
25-01-23 02:14:59 | I |           sum  error  = [    4.2577]
25-01-23 02:14:59 | I |           best error  = [    4.2577]
25-01-23 02:14:59 | I |       + error = [4.2577]
25-01-23 02:14:59 | I |     - Calibrating model.layers.4.mlp.up_proj.weight
25-01-23 02:15:00 | I |         - range scale = [    1.0000]
25-01-23 02:15:00 | I |           sum  error  = [    3.8742]
25-01-23 02:15:00 | I |           best error  = [    3.8742]
25-01-23 02:15:00 | I |       + error = [3.8742]
25-01-23 02:15:01 | I |         - range scale = [    1.0000]
25-01-23 02:15:01 | I |           sum  error  = [   41.0104]
25-01-23 02:15:01 | I |           best error  = [   41.0104]
25-01-23 02:15:01 | I |       + error = [41.0104]
25-01-23 02:15:01 | I |     - Calibrating model.layers.4.mlp.gate_proj.weight
25-01-23 02:15:02 | I |         - range scale = [    1.0000]
25-01-23 02:15:02 | I |           sum  error  = [    4.3603]
25-01-23 02:15:02 | I |           best error  = [    4.3603]
25-01-23 02:15:02 | I |       + error = [4.3603]
25-01-23 02:15:03 | I |         - range scale = [    1.0000]
25-01-23 02:15:03 | I |           sum  error  = [   45.6058]
25-01-23 02:15:03 | I |           best error  = [   45.6058]
25-01-23 02:15:03 | I |       + error = [45.6058]
25-01-23 02:15:03 | I |     - Calibrating model.layers.4.mlp.down_proj.weight
25-01-23 02:15:04 | I |         - range scale = [    1.0000]
25-01-23 02:15:04 | I |           sum  error  = [    0.9547]
25-01-23 02:15:04 | I |           best error  = [    0.9547]
25-01-23 02:15:04 | I |       + error = [0.9547]
25-01-23 02:15:05 | I |         - range scale = [    1.0000]
25-01-23 02:15:05 | I |           sum  error  = [    9.2298]
25-01-23 02:15:05 | I |           best error  = [    9.2298]
25-01-23 02:15:05 | I |       + error = [9.2298]
25-01-23 02:15:05 | I |     - Quantizing model.layers.4.self_attn.q_proj.weight
25-01-23 02:15:07 | I |     - Quantizing model.layers.4.self_attn.k_proj.weight
25-01-23 02:15:09 | I |     - Quantizing model.layers.4.self_attn.v_proj.weight
25-01-23 02:15:11 | I |     - Quantizing model.layers.4.self_attn.o_proj.weight
25-01-23 02:15:14 | I |     - Quantizing model.layers.4.mlp.up_proj.weight
25-01-23 02:15:16 | I |     - Quantizing model.layers.4.mlp.gate_proj.weight
25-01-23 02:15:18 | I |     - Quantizing model.layers.4.mlp.down_proj.weight
25-01-23 02:15:24 | I |   quantizing activations for layer model.layers.4
25-01-23 02:15:24 | I |   collecting calibration activations in model.layers.4
25-01-23 02:15:25 | I |   collecting calibration activations in model.layers.4
25-01-23 02:15:26 | I |   forward this layer
25-01-23 02:15:26 | I |   input_file: /data/gyy/lmquant-main/lmquant/data/data_without_preprocess_llama_7b_bs16/shard/val_input_args_layer_4/07.pt
25-01-23 02:15:26 | I |   output_file: /data/gyy/lmquant-main/lmquant/data/data_without_preprocess_llama_7b_bs16/shard/val_teacher_output_layer_4/07.pt
25-01-23 02:15:27 | I |   In layer model.layers.4, gradient of model.layers.4.self_attn.q_proj has nan: False
25-01-23 02:15:27 | I |   In layer model.layers.4, gradient of model.layers.4.self_attn.q_proj has inf: False
25-01-23 02:15:27 | I |   In layer model.layers.4, gradient of model.layers.4.self_attn.k_proj has nan: False
25-01-23 02:15:27 | I |   In layer model.layers.4, gradient of model.layers.4.self_attn.k_proj has inf: False
25-01-23 02:15:27 | I |   In layer model.layers.4, gradient of model.layers.4.self_attn.v_proj has nan: False
25-01-23 02:15:27 | I |   In layer model.layers.4, gradient of model.layers.4.self_attn.v_proj has inf: False
25-01-23 02:15:27 | I |   In layer model.layers.4, gradient of model.layers.4.self_attn.o_proj has nan: False
25-01-23 02:15:27 | I |   In layer model.layers.4, gradient of model.layers.4.self_attn.o_proj has inf: False
25-01-23 02:15:27 | I |   In layer model.layers.4, gradient of model.layers.4.mlp.up_proj has nan: False
25-01-23 02:15:27 | I |   In layer model.layers.4, gradient of model.layers.4.mlp.up_proj has inf: False
25-01-23 02:15:27 | I |   In layer model.layers.4, gradient of model.layers.4.mlp.gate_proj has nan: False
25-01-23 02:15:27 | I |   In layer model.layers.4, gradient of model.layers.4.mlp.gate_proj has inf: False
25-01-23 02:15:27 | I |   In layer model.layers.4, gradient of model.layers.4.mlp.down_proj has nan: False
25-01-23 02:15:27 | I |   In layer model.layers.4, gradient of model.layers.4.mlp.down_proj has inf: False
25-01-23 02:15:27 | I |   NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 1.0
25-01-23 02:15:27 | I |   TRAIN CURRENT LAYER_IDX = 4
25-01-23 02:15:27 | I |   in layer model.layers.4
25-01-23 02:15:27 | I |   quantizing weights for layer model.layers.4
25-01-23 02:15:27 | I |   collecting calibration activations in model.layers.4
25-01-23 02:15:28 | I |   collecting calibration activations in model.layers.4
25-01-23 02:15:28 | I |   - Quantizing decoder layer model.layers.4
25-01-23 02:15:28 | I |     - Calibrating model.layers.4.self_attn.q_proj.weight
25-01-23 02:15:29 | I |         - range scale = [    1.0000]
25-01-23 02:15:29 | I |           sum  error  = [    2.3709]
25-01-23 02:15:29 | I |           best error  = [    2.3709]
25-01-23 02:15:29 | I |       + error = [2.3709]
25-01-23 02:15:29 | I |         - range scale = [    1.0000]
25-01-23 02:15:29 | I |           sum  error  = [   22.7716]
25-01-23 02:15:29 | I |           best error  = [   22.7716]
25-01-23 02:15:29 | I |       + error = [22.7716]
25-01-23 02:15:30 | I |     - Calibrating model.layers.4.self_attn.k_proj.weight
25-01-23 02:15:30 | I |         - range scale = [    1.0000]
25-01-23 02:15:30 | I |           sum  error  = [    0.8553]
25-01-23 02:15:30 | I |           best error  = [    0.8553]
25-01-23 02:15:30 | I |       + error = [0.8553]
25-01-23 02:15:31 | I |         - range scale = [    1.0000]
25-01-23 02:15:31 | I |           sum  error  = [    8.8230]
25-01-23 02:15:31 | I |           best error  = [    8.8230]
25-01-23 02:15:31 | I |       + error = [8.8230]
25-01-23 02:15:31 | I |     - Calibrating model.layers.4.self_attn.v_proj.weight
25-01-23 02:15:32 | I |         - range scale = [    1.0000]
25-01-23 02:15:32 | I |           sum  error  = [    3.6204]
25-01-23 02:15:32 | I |           best error  = [    3.6204]
25-01-23 02:15:32 | I |       + error = [3.6204]
25-01-23 02:15:32 | I |         - range scale = [    1.0000]
25-01-23 02:15:32 | I |           sum  error  = [   38.6197]
25-01-23 02:15:32 | I |           best error  = [   38.6197]
25-01-23 02:15:32 | I |       + error = [38.6197]
25-01-23 02:15:33 | I |     - Calibrating model.layers.4.self_attn.o_proj.weight
25-01-23 02:15:33 | I |         - range scale = [    1.0000]
25-01-23 02:15:33 | I |           sum  error  = [    0.4373]
25-01-23 02:15:33 | I |           best error  = [    0.4373]
25-01-23 02:15:33 | I |       + error = [0.4373]
25-01-23 02:15:34 | I |         - range scale = [    1.0000]
25-01-23 02:15:34 | I |           sum  error  = [    4.4652]
25-01-23 02:15:34 | I |           best error  = [    4.4652]
25-01-23 02:15:34 | I |       + error = [4.4652]
25-01-23 02:15:34 | I |     - Calibrating model.layers.4.mlp.up_proj.weight
25-01-23 02:15:35 | I |         - range scale = [    1.0000]
25-01-23 02:15:35 | I |           sum  error  = [    3.9461]
25-01-23 02:15:35 | I |           best error  = [    3.9461]
25-01-23 02:15:35 | I |       + error = [3.9461]
