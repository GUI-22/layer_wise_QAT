25-01-23 07:24:05 | I | === Configurations ===
25-01-23 07:24:05 | I | LlmRunConfig(
25-01-23 07:24:05 | I |   model=LlmModelConfig(
25-01-23 07:24:05 | I |     name=llama2-7b,
25-01-23 07:24:05 | I |     path=/data/gyy/llama2-7b,
25-01-23 07:24:05 | I |     root=/dataset/models,
25-01-23 07:24:05 | I |     local_path=/dataset/models/llama2/llama2-7b,
25-01-23 07:24:05 | I |     local_root=/dataset/models,
25-01-23 07:24:05 | I |     family=llama2,
25-01-23 07:24:05 | I |     size=7.0),
25-01-23 07:24:05 | I |   eval=LlmEvalConfig(
25-01-23 07:24:05 | I |     num_gpus=4,
25-01-23 07:24:05 | I |     batch_size=8,
25-01-23 07:24:05 | I |     output_root=runs,
25-01-23 07:24:05 | I |     output_dirname=skip.y.[q]-krnl-rot-rodr.Seq.[fc2]-smth.xw.a0p3.b0p7.[AbsMax].[out+fc2].yx.a0p5.b0.[AbsMax].[qk]-pileval.128x1024.[0-0]-250123.072405,
25-01-23 07:24:05 | I |     attach_timestamp=True,
25-01-23 07:24:05 | I |     timestamp=250123.072405,
25-01-23 07:24:05 | I |     output_dirname_without_timestamp=skip.y.[q]-krnl-rot-rodr.Seq.[fc2]-smth.xw.a0p3.b0p7.[AbsMax].[out+fc2].yx.a0p5.b0.[AbsMax].[qk]-pileval.128x1024.[0-0],
25-01-23 07:24:05 | I |     tasks=['wikitext'],
25-01-23 07:24:05 | I |     max_seq_length=-4096,
25-01-23 07:24:05 | I |     evaluator=gptq),
25-01-23 07:24:05 | I |   calib=LlmCalibConfig(
25-01-23 07:24:05 | I |     data=pileval,
25-01-23 07:24:05 | I |     num_samples=128,
25-01-23 07:24:05 | I |     cache_root=runs,
25-01-23 07:24:05 | I |     cache_dirpath=runs/llm/cache/pileval.128x1024.[0-0],
25-01-23 07:24:05 | I |     dataset_path=mit-han-lab/pile-val-backup,
25-01-23 07:24:05 | I |     seq_length=1024,
25-01-23 07:24:05 | I |     min_seq_length=0,
25-01-23 07:24:05 | I |     max_seq_length=0,
25-01-23 07:24:05 | I |     local_dataset_path=/dataset/pile),
25-01-23 07:24:05 | I |   quant=LlmQuantConfig(
25-01-23 07:24:05 | I |     wgts=WeightQuantizerConfig(
25-01-23 07:24:05 | I |       dtype=zint4,
25-01-23 07:24:05 | I |       group_shapes=((1, -1, -1), (1, 128, -1)),
25-01-23 07:24:05 | I |       group_scale_dtypes=(torch.float16, sint8),
25-01-23 07:24:05 | I |       compute_dtype=sint8,
25-01-23 07:24:05 | I |       compute_group_level=0,
25-01-23 07:24:05 | I |       saturate_compute_dtype=False,
25-01-23 07:24:05 | I |       exponent_scaling_level=2,
25-01-23 07:24:05 | I |       skips=['embed', 'head', 'router'],
25-01-23 07:24:05 | I |       static=True,
25-01-23 07:24:05 | I |       calib_kernel=QuantizerKernelConfig(
25-01-23 07:24:05 | I |         _kernels={'proj_1st': QuantGPTQConfig(includes=['proj_1st', 'proj_2nd', 'proj_out', 'proj_qkv'], damp_percentage=0.01, block_size=128, num_inv_tries=250, hessian_block_size=512), 'proj_2nd': QuantGPTQConfig(includes=['proj_1st', 'proj_2nd', 'proj_out', 'proj_qkv'], damp_percentage=0.01, block_size=128, num_inv_tries=250, hessian_block_size=512), 'proj_out': QuantGPTQConfig(includes=['proj_1st', 'proj_2nd', 'proj_out', 'proj_qkv'], damp_percentage=0.01, block_size=128, num_inv_tries=250, hessian_block_size=512), 'proj_qkv': QuantGPTQConfig(includes=['proj_1st', 'proj_2nd', 'proj_out', 'proj_qkv'], damp_percentage=0.01, block_size=128, num_inv_tries=250, hessian_block_size=512)},
25-01-23 07:24:05 | I |         gptq=QuantGPTQConfig(
25-01-23 07:24:05 | I |           includes=['proj_1st', 'proj_2nd', 'proj_out', 'proj_qkv'],
25-01-23 07:24:05 | I |           damp_percentage=0.01,
25-01-23 07:24:05 | I |           block_size=128,
25-01-23 07:24:05 | I |           num_inv_tries=250,
25-01-23 07:24:05 | I |           hessian_block_size=512)),
25-01-23 07:24:05 | I |       calib_range=DynamicRangeCalibConfig(
25-01-23 07:24:05 | I |         degree=2,
25-01-23 07:24:05 | I |         skips=[],
25-01-23 07:24:05 | I |         objective=SearchBasedCalibObjective.OutputsError,
25-01-23 07:24:05 | I |         strategy=SearchBasedCalibStrategy.Manual,
25-01-23 07:24:05 | I |         granularity=SearchBasedCalibGranularity.Group,
25-01-23 07:24:05 | I |         element_batch_size=64,
25-01-23 07:24:05 | I |         sample_batch_size=-1,
25-01-23 07:24:05 | I |         element_size=512,
25-01-23 07:24:05 | I |         sample_size=-1,
25-01-23 07:24:05 | I |         pre_reshape=True,
25-01-23 07:24:05 | I |         outputs_device=cpu,
25-01-23 07:24:05 | I |         allow_kernel_calib=False,
25-01-23 07:24:05 | I |         ratio=1.0,
25-01-23 07:24:05 | I |         max_shrink=0.2,
25-01-23 07:24:05 | I |         max_expand=1.0,
25-01-23 07:24:05 | I |         num_grids=80)),
25-01-23 07:24:05 | I |     ipts=ActivationQuantizerConfig(
25-01-23 07:24:05 | I |       dtype=sint8,
25-01-23 07:24:05 | I |       group_shapes=((1, -1, -1),),
25-01-23 07:24:05 | I |       group_scale_dtypes=(torch.float16,),
25-01-23 07:24:05 | I |       compute_dtype=None,
25-01-23 07:24:05 | I |       compute_group_level=-1,
25-01-23 07:24:05 | I |       saturate_compute_dtype=False,
25-01-23 07:24:05 | I |       exponent_scaling_level=1,
25-01-23 07:24:05 | I |       skips=['embed', 'head', 'router'],
25-01-23 07:24:05 | I |       static=False,
25-01-23 07:24:05 | I |       calib_kernel=None,
25-01-23 07:24:05 | I |       calib_range=None),
25-01-23 07:24:05 | I |     opts=ActivationQuantizerConfig(
25-01-23 07:24:05 | I |       dtype=zint4,
25-01-23 07:24:05 | I |       group_shapes=((1, 128, -1),),
25-01-23 07:24:05 | I |       group_scale_dtypes=(torch.float16,),
25-01-23 07:24:05 | I |       compute_dtype=None,
25-01-23 07:24:05 | I |       compute_group_level=-1,
25-01-23 07:24:05 | I |       saturate_compute_dtype=False,
25-01-23 07:24:05 | I |       exponent_scaling_level=1,
25-01-23 07:24:05 | I |       skips=['attn_q'],
25-01-23 07:24:05 | I |       static=False,
25-01-23 07:24:05 | I |       calib_kernel=None,
25-01-23 07:24:05 | I |       calib_range=None),
25-01-23 07:24:05 | I |     rotation=QuantRotationConfig(
25-01-23 07:24:05 | I |       random=False,
25-01-23 07:24:05 | I |       transforms=[]),
25-01-23 07:24:05 | I |     reorder=QuantReorderConfig(
25-01-23 07:24:05 | I |       degree=2,
25-01-23 07:24:05 | I |       skips=['proj_out', 'proj_qkv', 'residual'],
25-01-23 07:24:05 | I |       objective=SearchBasedCalibObjective.OutputsError,
25-01-23 07:24:05 | I |       strategy=SearchBasedCalibStrategy.Manual,
25-01-23 07:24:05 | I |       granularity=SearchBasedCalibGranularity.Layer,
25-01-23 07:24:05 | I |       element_batch_size=-1,
25-01-23 07:24:05 | I |       sample_batch_size=-1,
25-01-23 07:24:05 | I |       element_size=-1,
25-01-23 07:24:05 | I |       sample_size=-1,
25-01-23 07:24:05 | I |       pre_reshape=True,
25-01-23 07:24:05 | I |       outputs_device=cpu,
25-01-23 07:24:05 | I |       allow_kernel_calib=False,
25-01-23 07:24:05 | I |       channel_metric=ChannelMetric.InputsAbsMax,
25-01-23 07:24:05 | I |       channel_index=ChannelIndex.Sequential,
25-01-23 07:24:05 | I |       dynamic=False),
25-01-23 07:24:05 | I |     smooth=QuantSmoothConfig(
25-01-23 07:24:05 | I |       xw=QuantSmoothCalibConfig(
25-01-23 07:24:05 | I |         degree=2,
25-01-23 07:24:05 | I |         skips=['proj_1st', 'proj_qkv'],
25-01-23 07:24:05 | I |         objective=SearchBasedCalibObjective.OutputsError,
25-01-23 07:24:05 | I |         strategy=SearchBasedCalibStrategy.Manual,
25-01-23 07:24:05 | I |         granularity=SearchBasedCalibGranularity.Layer,
25-01-23 07:24:05 | I |         element_batch_size=-1,
25-01-23 07:24:05 | I |         sample_batch_size=-1,
25-01-23 07:24:05 | I |         element_size=-1,
25-01-23 07:24:05 | I |         sample_size=-1,
25-01-23 07:24:05 | I |         pre_reshape=True,
25-01-23 07:24:05 | I |         outputs_device=cpu,
25-01-23 07:24:05 | I |         allow_kernel_calib=False,
25-01-23 07:24:05 | I |         ranges=[(<RangeMode.AbsMax: 1>, <RangeMode.AbsMax: 1>)],
25-01-23 07:24:05 | I |         x_ranges=[<RangeMode.AbsMax: 1>],
25-01-23 07:24:05 | I |         w_ranges=[<RangeMode.AbsMax: 1>],
25-01-23 07:24:05 | I |         alpha=0.3,
25-01-23 07:24:05 | I |         beta=0.7,
25-01-23 07:24:05 | I |         num_grids=20),
25-01-23 07:24:05 | I |       yx=QuantSmoothCalibConfig(
25-01-23 07:24:05 | I |         degree=2,
25-01-23 07:24:05 | I |         skips=[],
25-01-23 07:24:05 | I |         objective=SearchBasedCalibObjective.OutputsError,
25-01-23 07:24:05 | I |         strategy=SearchBasedCalibStrategy.Manual,
25-01-23 07:24:05 | I |         granularity=SearchBasedCalibGranularity.Layer,
25-01-23 07:24:05 | I |         element_batch_size=-1,
25-01-23 07:24:05 | I |         sample_batch_size=-1,
25-01-23 07:24:05 | I |         element_size=-1,
25-01-23 07:24:05 | I |         sample_size=-1,
25-01-23 07:24:05 | I |         pre_reshape=True,
25-01-23 07:24:05 | I |         outputs_device=cpu,
25-01-23 07:24:05 | I |         allow_kernel_calib=False,
25-01-23 07:24:05 | I |         ranges=[(<RangeMode.AbsMax: 1>, <RangeMode.AbsMax: 1>)],
25-01-23 07:24:05 | I |         x_ranges=[<RangeMode.AbsMax: 1>],
25-01-23 07:24:05 | I |         w_ranges=[<RangeMode.AbsMax: 1>],
25-01-23 07:24:05 | I |         alpha=0.5,
25-01-23 07:24:05 | I |         beta=0.0,
25-01-23 07:24:05 | I |         num_grids=20)),
25-01-23 07:24:05 | I |     bias_correction=False,
25-01-23 07:24:05 | I |     post_rotary=True,
25-01-23 07:24:05 | I |     develop_dtype=torch.float32,
25-01-23 07:24:05 | I |     select_wgts=None,
25-01-23 07:24:05 | I |     select_ipts=None,
25-01-23 07:24:05 | I |     select_opts=None,
25-01-23 07:24:05 | I |     keywords_i={'proj_qkv': ['q_proj', 'k_proj', 'v_proj'], 'proj_out': ['out_proj', 'o_proj'], 'proj_1st': ['fc1', 'up_proj', 'gate_proj', 'w1', 'w3'], 'proj_2nd': ['fc2', 'down_proj', 'w2'], 'head': ['output', 'score', 'qa_outputs'], 'embed': ['embed', 'lm_head', 'embed_out'], 'router': ['block_sparse_moe']},
25-01-23 07:24:05 | I |     keywords_w={'proj_qkv': ['q_proj', 'k_proj', 'v_proj'], 'proj_out': ['out_proj', 'o_proj'], 'proj_1st': ['fc1', 'up_proj', 'gate_proj', 'w1', 'w3'], 'proj_2nd': ['fc2', 'down_proj', 'w2'], 'head': ['output', 'score', 'qa_outputs'], 'embed': ['embed', 'lm_head', 'embed_out'], 'router': ['block_sparse_moe.gate']},
25-01-23 07:24:05 | I |     keywords_o={'attn_q': ['q_rotary_emb'], 'attn_k': ['k_rotary_emb'], 'attn_v': ['v_proj']},
25-01-23 07:24:05 | I |     module_types_i=(<class 'torch.nn.modules.linear.Linear'>, <class 'transformers.models.mixtral.modeling_mixtral.MixtralSparseMoeBlock'>),
25-01-23 07:24:05 | I |     module_types_w=(<class 'torch.nn.modules.linear.Linear'>,),
25-01-23 07:24:05 | I |     module_types_o=(<class 'torch.nn.modules.linear.Linear'>, <class 'lmquant.llm.nn.attention.RotaryEmbedding'>),
25-01-23 07:24:05 | I |     num_hidden_layers=-1),
25-01-23 07:24:05 | I |   seed=12345,
25-01-23 07:24:05 | I |   save_model=True,
25-01-23 07:24:05 | I |   output_dirpath=runs/llm/llama2/llama2-7b/w.4-x.8-y.4/w.zint4-x.sint8-y.zint4/w.gchn.fp16.g128.sint8.[gchn.sint8]-x.gchn.fp16-y.g128.fp16/rot-reorder-smooth.xw.yx-w.static.kernel/skip.y.[q]-krnl-rot-rodr.Seq.[fc2]-smth.xw.a0p3.b0p7.[AbsMax].[out+fc2].yx.a0p5.b0.[AbsMax].[qk]-pileval.128x1024.[0-0]-250123.072405,
25-01-23 07:24:05 | I |   cache_dirpath=LlmQuantCachePath(rotation='runs/llm/cache/rotation/hadamard', reorder='runs/llm/cache/pileval.128x1024.[0-0]/reorder/w.4-x.8-y.4/w.zint4-x.sint8-y.zint4/w.gchn.fp16.g128.sint8.[gchn.sint8]-x.gchn.fp16-y.g128.fp16/w.skip.[embed+head+router]-x.skip.[embed+head+router]-y.skip.[attn_q]/rotate.hadamard/reorder.OutputsError.Manual.Layer.d2.en1.sn1/reorder.InputsAbsMax.Sequential/reorder.skip.[proj_out+proj_qkv+residual]', smooth='runs/llm/cache/pileval.128x1024.[0-0]/smooth/w.4-x.8-y.4/w.zint4-x.sint8-y.zint4/w.gchn.fp16.g128.sint8.[gchn.sint8]-x.gchn.fp16-y.g128.fp16/w.skip.[embed+head+router]-x.skip.[embed+head+router]-y.skip.[attn_q]/rotate.hadamard/reorder.OutputsError.Manual.Layer.d2.en1.sn1/reorder.InputsAbsMax.Sequential/reorder.skip.[proj_out+proj_qkv+residual]/smooth.xw.OutputsError.Manual.Layer.d2.en1.sn1-yx.OutputsError.Manual.Layer.d2.en1.sn1/smooth.xw.[x.AbsMax.w.AbsMax]-yx.[x.AbsMax.w.AbsMax]/smooth.xw.a0p3.b0p7-yx.a0p5.b0/smooth.xw.skip.[proj_1st+proj_qkv]-yx.skip.[]', wgts='runs/llm/cache/pileval.128x1024.[0-0]/wgts/w.4-x.8-y.4/w.zint4-x.sint8-y.zint4/w.gchn.fp16.g128.sint8.[gchn.sint8]-x.gchn.fp16-y.g128.fp16/w.skip.[embed+head+router]-x.skip.[embed+head+router]-y.skip.[attn_q]/rotate.hadamard/reorder.OutputsError.Manual.Layer.d2.en1.sn1/reorder.InputsAbsMax.Sequential/reorder.skip.[proj_out+proj_qkv+residual]/smooth.xw.OutputsError.Manual.Layer.d2.en1.sn1-yx.OutputsError.Manual.Layer.d2.en1.sn1/smooth.xw.[x.AbsMax.w.AbsMax]-yx.[x.AbsMax.w.AbsMax]/smooth.xw.a0p3.b0p7-yx.a0p5.b0/smooth.xw.skip.[proj_1st+proj_qkv]-yx.skip.[]/w.kernel.gptq.d0p01.b128/w.kernel.gptq.include.[proj_1st+proj_2nd+proj_out+proj_qkv]/w.range.OutputsError.Manual.Group.d2.e512.sn1/w.range.r.[1].static/w.range.skip.[]', acts=''),
25-01-23 07:24:05 | I |   cache_path=LlmQuantCachePath(rotation='runs/llm/cache/rotation/hadamard/llama2-7b.pt', reorder='runs/llm/cache/pileval.128x1024.[0-0]/reorder/w.4-x.8-y.4/w.zint4-x.sint8-y.zint4/w.gchn.fp16.g128.sint8.[gchn.sint8]-x.gchn.fp16-y.g128.fp16/w.skip.[embed+head+router]-x.skip.[embed+head+router]-y.skip.[attn_q]/rotate.hadamard/reorder.OutputsError.Manual.Layer.d2.en1.sn1/reorder.InputsAbsMax.Sequential/reorder.skip.[proj_out+proj_qkv+residual]/llama2-7b.pt', smooth='runs/llm/cache/pileval.128x1024.[0-0]/smooth/w.4-x.8-y.4/w.zint4-x.sint8-y.zint4/w.gchn.fp16.g128.sint8.[gchn.sint8]-x.gchn.fp16-y.g128.fp16/w.skip.[embed+head+router]-x.skip.[embed+head+router]-y.skip.[attn_q]/rotate.hadamard/reorder.OutputsError.Manual.Layer.d2.en1.sn1/reorder.InputsAbsMax.Sequential/reorder.skip.[proj_out+proj_qkv+residual]/smooth.xw.OutputsError.Manual.Layer.d2.en1.sn1-yx.OutputsError.Manual.Layer.d2.en1.sn1/smooth.xw.[x.AbsMax.w.AbsMax]-yx.[x.AbsMax.w.AbsMax]/smooth.xw.a0p3.b0p7-yx.a0p5.b0/smooth.xw.skip.[proj_1st+proj_qkv]-yx.skip.[]/llama2-7b.pt', wgts='runs/llm/cache/pileval.128x1024.[0-0]/wgts/w.4-x.8-y.4/w.zint4-x.sint8-y.zint4/w.gchn.fp16.g128.sint8.[gchn.sint8]-x.gchn.fp16-y.g128.fp16/w.skip.[embed+head+router]-x.skip.[embed+head+router]-y.skip.[attn_q]/rotate.hadamard/reorder.OutputsError.Manual.Layer.d2.en1.sn1/reorder.InputsAbsMax.Sequential/reorder.skip.[proj_out+proj_qkv+residual]/smooth.xw.OutputsError.Manual.Layer.d2.en1.sn1-yx.OutputsError.Manual.Layer.d2.en1.sn1/smooth.xw.[x.AbsMax.w.AbsMax]-yx.[x.AbsMax.w.AbsMax]/smooth.xw.a0p3.b0p7-yx.a0p5.b0/smooth.xw.skip.[proj_1st+proj_qkv]-yx.skip.[]/w.kernel.gptq.d0p01.b128/w.kernel.gptq.include.[proj_1st+proj_2nd+proj_out+proj_qkv]/w.range.OutputsError.Manual.Group.d2.e512.sn1/w.range.r.[1].static/w.range.skip.[]/llama2-7b.pt', acts=''),
25-01-23 07:24:05 | I |   fairseq_args=/data/gyy/lmquant-main/projects/llm/configs/fairseq_args_7b_without_preprocess.json,
25-01-23 07:24:05 | I |   gen_teacher_opts=False,
25-01-23 07:24:05 | I |   enable_cache=True,
25-01-23 07:24:05 | I |   with_preprocess=False)
25-01-23 07:24:05 | I | === Dumped Configurations ===
25-01-23 07:24:05 | I | { 'calib': { 'cache_root': 'runs',
25-01-23 07:24:05 | I |              'data': 'pileval',
25-01-23 07:24:05 | I |              'dataset_path': 'mit-han-lab/pile-val-backup',
25-01-23 07:24:05 | I |              'local_dataset_path': '/dataset/pile',
25-01-23 07:24:05 | I |              'max_seq_length': 0,
25-01-23 07:24:05 | I |              'min_seq_length': 0,
25-01-23 07:24:05 | I |              'num_samples': 128,
25-01-23 07:24:05 | I |              'seq_length': 1024},
25-01-23 07:24:05 | I |   'enable_cache': True,
25-01-23 07:24:05 | I |   'eval': { 'attach_timestamp': True,
25-01-23 07:24:05 | I |             'batch_size': 8,
25-01-23 07:24:05 | I |             'evaluator': 'gptq',
25-01-23 07:24:05 | I |             'max_seq_length': -4096,
25-01-23 07:24:05 | I |             'num_gpus': 4,
25-01-23 07:24:05 | I |             'output_dirname': 'skip.y.[q]-krnl-rot-rodr.Seq.[fc2]-smth.xw.a0p3.b0p7.[AbsMax].[out+fc2].yx.a0p5.b0.[AbsMax].[qk]-pileval.128x1024.[0-0]-250123.072405',
25-01-23 07:24:05 | I |             'output_root': 'runs',
25-01-23 07:24:05 | I |             'tasks': ['wikitext']},
25-01-23 07:24:05 | I |   'fairseq_args': '/data/gyy/lmquant-main/projects/llm/configs/fairseq_args_7b_without_preprocess.json',
25-01-23 07:24:05 | I |   'gen_teacher_opts': False,
25-01-23 07:24:05 | I |   'model': { 'local_path': '/dataset/models/llama2/llama2-7b',
25-01-23 07:24:05 | I |              'local_root': '/dataset/models',
25-01-23 07:24:05 | I |              'name': 'llama2-7b',
25-01-23 07:24:05 | I |              'path': '/data/gyy/llama2-7b',
25-01-23 07:24:05 | I |              'root': '/dataset/models'},
25-01-23 07:24:05 | I |   'quant': { 'bias_correction': False,
25-01-23 07:24:05 | I |              'develop_dtype': 'torch.float32',
25-01-23 07:24:05 | I |              'enable_reorder': True,
25-01-23 07:24:05 | I |              'enable_rotation': True,
25-01-23 07:24:05 | I |              'enable_select_ipts': False,
25-01-23 07:24:05 | I |              'enable_select_opts': False,
25-01-23 07:24:05 | I |              'enable_select_wgts': False,
25-01-23 07:24:05 | I |              'enable_smooth': True,
25-01-23 07:24:05 | I |              'ipts': { 'compute_dtype': None,
25-01-23 07:24:05 | I |                        'compute_group_level': -1,
25-01-23 07:24:05 | I |                        'dtype': 'sint8',
25-01-23 07:24:05 | I |                        'enable_calib_range': False,
25-01-23 07:24:05 | I |                        'group_scale_dtypes': ['torch.float16'],
25-01-23 07:24:05 | I |                        'group_shapes': [[1, -1, -1]],
25-01-23 07:24:05 | I |                        'saturate_compute_dtype': False,
25-01-23 07:24:05 | I |                        'skips': ['embed', 'head', 'router'],
25-01-23 07:24:05 | I |                        'static': False},
25-01-23 07:24:05 | I |              'opts': { 'compute_dtype': None,
25-01-23 07:24:05 | I |                        'compute_group_level': -1,
25-01-23 07:24:05 | I |                        'dtype': 'zint4',
25-01-23 07:24:05 | I |                        'enable_calib_range': False,
25-01-23 07:24:05 | I |                        'group_scale_dtypes': ['torch.float16'],
25-01-23 07:24:05 | I |                        'group_shapes': [[1, 128, -1]],
25-01-23 07:24:05 | I |                        'saturate_compute_dtype': False,
25-01-23 07:24:05 | I |                        'skips': ['attn_q'],
25-01-23 07:24:05 | I |                        'static': False},
25-01-23 07:24:05 | I |              'post_rotary': True,
25-01-23 07:24:05 | I |              'reorder': { 'allow_kernel_calib': False,
25-01-23 07:24:05 | I |                           'channel_index': 'Sequential',
25-01-23 07:24:05 | I |                           'channel_metric': 'InputsAbsMax',
25-01-23 07:24:05 | I |                           'degree': 2,
25-01-23 07:24:05 | I |                           'dynamic': False,
25-01-23 07:24:05 | I |                           'element_batch_size': -1,
25-01-23 07:24:05 | I |                           'element_size': -1,
25-01-23 07:24:05 | I |                           'outputs_device': 'cpu',
25-01-23 07:24:05 | I |                           'pre_reshape': True,
25-01-23 07:24:05 | I |                           'sample_batch_size': -1,
25-01-23 07:24:05 | I |                           'sample_size': -1,
25-01-23 07:24:05 | I |                           'skips': ['proj_out', 'proj_qkv', 'residual'],
25-01-23 07:24:05 | I |                           'strategy': 'Manual'},
25-01-23 07:24:05 | I |              'rotation': {'random': False, 'transforms': []},
25-01-23 07:24:05 | I |              'smooth': { 'enable_xw': True,
25-01-23 07:24:05 | I |                          'enable_yx': True,
25-01-23 07:24:05 | I |                          'xw': { 'allow_kernel_calib': False,
25-01-23 07:24:05 | I |                                  'alpha': 0.3,
25-01-23 07:24:05 | I |                                  'beta': 0.7,
25-01-23 07:24:05 | I |                                  'degree': 2,
25-01-23 07:24:05 | I |                                  'element_batch_size': -1,
25-01-23 07:24:05 | I |                                  'element_size': -1,
25-01-23 07:24:05 | I |                                  'granularity': 'Layer',
25-01-23 07:24:05 | I |                                  'num_grids': 20,
25-01-23 07:24:05 | I |                                  'objective': 'OutputsError',
25-01-23 07:24:05 | I |                                  'outputs_device': 'cpu',
25-01-23 07:24:05 | I |                                  'pre_reshape': True,
25-01-23 07:24:05 | I |                                  'ranges': [['AbsMax', 'AbsMax']],
25-01-23 07:24:05 | I |                                  'sample_batch_size': -1,
25-01-23 07:24:05 | I |                                  'sample_size': -1,
25-01-23 07:24:05 | I |                                  'skips': ['proj_1st', 'proj_qkv'],
25-01-23 07:24:05 | I |                                  'strategy': 'Manual'},
25-01-23 07:24:05 | I |                          'yx': { 'allow_kernel_calib': False,
25-01-23 07:24:05 | I |                                  'alpha': 0.5,
25-01-23 07:24:05 | I |                                  'beta': 0.0,
25-01-23 07:24:05 | I |                                  'degree': 2,
25-01-23 07:24:05 | I |                                  'element_batch_size': -1,
25-01-23 07:24:05 | I |                                  'element_size': -1,
25-01-23 07:24:05 | I |                                  'granularity': 'Layer',
25-01-23 07:24:05 | I |                                  'num_grids': 20,
25-01-23 07:24:05 | I |                                  'objective': 'OutputsError',
25-01-23 07:24:05 | I |                                  'outputs_device': 'cpu',
25-01-23 07:24:05 | I |                                  'pre_reshape': True,
25-01-23 07:24:05 | I |                                  'ranges': [['AbsMax', 'AbsMax']],
25-01-23 07:24:05 | I |                                  'sample_batch_size': -1,
25-01-23 07:24:05 | I |                                  'sample_size': -1,
25-01-23 07:24:05 | I |                                  'skips': [],
25-01-23 07:24:05 | I |                                  'strategy': 'Manual'}},
25-01-23 07:24:05 | I |              'wgts': { 'calib_kernel': { 'enable_gptq': True,
25-01-23 07:24:05 | I |                                          'gptq': { 'block_size': 128,
25-01-23 07:24:05 | I |                                                    'damp_percentage': 0.01,
25-01-23 07:24:05 | I |                                                    'hessian_block_size': 512,
25-01-23 07:24:05 | I |                                                    'includes': ['proj_1st', 'proj_2nd', 'proj_out', 'proj_qkv'],
25-01-23 07:24:05 | I |                                                    'num_inv_tries': 250}},
25-01-23 07:24:05 | I |                        'calib_range': { 'allow_kernel_calib': False,
25-01-23 07:24:05 | I |                                         'degree': 2,
25-01-23 07:24:05 | I |                                         'element_batch_size': 64,
25-01-23 07:24:05 | I |                                         'element_size': 512,
25-01-23 07:24:05 | I |                                         'granularity': 'Group',
25-01-23 07:24:05 | I |                                         'max_expand': 1.0,
25-01-23 07:24:05 | I |                                         'max_shrink': 0.2,
25-01-23 07:24:05 | I |                                         'num_grids': 80,
25-01-23 07:24:05 | I |                                         'objective': 'OutputsError',
25-01-23 07:24:05 | I |                                         'outputs_device': 'cpu',
25-01-23 07:24:05 | I |                                         'pre_reshape': True,
25-01-23 07:24:05 | I |                                         'ratio': 1.0,
25-01-23 07:24:05 | I |                                         'sample_batch_size': -1,
25-01-23 07:24:05 | I |                                         'sample_size': -1,
25-01-23 07:24:05 | I |                                         'skips': [],
25-01-23 07:24:05 | I |                                         'strategy': 'Manual'},
25-01-23 07:24:05 | I |                        'compute_dtype': 'sint8',
25-01-23 07:24:05 | I |                        'compute_group_level': 0,
25-01-23 07:24:05 | I |                        'dtype': 'zint4',
25-01-23 07:24:05 | I |                        'enable_calib_kernel': True,
25-01-23 07:24:05 | I |                        'enable_calib_range': True,
25-01-23 07:24:05 | I |                        'group_scale_dtypes': ['torch.float16', 'sint8'],
25-01-23 07:24:05 | I |                        'group_shapes': [[1, -1, -1], [1, 128, -1]],
25-01-23 07:24:05 | I |                        'saturate_compute_dtype': False,
25-01-23 07:24:05 | I |                        'skips': ['embed', 'head', 'router']}},
25-01-23 07:24:05 | I |   'save_model': True,
25-01-23 07:24:05 | I |   'seed': 12345,
25-01-23 07:24:05 | I |   'with_preprocess': False}
25-01-23 07:24:05 | I | === Output Directory ===
25-01-23 07:24:05 | I | runs/llm/llama2/llama2-7b/w.4-x.8-y.4/w.zint4-x.sint8-y.zint4/w.gchn.fp16.g128.sint8.[gchn.sint8]-x.gchn.fp16-y.g128.fp16/rot-reorder-smooth.xw.yx-w.static.kernel/skip.y.[q]-krnl-rot-rodr.Seq.[fc2]-smth.xw.a0p3.b0p7.[AbsMax].[out+fc2].yx.a0p5.b0.[AbsMax].[qk]-pileval.128x1024.[0-0]-250123.072405
25-01-23 07:24:05 | I | === Start Evaluating ===
25-01-23 07:24:05 | I | * Building model llama2-7b from /data/gyy/llama2-7b
25-01-23 07:24:09 | I | - Patching LlamaSdpaAttention._old_forward in model.layers.0.self_attn
25-01-23 07:24:09 | I | - Patching LlamaSdpaAttention._old_forward in model.layers.1.self_attn
25-01-23 07:24:09 | I | - Patching LlamaSdpaAttention._old_forward in model.layers.2.self_attn
25-01-23 07:24:09 | I | - Patching LlamaSdpaAttention._old_forward in model.layers.3.self_attn
25-01-23 07:24:09 | I | - Patching LlamaSdpaAttention._old_forward in model.layers.4.self_attn
25-01-23 07:24:09 | I | - Patching LlamaSdpaAttention._old_forward in model.layers.5.self_attn
25-01-23 07:24:09 | I | - Patching LlamaSdpaAttention._old_forward in model.layers.6.self_attn
25-01-23 07:24:09 | I | - Patching LlamaSdpaAttention._old_forward in model.layers.7.self_attn
25-01-23 07:24:09 | I | - Patching LlamaSdpaAttention._old_forward in model.layers.8.self_attn
25-01-23 07:24:09 | I | - Patching LlamaSdpaAttention._old_forward in model.layers.9.self_attn
25-01-23 07:24:09 | I | - Patching LlamaSdpaAttention._old_forward in model.layers.10.self_attn
25-01-23 07:24:09 | I | - Patching LlamaSdpaAttention._old_forward in model.layers.11.self_attn
25-01-23 07:24:09 | I | - Patching LlamaSdpaAttention._old_forward in model.layers.12.self_attn
25-01-23 07:24:09 | I | - Patching LlamaSdpaAttention._old_forward in model.layers.13.self_attn
25-01-23 07:24:09 | I | - Patching LlamaSdpaAttention._old_forward in model.layers.14.self_attn
25-01-23 07:24:09 | I | - Patching LlamaSdpaAttention._old_forward in model.layers.15.self_attn
25-01-23 07:24:09 | I | - Patching LlamaSdpaAttention._old_forward in model.layers.16.self_attn
25-01-23 07:24:09 | I | - Patching LlamaSdpaAttention._old_forward in model.layers.17.self_attn
25-01-23 07:24:09 | I | - Patching LlamaSdpaAttention._old_forward in model.layers.18.self_attn
25-01-23 07:24:09 | I | - Patching LlamaSdpaAttention._old_forward in model.layers.19.self_attn
25-01-23 07:24:09 | I | - Patching LlamaSdpaAttention._old_forward in model.layers.20.self_attn
25-01-23 07:24:09 | I | - Patching LlamaSdpaAttention._old_forward in model.layers.21.self_attn
25-01-23 07:24:09 | I | - Patching LlamaSdpaAttention._old_forward in model.layers.22.self_attn
25-01-23 07:24:09 | I | - Patching LlamaSdpaAttention._old_forward in model.layers.23.self_attn
25-01-23 07:24:09 | I | - Patching LlamaSdpaAttention._old_forward in model.layers.24.self_attn
25-01-23 07:24:09 | I | - Patching LlamaSdpaAttention._old_forward in model.layers.25.self_attn
25-01-23 07:24:09 | I | - Patching LlamaSdpaAttention._old_forward in model.layers.26.self_attn
25-01-23 07:24:09 | I | - Patching LlamaSdpaAttention._old_forward in model.layers.27.self_attn
25-01-23 07:24:09 | I | - Patching LlamaSdpaAttention._old_forward in model.layers.28.self_attn
25-01-23 07:24:09 | I | - Patching LlamaSdpaAttention._old_forward in model.layers.29.self_attn
25-01-23 07:24:09 | I | - Patching LlamaSdpaAttention._old_forward in model.layers.30.self_attn
25-01-23 07:24:09 | I | - Patching LlamaSdpaAttention._old_forward in model.layers.31.self_attn
25-01-23 07:24:09 | I | * Development dtype is torch.float32
25-01-23 07:24:10 | I | * Begin to QAT
25-01-23 07:24:10 | D | Setting JobRuntime:name=UNKNOWN_NAME
25-01-23 07:24:10 | D | Setting JobRuntime:name=utils
25-01-23 07:24:11 | I | distributed init (rank 0): env://
25-01-23 07:24:11 | I | initialized host 3fb0cc53fbf9 as rank 0
25-01-23 07:24:12 | I | nvidia-smi stats: {'gpu_0_mem_used_gb': 2.080078125, 'gpu_1_mem_used_gb': 0.001953125, 'gpu_2_mem_used_gb': 0.001953125, 'gpu_3_mem_used_gb': 0.001953125, 'gpu_4_mem_used_gb': 6.150390625, 'gpu_5_mem_used_gb': 7.0859375, 'gpu_6_mem_used_gb': 7.0859375, 'gpu_7_mem_used_gb': 6.06640625}
25-01-23 07:24:12 | I | {'_name': None, 'common': {'_name': None, 'no_progress_bar': False, 'log_interval': 1, 'log_format': 'simple', 'log_file': None, 'tensorboard_logdir': None, 'wandb_project': 'layer_wise_quant', 'azureml_logging': False, 'seed': 1, 'cpu': False, 'tpu': False, 'bf16': False, 'memory_efficient_bf16': False, 'fp16': True, 'memory_efficient_fp16': True, 'fp16_no_flatten_grads': False, 'fp16_init_scale': 128, 'fp16_scale_window': None, 'fp16_scale_tolerance': 0.0, 'min_loss_scale': 0.0001, 'threshold_loss_scale': None, 'user_dir': None, 'empty_cache_freq': 0, 'all_gather_list_size': 16384, 'model_parallel_size': 1, 'quantization_config_path': None, 'profile': False, 'reset_logging': False, 'suppress_crashes': False, 'use_plasma_view': False, 'plasma_path': '/tmp/plasma', 'log_nvidia_smi': False}, 'common_eval': {'_name': None, 'path': None, 'post_process': None, 'quiet': False, 'model_overrides': '{}', 'results_path': None, 'is_moe': False, 'is_model_parallel': False}, 'distributed_training': {'_name': None, 'distributed_world_size': 1, 'distributed_rank': 0, 'distributed_backend': 'nccl', 'distributed_init_method': 'env://', 'distributed_port': -1, 'device_id': 0, 'distributed_no_spawn': True, 'ddp_backend': 'pytorch_ddp', 'bucket_cap_mb': 25, 'fix_batches_to_gpus': False, 'find_unused_parameters': False, 'fast_stat_sync': False, 'heartbeat_timeout': -1, 'broadcast_buffers': False, 'slowmo_momentum': None, 'slowmo_algorithm': 'LocalSGD', 'localsgd_frequency': 3, 'nprocs_per_node': 4, 'pipeline_model_parallel': False, 'pipeline_balance': None, 'pipeline_devices': None, 'pipeline_chunks': 0, 'pipeline_encoder_balance': None, 'pipeline_encoder_devices': None, 'pipeline_decoder_balance': None, 'pipeline_decoder_devices': None, 'pipeline_checkpoint': 'never', 'zero_sharding': 'none', 'zero_group_size': -1, 'save_zero_ckpt_fast': False, 'fp16': True, 'memory_efficient_fp16': True, 'tpu': False, 'no_reshard_after_forward': False, 'fp32_reduce_scatter': False, 'cpu_offload': False, 'use_sharded_state': False}, 'dataset': {'_name': None, 'num_workers': 1, 'num_workers_valid': 0, 'skip_invalid_size_inputs_valid_test': False, 'max_tokens': None, 'batch_size': 16, 'required_batch_size_multiple': 8, 'required_seq_len_multiple': 1, 'dataset_impl': None, 'data_buffer_size': 10, 'train_subset': 'train', 'valid_subset': 'valid', 'combine_valid_subsets': None, 'ignore_unused_valid_subsets': False, 'validate_interval': 1, 'validate_interval_updates': 10, 'validate_after_updates': 0, 'fixed_validation_seed': None, 'disable_validation': False, 'max_tokens_valid': None, 'batch_size_valid': 16, 'max_valid_steps': None, 'curriculum': 0, 'gen_subset': 'test', 'num_shards': 1, 'shard_id': 0}, 'optimization': {'_name': None, 'max_epoch': 0, 'max_update': 100, 'stop_time_hours': 0.0, 'clip_norm': 2.0, 'sentence_avg': False, 'update_freq': [1], 'lr': [0.005], 'stop_min_lr': -1.0, 'use_bmuf': False}, 'checkpoint': {'_name': None, 'save_dir': 'checkpoints', 'restore_file': 'checkpoint_last.pt', 'finetune_from_model': None, 'reset_dataloader': False, 'reset_lr_scheduler': False, 'reset_meters': False, 'reset_optimizer': False, 'optimizer_overrides': '{}', 'save_interval': 1, 'save_interval_updates': 1000, 'keep_interval_updates': -1, 'keep_last_epochs': -1, 'keep_best_checkpoints': -1, 'no_save': True, 'no_epoch_checkpoints': True, 'no_last_checkpoints': False, 'no_best_checkpoints': False, 'no_save_optimizer_state': False, 'no_save_optimizer_state_on_training_finished': False, 'symlink_best_and_last_checkpoints': False, 'best_checkpoint_metric': 'loss_valid', 'maximize_best_checkpoint_metric': False, 'patience': -1, 'checkpoint_suffix': '', 'checkpoint_shard_count': 1, 'load_checkpoint_on_all_dp_ranks': False, 'write_checkpoints_asynchronously': False, 's3_upload_path': None, 'model_parallel_size': 1}, 'bmuf': {'_name': None, 'block_lr': 1.0, 'block_momentum': 0.875, 'global_sync_iter': 50, 'warmup_iterations': 500, 'use_nbm': False, 'average_sync': False, 'distributed_world_size': 4}, 'generation': {'_name': None, 'beam': 5, 'nbest': 1, 'max_len_a': 0.0, 'max_len_b': 200, 'min_len': 1, 'match_source_len': False, 'unnormalized': False, 'no_early_stop': False, 'no_beamable_mm': False, 'lenpen': 1.0, 'unkpen': 0.0, 'replace_unk': None, 'sacrebleu': False, 'score_reference': False, 'prefix_size': 0, 'no_repeat_ngram_size': 0, 'sampling': False, 'sampling_topk': -1, 'sampling_topp': -1.0, 'constraints': None, 'temperature': 1.0, 'diverse_beam_groups': -1, 'diverse_beam_strength': 0.5, 'diversity_rate': -1.0, 'print_alignment': None, 'print_step': False, 'lm_path': None, 'lm_weight': 0.0, 'iter_decode_eos_penalty': 0.0, 'iter_decode_max_iter': 10, 'iter_decode_force_max_iter': False, 'iter_decode_with_beam': 1, 'iter_decode_with_external_reranker': False, 'retain_iter_history': False, 'retain_dropout': False, 'retain_dropout_modules': None, 'decoding_format': None, 'no_seed_provided': False}, 'eval_lm': {'_name': None, 'input_quant_method': '', 'input_bits': -1, 'weight_quant_method': '', 'weight_bits': -1, 'smoothquant': False, 'smoothquant_alpha': 0.5, 'smoothquant_bitnet': False, 'input_bits_post': 8, 'hadamard_group': -1, 'cal_input_stat': 'none', 'output_word_probs': False, 'output_word_stats': False, 'context_window': 0, 'softmax_batch': 9223372036854775807, 'stats_path': None, 'max_valid_steps': None}, 'interactive': {'_name': None, 'buffer_size': 0, 'input': '-'}, 'model': {'_name': 'llama_for_layer_wise_qat', 'load_ckpt': None, 'batch_size': 16, 'share_input_output_embed': False, 'sliding_window': None, 'rope_theta': 10000.0, 'checkpoint_activations': False, 'tokens_per_sample': 512, 'model_parallel_size': 1}, 'task': {'_name': 'kd', 'path_to_labels': None, 'data': '/data/gyy/lmquant-main/lmquant/data/data_without_preprocess_llama_7b_bs16', 'tokens_per_sample': 512, 'batch_size_in_quant': 8, 'max_target_positions': None, 'llama_model': None, 'quant_acts_when_training': True, 'tiktoken_model': 'cl100k_base', 'batch_read_ahead': 1, 'pad_to_max_len': True, 'absolute_path': False, 'tokenizer_pad_to_multiple': 1, 'seed': 1, 'batch_size': 16}, 'criterion': {'_name': 'mse', 'sentence_avg': False}, 'optimizer': {'_name': 'adam', 'adam_betas': '(0.9, 0.95)', 'adam_eps': 1e-06, 'weight_decay': 0.05, 'use_old_adam': False, 'fp16_adam_stats': False, 'tpu': False, 'bf16': False, 'lr': [0.005], 'block_wise': False}, 'lr_scheduler': {'_name': 'polynomial_decay', 'warmup_updates': 50, 'force_anneal': None, 'end_learning_rate': 0.0, 'power': 1.0, 'total_num_update': 300000.0, 'lr': [0.005]}, 'scoring': {'_name': 'bleu', 'pad': 1, 'eos': 2, 'unk': 3}, 'bpe': None, 'tokenizer': None}
25-01-23 07:24:12 | I | LlamaModelDecoderLayer(
  (decoder): LlamaDecoderLayerInFairseq(
    (model): LlamaDecoderLayer(
      (self_attn): LlamaSdpaAttention(
        (q_proj): Linear(in_features=4096, out_features=4096, bias=False)
        (k_proj): Linear(in_features=4096, out_features=4096, bias=False)
        (v_proj): Linear(in_features=4096, out_features=4096, bias=False)
        (o_proj): Linear(in_features=4096, out_features=4096, bias=False)
        (rotary_emb): LlamaRotaryEmbedding()
        (q_rotary_emb): RotaryEmbedding()
        (k_rotary_emb): RotaryEmbedding()
      )
      (mlp): LlamaMLP(
        (gate_proj): Linear(in_features=4096, out_features=11008, bias=False)
        (up_proj): Linear(in_features=4096, out_features=11008, bias=False)
        (down_proj): Linear(in_features=11008, out_features=4096, bias=False)
        (act_fn): SiLU()
      )
      (input_layernorm): LlamaRMSNorm((4096,), eps=1e-05)
      (post_attention_layernorm): LlamaRMSNorm((4096,), eps=1e-05)
    )
  )
  (model): LlamaDecoderLayerInFairseq(
    (model): LlamaDecoderLayer(
      (self_attn): LlamaSdpaAttention(
        (q_proj): Linear(in_features=4096, out_features=4096, bias=False)
        (k_proj): Linear(in_features=4096, out_features=4096, bias=False)
        (v_proj): Linear(in_features=4096, out_features=4096, bias=False)
        (o_proj): Linear(in_features=4096, out_features=4096, bias=False)
        (rotary_emb): LlamaRotaryEmbedding()
        (q_rotary_emb): RotaryEmbedding()
        (k_rotary_emb): RotaryEmbedding()
      )
      (mlp): LlamaMLP(
        (gate_proj): Linear(in_features=4096, out_features=11008, bias=False)
        (up_proj): Linear(in_features=4096, out_features=11008, bias=False)
        (down_proj): Linear(in_features=11008, out_features=4096, bias=False)
        (act_fn): SiLU()
      )
      (input_layernorm): LlamaRMSNorm((4096,), eps=1e-05)
      (post_attention_layernorm): LlamaRMSNorm((4096,), eps=1e-05)
    )
  )
)
25-01-23 07:24:12 | I | task: KDTask
25-01-23 07:24:12 | I | model: LlamaModelDecoderLayer
25-01-23 07:24:12 | I | criterion: MSECriterion
25-01-23 07:24:12 | I | num. non-expert model params: 202,383,360 (num. trained: 202,383,360)
25-01-23 07:24:12 | I | num. expert model params: 0 (num. trained: 0)
25-01-23 07:24:12 | I | nvidia-smi stats: {'gpu_0_mem_used_gb': 2.080078125, 'gpu_1_mem_used_gb': 0.001953125, 'gpu_2_mem_used_gb': 0.001953125, 'gpu_3_mem_used_gb': 0.001953125, 'gpu_4_mem_used_gb': 6.150390625, 'gpu_5_mem_used_gb': 7.0859375, 'gpu_6_mem_used_gb': 7.0859375, 'gpu_7_mem_used_gb': 6.06640625}
25-01-23 07:24:12 | I | detected shared parameter: decoder.model.self_attn.q_proj.weight <- model.model.self_attn.q_proj.weight
25-01-23 07:24:12 | I | detected shared parameter: decoder.model.self_attn.q_proj.bias <- decoder.model.self_attn.k_proj.bias
25-01-23 07:24:12 | I | detected shared parameter: decoder.model.self_attn.q_proj.bias <- decoder.model.self_attn.v_proj.bias
25-01-23 07:24:12 | I | detected shared parameter: decoder.model.self_attn.q_proj.bias <- decoder.model.self_attn.o_proj.bias
25-01-23 07:24:12 | I | detected shared parameter: decoder.model.self_attn.q_proj.bias <- decoder.model.mlp.gate_proj.bias
25-01-23 07:24:12 | I | detected shared parameter: decoder.model.self_attn.q_proj.bias <- decoder.model.mlp.up_proj.bias
25-01-23 07:24:12 | I | detected shared parameter: decoder.model.self_attn.q_proj.bias <- decoder.model.mlp.down_proj.bias
25-01-23 07:24:12 | I | detected shared parameter: decoder.model.self_attn.q_proj.bias <- model.model.self_attn.q_proj.bias
25-01-23 07:24:12 | I | detected shared parameter: decoder.model.self_attn.q_proj.bias <- model.model.self_attn.k_proj.bias
25-01-23 07:24:12 | I | detected shared parameter: decoder.model.self_attn.q_proj.bias <- model.model.self_attn.v_proj.bias
25-01-23 07:24:12 | I | detected shared parameter: decoder.model.self_attn.q_proj.bias <- model.model.self_attn.o_proj.bias
25-01-23 07:24:12 | I | detected shared parameter: decoder.model.self_attn.q_proj.bias <- model.model.mlp.gate_proj.bias
25-01-23 07:24:12 | I | detected shared parameter: decoder.model.self_attn.q_proj.bias <- model.model.mlp.up_proj.bias
25-01-23 07:24:12 | I | detected shared parameter: decoder.model.self_attn.q_proj.bias <- model.model.mlp.down_proj.bias
25-01-23 07:24:12 | I | detected shared parameter: decoder.model.self_attn.k_proj.weight <- model.model.self_attn.k_proj.weight
25-01-23 07:24:12 | I | detected shared parameter: decoder.model.self_attn.v_proj.weight <- model.model.self_attn.v_proj.weight
25-01-23 07:24:12 | I | detected shared parameter: decoder.model.self_attn.o_proj.weight <- model.model.self_attn.o_proj.weight
25-01-23 07:24:12 | I | detected shared parameter: decoder.model.mlp.gate_proj.weight <- model.model.mlp.gate_proj.weight
25-01-23 07:24:12 | I | detected shared parameter: decoder.model.mlp.up_proj.weight <- model.model.mlp.up_proj.weight
25-01-23 07:24:12 | I | detected shared parameter: decoder.model.mlp.down_proj.weight <- model.model.mlp.down_proj.weight
25-01-23 07:24:12 | I | detected shared parameter: decoder.model.input_layernorm.weight <- model.model.input_layernorm.weight
25-01-23 07:24:12 | I | detected shared parameter: decoder.model.post_attention_layernorm.weight <- model.model.post_attention_layernorm.weight
25-01-23 07:24:12 | I | nvidia-smi stats: {'gpu_0_mem_used_gb': 2.080078125, 'gpu_1_mem_used_gb': 0.001953125, 'gpu_2_mem_used_gb': 0.001953125, 'gpu_3_mem_used_gb': 0.001953125, 'gpu_4_mem_used_gb': 6.19140625, 'gpu_5_mem_used_gb': 7.126953125, 'gpu_6_mem_used_gb': 7.126953125, 'gpu_7_mem_used_gb': 6.107421875}
25-01-23 07:24:12 | I | ***********************CUDA enviroments for all 1 workers***********************
25-01-23 07:24:12 | I | rank   0: capabilities =  8.6  ; total memory = 23.691 GB ; name = NVIDIA GeForce RTX 3090                 
25-01-23 07:24:12 | I | ***********************CUDA enviroments for all 1 workers***********************
25-01-23 07:24:12 | I | training on 1 devices (GPUs/TPUs)
25-01-23 07:24:12 | I | max tokens per GPU = None and batch size per GPU = 16
25-01-23 07:24:12 | I | nvidia-smi stats: {'gpu_0_mem_used_gb': 2.080078125, 'gpu_1_mem_used_gb': 0.001953125, 'gpu_2_mem_used_gb': 0.001953125, 'gpu_3_mem_used_gb': 0.001953125, 'gpu_4_mem_used_gb': 6.19140625, 'gpu_5_mem_used_gb': 7.126953125, 'gpu_6_mem_used_gb': 7.126953125, 'gpu_7_mem_used_gb': 6.107421875}
25-01-23 07:24:12 | I | loading train data for epoch 1
25-01-23 07:24:14 | I | *** 
 now in /data/gyy/lmquant-main/lmquant/data/data_without_preprocess_llama_1.1b/shard/val/0.jsonl 
 ***
25-01-23 07:24:14 | I | *** 
 total_rows = 700 
 ***
25-01-23 07:24:14 | I | current row num = 0
25-01-23 07:24:14 | I | in forward_and_gen_teacher_outputs, Start iterating over samples
25-01-23 07:24:14 | I | loading train data for epoch 1
25-01-23 07:24:15 | I | *** 
 now in /data/gyy/lmquant-main/lmquant/data/data_without_preprocess_llama_1.1b/shard/val/0.jsonl 
 ***
25-01-23 07:24:15 | I | *** 
 total_rows = 700 
 ***
25-01-23 07:24:15 | I | current row num = 0
25-01-23 07:24:15 | I | in forward_and_gen_args_and_kwargs, Start iterating over samples
25-01-23 07:24:15 | I | No existing checkpoint found checkpoints/checkpoint_last.pt
25-01-23 07:24:15 | I | loading train data for epoch 1
25-01-23 07:24:15 | I | loading valid data for epoch 1
25-01-23 07:24:18 | I | begin training epoch 1
25-01-23 07:24:18 | I | Start iterating over samples
25-01-23 07:24:18 | I | TRAIN CURRENT LAYER_IDX = 0
25-01-23 07:24:18 | I | in layer model.layers.0
25-01-23 07:24:18 | I | quantizing weights for layer model.layers.0
25-01-23 07:24:19 | I | collecting calibration activations in model.layers.0
25-01-23 07:24:19 | I | collecting calibration activations in model.layers.0
25-01-23 07:24:20 | I | - Quantizing decoder layer model.layers.0
25-01-23 07:24:20 | I |   - Calibrating model.layers.0.self_attn.q_proj.weight
25-01-23 07:24:21 | I |       - range scale = [    1.0000]
25-01-23 07:24:21 | I |         sum  error  = [    0.0441]
25-01-23 07:24:21 | I |         best error  = [    0.0441]
25-01-23 07:24:21 | I |     + error = [0.0441]
25-01-23 07:24:22 | I |       - range scale = [    1.0000]
25-01-23 07:24:22 | I |         sum  error  = [    0.6452]
25-01-23 07:24:22 | I |         best error  = [    0.6452]
25-01-23 07:24:22 | I |     + error = [0.6452]
25-01-23 07:24:22 | I |   - Calibrating model.layers.0.self_attn.k_proj.weight
25-01-23 07:24:23 | I |       - range scale = [    1.0000]
25-01-23 07:24:23 | I |         sum  error  = [    0.0524]
25-01-23 07:24:23 | I |         best error  = [    0.0524]
25-01-23 07:24:23 | I |     + error = [0.0524]
25-01-23 07:24:23 | I |       - range scale = [    1.0000]
25-01-23 07:24:23 | I |         sum  error  = [    0.9603]
25-01-23 07:24:23 | I |         best error  = [    0.9603]
25-01-23 07:24:23 | I |     + error = [0.9603]
25-01-23 07:24:23 | I |   - Calibrating model.layers.0.self_attn.v_proj.weight
25-01-23 07:24:24 | I |       - range scale = [    1.0000]
25-01-23 07:24:24 | I |         sum  error  = [    0.6770]
25-01-23 07:24:24 | I |         best error  = [    0.6770]
25-01-23 07:24:24 | I |     + error = [0.6770]
25-01-23 07:24:25 | I |       - range scale = [    1.0000]
25-01-23 07:24:25 | I |         sum  error  = [    6.0392]
25-01-23 07:24:25 | I |         best error  = [    6.0392]
25-01-23 07:24:25 | I |     + error = [6.0392]
25-01-23 07:24:25 | I |   - Calibrating model.layers.0.self_attn.o_proj.weight
25-01-23 07:24:26 | I |       - range scale = [    1.0000]
25-01-23 07:24:26 | I |         sum  error  = [    0.1914]
25-01-23 07:24:26 | I |         best error  = [    0.1914]
25-01-23 07:24:26 | I |     + error = [0.1914]
25-01-23 07:24:27 | I |       - range scale = [    1.0000]
25-01-23 07:24:27 | I |         sum  error  = [    1.5861]
25-01-23 07:24:27 | I |         best error  = [    1.5861]
25-01-23 07:24:27 | I |     + error = [1.5861]
25-01-23 07:24:27 | I |   - Calibrating model.layers.0.mlp.up_proj.weight
25-01-23 07:24:28 | I |       - range scale = [    1.0000]
25-01-23 07:24:28 | I |         sum  error  = [    1.0921]
25-01-23 07:24:28 | I |         best error  = [    1.0921]
25-01-23 07:24:28 | I |     + error = [1.0921]
25-01-23 07:24:29 | I |       - range scale = [    1.0000]
25-01-23 07:24:29 | I |         sum  error  = [   11.6534]
25-01-23 07:24:29 | I |         best error  = [   11.6534]
25-01-23 07:24:29 | I |     + error = [11.6534]
25-01-23 07:24:29 | I |   - Calibrating model.layers.0.mlp.gate_proj.weight
25-01-23 07:24:30 | I |       - range scale = [    1.0000]
25-01-23 07:24:30 | I |         sum  error  = [    1.1500]
25-01-23 07:24:30 | I |         best error  = [    1.1500]
25-01-23 07:24:30 | I |     + error = [1.1500]
25-01-23 07:24:31 | I |       - range scale = [    1.0000]
25-01-23 07:24:31 | I |         sum  error  = [   11.9849]
25-01-23 07:24:31 | I |         best error  = [   11.9849]
25-01-23 07:24:31 | I |     + error = [11.9849]
25-01-23 07:24:31 | I |   - Calibrating model.layers.0.mlp.down_proj.weight
25-01-23 07:24:32 | I |       - range scale = [    1.0000]
25-01-23 07:24:32 | I |         sum  error  = [    0.1764]
25-01-23 07:24:32 | I |         best error  = [    0.1764]
25-01-23 07:24:32 | I |     + error = [0.1764]
25-01-23 07:24:33 | I |       - range scale = [    1.0000]
25-01-23 07:24:33 | I |         sum  error  = [    1.7234]
25-01-23 07:24:33 | I |         best error  = [    1.7234]
25-01-23 07:24:33 | I |     + error = [1.7234]
25-01-23 07:24:33 | I |   - Quantizing model.layers.0.self_attn.q_proj.weight
25-01-23 07:24:35 | I |   - Quantizing model.layers.0.self_attn.k_proj.weight
25-01-23 07:24:38 | I |   - Quantizing model.layers.0.self_attn.v_proj.weight
25-01-23 07:24:40 | I |   - Quantizing model.layers.0.self_attn.o_proj.weight
25-01-23 07:24:43 | I |   - Quantizing model.layers.0.mlp.up_proj.weight
25-01-23 07:24:46 | I |   - Quantizing model.layers.0.mlp.gate_proj.weight
25-01-23 07:24:49 | I |   - Quantizing model.layers.0.mlp.down_proj.weight
25-01-23 07:24:56 | I | quantizing activations for layer model.layers.0
25-01-23 07:24:57 | I | collecting calibration activations in model.layers.0
25-01-23 07:24:57 | I | collecting calibration activations in model.layers.0
25-01-23 07:24:59 | I | forward this layer
25-01-23 07:24:59 | I | input_file: /data/gyy/lmquant-main/lmquant/data/data_without_preprocess_llama_7b_bs16/shard/val_input_args_layer_0/00.pt
25-01-23 07:24:59 | I | output_file: /data/gyy/lmquant-main/lmquant/data/data_without_preprocess_llama_7b_bs16/shard/val_teacher_output_layer_0/00.pt
25-01-23 07:24:59 | I | In layer model.layers.0, gradient of model.layers.0.self_attn.q_proj has nan: False
25-01-23 07:24:59 | I | In layer model.layers.0, gradient of model.layers.0.self_attn.q_proj has inf: False
25-01-23 07:24:59 | I | In layer model.layers.0, gradient of model.layers.0.self_attn.k_proj has nan: False
25-01-23 07:24:59 | I | In layer model.layers.0, gradient of model.layers.0.self_attn.k_proj has inf: False
25-01-23 07:24:59 | I | In layer model.layers.0, gradient of model.layers.0.self_attn.v_proj has nan: False
25-01-23 07:24:59 | I | In layer model.layers.0, gradient of model.layers.0.self_attn.v_proj has inf: True
25-01-23 07:24:59 | I | inf: first position tensor([  70, 1411], device='cuda:0')
25-01-23 07:24:59 | I | In layer model.layers.0, gradient of model.layers.0.self_attn.o_proj has nan: False
25-01-23 07:24:59 | I | In layer model.layers.0, gradient of model.layers.0.self_attn.o_proj has inf: False
25-01-23 07:24:59 | I | In layer model.layers.0, gradient of model.layers.0.mlp.up_proj has nan: False
25-01-23 07:24:59 | I | In layer model.layers.0, gradient of model.layers.0.mlp.up_proj has inf: False
25-01-23 07:24:59 | I | In layer model.layers.0, gradient of model.layers.0.mlp.gate_proj has nan: False
25-01-23 07:24:59 | I | In layer model.layers.0, gradient of model.layers.0.mlp.gate_proj has inf: False
25-01-23 07:24:59 | I | In layer model.layers.0, gradient of model.layers.0.mlp.down_proj has nan: False
25-01-23 07:24:59 | I | In layer model.layers.0, gradient of model.layers.0.mlp.down_proj has inf: False
25-01-23 07:24:59 | I | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 64.0
25-01-23 07:25:00 | I | TRAIN CURRENT LAYER_IDX = 0
25-01-23 07:25:00 | I | in layer model.layers.0
25-01-23 07:25:00 | I | quantizing weights for layer model.layers.0
25-01-23 07:25:00 | I | collecting calibration activations in model.layers.0
25-01-23 07:25:00 | I | collecting calibration activations in model.layers.0
25-01-23 07:25:01 | I | - Quantizing decoder layer model.layers.0
25-01-23 07:25:01 | I |   - Calibrating model.layers.0.self_attn.q_proj.weight
25-01-23 07:25:01 | I |       - range scale = [    1.0000]
25-01-23 07:25:01 | I |         sum  error  = [    0.0434]
25-01-23 07:25:01 | I |         best error  = [    0.0434]
25-01-23 07:25:01 | I |     + error = [0.0434]
25-01-23 07:25:02 | I |       - range scale = [    1.0000]
25-01-23 07:25:02 | I |         sum  error  = [    0.6479]
25-01-23 07:25:02 | I |         best error  = [    0.6479]
25-01-23 07:25:02 | I |     + error = [0.6479]
25-01-23 07:25:02 | I |   - Calibrating model.layers.0.self_attn.k_proj.weight
25-01-23 07:25:03 | I |       - range scale = [    1.0000]
25-01-23 07:25:03 | I |         sum  error  = [    0.0528]
25-01-23 07:25:03 | I |         best error  = [    0.0528]
25-01-23 07:25:03 | I |     + error = [0.0528]
25-01-23 07:25:03 | I |       - range scale = [    1.0000]
25-01-23 07:25:03 | I |         sum  error  = [    0.9676]
25-01-23 07:25:03 | I |         best error  = [    0.9676]
25-01-23 07:25:03 | I |     + error = [0.9676]
25-01-23 07:25:04 | I |   - Calibrating model.layers.0.self_attn.v_proj.weight
25-01-23 07:25:04 | I |       - range scale = [    1.0000]
25-01-23 07:25:04 | I |         sum  error  = [    0.6746]
25-01-23 07:25:04 | I |         best error  = [    0.6746]
25-01-23 07:25:04 | I |     + error = [0.6746]
25-01-23 07:25:05 | I |       - range scale = [    1.0000]
25-01-23 07:25:05 | I |         sum  error  = [    6.0677]
25-01-23 07:25:05 | I |         best error  = [    6.0677]
25-01-23 07:25:05 | I |     + error = [6.0677]
25-01-23 07:25:05 | I |   - Calibrating model.layers.0.self_attn.o_proj.weight
25-01-23 07:25:06 | I |       - range scale = [    1.0000]
25-01-23 07:25:06 | I |         sum  error  = [    0.1921]
25-01-23 07:25:06 | I |         best error  = [    0.1921]
25-01-23 07:25:06 | I |     + error = [0.1921]
25-01-23 07:25:07 | I |       - range scale = [    1.0000]
25-01-23 07:25:07 | I |         sum  error  = [    1.5878]
25-01-23 07:25:07 | I |         best error  = [    1.5878]
25-01-23 07:25:07 | I |     + error = [1.5878]
25-01-23 07:25:07 | I |   - Calibrating model.layers.0.mlp.up_proj.weight
25-01-23 07:25:08 | I |       - range scale = [    1.0000]
25-01-23 07:25:08 | I |         sum  error  = [    1.1089]
25-01-23 07:25:08 | I |         best error  = [    1.1089]
25-01-23 07:25:08 | I |     + error = [1.1089]
25-01-23 07:25:09 | I |       - range scale = [    1.0000]
25-01-23 07:25:09 | I |         sum  error  = [   11.8233]
25-01-23 07:25:09 | I |         best error  = [   11.8233]
25-01-23 07:25:09 | I |     + error = [11.8233]
25-01-23 07:25:09 | I |   - Calibrating model.layers.0.mlp.gate_proj.weight
25-01-23 07:25:10 | I |       - range scale = [    1.0000]
25-01-23 07:25:10 | I |         sum  error  = [    1.1697]
25-01-23 07:25:10 | I |         best error  = [    1.1697]
25-01-23 07:25:10 | I |     + error = [1.1697]
25-01-23 07:25:11 | I |       - range scale = [    1.0000]
25-01-23 07:25:11 | I |         sum  error  = [   12.1586]
25-01-23 07:25:11 | I |         best error  = [   12.1586]
25-01-23 07:25:11 | I |     + error = [12.1586]
25-01-23 07:25:11 | I |   - Calibrating model.layers.0.mlp.down_proj.weight
25-01-23 07:25:12 | I |       - range scale = [    1.0000]
25-01-23 07:25:12 | I |         sum  error  = [    0.1731]
25-01-23 07:25:12 | I |         best error  = [    0.1731]
25-01-23 07:25:12 | I |     + error = [0.1731]
25-01-23 07:25:13 | I |       - range scale = [    1.0000]
25-01-23 07:25:13 | I |         sum  error  = [    1.6879]
25-01-23 07:25:13 | I |         best error  = [    1.6879]
25-01-23 07:25:13 | I |     + error = [1.6879]
25-01-23 07:25:13 | I |   - Quantizing model.layers.0.self_attn.q_proj.weight
25-01-23 07:25:16 | I |   - Quantizing model.layers.0.self_attn.k_proj.weight
25-01-23 07:25:19 | I |   - Quantizing model.layers.0.self_attn.v_proj.weight
25-01-23 07:25:21 | I |   - Quantizing model.layers.0.self_attn.o_proj.weight
25-01-23 07:25:24 | I |   - Quantizing model.layers.0.mlp.up_proj.weight
25-01-23 07:25:27 | I |   - Quantizing model.layers.0.mlp.gate_proj.weight
25-01-23 07:25:30 | I |   - Quantizing model.layers.0.mlp.down_proj.weight
25-01-23 07:25:38 | I | quantizing activations for layer model.layers.0
25-01-23 07:25:38 | I | collecting calibration activations in model.layers.0
25-01-23 07:25:39 | I | collecting calibration activations in model.layers.0
25-01-23 07:25:41 | I | forward this layer
25-01-23 07:25:41 | I | input_file: /data/gyy/lmquant-main/lmquant/data/data_without_preprocess_llama_7b_bs16/shard/val_input_args_layer_0/01.pt
25-01-23 07:25:41 | I | output_file: /data/gyy/lmquant-main/lmquant/data/data_without_preprocess_llama_7b_bs16/shard/val_teacher_output_layer_0/01.pt
25-01-23 07:25:41 | I | In layer model.layers.0, gradient of model.layers.0.self_attn.q_proj has nan: False
25-01-23 07:25:41 | I | In layer model.layers.0, gradient of model.layers.0.self_attn.q_proj has inf: False
25-01-23 07:25:41 | I | In layer model.layers.0, gradient of model.layers.0.self_attn.k_proj has nan: False
25-01-23 07:25:41 | I | In layer model.layers.0, gradient of model.layers.0.self_attn.k_proj has inf: False
25-01-23 07:25:41 | I | In layer model.layers.0, gradient of model.layers.0.self_attn.v_proj has nan: False
25-01-23 07:25:41 | I | In layer model.layers.0, gradient of model.layers.0.self_attn.v_proj has inf: True
25-01-23 07:25:41 | I | inf: first position tensor([386,  22], device='cuda:0')
25-01-23 07:25:41 | I | In layer model.layers.0, gradient of model.layers.0.self_attn.o_proj has nan: False
25-01-23 07:25:41 | I | In layer model.layers.0, gradient of model.layers.0.self_attn.o_proj has inf: False
25-01-23 07:25:41 | I | In layer model.layers.0, gradient of model.layers.0.mlp.up_proj has nan: False
25-01-23 07:25:41 | I | In layer model.layers.0, gradient of model.layers.0.mlp.up_proj has inf: False
25-01-23 07:25:41 | I | In layer model.layers.0, gradient of model.layers.0.mlp.gate_proj has nan: False
25-01-23 07:25:41 | I | In layer model.layers.0, gradient of model.layers.0.mlp.gate_proj has inf: False
25-01-23 07:25:41 | I | In layer model.layers.0, gradient of model.layers.0.mlp.down_proj has nan: False
25-01-23 07:25:41 | I | In layer model.layers.0, gradient of model.layers.0.mlp.down_proj has inf: False
25-01-23 07:25:41 | I | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 32.0
25-01-23 07:25:41 | I | TRAIN CURRENT LAYER_IDX = 0
25-01-23 07:25:41 | I | in layer model.layers.0
25-01-23 07:25:41 | I | quantizing weights for layer model.layers.0
25-01-23 07:25:42 | I | collecting calibration activations in model.layers.0
25-01-23 07:25:42 | I | collecting calibration activations in model.layers.0
25-01-23 07:25:42 | I | - Quantizing decoder layer model.layers.0
25-01-23 07:25:42 | I |   - Calibrating model.layers.0.self_attn.q_proj.weight
25-01-23 07:25:43 | I |       - range scale = [    1.0000]
25-01-23 07:25:43 | I |         sum  error  = [    0.0418]
25-01-23 07:25:43 | I |         best error  = [    0.0418]
25-01-23 07:25:43 | I |     + error = [0.0418]
25-01-23 07:25:44 | I |       - range scale = [    1.0000]
25-01-23 07:25:44 | I |         sum  error  = [    0.6217]
25-01-23 07:25:44 | I |         best error  = [    0.6217]
25-01-23 07:25:44 | I |     + error = [0.6217]
25-01-23 07:25:44 | I |   - Calibrating model.layers.0.self_attn.k_proj.weight
25-01-23 07:25:45 | I |       - range scale = [    1.0000]
25-01-23 07:25:45 | I |         sum  error  = [    0.0490]
25-01-23 07:25:45 | I |         best error  = [    0.0490]
25-01-23 07:25:45 | I |     + error = [0.0490]
25-01-23 07:25:45 | I |       - range scale = [    1.0000]
25-01-23 07:25:45 | I |         sum  error  = [    0.9608]
25-01-23 07:25:45 | I |         best error  = [    0.9608]
25-01-23 07:25:45 | I |     + error = [0.9608]
25-01-23 07:25:45 | I |   - Calibrating model.layers.0.self_attn.v_proj.weight
25-01-23 07:25:46 | I |       - range scale = [    1.0000]
25-01-23 07:25:46 | I |         sum  error  = [    0.6681]
25-01-23 07:25:46 | I |         best error  = [    0.6681]
25-01-23 07:25:46 | I |     + error = [0.6681]
25-01-23 07:25:47 | I |       - range scale = [    1.0000]
25-01-23 07:25:47 | I |         sum  error  = [    6.0241]
25-01-23 07:25:47 | I |         best error  = [    6.0241]
25-01-23 07:25:47 | I |     + error = [6.0241]
25-01-23 07:25:47 | I |   - Calibrating model.layers.0.self_attn.o_proj.weight
25-01-23 07:25:48 | I |       - range scale = [    1.0000]
25-01-23 07:25:48 | I |         sum  error  = [    0.1875]
25-01-23 07:25:48 | I |         best error  = [    0.1875]
25-01-23 07:25:48 | I |     + error = [0.1875]
25-01-23 07:25:49 | I |       - range scale = [    1.0000]
25-01-23 07:25:49 | I |         sum  error  = [    1.5419]
25-01-23 07:25:49 | I |         best error  = [    1.5419]
25-01-23 07:25:49 | I |     + error = [1.5419]
25-01-23 07:25:49 | I |   - Calibrating model.layers.0.mlp.up_proj.weight
25-01-23 07:25:50 | I |       - range scale = [    1.0000]
25-01-23 07:25:50 | I |         sum  error  = [    1.1100]
25-01-23 07:25:50 | I |         best error  = [    1.1100]
25-01-23 07:25:50 | I |     + error = [1.1100]
25-01-23 07:25:51 | I |       - range scale = [    1.0000]
25-01-23 07:25:51 | I |         sum  error  = [   11.8287]
25-01-23 07:25:51 | I |         best error  = [   11.8287]
25-01-23 07:25:51 | I |     + error = [11.8287]
25-01-23 07:25:51 | I |   - Calibrating model.layers.0.mlp.gate_proj.weight
25-01-23 07:25:51 | I |       - range scale = [    1.0000]
25-01-23 07:25:51 | I |         sum  error  = [    1.1669]
25-01-23 07:25:51 | I |         best error  = [    1.1669]
25-01-23 07:25:51 | I |     + error = [1.1669]
25-01-23 07:25:52 | I |       - range scale = [    1.0000]
25-01-23 07:25:52 | I |         sum  error  = [   12.1587]
25-01-23 07:25:52 | I |         best error  = [   12.1587]
25-01-23 07:25:52 | I |     + error = [12.1587]
25-01-23 07:25:53 | I |   - Calibrating model.layers.0.mlp.down_proj.weight
25-01-23 07:25:53 | I |       - range scale = [    1.0000]
25-01-23 07:25:53 | I |         sum  error  = [    0.1733]
25-01-23 07:25:53 | I |         best error  = [    0.1733]
25-01-23 07:25:53 | I |     + error = [0.1733]
25-01-23 07:25:54 | I |       - range scale = [    1.0000]
25-01-23 07:25:54 | I |         sum  error  = [    1.6914]
25-01-23 07:25:54 | I |         best error  = [    1.6914]
25-01-23 07:25:54 | I |     + error = [1.6914]
25-01-23 07:25:55 | I |   - Quantizing model.layers.0.self_attn.q_proj.weight
25-01-23 07:25:58 | I |   - Quantizing model.layers.0.self_attn.k_proj.weight
25-01-23 07:26:00 | I |   - Quantizing model.layers.0.self_attn.v_proj.weight
25-01-23 07:26:03 | I |   - Quantizing model.layers.0.self_attn.o_proj.weight
25-01-23 07:26:06 | I |   - Quantizing model.layers.0.mlp.up_proj.weight
25-01-23 07:26:09 | I |   - Quantizing model.layers.0.mlp.gate_proj.weight
25-01-23 07:26:12 | I |   - Quantizing model.layers.0.mlp.down_proj.weight
25-01-23 07:26:18 | I | quantizing activations for layer model.layers.0
25-01-23 07:26:19 | I | collecting calibration activations in model.layers.0
25-01-23 07:26:19 | I | collecting calibration activations in model.layers.0
25-01-23 07:26:21 | I | forward this layer
25-01-23 07:26:21 | I | input_file: /data/gyy/lmquant-main/lmquant/data/data_without_preprocess_llama_7b_bs16/shard/val_input_args_layer_0/02.pt
25-01-23 07:26:21 | I | output_file: /data/gyy/lmquant-main/lmquant/data/data_without_preprocess_llama_7b_bs16/shard/val_teacher_output_layer_0/02.pt
25-01-23 07:26:21 | I | In layer model.layers.0, gradient of model.layers.0.self_attn.q_proj has nan: False
25-01-23 07:26:21 | I | In layer model.layers.0, gradient of model.layers.0.self_attn.q_proj has inf: False
25-01-23 07:26:21 | I | In layer model.layers.0, gradient of model.layers.0.self_attn.k_proj has nan: False
25-01-23 07:26:21 | I | In layer model.layers.0, gradient of model.layers.0.self_attn.k_proj has inf: False
25-01-23 07:26:21 | I | In layer model.layers.0, gradient of model.layers.0.self_attn.v_proj has nan: False
25-01-23 07:26:21 | I | In layer model.layers.0, gradient of model.layers.0.self_attn.v_proj has inf: True
25-01-23 07:26:21 | I | inf: first position tensor([ 430, 3964], device='cuda:0')
25-01-23 07:26:21 | I | In layer model.layers.0, gradient of model.layers.0.self_attn.o_proj has nan: False
25-01-23 07:26:21 | I | In layer model.layers.0, gradient of model.layers.0.self_attn.o_proj has inf: False
25-01-23 07:26:21 | I | In layer model.layers.0, gradient of model.layers.0.mlp.up_proj has nan: False
25-01-23 07:26:21 | I | In layer model.layers.0, gradient of model.layers.0.mlp.up_proj has inf: False
25-01-23 07:26:21 | I | In layer model.layers.0, gradient of model.layers.0.mlp.gate_proj has nan: False
25-01-23 07:26:21 | I | In layer model.layers.0, gradient of model.layers.0.mlp.gate_proj has inf: False
25-01-23 07:26:21 | I | In layer model.layers.0, gradient of model.layers.0.mlp.down_proj has nan: False
25-01-23 07:26:21 | I | In layer model.layers.0, gradient of model.layers.0.mlp.down_proj has inf: False
25-01-23 07:26:21 | I | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
25-01-23 07:26:21 | I | TRAIN CURRENT LAYER_IDX = 0
25-01-23 07:26:21 | I | in layer model.layers.0
25-01-23 07:26:21 | I | quantizing weights for layer model.layers.0
25-01-23 07:26:22 | I | collecting calibration activations in model.layers.0
25-01-23 07:26:22 | I | collecting calibration activations in model.layers.0
25-01-23 07:26:22 | I | - Quantizing decoder layer model.layers.0
25-01-23 07:26:22 | I |   - Calibrating model.layers.0.self_attn.q_proj.weight
25-01-23 07:26:23 | I |       - range scale = [    1.0000]
25-01-23 07:26:23 | I |         sum  error  = [    0.0452]
25-01-23 07:26:23 | I |         best error  = [    0.0452]
25-01-23 07:26:23 | I |     + error = [0.0452]
25-01-23 07:26:24 | I |       - range scale = [    1.0000]
25-01-23 07:26:24 | I |         sum  error  = [    0.6561]
25-01-23 07:26:24 | I |         best error  = [    0.6561]
25-01-23 07:26:24 | I |     + error = [0.6561]
25-01-23 07:26:24 | I |   - Calibrating model.layers.0.self_attn.k_proj.weight
25-01-23 07:26:25 | I |       - range scale = [    1.0000]
25-01-23 07:26:25 | I |         sum  error  = [    0.0543]
25-01-23 07:26:25 | I |         best error  = [    0.0543]
25-01-23 07:26:25 | I |     + error = [0.0543]
25-01-23 07:26:25 | I |       - range scale = [    1.0000]
25-01-23 07:26:25 | I |         sum  error  = [    0.9254]
25-01-23 07:26:25 | I |         best error  = [    0.9254]
25-01-23 07:26:25 | I |     + error = [0.9254]
25-01-23 07:26:26 | I |   - Calibrating model.layers.0.self_attn.v_proj.weight
25-01-23 07:26:26 | I |       - range scale = [    1.0000]
25-01-23 07:26:26 | I |         sum  error  = [    0.7058]
25-01-23 07:26:26 | I |         best error  = [    0.7058]
25-01-23 07:26:26 | I |     + error = [0.7058]
25-01-23 07:26:27 | I |       - range scale = [    1.0000]
25-01-23 07:26:27 | I |         sum  error  = [    6.3541]
25-01-23 07:26:27 | I |         best error  = [    6.3541]
25-01-23 07:26:27 | I |     + error = [6.3541]
25-01-23 07:26:27 | I |   - Calibrating model.layers.0.self_attn.o_proj.weight
25-01-23 07:26:28 | I |       - range scale = [    1.0000]
25-01-23 07:26:28 | I |         sum  error  = [    0.1958]
25-01-23 07:26:28 | I |         best error  = [    0.1958]
25-01-23 07:26:28 | I |     + error = [0.1958]
25-01-23 07:26:29 | I |       - range scale = [    1.0000]
25-01-23 07:26:29 | I |         sum  error  = [    1.6216]
25-01-23 07:26:29 | I |         best error  = [    1.6216]
25-01-23 07:26:29 | I |     + error = [1.6216]
25-01-23 07:26:29 | I |   - Calibrating model.layers.0.mlp.up_proj.weight
25-01-23 07:26:30 | I |       - range scale = [    1.0000]
25-01-23 07:26:30 | I |         sum  error  = [    1.0875]
25-01-23 07:26:30 | I |         best error  = [    1.0875]
25-01-23 07:26:30 | I |     + error = [1.0875]
25-01-23 07:26:31 | I |       - range scale = [    1.0000]
25-01-23 07:26:31 | I |         sum  error  = [   11.5986]
25-01-23 07:26:31 | I |         best error  = [   11.5986]
25-01-23 07:26:31 | I |     + error = [11.5986]
25-01-23 07:26:31 | I |   - Calibrating model.layers.0.mlp.gate_proj.weight
25-01-23 07:26:32 | I |       - range scale = [    1.0000]
25-01-23 07:26:32 | I |         sum  error  = [    1.1464]
25-01-23 07:26:32 | I |         best error  = [    1.1464]
25-01-23 07:26:32 | I |     + error = [1.1464]
25-01-23 07:26:33 | I |       - range scale = [    1.0000]
25-01-23 07:26:33 | I |         sum  error  = [   11.9272]
25-01-23 07:26:33 | I |         best error  = [   11.9272]
25-01-23 07:26:33 | I |     + error = [11.9272]
25-01-23 07:26:33 | I |   - Calibrating model.layers.0.mlp.down_proj.weight
25-01-23 07:26:34 | I |       - range scale = [    1.0000]
25-01-23 07:26:34 | I |         sum  error  = [    0.1788]
25-01-23 07:26:34 | I |         best error  = [    0.1788]
25-01-23 07:26:34 | I |     + error = [0.1788]
25-01-23 07:26:35 | I |       - range scale = [    1.0000]
25-01-23 07:26:35 | I |         sum  error  = [    1.7479]
25-01-23 07:26:35 | I |         best error  = [    1.7479]
25-01-23 07:26:35 | I |     + error = [1.7479]
25-01-23 07:26:35 | I |   - Quantizing model.layers.0.self_attn.q_proj.weight
25-01-23 07:26:37 | I |   - Quantizing model.layers.0.self_attn.k_proj.weight
25-01-23 07:26:40 | I |   - Quantizing model.layers.0.self_attn.v_proj.weight
25-01-23 07:26:42 | I |   - Quantizing model.layers.0.self_attn.o_proj.weight
25-01-23 07:26:44 | I |   - Quantizing model.layers.0.mlp.up_proj.weight
25-01-23 07:26:46 | I |   - Quantizing model.layers.0.mlp.gate_proj.weight
25-01-23 07:26:49 | I |   - Quantizing model.layers.0.mlp.down_proj.weight
25-01-23 07:26:55 | I | quantizing activations for layer model.layers.0
25-01-23 07:26:55 | I | collecting calibration activations in model.layers.0
25-01-23 07:26:55 | I | collecting calibration activations in model.layers.0
25-01-23 07:26:57 | I | forward this layer
25-01-23 07:26:57 | I | input_file: /data/gyy/lmquant-main/lmquant/data/data_without_preprocess_llama_7b_bs16/shard/val_input_args_layer_0/03.pt
25-01-23 07:26:57 | I | output_file: /data/gyy/lmquant-main/lmquant/data/data_without_preprocess_llama_7b_bs16/shard/val_teacher_output_layer_0/03.pt
25-01-23 07:26:58 | I | In layer model.layers.0, gradient of model.layers.0.self_attn.q_proj has nan: False
25-01-23 07:26:58 | I | In layer model.layers.0, gradient of model.layers.0.self_attn.q_proj has inf: False
25-01-23 07:26:58 | I | In layer model.layers.0, gradient of model.layers.0.self_attn.k_proj has nan: False
25-01-23 07:26:58 | I | In layer model.layers.0, gradient of model.layers.0.self_attn.k_proj has inf: False
25-01-23 07:26:58 | I | In layer model.layers.0, gradient of model.layers.0.self_attn.v_proj has nan: False
25-01-23 07:26:58 | I | In layer model.layers.0, gradient of model.layers.0.self_attn.v_proj has inf: False
25-01-23 07:26:58 | I | In layer model.layers.0, gradient of model.layers.0.self_attn.o_proj has nan: False
25-01-23 07:26:58 | I | In layer model.layers.0, gradient of model.layers.0.self_attn.o_proj has inf: False
25-01-23 07:26:58 | I | In layer model.layers.0, gradient of model.layers.0.mlp.up_proj has nan: False
25-01-23 07:26:58 | I | In layer model.layers.0, gradient of model.layers.0.mlp.up_proj has inf: False
25-01-23 07:26:58 | I | In layer model.layers.0, gradient of model.layers.0.mlp.gate_proj has nan: False
25-01-23 07:26:58 | I | In layer model.layers.0, gradient of model.layers.0.mlp.gate_proj has inf: False
25-01-23 07:26:58 | I | In layer model.layers.0, gradient of model.layers.0.mlp.down_proj has nan: False
25-01-23 07:26:58 | I | In layer model.layers.0, gradient of model.layers.0.mlp.down_proj has inf: False
25-01-23 07:26:58 | I | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
25-01-23 07:26:58 | I | TRAIN CURRENT LAYER_IDX = 0
25-01-23 07:26:58 | I | in layer model.layers.0
25-01-23 07:26:58 | I | quantizing weights for layer model.layers.0
25-01-23 07:26:58 | I | collecting calibration activations in model.layers.0
25-01-23 07:26:58 | I | collecting calibration activations in model.layers.0
25-01-23 07:26:59 | I | - Quantizing decoder layer model.layers.0
25-01-23 07:26:59 | I |   - Calibrating model.layers.0.self_attn.q_proj.weight
25-01-23 07:26:59 | I |       - range scale = [    1.0000]
25-01-23 07:26:59 | I |         sum  error  = [    0.0414]
25-01-23 07:26:59 | I |         best error  = [    0.0414]
25-01-23 07:26:59 | I |     + error = [0.0414]
25-01-23 07:27:00 | I |       - range scale = [    1.0000]
25-01-23 07:27:00 | I |         sum  error  = [    0.6158]
25-01-23 07:27:00 | I |         best error  = [    0.6158]
25-01-23 07:27:00 | I |     + error = [0.6158]
25-01-23 07:27:00 | I |   - Calibrating model.layers.0.self_attn.k_proj.weight
25-01-23 07:27:01 | I |       - range scale = [    1.0000]
25-01-23 07:27:01 | I |         sum  error  = [    0.0490]
25-01-23 07:27:01 | I |         best error  = [    0.0490]
25-01-23 07:27:01 | I |     + error = [0.0490]
25-01-23 07:27:02 | I |       - range scale = [    1.0000]
25-01-23 07:27:02 | I |         sum  error  = [    0.9699]
25-01-23 07:27:02 | I |         best error  = [    0.9699]
25-01-23 07:27:02 | I |     + error = [0.9699]
25-01-23 07:27:02 | I |   - Calibrating model.layers.0.self_attn.v_proj.weight
25-01-23 07:27:03 | I |       - range scale = [    1.0000]
25-01-23 07:27:03 | I |         sum  error  = [    0.6681]
25-01-23 07:27:03 | I |         best error  = [    0.6681]
25-01-23 07:27:03 | I |     + error = [0.6681]
25-01-23 07:27:03 | I |       - range scale = [    1.0000]
25-01-23 07:27:03 | I |         sum  error  = [    6.0124]
25-01-23 07:27:03 | I |         best error  = [    6.0124]
25-01-23 07:27:03 | I |     + error = [6.0124]
25-01-23 07:27:04 | I |   - Calibrating model.layers.0.self_attn.o_proj.weight
25-01-23 07:27:04 | I |       - range scale = [    1.0000]
25-01-23 07:27:04 | I |         sum  error  = [    0.1919]
25-01-23 07:27:04 | I |         best error  = [    0.1919]
25-01-23 07:27:04 | I |     + error = [0.1919]
25-01-23 07:27:05 | I |       - range scale = [    1.0000]
25-01-23 07:27:05 | I |         sum  error  = [    1.5882]
25-01-23 07:27:05 | I |         best error  = [    1.5882]
25-01-23 07:27:05 | I |     + error = [1.5882]
25-01-23 07:27:05 | I |   - Calibrating model.layers.0.mlp.up_proj.weight
25-01-23 07:27:06 | I |       - range scale = [    1.0000]
25-01-23 07:27:06 | I |         sum  error  = [    1.1170]
25-01-23 07:27:06 | I |         best error  = [    1.1170]
25-01-23 07:27:06 | I |     + error = [1.1170]
25-01-23 07:27:07 | I |       - range scale = [    1.0000]
25-01-23 07:27:07 | I |         sum  error  = [   11.9033]
25-01-23 07:27:07 | I |         best error  = [   11.9033]
25-01-23 07:27:07 | I |     + error = [11.9033]
25-01-23 07:27:07 | I |   - Calibrating model.layers.0.mlp.gate_proj.weight
25-01-23 07:27:08 | I |       - range scale = [    1.0000]
25-01-23 07:27:08 | I |         sum  error  = [    1.1762]
25-01-23 07:27:08 | I |         best error  = [    1.1762]
25-01-23 07:27:08 | I |     + error = [1.1762]
25-01-23 07:27:09 | I |       - range scale = [    1.0000]
25-01-23 07:27:09 | I |         sum  error  = [   12.2417]
25-01-23 07:27:09 | I |         best error  = [   12.2417]
25-01-23 07:27:09 | I |     + error = [12.2417]
25-01-23 07:27:09 | I |   - Calibrating model.layers.0.mlp.down_proj.weight
25-01-23 07:27:10 | I |       - range scale = [    1.0000]
25-01-23 07:27:10 | I |         sum  error  = [    0.1732]
25-01-23 07:27:10 | I |         best error  = [    0.1732]
25-01-23 07:27:10 | I |     + error = [0.1732]
25-01-23 07:27:11 | I |       - range scale = [    1.0000]
25-01-23 07:27:11 | I |         sum  error  = [    1.6907]
25-01-23 07:27:11 | I |         best error  = [    1.6907]
25-01-23 07:27:11 | I |     + error = [1.6907]
25-01-23 07:27:11 | I |   - Quantizing model.layers.0.self_attn.q_proj.weight
25-01-23 07:27:14 | I |   - Quantizing model.layers.0.self_attn.k_proj.weight
25-01-23 07:27:16 | I |   - Quantizing model.layers.0.self_attn.v_proj.weight
25-01-23 07:27:18 | I |   - Quantizing model.layers.0.self_attn.o_proj.weight
25-01-23 07:27:20 | I |   - Quantizing model.layers.0.mlp.up_proj.weight
25-01-23 07:27:23 | I |   - Quantizing model.layers.0.mlp.gate_proj.weight
25-01-23 07:27:25 | I |   - Quantizing model.layers.0.mlp.down_proj.weight
25-01-23 07:27:31 | I | quantizing activations for layer model.layers.0
25-01-23 07:27:31 | I | collecting calibration activations in model.layers.0
25-01-23 07:27:32 | I | collecting calibration activations in model.layers.0
25-01-23 07:27:34 | I | forward this layer
25-01-23 07:27:34 | I | input_file: /data/gyy/lmquant-main/lmquant/data/data_without_preprocess_llama_7b_bs16/shard/val_input_args_layer_0/04.pt
25-01-23 07:27:34 | I | output_file: /data/gyy/lmquant-main/lmquant/data/data_without_preprocess_llama_7b_bs16/shard/val_teacher_output_layer_0/04.pt
25-01-23 07:27:34 | I | In layer model.layers.0, gradient of model.layers.0.self_attn.q_proj has nan: False
25-01-23 07:27:34 | I | In layer model.layers.0, gradient of model.layers.0.self_attn.q_proj has inf: False
25-01-23 07:27:34 | I | In layer model.layers.0, gradient of model.layers.0.self_attn.k_proj has nan: False
25-01-23 07:27:34 | I | In layer model.layers.0, gradient of model.layers.0.self_attn.k_proj has inf: False
25-01-23 07:27:34 | I | In layer model.layers.0, gradient of model.layers.0.self_attn.v_proj has nan: False
25-01-23 07:27:34 | I | In layer model.layers.0, gradient of model.layers.0.self_attn.v_proj has inf: False
25-01-23 07:27:34 | I | In layer model.layers.0, gradient of model.layers.0.self_attn.o_proj has nan: False
25-01-23 07:27:34 | I | In layer model.layers.0, gradient of model.layers.0.self_attn.o_proj has inf: False
25-01-23 07:27:34 | I | In layer model.layers.0, gradient of model.layers.0.mlp.up_proj has nan: False
25-01-23 07:27:34 | I | In layer model.layers.0, gradient of model.layers.0.mlp.up_proj has inf: False
25-01-23 07:27:34 | I | In layer model.layers.0, gradient of model.layers.0.mlp.gate_proj has nan: False
25-01-23 07:27:34 | I | In layer model.layers.0, gradient of model.layers.0.mlp.gate_proj has inf: False
25-01-23 07:27:34 | I | In layer model.layers.0, gradient of model.layers.0.mlp.down_proj has nan: False
25-01-23 07:27:34 | I | In layer model.layers.0, gradient of model.layers.0.mlp.down_proj has inf: False
25-01-23 07:27:34 | I | [0] done with optimizer step
25-01-23 07:27:34 | I | epoch 001:      5 / 819200000 loss=8.2202e-06, loss_per_token=0.0336698, loss_sum=275.823, wps=0, ups=0, wpb=8192, bsz=16, num_updates=1, lr=0.0001, gnorm=2.741, clip=100, loss_scale=8, train_wall=195, cuda_gb_allocated=14.8, cuda_gb_reserved=16.3, cuda_gb_free=8.9, wall=202
25-01-23 07:27:34 | I | TRAIN CURRENT LAYER_IDX = 0
25-01-23 07:27:34 | I | in layer model.layers.0
25-01-23 07:27:34 | I | quantizing weights for layer model.layers.0
25-01-23 07:27:35 | I | collecting calibration activations in model.layers.0
25-01-23 07:27:35 | I | collecting calibration activations in model.layers.0
25-01-23 07:27:35 | I | - Quantizing decoder layer model.layers.0
25-01-23 07:27:35 | I |   - Calibrating model.layers.0.self_attn.q_proj.weight
25-01-23 07:27:36 | I |       - range scale = [    1.0000]
25-01-23 07:27:36 | I |         sum  error  = [    0.0440]
25-01-23 07:27:36 | I |         best error  = [    0.0440]
25-01-23 07:27:36 | I |     + error = [0.0440]
25-01-23 07:27:37 | I |       - range scale = [    1.0000]
25-01-23 07:27:37 | I |         sum  error  = [    0.6461]
25-01-23 07:27:37 | I |         best error  = [    0.6461]
25-01-23 07:27:37 | I |     + error = [0.6461]
25-01-23 07:27:37 | I |   - Calibrating model.layers.0.self_attn.k_proj.weight
25-01-23 07:27:37 | I |       - range scale = [    1.0000]
25-01-23 07:27:37 | I |         sum  error  = [    0.0517]
25-01-23 07:27:37 | I |         best error  = [    0.0517]
25-01-23 07:27:37 | I |     + error = [0.0517]
25-01-23 07:27:38 | I |       - range scale = [    1.0000]
25-01-23 07:27:38 | I |         sum  error  = [    0.9224]
25-01-23 07:27:38 | I |         best error  = [    0.9224]
25-01-23 07:27:38 | I |     + error = [0.9224]
25-01-23 07:27:38 | I |   - Calibrating model.layers.0.self_attn.v_proj.weight
25-01-23 07:27:39 | I |       - range scale = [    1.0000]
25-01-23 07:27:39 | I |         sum  error  = [    0.6851]
25-01-23 07:27:39 | I |         best error  = [    0.6851]
25-01-23 07:27:39 | I |     + error = [0.6851]
25-01-23 07:27:40 | I |       - range scale = [    1.0000]
25-01-23 07:27:40 | I |         sum  error  = [    6.1817]
25-01-23 07:27:40 | I |         best error  = [    6.1817]
25-01-23 07:27:40 | I |     + error = [6.1817]
25-01-23 07:27:40 | I |   - Calibrating model.layers.0.self_attn.o_proj.weight
25-01-23 07:27:41 | I |       - range scale = [    1.0000]
25-01-23 07:27:41 | I |         sum  error  = [    0.1895]
25-01-23 07:27:41 | I |         best error  = [    0.1895]
25-01-23 07:27:41 | I |     + error = [0.1895]
25-01-23 07:27:42 | I |       - range scale = [    1.0000]
25-01-23 07:27:42 | I |         sum  error  = [    1.5626]
25-01-23 07:27:42 | I |         best error  = [    1.5626]
25-01-23 07:27:42 | I |     + error = [1.5626]
25-01-23 07:27:42 | I |   - Calibrating model.layers.0.mlp.up_proj.weight
25-01-23 07:27:42 | I |       - range scale = [    1.0000]
25-01-23 07:27:42 | I |         sum  error  = [    1.0981]
25-01-23 07:27:42 | I |         best error  = [    1.0981]
25-01-23 07:27:42 | I |     + error = [1.0981]
25-01-23 07:27:44 | I |       - range scale = [    1.0000]
25-01-23 07:27:44 | I |         sum  error  = [   11.7067]
25-01-23 07:27:44 | I |         best error  = [   11.7067]
25-01-23 07:27:44 | I |     + error = [11.7067]
25-01-23 07:27:44 | I |   - Calibrating model.layers.0.mlp.gate_proj.weight
25-01-23 07:27:44 | I |       - range scale = [    1.0000]
25-01-23 07:27:44 | I |         sum  error  = [    1.1603]
25-01-23 07:27:44 | I |         best error  = [    1.1603]
25-01-23 07:27:44 | I |     + error = [1.1603]
25-01-23 07:27:46 | I |       - range scale = [    1.0000]
25-01-23 07:27:46 | I |         sum  error  = [   12.0391]
25-01-23 07:27:46 | I |         best error  = [   12.0391]
25-01-23 07:27:46 | I |     + error = [12.0391]
25-01-23 07:27:46 | I |   - Calibrating model.layers.0.mlp.down_proj.weight
25-01-23 07:27:47 | I |       - range scale = [    1.0000]
25-01-23 07:27:47 | I |         sum  error  = [    0.1752]
25-01-23 07:27:47 | I |         best error  = [    0.1752]
25-01-23 07:27:47 | I |     + error = [0.1752]
25-01-23 07:27:48 | I |       - range scale = [    1.0000]
25-01-23 07:27:48 | I |         sum  error  = [    1.7100]
25-01-23 07:27:48 | I |         best error  = [    1.7100]
25-01-23 07:27:48 | I |     + error = [1.7100]
25-01-23 07:27:48 | I |   - Quantizing model.layers.0.self_attn.q_proj.weight
25-01-23 07:27:50 | I |   - Quantizing model.layers.0.self_attn.k_proj.weight
25-01-23 07:27:52 | I |   - Quantizing model.layers.0.self_attn.v_proj.weight
25-01-23 07:27:55 | I |   - Quantizing model.layers.0.self_attn.o_proj.weight
25-01-23 07:27:57 | I |   - Quantizing model.layers.0.mlp.up_proj.weight
25-01-23 07:27:59 | I |   - Quantizing model.layers.0.mlp.gate_proj.weight
25-01-23 07:28:02 | I |   - Quantizing model.layers.0.mlp.down_proj.weight
25-01-23 07:28:08 | I | quantizing activations for layer model.layers.0
25-01-23 07:28:08 | I | collecting calibration activations in model.layers.0
25-01-23 07:28:08 | I | collecting calibration activations in model.layers.0
25-01-23 07:28:10 | I | forward this layer
25-01-23 07:28:10 | I | input_file: /data/gyy/lmquant-main/lmquant/data/data_without_preprocess_llama_7b_bs16/shard/val_input_args_layer_0/05.pt
25-01-23 07:28:10 | I | output_file: /data/gyy/lmquant-main/lmquant/data/data_without_preprocess_llama_7b_bs16/shard/val_teacher_output_layer_0/05.pt
25-01-23 07:28:11 | I | In layer model.layers.0, gradient of model.layers.0.self_attn.q_proj has nan: False
25-01-23 07:28:11 | I | In layer model.layers.0, gradient of model.layers.0.self_attn.q_proj has inf: False
25-01-23 07:28:11 | I | In layer model.layers.0, gradient of model.layers.0.self_attn.k_proj has nan: False
25-01-23 07:28:11 | I | In layer model.layers.0, gradient of model.layers.0.self_attn.k_proj has inf: False
25-01-23 07:28:11 | I | In layer model.layers.0, gradient of model.layers.0.self_attn.v_proj has nan: False
25-01-23 07:28:11 | I | In layer model.layers.0, gradient of model.layers.0.self_attn.v_proj has inf: False
25-01-23 07:28:11 | I | In layer model.layers.0, gradient of model.layers.0.self_attn.o_proj has nan: False
25-01-23 07:28:11 | I | In layer model.layers.0, gradient of model.layers.0.self_attn.o_proj has inf: False
25-01-23 07:28:11 | I | In layer model.layers.0, gradient of model.layers.0.mlp.up_proj has nan: False
25-01-23 07:28:11 | I | In layer model.layers.0, gradient of model.layers.0.mlp.up_proj has inf: False
25-01-23 07:28:11 | I | In layer model.layers.0, gradient of model.layers.0.mlp.gate_proj has nan: False
25-01-23 07:28:11 | I | In layer model.layers.0, gradient of model.layers.0.mlp.gate_proj has inf: False
25-01-23 07:28:11 | I | In layer model.layers.0, gradient of model.layers.0.mlp.down_proj has nan: False
25-01-23 07:28:11 | I | In layer model.layers.0, gradient of model.layers.0.mlp.down_proj has inf: False
25-01-23 07:28:11 | I | [1] done with optimizer step
25-01-23 07:28:11 | I | epoch 001:      6 / 819200000 loss=9.4858e-06, loss_per_token=0.0388539, loss_sum=318.291, wps=223.5, ups=0.03, wpb=8192, bsz=16, num_updates=2, lr=0.0002, gnorm=3.586, clip=100, loss_scale=8, train_wall=37, cuda_gb_allocated=16.3, cuda_gb_reserved=17.7, cuda_gb_free=7.4, wall=238
25-01-23 07:28:11 | I | TRAIN CURRENT LAYER_IDX = 0
25-01-23 07:28:11 | I | in layer model.layers.0
25-01-23 07:28:11 | I | quantizing weights for layer model.layers.0
25-01-23 07:28:11 | I | collecting calibration activations in model.layers.0
25-01-23 07:28:11 | I | collecting calibration activations in model.layers.0
25-01-23 07:28:12 | I | - Quantizing decoder layer model.layers.0
25-01-23 07:28:12 | I |   - Calibrating model.layers.0.self_attn.q_proj.weight
25-01-23 07:28:13 | I |       - range scale = [    1.0000]
25-01-23 07:28:13 | I |         sum  error  = [    0.0428]
25-01-23 07:28:13 | I |         best error  = [    0.0428]
25-01-23 07:28:13 | I |     + error = [0.0428]
25-01-23 07:28:13 | I |       - range scale = [    1.0000]
25-01-23 07:28:13 | I |         sum  error  = [    0.6169]
25-01-23 07:28:13 | I |         best error  = [    0.6169]
25-01-23 07:28:13 | I |     + error = [0.6169]
25-01-23 07:28:13 | I |   - Calibrating model.layers.0.self_attn.k_proj.weight
25-01-23 07:28:14 | I |       - range scale = [    1.0000]
25-01-23 07:28:14 | I |         sum  error  = [    0.0579]
25-01-23 07:28:14 | I |         best error  = [    0.0579]
25-01-23 07:28:14 | I |     + error = [0.0579]
25-01-23 07:28:15 | I |       - range scale = [    1.0000]
25-01-23 07:28:15 | I |         sum  error  = [    0.8956]
25-01-23 07:28:15 | I |         best error  = [    0.8956]
25-01-23 07:28:15 | I |     + error = [0.8956]
25-01-23 07:28:15 | I |   - Calibrating model.layers.0.self_attn.v_proj.weight
25-01-23 07:28:16 | I |       - range scale = [    1.0000]
25-01-23 07:28:16 | I |         sum  error  = [    0.7017]
25-01-23 07:28:16 | I |         best error  = [    0.7017]
25-01-23 07:28:16 | I |     + error = [0.7017]
25-01-23 07:28:16 | I |       - range scale = [    1.0000]
25-01-23 07:28:16 | I |         sum  error  = [    6.2451]
25-01-23 07:28:16 | I |         best error  = [    6.2451]
25-01-23 07:28:16 | I |     + error = [6.2451]
25-01-23 07:28:17 | I |   - Calibrating model.layers.0.self_attn.o_proj.weight
25-01-23 07:28:17 | I |       - range scale = [    1.0000]
25-01-23 07:28:17 | I |         sum  error  = [    0.1996]
25-01-23 07:28:17 | I |         best error  = [    0.1996]
25-01-23 07:28:17 | I |     + error = [0.1996]
25-01-23 07:28:18 | I |       - range scale = [    1.0000]
25-01-23 07:28:18 | I |         sum  error  = [    1.6147]
25-01-23 07:28:18 | I |         best error  = [    1.6147]
25-01-23 07:28:18 | I |     + error = [1.6147]
25-01-23 07:28:18 | I |   - Calibrating model.layers.0.mlp.up_proj.weight
25-01-23 07:28:19 | I |       - range scale = [    1.0000]
25-01-23 07:28:19 | I |         sum  error  = [    1.1044]
25-01-23 07:28:19 | I |         best error  = [    1.1044]
25-01-23 07:28:19 | I |     + error = [1.1044]
25-01-23 07:28:20 | I |       - range scale = [    1.0000]
25-01-23 07:28:20 | I |         sum  error  = [   11.7788]
25-01-23 07:28:20 | I |         best error  = [   11.7788]
25-01-23 07:28:20 | I |     + error = [11.7788]
25-01-23 07:28:20 | I |   - Calibrating model.layers.0.mlp.gate_proj.weight
25-01-23 07:28:21 | I |       - range scale = [    1.0000]
25-01-23 07:28:21 | I |         sum  error  = [    1.1685]
25-01-23 07:28:21 | I |         best error  = [    1.1685]
25-01-23 07:28:21 | I |     + error = [1.1685]
25-01-23 07:28:22 | I |       - range scale = [    1.0000]
25-01-23 07:28:22 | I |         sum  error  = [   12.1206]
25-01-23 07:28:22 | I |         best error  = [   12.1206]
25-01-23 07:28:22 | I |     + error = [12.1206]
25-01-23 07:28:22 | I |   - Calibrating model.layers.0.mlp.down_proj.weight
25-01-23 07:28:23 | I |       - range scale = [    1.0000]
25-01-23 07:28:23 | I |         sum  error  = [    0.2091]
25-01-23 07:28:23 | I |         best error  = [    0.2091]
25-01-23 07:28:23 | I |     + error = [0.2091]
25-01-23 07:28:24 | I |       - range scale = [    1.0000]
25-01-23 07:28:24 | I |         sum  error  = [    2.0368]
25-01-23 07:28:24 | I |         best error  = [    2.0368]
25-01-23 07:28:24 | I |     + error = [2.0368]
25-01-23 07:28:25 | I |   - Quantizing model.layers.0.self_attn.q_proj.weight
25-01-23 07:28:27 | I |   - Quantizing model.layers.0.self_attn.k_proj.weight
25-01-23 07:28:29 | I |   - Quantizing model.layers.0.self_attn.v_proj.weight
25-01-23 07:28:31 | I |   - Quantizing model.layers.0.self_attn.o_proj.weight
25-01-23 07:28:34 | I |   - Quantizing model.layers.0.mlp.up_proj.weight
25-01-23 07:28:36 | I |   - Quantizing model.layers.0.mlp.gate_proj.weight
25-01-23 07:28:38 | I |   - Quantizing model.layers.0.mlp.down_proj.weight
25-01-23 07:28:44 | I | quantizing activations for layer model.layers.0
25-01-23 07:28:45 | I | collecting calibration activations in model.layers.0
25-01-23 07:28:45 | I | collecting calibration activations in model.layers.0
25-01-23 07:28:47 | I | forward this layer
25-01-23 07:28:47 | I | input_file: /data/gyy/lmquant-main/lmquant/data/data_without_preprocess_llama_7b_bs16/shard/val_input_args_layer_0/06.pt
25-01-23 07:28:47 | I | output_file: /data/gyy/lmquant-main/lmquant/data/data_without_preprocess_llama_7b_bs16/shard/val_teacher_output_layer_0/06.pt
25-01-23 07:28:47 | I | In layer model.layers.0, gradient of model.layers.0.self_attn.q_proj has nan: False
25-01-23 07:28:47 | I | In layer model.layers.0, gradient of model.layers.0.self_attn.q_proj has inf: False
25-01-23 07:28:47 | I | In layer model.layers.0, gradient of model.layers.0.self_attn.k_proj has nan: False
25-01-23 07:28:47 | I | In layer model.layers.0, gradient of model.layers.0.self_attn.k_proj has inf: False
25-01-23 07:28:47 | I | In layer model.layers.0, gradient of model.layers.0.self_attn.v_proj has nan: False
25-01-23 07:28:47 | I | In layer model.layers.0, gradient of model.layers.0.self_attn.v_proj has inf: False
25-01-23 07:28:47 | I | In layer model.layers.0, gradient of model.layers.0.self_attn.o_proj has nan: False
25-01-23 07:28:47 | I | In layer model.layers.0, gradient of model.layers.0.self_attn.o_proj has inf: False
25-01-23 07:28:47 | I | In layer model.layers.0, gradient of model.layers.0.mlp.up_proj has nan: False
25-01-23 07:28:47 | I | In layer model.layers.0, gradient of model.layers.0.mlp.up_proj has inf: False
25-01-23 07:28:47 | I | In layer model.layers.0, gradient of model.layers.0.mlp.gate_proj has nan: False
25-01-23 07:28:47 | I | In layer model.layers.0, gradient of model.layers.0.mlp.gate_proj has inf: False
25-01-23 07:28:47 | I | In layer model.layers.0, gradient of model.layers.0.mlp.down_proj has nan: False
25-01-23 07:28:47 | I | In layer model.layers.0, gradient of model.layers.0.mlp.down_proj has inf: False
25-01-23 07:28:47 | I | [2] done with optimizer step
25-01-23 07:28:47 | I | epoch 001:      7 / 819200000 loss=1.07059e-05, loss_per_token=0.0438513, loss_sum=359.23, wps=223.1, ups=0.03, wpb=8192, bsz=16, num_updates=3, lr=0.0003, gnorm=3.566, clip=100, loss_scale=8, train_wall=37, cuda_gb_allocated=16.3, cuda_gb_reserved=17.7, cuda_gb_free=7.4, wall=275
25-01-23 07:28:48 | I | TRAIN CURRENT LAYER_IDX = 0
25-01-23 07:28:48 | I | in layer model.layers.0
25-01-23 07:28:48 | I | quantizing weights for layer model.layers.0
25-01-23 07:28:48 | I | collecting calibration activations in model.layers.0
25-01-23 07:28:48 | I | collecting calibration activations in model.layers.0
25-01-23 07:28:49 | I | - Quantizing decoder layer model.layers.0
25-01-23 07:28:49 | I |   - Calibrating model.layers.0.self_attn.q_proj.weight
25-01-23 07:28:49 | I |       - range scale = [    1.0000]
25-01-23 07:28:49 | I |         sum  error  = [    0.0431]
25-01-23 07:28:49 | I |         best error  = [    0.0431]
25-01-23 07:28:49 | I |     + error = [0.0431]
25-01-23 07:28:50 | I |       - range scale = [    1.0000]
25-01-23 07:28:50 | I |         sum  error  = [    0.5903]
25-01-23 07:28:50 | I |         best error  = [    0.5903]
25-01-23 07:28:50 | I |     + error = [0.5903]
25-01-23 07:28:50 | I |   - Calibrating model.layers.0.self_attn.k_proj.weight
25-01-23 07:28:51 | I |       - range scale = [    1.0000]
25-01-23 07:28:51 | I |         sum  error  = [    0.0512]
25-01-23 07:28:51 | I |         best error  = [    0.0512]
25-01-23 07:28:51 | I |     + error = [0.0512]
25-01-23 07:28:52 | I |       - range scale = [    1.0000]
25-01-23 07:28:52 | I |         sum  error  = [    0.9235]
25-01-23 07:28:52 | I |         best error  = [    0.9235]
25-01-23 07:28:52 | I |     + error = [0.9235]
25-01-23 07:28:52 | I |   - Calibrating model.layers.0.self_attn.v_proj.weight
25-01-23 07:28:52 | I |       - range scale = [    1.0000]
25-01-23 07:28:52 | I |         sum  error  = [    0.6834]
25-01-23 07:28:52 | I |         best error  = [    0.6834]
25-01-23 07:28:52 | I |     + error = [0.6834]
25-01-23 07:28:53 | I |       - range scale = [    1.0000]
25-01-23 07:28:53 | I |         sum  error  = [    6.0938]
25-01-23 07:28:53 | I |         best error  = [    6.0938]
25-01-23 07:28:53 | I |     + error = [6.0938]
25-01-23 07:28:53 | I |   - Calibrating model.layers.0.self_attn.o_proj.weight
25-01-23 07:28:54 | I |       - range scale = [    1.0000]
25-01-23 07:28:54 | I |         sum  error  = [    0.1943]
25-01-23 07:28:54 | I |         best error  = [    0.1943]
25-01-23 07:28:54 | I |     + error = [0.1943]
25-01-23 07:28:55 | I |       - range scale = [    1.0000]
25-01-23 07:28:55 | I |         sum  error  = [    1.5914]
25-01-23 07:28:55 | I |         best error  = [    1.5914]
25-01-23 07:28:55 | I |     + error = [1.5914]
25-01-23 07:28:55 | I |   - Calibrating model.layers.0.mlp.up_proj.weight
25-01-23 07:28:56 | I |       - range scale = [    1.0000]
25-01-23 07:28:56 | I |         sum  error  = [    1.1079]
25-01-23 07:28:56 | I |         best error  = [    1.1079]
25-01-23 07:28:56 | I |     + error = [1.1079]
25-01-23 07:28:57 | I |       - range scale = [    1.0000]
25-01-23 07:28:57 | I |         sum  error  = [   11.8013]
25-01-23 07:28:57 | I |         best error  = [   11.8013]
25-01-23 07:28:57 | I |     + error = [11.8013]
25-01-23 07:28:57 | I |   - Calibrating model.layers.0.mlp.gate_proj.weight
25-01-23 07:28:58 | I |       - range scale = [    1.0000]
25-01-23 07:28:58 | I |         sum  error  = [    1.1745]
25-01-23 07:28:58 | I |         best error  = [    1.1745]
25-01-23 07:28:58 | I |     + error = [1.1745]
25-01-23 07:28:59 | I |       - range scale = [    1.0000]
25-01-23 07:28:59 | I |         sum  error  = [   12.1511]
25-01-23 07:28:59 | I |         best error  = [   12.1511]
25-01-23 07:28:59 | I |     + error = [12.1511]
25-01-23 07:28:59 | I |   - Calibrating model.layers.0.mlp.down_proj.weight
25-01-23 07:29:00 | I |       - range scale = [    1.0000]
25-01-23 07:29:00 | I |         sum  error  = [    0.1704]
25-01-23 07:29:00 | I |         best error  = [    0.1704]
25-01-23 07:29:00 | I |     + error = [0.1704]
25-01-23 07:29:01 | I |       - range scale = [    1.0000]
25-01-23 07:29:01 | I |         sum  error  = [    1.6605]
25-01-23 07:29:01 | I |         best error  = [    1.6605]
25-01-23 07:29:01 | I |     + error = [1.6605]
25-01-23 07:29:01 | I |   - Quantizing model.layers.0.self_attn.q_proj.weight
25-01-23 07:29:04 | I |   - Quantizing model.layers.0.self_attn.k_proj.weight
25-01-23 07:29:06 | I |   - Quantizing model.layers.0.self_attn.v_proj.weight
25-01-23 07:29:08 | I |   - Quantizing model.layers.0.self_attn.o_proj.weight
25-01-23 07:29:11 | I |   - Quantizing model.layers.0.mlp.up_proj.weight
25-01-23 07:29:13 | I |   - Quantizing model.layers.0.mlp.gate_proj.weight
25-01-23 07:29:15 | I |   - Quantizing model.layers.0.mlp.down_proj.weight
25-01-23 07:29:21 | I | quantizing activations for layer model.layers.0
25-01-23 07:29:22 | I | collecting calibration activations in model.layers.0
25-01-23 07:29:22 | I | collecting calibration activations in model.layers.0
25-01-23 07:29:24 | I | forward this layer
25-01-23 07:29:24 | I | input_file: /data/gyy/lmquant-main/lmquant/data/data_without_preprocess_llama_7b_bs16/shard/val_input_args_layer_0/07.pt
25-01-23 07:29:24 | I | output_file: /data/gyy/lmquant-main/lmquant/data/data_without_preprocess_llama_7b_bs16/shard/val_teacher_output_layer_0/07.pt
25-01-23 07:29:24 | I | In layer model.layers.0, gradient of model.layers.0.self_attn.q_proj has nan: False
25-01-23 07:29:24 | I | In layer model.layers.0, gradient of model.layers.0.self_attn.q_proj has inf: False
25-01-23 07:29:24 | I | In layer model.layers.0, gradient of model.layers.0.self_attn.k_proj has nan: False
25-01-23 07:29:24 | I | In layer model.layers.0, gradient of model.layers.0.self_attn.k_proj has inf: False
25-01-23 07:29:24 | I | In layer model.layers.0, gradient of model.layers.0.self_attn.v_proj has nan: False
25-01-23 07:29:24 | I | In layer model.layers.0, gradient of model.layers.0.self_attn.v_proj has inf: False
25-01-23 07:29:24 | I | In layer model.layers.0, gradient of model.layers.0.self_attn.o_proj has nan: False
25-01-23 07:29:24 | I | In layer model.layers.0, gradient of model.layers.0.self_attn.o_proj has inf: False
25-01-23 07:29:24 | I | In layer model.layers.0, gradient of model.layers.0.mlp.up_proj has nan: False
25-01-23 07:29:24 | I | In layer model.layers.0, gradient of model.layers.0.mlp.up_proj has inf: False
25-01-23 07:29:24 | I | In layer model.layers.0, gradient of model.layers.0.mlp.gate_proj has nan: False
25-01-23 07:29:24 | I | In layer model.layers.0, gradient of model.layers.0.mlp.gate_proj has inf: False
25-01-23 07:29:24 | I | In layer model.layers.0, gradient of model.layers.0.mlp.down_proj has nan: False
25-01-23 07:29:24 | I | In layer model.layers.0, gradient of model.layers.0.mlp.down_proj has inf: False
25-01-23 07:29:24 | I | [3] done with optimizer step
25-01-23 07:29:24 | I | epoch 001:      8 / 819200000 loss=1.35938e-05, loss_per_token=0.0556803, loss_sum=456.133, wps=223.3, ups=0.03, wpb=8192, bsz=16, num_updates=4, lr=0.0004, gnorm=4.303, clip=100, loss_scale=8, train_wall=37, cuda_gb_allocated=16.3, cuda_gb_reserved=17.7, cuda_gb_free=7.4, wall=312
25-01-23 07:29:24 | I | TRAIN CURRENT LAYER_IDX = 0
25-01-23 07:29:24 | I | in layer model.layers.0
25-01-23 07:29:24 | I | quantizing weights for layer model.layers.0
25-01-23 07:29:25 | I | collecting calibration activations in model.layers.0
25-01-23 07:29:25 | I | collecting calibration activations in model.layers.0
25-01-23 07:29:25 | I | - Quantizing decoder layer model.layers.0
25-01-23 07:29:25 | I |   - Calibrating model.layers.0.self_attn.q_proj.weight
25-01-23 07:29:26 | I |       - range scale = [    1.0000]
25-01-23 07:29:26 | I |         sum  error  = [    0.0558]
25-01-23 07:29:26 | I |         best error  = [    0.0558]
25-01-23 07:29:26 | I |     + error = [0.0558]
25-01-23 07:29:27 | I |       - range scale = [    1.0000]
25-01-23 07:29:27 | I |         sum  error  = [    0.6461]
25-01-23 07:29:27 | I |         best error  = [    0.6461]
25-01-23 07:29:27 | I |     + error = [0.6461]
25-01-23 07:29:27 | I |   - Calibrating model.layers.0.self_attn.k_proj.weight
25-01-23 07:29:28 | I |       - range scale = [    1.0000]
25-01-23 07:29:28 | I |         sum  error  = [    0.0609]
25-01-23 07:29:28 | I |         best error  = [    0.0609]
25-01-23 07:29:28 | I |     + error = [0.0609]
25-01-23 07:29:28 | I |       - range scale = [    1.0000]
25-01-23 07:29:28 | I |         sum  error  = [    0.9422]
25-01-23 07:29:28 | I |         best error  = [    0.9422]
25-01-23 07:29:28 | I |     + error = [0.9422]
25-01-23 07:29:28 | I |   - Calibrating model.layers.0.self_attn.v_proj.weight
25-01-23 07:29:29 | I |       - range scale = [    1.0000]
25-01-23 07:29:29 | I |         sum  error  = [    0.6817]
25-01-23 07:29:29 | I |         best error  = [    0.6817]
25-01-23 07:29:29 | I |     + error = [0.6817]
25-01-23 07:29:30 | I |       - range scale = [    1.0000]
25-01-23 07:29:30 | I |         sum  error  = [    6.2286]
25-01-23 07:29:30 | I |         best error  = [    6.2286]
25-01-23 07:29:30 | I |     + error = [6.2286]
25-01-23 07:29:30 | I |   - Calibrating model.layers.0.self_attn.o_proj.weight
25-01-23 07:29:31 | I |       - range scale = [    1.0000]
25-01-23 07:29:31 | I |         sum  error  = [    0.2002]
25-01-23 07:29:31 | I |         best error  = [    0.2002]
25-01-23 07:29:31 | I |     + error = [0.2002]
25-01-23 07:29:32 | I |       - range scale = [    1.0000]
25-01-23 07:29:32 | I |         sum  error  = [    1.6311]
25-01-23 07:29:32 | I |         best error  = [    1.6311]
25-01-23 07:29:32 | I |     + error = [1.6311]
25-01-23 07:29:32 | I |   - Calibrating model.layers.0.mlp.up_proj.weight
25-01-23 07:29:33 | I |       - range scale = [    1.0000]
25-01-23 07:29:33 | I |         sum  error  = [    1.1022]
25-01-23 07:29:33 | I |         best error  = [    1.1022]
25-01-23 07:29:33 | I |     + error = [1.1022]
25-01-23 07:29:34 | I |       - range scale = [    1.0000]
25-01-23 07:29:34 | I |         sum  error  = [   11.7934]
25-01-23 07:29:34 | I |         best error  = [   11.7934]
25-01-23 07:29:34 | I |     + error = [11.7934]
25-01-23 07:29:34 | I |   - Calibrating model.layers.0.mlp.gate_proj.weight
25-01-23 07:29:35 | I |       - range scale = [    1.0000]
25-01-23 07:29:35 | I |         sum  error  = [    1.1671]
25-01-23 07:29:35 | I |         best error  = [    1.1671]
25-01-23 07:29:35 | I |     + error = [1.1671]
25-01-23 07:29:36 | I |       - range scale = [    1.0000]
25-01-23 07:29:36 | I |         sum  error  = [   12.1540]
25-01-23 07:29:36 | I |         best error  = [   12.1540]
25-01-23 07:29:36 | I |     + error = [12.1540]
25-01-23 07:29:36 | I |   - Calibrating model.layers.0.mlp.down_proj.weight
25-01-23 07:29:37 | I |       - range scale = [    1.0000]
25-01-23 07:29:37 | I |         sum  error  = [    0.1624]
25-01-23 07:29:37 | I |         best error  = [    0.1624]
25-01-23 07:29:37 | I |     + error = [0.1624]
25-01-23 07:29:38 | I |       - range scale = [    1.0000]
25-01-23 07:29:38 | I |         sum  error  = [    1.5789]
25-01-23 07:29:38 | I |         best error  = [    1.5789]
25-01-23 07:29:38 | I |     + error = [1.5789]
25-01-23 07:29:38 | I |   - Quantizing model.layers.0.self_attn.q_proj.weight
25-01-23 07:29:40 | I |   - Quantizing model.layers.0.self_attn.k_proj.weight
25-01-23 07:29:43 | I |   - Quantizing model.layers.0.self_attn.v_proj.weight
25-01-23 07:29:45 | I |   - Quantizing model.layers.0.self_attn.o_proj.weight
25-01-23 07:29:47 | I |   - Quantizing model.layers.0.mlp.up_proj.weight
25-01-23 07:29:49 | I |   - Quantizing model.layers.0.mlp.gate_proj.weight
25-01-23 07:29:52 | I |   - Quantizing model.layers.0.mlp.down_proj.weight
25-01-23 07:29:58 | I | quantizing activations for layer model.layers.0
25-01-23 07:29:58 | I | collecting calibration activations in model.layers.0
25-01-23 07:29:58 | I | collecting calibration activations in model.layers.0
25-01-23 07:30:00 | I | forward this layer
25-01-23 07:30:00 | I | input_file: /data/gyy/lmquant-main/lmquant/data/data_without_preprocess_llama_7b_bs16/shard/val_input_args_layer_0/08.pt
25-01-23 07:30:00 | I | output_file: /data/gyy/lmquant-main/lmquant/data/data_without_preprocess_llama_7b_bs16/shard/val_teacher_output_layer_0/08.pt
25-01-23 07:30:01 | I | In layer model.layers.0, gradient of model.layers.0.self_attn.q_proj has nan: False
25-01-23 07:30:01 | I | In layer model.layers.0, gradient of model.layers.0.self_attn.q_proj has inf: False
25-01-23 07:30:01 | I | In layer model.layers.0, gradient of model.layers.0.self_attn.k_proj has nan: False
25-01-23 07:30:01 | I | In layer model.layers.0, gradient of model.layers.0.self_attn.k_proj has inf: False
25-01-23 07:30:01 | I | In layer model.layers.0, gradient of model.layers.0.self_attn.v_proj has nan: False
25-01-23 07:30:01 | I | In layer model.layers.0, gradient of model.layers.0.self_attn.v_proj has inf: False
25-01-23 07:30:01 | I | In layer model.layers.0, gradient of model.layers.0.self_attn.o_proj has nan: False
25-01-23 07:30:01 | I | In layer model.layers.0, gradient of model.layers.0.self_attn.o_proj has inf: False
25-01-23 07:30:01 | I | In layer model.layers.0, gradient of model.layers.0.mlp.up_proj has nan: False
25-01-23 07:30:01 | I | In layer model.layers.0, gradient of model.layers.0.mlp.up_proj has inf: False
25-01-23 07:30:01 | I | In layer model.layers.0, gradient of model.layers.0.mlp.gate_proj has nan: False
25-01-23 07:30:01 | I | In layer model.layers.0, gradient of model.layers.0.mlp.gate_proj has inf: False
25-01-23 07:30:01 | I | In layer model.layers.0, gradient of model.layers.0.mlp.down_proj has nan: False
25-01-23 07:30:01 | I | In layer model.layers.0, gradient of model.layers.0.mlp.down_proj has inf: False
25-01-23 07:30:01 | I | [4] done with optimizer step
25-01-23 07:30:01 | I | epoch 001:      9 / 819200000 loss=1.25557e-05, loss_per_token=0.0514281, loss_sum=421.299, wps=223.9, ups=0.03, wpb=8192, bsz=16, num_updates=5, lr=0.0005, gnorm=1.827, clip=0, loss_scale=8, train_wall=36, cuda_gb_allocated=16.3, cuda_gb_reserved=17.7, cuda_gb_free=7.4, wall=348
25-01-23 07:30:01 | I | TRAIN CURRENT LAYER_IDX = 0
25-01-23 07:30:01 | I | in layer model.layers.0
25-01-23 07:30:01 | I | quantizing weights for layer model.layers.0
25-01-23 07:30:01 | I | collecting calibration activations in model.layers.0
25-01-23 07:30:01 | I | collecting calibration activations in model.layers.0
25-01-23 07:30:02 | I | - Quantizing decoder layer model.layers.0
25-01-23 07:30:02 | I |   - Calibrating model.layers.0.self_attn.q_proj.weight
25-01-23 07:30:03 | I |       - range scale = [    1.0000]
25-01-23 07:30:03 | I |         sum  error  = [    0.0461]
25-01-23 07:30:03 | I |         best error  = [    0.0461]
25-01-23 07:30:03 | I |     + error = [0.0461]
25-01-23 07:30:03 | I |       - range scale = [    1.0000]
25-01-23 07:30:03 | I |         sum  error  = [    0.6017]
25-01-23 07:30:03 | I |         best error  = [    0.6017]
25-01-23 07:30:03 | I |     + error = [0.6017]
25-01-23 07:30:03 | I |   - Calibrating model.layers.0.self_attn.k_proj.weight
25-01-23 07:30:04 | I |       - range scale = [    1.0000]
25-01-23 07:30:04 | I |         sum  error  = [    0.0588]
25-01-23 07:30:04 | I |         best error  = [    0.0588]
25-01-23 07:30:04 | I |     + error = [0.0588]
25-01-23 07:30:05 | I |       - range scale = [    1.0000]
25-01-23 07:30:05 | I |         sum  error  = [    0.9586]
25-01-23 07:30:05 | I |         best error  = [    0.9586]
25-01-23 07:30:05 | I |     + error = [0.9586]
25-01-23 07:30:05 | I |   - Calibrating model.layers.0.self_attn.v_proj.weight
25-01-23 07:30:06 | I |       - range scale = [    1.0000]
25-01-23 07:30:06 | I |         sum  error  = [    0.6621]
25-01-23 07:30:06 | I |         best error  = [    0.6621]
25-01-23 07:30:06 | I |     + error = [0.6621]
25-01-23 07:30:06 | I |       - range scale = [    1.0000]
25-01-23 07:30:06 | I |         sum  error  = [    6.0568]
25-01-23 07:30:06 | I |         best error  = [    6.0568]
25-01-23 07:30:06 | I |     + error = [6.0568]
25-01-23 07:30:07 | I |   - Calibrating model.layers.0.self_attn.o_proj.weight
25-01-23 07:30:07 | I |       - range scale = [    1.0000]
25-01-23 07:30:07 | I |         sum  error  = [    0.2038]
25-01-23 07:30:07 | I |         best error  = [    0.2038]
25-01-23 07:30:07 | I |     + error = [0.2038]
25-01-23 07:30:08 | I |       - range scale = [    1.0000]
25-01-23 07:30:08 | I |         sum  error  = [    1.6269]
25-01-23 07:30:08 | I |         best error  = [    1.6269]
25-01-23 07:30:08 | I |     + error = [1.6269]
25-01-23 07:30:08 | I |   - Calibrating model.layers.0.mlp.up_proj.weight
25-01-23 07:30:09 | I |       - range scale = [    1.0000]
25-01-23 07:30:09 | I |         sum  error  = [    1.1056]
25-01-23 07:30:09 | I |         best error  = [    1.1056]
25-01-23 07:30:09 | I |     + error = [1.1056]
25-01-23 07:30:10 | I |       - range scale = [    1.0000]
25-01-23 07:30:10 | I |         sum  error  = [   11.7840]
25-01-23 07:30:10 | I |         best error  = [   11.7840]
25-01-23 07:30:10 | I |     + error = [11.7840]
25-01-23 07:30:10 | I |   - Calibrating model.layers.0.mlp.gate_proj.weight
25-01-23 07:30:11 | I |       - range scale = [    1.0000]
25-01-23 07:30:11 | I |         sum  error  = [    1.1623]
25-01-23 07:30:11 | I |         best error  = [    1.1623]
25-01-23 07:30:11 | I |     + error = [1.1623]
25-01-23 07:30:12 | I |       - range scale = [    1.0000]
25-01-23 07:30:12 | I |         sum  error  = [   12.1620]
25-01-23 07:30:12 | I |         best error  = [   12.1620]
25-01-23 07:30:12 | I |     + error = [12.1620]
25-01-23 07:30:12 | I |   - Calibrating model.layers.0.mlp.down_proj.weight
25-01-23 07:30:13 | I |       - range scale = [    1.0000]
25-01-23 07:30:13 | I |         sum  error  = [    0.1862]
25-01-23 07:30:13 | I |         best error  = [    0.1862]
25-01-23 07:30:13 | I |     + error = [0.1862]
25-01-23 07:30:14 | I |       - range scale = [    1.0000]
25-01-23 07:30:14 | I |         sum  error  = [    1.8372]
25-01-23 07:30:14 | I |         best error  = [    1.8372]
25-01-23 07:30:14 | I |     + error = [1.8372]
25-01-23 07:30:15 | I |   - Quantizing model.layers.0.self_attn.q_proj.weight
25-01-23 07:30:17 | I |   - Quantizing model.layers.0.self_attn.k_proj.weight
25-01-23 07:30:19 | I |   - Quantizing model.layers.0.self_attn.v_proj.weight
25-01-23 07:30:21 | I |   - Quantizing model.layers.0.self_attn.o_proj.weight
25-01-23 07:30:24 | I |   - Quantizing model.layers.0.mlp.up_proj.weight
25-01-23 07:30:26 | I |   - Quantizing model.layers.0.mlp.gate_proj.weight
25-01-23 07:30:28 | I |   - Quantizing model.layers.0.mlp.down_proj.weight
25-01-23 07:30:34 | I | quantizing activations for layer model.layers.0
25-01-23 07:30:34 | I | collecting calibration activations in model.layers.0
25-01-23 07:30:35 | I | collecting calibration activations in model.layers.0
25-01-23 07:30:36 | I | forward this layer
25-01-23 07:30:36 | I | input_file: /data/gyy/lmquant-main/lmquant/data/data_without_preprocess_llama_7b_bs16/shard/val_input_args_layer_0/09.pt
25-01-23 07:30:36 | I | output_file: /data/gyy/lmquant-main/lmquant/data/data_without_preprocess_llama_7b_bs16/shard/val_teacher_output_layer_0/09.pt
25-01-23 07:30:37 | I | In layer model.layers.0, gradient of model.layers.0.self_attn.q_proj has nan: False
25-01-23 07:30:37 | I | In layer model.layers.0, gradient of model.layers.0.self_attn.q_proj has inf: False
25-01-23 07:30:37 | I | In layer model.layers.0, gradient of model.layers.0.self_attn.k_proj has nan: False
25-01-23 07:30:37 | I | In layer model.layers.0, gradient of model.layers.0.self_attn.k_proj has inf: False
25-01-23 07:30:37 | I | In layer model.layers.0, gradient of model.layers.0.self_attn.v_proj has nan: False
25-01-23 07:30:37 | I | In layer model.layers.0, gradient of model.layers.0.self_attn.v_proj has inf: False
25-01-23 07:30:37 | I | In layer model.layers.0, gradient of model.layers.0.self_attn.o_proj has nan: False
25-01-23 07:30:37 | I | In layer model.layers.0, gradient of model.layers.0.self_attn.o_proj has inf: False
25-01-23 07:30:37 | I | In layer model.layers.0, gradient of model.layers.0.mlp.up_proj has nan: False
25-01-23 07:30:37 | I | In layer model.layers.0, gradient of model.layers.0.mlp.up_proj has inf: False
25-01-23 07:30:37 | I | In layer model.layers.0, gradient of model.layers.0.mlp.gate_proj has nan: False
25-01-23 07:30:37 | I | In layer model.layers.0, gradient of model.layers.0.mlp.gate_proj has inf: False
25-01-23 07:30:37 | I | In layer model.layers.0, gradient of model.layers.0.mlp.down_proj has nan: False
25-01-23 07:30:37 | I | In layer model.layers.0, gradient of model.layers.0.mlp.down_proj has inf: False
25-01-23 07:30:37 | I | [5] done with optimizer step
25-01-23 07:30:37 | I | epoch 001:     10 / 819200000 loss=1.53089e-05, loss_per_token=0.0627052, loss_sum=513.681, wps=225.8, ups=0.03, wpb=8192, bsz=16, num_updates=6, lr=0.0006, gnorm=4.009, clip=100, loss_scale=8, train_wall=36, cuda_gb_allocated=16.3, cuda_gb_reserved=17.7, cuda_gb_free=7.4, wall=385
25-01-23 07:30:37 | I | TRAIN CURRENT LAYER_IDX = 0
25-01-23 07:30:37 | I | in layer model.layers.0
25-01-23 07:30:37 | I | quantizing weights for layer model.layers.0
25-01-23 07:30:38 | I | collecting calibration activations in model.layers.0
25-01-23 07:30:38 | I | collecting calibration activations in model.layers.0
25-01-23 07:30:38 | I | - Quantizing decoder layer model.layers.0
25-01-23 07:30:38 | I |   - Calibrating model.layers.0.self_attn.q_proj.weight
25-01-23 07:30:39 | I |       - range scale = [    1.0000]
25-01-23 07:30:39 | I |         sum  error  = [    0.0478]
25-01-23 07:30:39 | I |         best error  = [    0.0478]
25-01-23 07:30:39 | I |     + error = [0.0478]
25-01-23 07:30:40 | I |       - range scale = [    1.0000]
25-01-23 07:30:40 | I |         sum  error  = [    0.6350]
25-01-23 07:30:40 | I |         best error  = [    0.6350]
25-01-23 07:30:40 | I |     + error = [0.6350]
25-01-23 07:30:40 | I |   - Calibrating model.layers.0.self_attn.k_proj.weight
25-01-23 07:30:40 | I |       - range scale = [    1.0000]
25-01-23 07:30:41 | I |         sum  error  = [    0.0596]
25-01-23 07:30:41 | I |         best error  = [    0.0596]
25-01-23 07:30:41 | I |     + error = [0.0596]
25-01-23 07:30:41 | I |       - range scale = [    1.0000]
25-01-23 07:30:41 | I |         sum  error  = [    0.9859]
25-01-23 07:30:41 | I |         best error  = [    0.9859]
25-01-23 07:30:41 | I |     + error = [0.9859]
25-01-23 07:30:41 | I |   - Calibrating model.layers.0.self_attn.v_proj.weight
25-01-23 07:30:42 | I |       - range scale = [    1.0000]
25-01-23 07:30:42 | I |         sum  error  = [    0.6617]
25-01-23 07:30:42 | I |         best error  = [    0.6617]
25-01-23 07:30:42 | I |     + error = [0.6617]
25-01-23 07:30:43 | I |       - range scale = [    1.0000]
25-01-23 07:30:43 | I |         sum  error  = [    6.1672]
25-01-23 07:30:43 | I |         best error  = [    6.1672]
25-01-23 07:30:43 | I |     + error = [6.1672]
25-01-23 07:30:43 | I |   - Calibrating model.layers.0.self_attn.o_proj.weight
25-01-23 07:30:44 | I |       - range scale = [    1.0000]
25-01-23 07:30:44 | I |         sum  error  = [    0.2038]
25-01-23 07:30:44 | I |         best error  = [    0.2038]
25-01-23 07:30:44 | I |     + error = [0.2038]
25-01-23 07:30:45 | I |       - range scale = [    1.0000]
25-01-23 07:30:45 | I |         sum  error  = [    1.6336]
25-01-23 07:30:45 | I |         best error  = [    1.6336]
25-01-23 07:30:45 | I |     + error = [1.6336]
25-01-23 07:30:45 | I |   - Calibrating model.layers.0.mlp.up_proj.weight
25-01-23 07:30:45 | I |       - range scale = [    1.0000]
25-01-23 07:30:45 | I |         sum  error  = [    1.0886]
25-01-23 07:30:45 | I |         best error  = [    1.0886]
25-01-23 07:30:45 | I |     + error = [1.0886]
25-01-23 07:30:47 | I |       - range scale = [    1.0000]
25-01-23 07:30:47 | I |         sum  error  = [   11.6333]
25-01-23 07:30:47 | I |         best error  = [   11.6333]
25-01-23 07:30:47 | I |     + error = [11.6333]
25-01-23 07:30:47 | I |   - Calibrating model.layers.0.mlp.gate_proj.weight
25-01-23 07:30:47 | I |       - range scale = [    1.0000]
25-01-23 07:30:47 | I |         sum  error  = [    1.1460]
25-01-23 07:30:47 | I |         best error  = [    1.1460]
25-01-23 07:30:47 | I |     + error = [1.1460]
25-01-23 07:30:49 | I |       - range scale = [    1.0000]
25-01-23 07:30:49 | I |         sum  error  = [   12.0134]
25-01-23 07:30:49 | I |         best error  = [   12.0134]
25-01-23 07:30:49 | I |     + error = [12.0134]
25-01-23 07:30:49 | I |   - Calibrating model.layers.0.mlp.down_proj.weight
25-01-23 07:30:50 | I |       - range scale = [    1.0000]
25-01-23 07:30:50 | I |         sum  error  = [    0.1821]
25-01-23 07:30:50 | I |         best error  = [    0.1821]
25-01-23 07:30:50 | I |     + error = [0.1821]
25-01-23 07:30:51 | I |       - range scale = [    1.0000]
25-01-23 07:30:51 | I |         sum  error  = [    1.7764]
25-01-23 07:30:51 | I |         best error  = [    1.7764]
25-01-23 07:30:51 | I |     + error = [1.7764]
25-01-23 07:30:51 | I |   - Quantizing model.layers.0.self_attn.q_proj.weight
25-01-23 07:30:54 | I |   - Quantizing model.layers.0.self_attn.k_proj.weight
25-01-23 07:30:57 | I |   - Quantizing model.layers.0.self_attn.v_proj.weight
25-01-23 07:31:00 | I |   - Quantizing model.layers.0.self_attn.o_proj.weight
25-01-23 07:31:02 | I |   - Quantizing model.layers.0.mlp.up_proj.weight
25-01-23 07:31:05 | I |   - Quantizing model.layers.0.mlp.gate_proj.weight
25-01-23 07:31:08 | I |   - Quantizing model.layers.0.mlp.down_proj.weight
25-01-23 07:31:16 | I | quantizing activations for layer model.layers.0
25-01-23 07:31:16 | I | collecting calibration activations in model.layers.0
25-01-23 07:31:17 | I | collecting calibration activations in model.layers.0
25-01-23 07:31:19 | I | forward this layer
25-01-23 07:31:19 | I | input_file: /data/gyy/lmquant-main/lmquant/data/data_without_preprocess_llama_7b_bs16/shard/val_input_args_layer_0/10.pt
25-01-23 07:31:19 | I | output_file: /data/gyy/lmquant-main/lmquant/data/data_without_preprocess_llama_7b_bs16/shard/val_teacher_output_layer_0/10.pt
25-01-23 07:31:19 | I | In layer model.layers.0, gradient of model.layers.0.self_attn.q_proj has nan: False
25-01-23 07:31:19 | I | In layer model.layers.0, gradient of model.layers.0.self_attn.q_proj has inf: False
25-01-23 07:31:19 | I | In layer model.layers.0, gradient of model.layers.0.self_attn.k_proj has nan: False
25-01-23 07:31:19 | I | In layer model.layers.0, gradient of model.layers.0.self_attn.k_proj has inf: False
25-01-23 07:31:19 | I | In layer model.layers.0, gradient of model.layers.0.self_attn.v_proj has nan: False
25-01-23 07:31:19 | I | In layer model.layers.0, gradient of model.layers.0.self_attn.v_proj has inf: False
25-01-23 07:31:19 | I | In layer model.layers.0, gradient of model.layers.0.self_attn.o_proj has nan: False
25-01-23 07:31:19 | I | In layer model.layers.0, gradient of model.layers.0.self_attn.o_proj has inf: False
25-01-23 07:31:19 | I | In layer model.layers.0, gradient of model.layers.0.mlp.up_proj has nan: False
25-01-23 07:31:19 | I | In layer model.layers.0, gradient of model.layers.0.mlp.up_proj has inf: False
25-01-23 07:31:19 | I | In layer model.layers.0, gradient of model.layers.0.mlp.gate_proj has nan: False
25-01-23 07:31:19 | I | In layer model.layers.0, gradient of model.layers.0.mlp.gate_proj has inf: False
25-01-23 07:31:19 | I | In layer model.layers.0, gradient of model.layers.0.mlp.down_proj has nan: False
25-01-23 07:31:19 | I | In layer model.layers.0, gradient of model.layers.0.mlp.down_proj has inf: False
25-01-23 07:31:19 | I | [6] done with optimizer step
25-01-23 07:31:19 | I | epoch 001:     11 / 819200000 loss=2.01709e-05, loss_per_token=0.0826202, loss_sum=676.825, wps=195.2, ups=0.02, wpb=8192, bsz=16, num_updates=7, lr=0.0007, gnorm=3.944, clip=100, loss_scale=8, train_wall=42, cuda_gb_allocated=16.3, cuda_gb_reserved=17.7, cuda_gb_free=7.4, wall=427
25-01-23 07:31:19 | I | TRAIN CURRENT LAYER_IDX = 0
25-01-23 07:31:19 | I | in layer model.layers.0
25-01-23 07:31:19 | I | quantizing weights for layer model.layers.0
25-01-23 07:31:20 | I | collecting calibration activations in model.layers.0
25-01-23 07:31:20 | I | collecting calibration activations in model.layers.0
25-01-23 07:31:20 | I | - Quantizing decoder layer model.layers.0
25-01-23 07:31:20 | I |   - Calibrating model.layers.0.self_attn.q_proj.weight
25-01-23 07:31:21 | I |       - range scale = [    1.0000]
25-01-23 07:31:21 | I |         sum  error  = [    0.0494]
25-01-23 07:31:21 | I |         best error  = [    0.0494]
25-01-23 07:31:21 | I |     + error = [0.0494]
25-01-23 07:31:22 | I |       - range scale = [    1.0000]
25-01-23 07:31:22 | I |         sum  error  = [    0.6331]
25-01-23 07:31:22 | I |         best error  = [    0.6331]
25-01-23 07:31:22 | I |     + error = [0.6331]
25-01-23 07:31:22 | I |   - Calibrating model.layers.0.self_attn.k_proj.weight
25-01-23 07:31:22 | I |       - range scale = [    1.0000]
25-01-23 07:31:22 | I |         sum  error  = [    0.0654]
25-01-23 07:31:22 | I |         best error  = [    0.0654]
25-01-23 07:31:22 | I |     + error = [0.0654]
25-01-23 07:31:23 | I |       - range scale = [    1.0000]
25-01-23 07:31:23 | I |         sum  error  = [    0.9711]
25-01-23 07:31:23 | I |         best error  = [    0.9711]
25-01-23 07:31:23 | I |     + error = [0.9711]
25-01-23 07:31:23 | I |   - Calibrating model.layers.0.self_attn.v_proj.weight
25-01-23 07:31:24 | I |       - range scale = [    1.0000]
25-01-23 07:31:24 | I |         sum  error  = [    0.7137]
25-01-23 07:31:24 | I |         best error  = [    0.7137]
25-01-23 07:31:24 | I |     + error = [0.7137]
25-01-23 07:31:25 | I |       - range scale = [    1.0000]
25-01-23 07:31:25 | I |         sum  error  = [    6.5519]
25-01-23 07:31:25 | I |         best error  = [    6.5519]
25-01-23 07:31:25 | I |     + error = [6.5519]
25-01-23 07:31:25 | I |   - Calibrating model.layers.0.self_attn.o_proj.weight
25-01-23 07:31:26 | I |       - range scale = [    1.0000]
25-01-23 07:31:26 | I |         sum  error  = [    0.2066]
25-01-23 07:31:26 | I |         best error  = [    0.2066]
25-01-23 07:31:26 | I |     + error = [0.2066]
25-01-23 07:31:27 | I |       - range scale = [    1.0000]
25-01-23 07:31:27 | I |         sum  error  = [    1.7077]
25-01-23 07:31:27 | I |         best error  = [    1.7077]
25-01-23 07:31:27 | I |     + error = [1.7077]
25-01-23 07:31:27 | I |   - Calibrating model.layers.0.mlp.up_proj.weight
25-01-23 07:31:28 | I |       - range scale = [    1.0000]
25-01-23 07:31:28 | I |         sum  error  = [    1.0767]
25-01-23 07:31:28 | I |         best error  = [    1.0767]
25-01-23 07:31:28 | I |     + error = [1.0767]
25-01-23 07:31:29 | I |       - range scale = [    1.0000]
25-01-23 07:31:29 | I |         sum  error  = [   11.5453]
25-01-23 07:31:29 | I |         best error  = [   11.5453]
25-01-23 07:31:29 | I |     + error = [11.5453]
25-01-23 07:31:29 | I |   - Calibrating model.layers.0.mlp.gate_proj.weight
25-01-23 07:31:30 | I |       - range scale = [    1.0000]
25-01-23 07:31:30 | I |         sum  error  = [    1.1416]
25-01-23 07:31:30 | I |         best error  = [    1.1416]
25-01-23 07:31:30 | I |     + error = [1.1416]
25-01-23 07:31:31 | I |       - range scale = [    1.0000]
25-01-23 07:31:31 | I |         sum  error  = [   11.9088]
25-01-23 07:31:31 | I |         best error  = [   11.9088]
25-01-23 07:31:31 | I |     + error = [11.9088]
25-01-23 07:31:31 | I |   - Calibrating model.layers.0.mlp.down_proj.weight
25-01-23 07:31:32 | I |       - range scale = [    1.0000]
25-01-23 07:31:32 | I |         sum  error  = [    0.1424]
25-01-23 07:31:32 | I |         best error  = [    0.1424]
25-01-23 07:31:32 | I |     + error = [0.1424]
25-01-23 07:31:33 | I |       - range scale = [    1.0000]
25-01-23 07:31:33 | I |         sum  error  = [    1.3938]
25-01-23 07:31:33 | I |         best error  = [    1.3938]
25-01-23 07:31:33 | I |     + error = [1.3938]
25-01-23 07:31:33 | I |   - Quantizing model.layers.0.self_attn.q_proj.weight
25-01-23 07:31:35 | I |   - Quantizing model.layers.0.self_attn.k_proj.weight
25-01-23 07:31:38 | I |   - Quantizing model.layers.0.self_attn.v_proj.weight
25-01-23 07:31:40 | I |   - Quantizing model.layers.0.self_attn.o_proj.weight
25-01-23 07:31:42 | I |   - Quantizing model.layers.0.mlp.up_proj.weight
25-01-23 07:31:45 | I |   - Quantizing model.layers.0.mlp.gate_proj.weight
25-01-23 07:31:47 | I |   - Quantizing model.layers.0.mlp.down_proj.weight
25-01-23 07:31:53 | I | quantizing activations for layer model.layers.0
25-01-23 07:31:53 | I | collecting calibration activations in model.layers.0
25-01-23 07:31:54 | I | collecting calibration activations in model.layers.0
25-01-23 07:31:56 | I | forward this layer
25-01-23 07:31:56 | I | input_file: /data/gyy/lmquant-main/lmquant/data/data_without_preprocess_llama_7b_bs16/shard/val_input_args_layer_0/11.pt
25-01-23 07:31:56 | I | output_file: /data/gyy/lmquant-main/lmquant/data/data_without_preprocess_llama_7b_bs16/shard/val_teacher_output_layer_0/11.pt
25-01-23 07:31:56 | I | In layer model.layers.0, gradient of model.layers.0.self_attn.q_proj has nan: False
25-01-23 07:31:56 | I | In layer model.layers.0, gradient of model.layers.0.self_attn.q_proj has inf: False
25-01-23 07:31:56 | I | In layer model.layers.0, gradient of model.layers.0.self_attn.k_proj has nan: False
25-01-23 07:31:56 | I | In layer model.layers.0, gradient of model.layers.0.self_attn.k_proj has inf: False
25-01-23 07:31:56 | I | In layer model.layers.0, gradient of model.layers.0.self_attn.v_proj has nan: False
25-01-23 07:31:56 | I | In layer model.layers.0, gradient of model.layers.0.self_attn.v_proj has inf: False
25-01-23 07:31:56 | I | In layer model.layers.0, gradient of model.layers.0.self_attn.o_proj has nan: False
25-01-23 07:31:56 | I | In layer model.layers.0, gradient of model.layers.0.self_attn.o_proj has inf: False
25-01-23 07:31:56 | I | In layer model.layers.0, gradient of model.layers.0.mlp.up_proj has nan: False
25-01-23 07:31:56 | I | In layer model.layers.0, gradient of model.layers.0.mlp.up_proj has inf: False
25-01-23 07:31:56 | I | In layer model.layers.0, gradient of model.layers.0.mlp.gate_proj has nan: False
25-01-23 07:31:56 | I | In layer model.layers.0, gradient of model.layers.0.mlp.gate_proj has inf: False
25-01-23 07:31:56 | I | In layer model.layers.0, gradient of model.layers.0.mlp.down_proj has nan: False
25-01-23 07:31:56 | I | In layer model.layers.0, gradient of model.layers.0.mlp.down_proj has inf: False
25-01-23 07:31:56 | I | [7] done with optimizer step
25-01-23 07:31:56 | I | epoch 001:     12 / 819200000 loss=1.33067e-05, loss_per_token=0.0545044, loss_sum=446.5, wps=221.5, ups=0.03, wpb=8192, bsz=16, num_updates=8, lr=0.0008, gnorm=1.657, clip=0, loss_scale=8, train_wall=37, cuda_gb_allocated=16.3, cuda_gb_reserved=17.7, cuda_gb_free=7.4, wall=464
25-01-23 07:31:56 | I | TRAIN CURRENT LAYER_IDX = 0
25-01-23 07:31:56 | I | in layer model.layers.0
25-01-23 07:31:56 | I | quantizing weights for layer model.layers.0
25-01-23 07:31:57 | I | collecting calibration activations in model.layers.0
25-01-23 07:31:57 | I | collecting calibration activations in model.layers.0
25-01-23 07:31:57 | I | - Quantizing decoder layer model.layers.0
25-01-23 07:31:57 | I |   - Calibrating model.layers.0.self_attn.q_proj.weight
25-01-23 07:31:58 | I |       - range scale = [    1.0000]
25-01-23 07:31:58 | I |         sum  error  = [    0.0439]
25-01-23 07:31:58 | I |         best error  = [    0.0439]
25-01-23 07:31:58 | I |     + error = [0.0439]
25-01-23 07:31:59 | I |       - range scale = [    1.0000]
25-01-23 07:31:59 | I |         sum  error  = [    0.6263]
25-01-23 07:31:59 | I |         best error  = [    0.6263]
25-01-23 07:31:59 | I |     + error = [0.6263]
25-01-23 07:31:59 | I |   - Calibrating model.layers.0.self_attn.k_proj.weight
25-01-23 07:31:59 | I |       - range scale = [    1.0000]
25-01-23 07:31:59 | I |         sum  error  = [    0.0639]
25-01-23 07:31:59 | I |         best error  = [    0.0639]
25-01-23 07:31:59 | I |     + error = [0.0639]
25-01-23 07:32:00 | I |       - range scale = [    1.0000]
25-01-23 07:32:00 | I |         sum  error  = [    1.0077]
25-01-23 07:32:00 | I |         best error  = [    1.0077]
25-01-23 07:32:00 | I |     + error = [1.0077]
25-01-23 07:32:00 | I |   - Calibrating model.layers.0.self_attn.v_proj.weight
25-01-23 07:32:01 | I |       - range scale = [    1.0000]
25-01-23 07:32:01 | I |         sum  error  = [    0.7107]
25-01-23 07:32:01 | I |         best error  = [    0.7107]
25-01-23 07:32:01 | I |     + error = [0.7107]
25-01-23 07:32:02 | I |       - range scale = [    1.0000]
25-01-23 07:32:02 | I |         sum  error  = [    6.4967]
25-01-23 07:32:02 | I |         best error  = [    6.4967]
25-01-23 07:32:02 | I |     + error = [6.4967]
25-01-23 07:32:02 | I |   - Calibrating model.layers.0.self_attn.o_proj.weight
25-01-23 07:32:03 | I |       - range scale = [    1.0000]
25-01-23 07:32:03 | I |         sum  error  = [    0.2189]
25-01-23 07:32:03 | I |         best error  = [    0.2189]
25-01-23 07:32:03 | I |     + error = [0.2189]
25-01-23 07:32:04 | I |       - range scale = [    1.0000]
25-01-23 07:32:04 | I |         sum  error  = [    1.7275]
25-01-23 07:32:04 | I |         best error  = [    1.7275]
25-01-23 07:32:04 | I |     + error = [1.7275]
25-01-23 07:32:04 | I |   - Calibrating model.layers.0.mlp.up_proj.weight
25-01-23 07:32:04 | I |       - range scale = [    1.0000]
25-01-23 07:32:04 | I |         sum  error  = [    1.1004]
25-01-23 07:32:04 | I |         best error  = [    1.1004]
25-01-23 07:32:04 | I |     + error = [1.1004]
25-01-23 07:32:06 | I |       - range scale = [    1.0000]
25-01-23 07:32:06 | I |         sum  error  = [   11.7399]
25-01-23 07:32:06 | I |         best error  = [   11.7399]
25-01-23 07:32:06 | I |     + error = [11.7399]
25-01-23 07:32:06 | I |   - Calibrating model.layers.0.mlp.gate_proj.weight
25-01-23 07:32:06 | I |       - range scale = [    1.0000]
25-01-23 07:32:06 | I |         sum  error  = [    1.1556]
25-01-23 07:32:06 | I |         best error  = [    1.1556]
25-01-23 07:32:06 | I |     + error = [1.1556]
25-01-23 07:32:08 | I |       - range scale = [    1.0000]
25-01-23 07:32:08 | I |         sum  error  = [   12.1235]
25-01-23 07:32:08 | I |         best error  = [   12.1235]
25-01-23 07:32:08 | I |     + error = [12.1235]
25-01-23 07:32:08 | I |   - Calibrating model.layers.0.mlp.down_proj.weight
25-01-23 07:32:08 | I |       - range scale = [    1.0000]
25-01-23 07:32:08 | I |         sum  error  = [    0.1902]
25-01-23 07:32:08 | I |         best error  = [    0.1902]
25-01-23 07:32:09 | I |     + error = [0.1902]
25-01-23 07:32:10 | I |       - range scale = [    1.0000]
25-01-23 07:32:10 | I |         sum  error  = [    1.8769]
25-01-23 07:32:10 | I |         best error  = [    1.8769]
25-01-23 07:32:10 | I |     + error = [1.8769]
25-01-23 07:32:10 | I |   - Quantizing model.layers.0.self_attn.q_proj.weight
25-01-23 07:32:12 | I |   - Quantizing model.layers.0.self_attn.k_proj.weight
25-01-23 07:32:14 | I |   - Quantizing model.layers.0.self_attn.v_proj.weight
25-01-23 07:32:17 | I |   - Quantizing model.layers.0.self_attn.o_proj.weight
25-01-23 07:32:19 | I |   - Quantizing model.layers.0.mlp.up_proj.weight
25-01-23 07:32:21 | I |   - Quantizing model.layers.0.mlp.gate_proj.weight
25-01-23 07:32:24 | I |   - Quantizing model.layers.0.mlp.down_proj.weight
25-01-23 07:32:30 | I | quantizing activations for layer model.layers.0
25-01-23 07:32:30 | I | collecting calibration activations in model.layers.0
25-01-23 07:32:30 | I | collecting calibration activations in model.layers.0
25-01-23 07:32:32 | I | forward this layer
25-01-23 07:32:32 | I | input_file: /data/gyy/lmquant-main/lmquant/data/data_without_preprocess_llama_7b_bs16/shard/val_input_args_layer_0/12.pt
25-01-23 07:32:32 | I | output_file: /data/gyy/lmquant-main/lmquant/data/data_without_preprocess_llama_7b_bs16/shard/val_teacher_output_layer_0/12.pt
25-01-23 07:32:33 | I | In layer model.layers.0, gradient of model.layers.0.self_attn.q_proj has nan: False
25-01-23 07:32:33 | I | In layer model.layers.0, gradient of model.layers.0.self_attn.q_proj has inf: False
25-01-23 07:32:33 | I | In layer model.layers.0, gradient of model.layers.0.self_attn.k_proj has nan: False
25-01-23 07:32:33 | I | In layer model.layers.0, gradient of model.layers.0.self_attn.k_proj has inf: False
25-01-23 07:32:33 | I | In layer model.layers.0, gradient of model.layers.0.self_attn.v_proj has nan: False
25-01-23 07:32:33 | I | In layer model.layers.0, gradient of model.layers.0.self_attn.v_proj has inf: False
25-01-23 07:32:33 | I | In layer model.layers.0, gradient of model.layers.0.self_attn.o_proj has nan: False
25-01-23 07:32:33 | I | In layer model.layers.0, gradient of model.layers.0.self_attn.o_proj has inf: False
25-01-23 07:32:33 | I | In layer model.layers.0, gradient of model.layers.0.mlp.up_proj has nan: False
25-01-23 07:32:33 | I | In layer model.layers.0, gradient of model.layers.0.mlp.up_proj has inf: False
25-01-23 07:32:33 | I | In layer model.layers.0, gradient of model.layers.0.mlp.gate_proj has nan: False
25-01-23 07:32:33 | I | In layer model.layers.0, gradient of model.layers.0.mlp.gate_proj has inf: False
25-01-23 07:32:33 | I | In layer model.layers.0, gradient of model.layers.0.mlp.down_proj has nan: False
25-01-23 07:32:33 | I | In layer model.layers.0, gradient of model.layers.0.mlp.down_proj has inf: False
25-01-23 07:32:33 | I | [8] done with optimizer step
25-01-23 07:32:33 | I | epoch 001:     13 / 819200000 loss=1.70786e-05, loss_per_token=0.0699539, loss_sum=573.062, wps=223.3, ups=0.03, wpb=8192, bsz=16, num_updates=9, lr=0.0009, gnorm=3.922, clip=100, loss_scale=8, train_wall=37, cuda_gb_allocated=16.3, cuda_gb_reserved=17.7, cuda_gb_free=7.4, wall=500
25-01-23 07:32:33 | I | TRAIN CURRENT LAYER_IDX = 0
25-01-23 07:32:33 | I | in layer model.layers.0
25-01-23 07:32:33 | I | quantizing weights for layer model.layers.0
25-01-23 07:32:33 | I | collecting calibration activations in model.layers.0
25-01-23 07:32:33 | I | collecting calibration activations in model.layers.0
25-01-23 07:32:34 | I | - Quantizing decoder layer model.layers.0
25-01-23 07:32:34 | I |   - Calibrating model.layers.0.self_attn.q_proj.weight
25-01-23 07:32:35 | I |       - range scale = [    1.0000]
25-01-23 07:32:35 | I |         sum  error  = [    0.0521]
25-01-23 07:32:35 | I |         best error  = [    0.0521]
25-01-23 07:32:35 | I |     + error = [0.0521]
25-01-23 07:32:35 | I |       - range scale = [    1.0000]
25-01-23 07:32:35 | I |         sum  error  = [    0.6872]
25-01-23 07:32:35 | I |         best error  = [    0.6872]
25-01-23 07:32:35 | I |     + error = [0.6872]
25-01-23 07:32:35 | I |   - Calibrating model.layers.0.self_attn.k_proj.weight
25-01-23 07:32:36 | I |       - range scale = [    1.0000]
25-01-23 07:32:36 | I |         sum  error  = [    0.0582]
25-01-23 07:32:36 | I |         best error  = [    0.0582]
25-01-23 07:32:36 | I |     + error = [0.0582]
25-01-23 07:32:37 | I |       - range scale = [    1.0000]
25-01-23 07:32:37 | I |         sum  error  = [    0.9837]
25-01-23 07:32:37 | I |         best error  = [    0.9837]
25-01-23 07:32:37 | I |     + error = [0.9837]
25-01-23 07:32:37 | I |   - Calibrating model.layers.0.self_attn.v_proj.weight
25-01-23 07:32:38 | I |       - range scale = [    1.0000]
25-01-23 07:32:38 | I |         sum  error  = [    0.6920]
25-01-23 07:32:38 | I |         best error  = [    0.6920]
25-01-23 07:32:38 | I |     + error = [0.6920]
25-01-23 07:32:39 | I |       - range scale = [    1.0000]
25-01-23 07:32:39 | I |         sum  error  = [    6.4157]
25-01-23 07:32:39 | I |         best error  = [    6.4157]
25-01-23 07:32:39 | I |     + error = [6.4157]
25-01-23 07:32:39 | I |   - Calibrating model.layers.0.self_attn.o_proj.weight
25-01-23 07:32:39 | I |       - range scale = [    1.0000]
25-01-23 07:32:39 | I |         sum  error  = [    0.2116]
25-01-23 07:32:39 | I |         best error  = [    0.2116]
25-01-23 07:32:39 | I |     + error = [0.2116]
25-01-23 07:32:40 | I |       - range scale = [    1.0000]
25-01-23 07:32:40 | I |         sum  error  = [    1.6616]
25-01-23 07:32:40 | I |         best error  = [    1.6616]
25-01-23 07:32:40 | I |     + error = [1.6616]
25-01-23 07:32:40 | I |   - Calibrating model.layers.0.mlp.up_proj.weight
25-01-23 07:32:41 | I |       - range scale = [    1.0000]
25-01-23 07:32:41 | I |         sum  error  = [    1.0467]
25-01-23 07:32:41 | I |         best error  = [    1.0467]
25-01-23 07:32:41 | I |     + error = [1.0467]
25-01-23 07:32:42 | I |       - range scale = [    1.0000]
25-01-23 07:32:42 | I |         sum  error  = [   11.2130]
25-01-23 07:32:42 | I |         best error  = [   11.2130]
25-01-23 07:32:42 | I |     + error = [11.2130]
25-01-23 07:32:42 | I |   - Calibrating model.layers.0.mlp.gate_proj.weight
25-01-23 07:32:43 | I |       - range scale = [    1.0000]
25-01-23 07:32:43 | I |         sum  error  = [    1.1199]
25-01-23 07:32:43 | I |         best error  = [    1.1199]
25-01-23 07:32:43 | I |     + error = [1.1199]
25-01-23 07:32:44 | I |       - range scale = [    1.0000]
25-01-23 07:32:44 | I |         sum  error  = [   11.5850]
25-01-23 07:32:44 | I |         best error  = [   11.5850]
25-01-23 07:32:44 | I |     + error = [11.5850]
25-01-23 07:32:44 | I |   - Calibrating model.layers.0.mlp.down_proj.weight
25-01-23 07:32:45 | I |       - range scale = [    1.0000]
25-01-23 07:32:45 | I |         sum  error  = [    0.1533]
25-01-23 07:32:45 | I |         best error  = [    0.1533]
25-01-23 07:32:45 | I |     + error = [0.1533]
25-01-23 07:32:46 | I |       - range scale = [    1.0000]
25-01-23 07:32:46 | I |         sum  error  = [    1.5066]
25-01-23 07:32:46 | I |         best error  = [    1.5066]
25-01-23 07:32:46 | I |     + error = [1.5066]
25-01-23 07:32:46 | I |   - Quantizing model.layers.0.self_attn.q_proj.weight
25-01-23 07:32:49 | I |   - Quantizing model.layers.0.self_attn.k_proj.weight
25-01-23 07:32:51 | I |   - Quantizing model.layers.0.self_attn.v_proj.weight
25-01-23 07:32:53 | I |   - Quantizing model.layers.0.self_attn.o_proj.weight
25-01-23 07:32:55 | I |   - Quantizing model.layers.0.mlp.up_proj.weight
25-01-23 07:32:58 | I |   - Quantizing model.layers.0.mlp.gate_proj.weight
25-01-23 07:33:00 | I |   - Quantizing model.layers.0.mlp.down_proj.weight
25-01-23 07:33:06 | I | quantizing activations for layer model.layers.0
25-01-23 07:33:06 | I | collecting calibration activations in model.layers.0
25-01-23 07:33:06 | I | collecting calibration activations in model.layers.0
25-01-23 07:33:08 | I | forward this layer
25-01-23 07:33:08 | I | input_file: /data/gyy/lmquant-main/lmquant/data/data_without_preprocess_llama_7b_bs16/shard/val_input_args_layer_0/13.pt
25-01-23 07:33:08 | I | output_file: /data/gyy/lmquant-main/lmquant/data/data_without_preprocess_llama_7b_bs16/shard/val_teacher_output_layer_0/13.pt
25-01-23 07:33:09 | I | In layer model.layers.0, gradient of model.layers.0.self_attn.q_proj has nan: False
25-01-23 07:33:09 | I | In layer model.layers.0, gradient of model.layers.0.self_attn.q_proj has inf: False
25-01-23 07:33:09 | I | In layer model.layers.0, gradient of model.layers.0.self_attn.k_proj has nan: False
25-01-23 07:33:09 | I | In layer model.layers.0, gradient of model.layers.0.self_attn.k_proj has inf: False
25-01-23 07:33:09 | I | In layer model.layers.0, gradient of model.layers.0.self_attn.v_proj has nan: False
25-01-23 07:33:09 | I | In layer model.layers.0, gradient of model.layers.0.self_attn.v_proj has inf: False
25-01-23 07:33:09 | I | In layer model.layers.0, gradient of model.layers.0.self_attn.o_proj has nan: False
25-01-23 07:33:09 | I | In layer model.layers.0, gradient of model.layers.0.self_attn.o_proj has inf: False
25-01-23 07:33:09 | I | In layer model.layers.0, gradient of model.layers.0.mlp.up_proj has nan: False
25-01-23 07:33:09 | I | In layer model.layers.0, gradient of model.layers.0.mlp.up_proj has inf: False
25-01-23 07:33:09 | I | In layer model.layers.0, gradient of model.layers.0.mlp.gate_proj has nan: False
25-01-23 07:33:09 | I | In layer model.layers.0, gradient of model.layers.0.mlp.gate_proj has inf: False
25-01-23 07:33:09 | I | In layer model.layers.0, gradient of model.layers.0.mlp.down_proj has nan: False
25-01-23 07:33:09 | I | In layer model.layers.0, gradient of model.layers.0.mlp.down_proj has inf: False
25-01-23 07:33:09 | I | [9] done with optimizer step
25-01-23 07:33:09 | I | epoch 001:     14 / 819200000 loss=1.62776e-05, loss_per_token=0.0666731, loss_sum=546.186, wps=227.7, ups=0.03, wpb=8192, bsz=16, num_updates=10, lr=0.001, gnorm=2.885, clip=100, loss_scale=8, train_wall=36, cuda_gb_allocated=16.3, cuda_gb_reserved=17.7, cuda_gb_free=7.4, wall=536
25-01-23 07:33:09 | I | begin validation on "valid" subset on rank 0
25-01-23 07:33:09 | I | got valid iterator on "valid" subset on rank 0
25-01-23 07:33:09 | I | Valid: Start iterating over samples
25-01-23 07:33:09 | W |   `pretrained` model kwarg is not of type `str`. Many other model arguments may be ignored. Please do not launch via accelerate or use `parallelize=True` if passing an existing model this way.
25-01-23 07:33:09 | W |   Passed an already-initialized model through `pretrained`, assuming single-process call to evaluate() or custom distributed integration
25-01-23 07:33:09 | I | - Evaluator: gptq
25-01-23 07:33:09 | I | - Tasks: ['wikitext', 'val_valid']
25-01-23 07:33:09 | I | - Batch_size: 8
25-01-23 07:33:09 | I |   + Max_seq_length: 2048
25-01-23 07:34:44 | I |     - Results:
25-01-23 07:34:44 | I |       |  Task   |Version|    Metric     |Value |   |Stderr|
25-01-23 07:34:44 | I |       |---------|------:|---------------|-----:|---|-----:|
25-01-23 07:34:44 | I |       |wikitext |      1|word_perplexity|5.4853|±  |5.4853|
25-01-23 07:34:44 | I |       |val_valid|      1|word_perplexity|5.0286|±  |5.0286|
25-01-23 07:34:44 | I |       
25-01-23 07:34:44 | I |   + Max_seq_length: 4096
25-01-23 07:36:17 | I |     - Results:
25-01-23 07:36:17 | I |       |  Task   |Version|    Metric     |Value |   |Stderr|
25-01-23 07:36:17 | I |       |---------|------:|---------------|-----:|---|-----:|
25-01-23 07:36:17 | I |       |wikitext |      1|word_perplexity|5.1260|±  |5.1260|
25-01-23 07:36:17 | I |       |val_valid|      1|word_perplexity|4.8219|±  |4.8219|
25-01-23 07:36:17 | I |       
25-01-23 07:36:17 | I | in valid, quantize current layer weights
25-01-23 07:36:18 | W | Repo card metadata block was not found. Setting CardData to empty.
25-01-23 07:37:04 | I | collecting calibration activations in model.layers.0
25-01-23 07:37:04 | I | collecting calibration activations in model.layers.0
25-01-23 07:37:04 | I | collecting calibration activations in model.layers.0
25-01-23 07:37:04 | I | collecting calibration activations in model.layers.0
25-01-23 07:37:04 | I | collecting calibration activations in model.layers.0
25-01-23 07:37:04 | I | collecting calibration activations in model.layers.0
25-01-23 07:37:04 | I | collecting calibration activations in model.layers.0
25-01-23 07:37:04 | I | collecting calibration activations in model.layers.0
25-01-23 07:37:04 | I | collecting calibration activations in model.layers.0
25-01-23 07:37:04 | I | collecting calibration activations in model.layers.0
25-01-23 07:37:04 | I | collecting calibration activations in model.layers.0
25-01-23 07:37:04 | I | collecting calibration activations in model.layers.0
25-01-23 07:37:04 | I | collecting calibration activations in model.layers.0
25-01-23 07:37:05 | I | collecting calibration activations in model.layers.0
25-01-23 07:37:05 | I | collecting calibration activations in model.layers.0
25-01-23 07:37:05 | I | collecting calibration activations in model.layers.0
25-01-23 07:37:05 | I | collecting calibration activations in model.layers.0
25-01-23 07:37:05 | I | collecting calibration activations in model.layers.0
25-01-23 07:37:05 | I | collecting calibration activations in model.layers.0
25-01-23 07:37:05 | I | collecting calibration activations in model.layers.0
25-01-23 07:37:05 | I | collecting calibration activations in model.layers.0
25-01-23 07:37:05 | I | collecting calibration activations in model.layers.0
25-01-23 07:37:05 | I | collecting calibration activations in model.layers.0
25-01-23 07:37:05 | I | collecting calibration activations in model.layers.0
25-01-23 07:37:05 | I | collecting calibration activations in model.layers.0
25-01-23 07:37:05 | I | collecting calibration activations in model.layers.0
25-01-23 07:37:05 | I | collecting calibration activations in model.layers.0
25-01-23 07:37:05 | I | collecting calibration activations in model.layers.0
25-01-23 07:37:05 | I | collecting calibration activations in model.layers.0
25-01-23 07:37:05 | I | collecting calibration activations in model.layers.0
25-01-23 07:37:05 | I | collecting calibration activations in model.layers.0
25-01-23 07:37:05 | I | collecting calibration activations in model.layers.0
25-01-23 07:37:06 | I | collecting calibration activations in model.layers.0
25-01-23 07:37:06 | I | collecting calibration activations in model.layers.0
25-01-23 07:37:06 | I | collecting calibration activations in model.layers.0
25-01-23 07:37:06 | I | collecting calibration activations in model.layers.0
25-01-23 07:37:06 | I | collecting calibration activations in model.layers.0
25-01-23 07:37:06 | I | collecting calibration activations in model.layers.0
25-01-23 07:37:06 | I | collecting calibration activations in model.layers.0
25-01-23 07:37:06 | I | collecting calibration activations in model.layers.0
25-01-23 07:37:06 | I | collecting calibration activations in model.layers.0
25-01-23 07:37:06 | I | collecting calibration activations in model.layers.0
25-01-23 07:37:06 | I | collecting calibration activations in model.layers.0
25-01-23 07:37:06 | I | collecting calibration activations in model.layers.0
25-01-23 07:37:06 | I | collecting calibration activations in model.layers.0
25-01-23 07:37:06 | I | collecting calibration activations in model.layers.0
25-01-23 07:37:06 | I | collecting calibration activations in model.layers.0
25-01-23 07:37:06 | I | collecting calibration activations in model.layers.0
25-01-23 07:37:06 | I | collecting calibration activations in model.layers.0
25-01-23 07:37:06 | I | collecting calibration activations in model.layers.0
25-01-23 07:37:06 | I | collecting calibration activations in model.layers.0
25-01-23 07:37:07 | I | collecting calibration activations in model.layers.0
25-01-23 07:37:07 | I | collecting calibration activations in model.layers.0
25-01-23 07:37:07 | I | collecting calibration activations in model.layers.0
25-01-23 07:37:07 | I | collecting calibration activations in model.layers.0
25-01-23 07:37:07 | I | collecting calibration activations in model.layers.0
25-01-23 07:37:07 | I | collecting calibration activations in model.layers.0
25-01-23 07:37:07 | I | collecting calibration activations in model.layers.0
25-01-23 07:37:07 | I | collecting calibration activations in model.layers.0
25-01-23 07:37:07 | I | collecting calibration activations in model.layers.0
25-01-23 07:37:07 | I | collecting calibration activations in model.layers.0
25-01-23 07:37:07 | I | collecting calibration activations in model.layers.0
25-01-23 07:37:07 | I | collecting calibration activations in model.layers.0
25-01-23 07:37:07 | I | collecting calibration activations in model.layers.0
25-01-23 07:37:07 | I | collecting calibration activations in model.layers.0
25-01-23 07:37:07 | I | collecting calibration activations in model.layers.0
25-01-23 07:37:07 | I | collecting calibration activations in model.layers.0
25-01-23 07:37:07 | I | collecting calibration activations in model.layers.0
25-01-23 07:37:07 | I | collecting calibration activations in model.layers.0
25-01-23 07:37:08 | I | collecting calibration activations in model.layers.0
25-01-23 07:37:08 | I | collecting calibration activations in model.layers.0
25-01-23 07:37:08 | I | collecting calibration activations in model.layers.0
25-01-23 07:37:08 | I | collecting calibration activations in model.layers.0
25-01-23 07:37:08 | I | collecting calibration activations in model.layers.0
25-01-23 07:37:08 | I | collecting calibration activations in model.layers.0
25-01-23 07:37:08 | I | collecting calibration activations in model.layers.0
25-01-23 07:37:08 | I | collecting calibration activations in model.layers.0
25-01-23 07:37:08 | I | collecting calibration activations in model.layers.0
25-01-23 07:37:08 | I | collecting calibration activations in model.layers.0
25-01-23 07:37:08 | I | collecting calibration activations in model.layers.0
25-01-23 07:37:08 | I | collecting calibration activations in model.layers.0
25-01-23 07:37:08 | I | collecting calibration activations in model.layers.0
25-01-23 07:37:08 | I | collecting calibration activations in model.layers.0
25-01-23 07:37:08 | I | collecting calibration activations in model.layers.0
25-01-23 07:37:08 | I | collecting calibration activations in model.layers.0
25-01-23 07:37:08 | I | collecting calibration activations in model.layers.0
25-01-23 07:37:08 | I | collecting calibration activations in model.layers.0
25-01-23 07:37:08 | I | collecting calibration activations in model.layers.0
25-01-23 07:37:09 | I | collecting calibration activations in model.layers.0
25-01-23 07:37:09 | I | collecting calibration activations in model.layers.0
25-01-23 07:37:09 | I | collecting calibration activations in model.layers.0
25-01-23 07:37:09 | I | collecting calibration activations in model.layers.0
25-01-23 07:37:09 | I | collecting calibration activations in model.layers.0
25-01-23 07:37:09 | I | collecting calibration activations in model.layers.0
25-01-23 07:37:09 | I | collecting calibration activations in model.layers.0
25-01-23 07:37:09 | I | collecting calibration activations in model.layers.0
25-01-23 07:37:09 | I | collecting calibration activations in model.layers.0
25-01-23 07:37:09 | I | collecting calibration activations in model.layers.0
25-01-23 07:37:09 | I | collecting calibration activations in model.layers.0
25-01-23 07:37:09 | I | collecting calibration activations in model.layers.0
25-01-23 07:37:09 | I | collecting calibration activations in model.layers.0
25-01-23 07:37:09 | I | collecting calibration activations in model.layers.0
25-01-23 07:37:09 | I | collecting calibration activations in model.layers.0
25-01-23 07:37:09 | I | collecting calibration activations in model.layers.0
25-01-23 07:37:09 | I | collecting calibration activations in model.layers.0
25-01-23 07:37:09 | I | collecting calibration activations in model.layers.0
25-01-23 07:37:09 | I | collecting calibration activations in model.layers.0
25-01-23 07:37:10 | I | collecting calibration activations in model.layers.0
25-01-23 07:37:10 | I | collecting calibration activations in model.layers.0
25-01-23 07:37:10 | I | collecting calibration activations in model.layers.0
25-01-23 07:37:10 | I | collecting calibration activations in model.layers.0
25-01-23 07:37:10 | I | collecting calibration activations in model.layers.0
25-01-23 07:37:10 | I | collecting calibration activations in model.layers.0
25-01-23 07:37:10 | I | collecting calibration activations in model.layers.0
25-01-23 07:37:10 | I | collecting calibration activations in model.layers.0
25-01-23 07:37:10 | I | collecting calibration activations in model.layers.0
25-01-23 07:37:10 | I | collecting calibration activations in model.layers.0
25-01-23 07:37:10 | I | collecting calibration activations in model.layers.0
25-01-23 07:37:10 | I | collecting calibration activations in model.layers.0
25-01-23 07:37:10 | I | collecting calibration activations in model.layers.0
25-01-23 07:37:10 | I | collecting calibration activations in model.layers.0
25-01-23 07:37:10 | I | collecting calibration activations in model.layers.0
25-01-23 07:37:10 | I | collecting calibration activations in model.layers.0
25-01-23 07:37:10 | I | collecting calibration activations in model.layers.0
25-01-23 07:37:10 | I | collecting calibration activations in model.layers.0
25-01-23 07:37:10 | I | collecting calibration activations in model.layers.0
25-01-23 07:37:10 | I | collecting calibration activations in model.layers.0
25-01-23 07:37:11 | I | collecting calibration activations in model.layers.0
25-01-23 07:37:12 | I |       - range scale = [    1.0000]
25-01-23 07:37:12 | I |         sum  error  = [    0.1608]
25-01-23 07:37:12 | I |         best error  = [    0.1608]
25-01-23 07:37:12 | I |     + error = [0.1608]
25-01-23 07:37:14 | I |       - range scale = [    1.0000]
25-01-23 07:37:14 | I |         sum  error  = [    2.8225]
25-01-23 07:37:14 | I |         best error  = [    2.8225]
25-01-23 07:37:14 | I |     + error = [2.8225]
25-01-23 07:37:16 | I |       - range scale = [    1.0000]
25-01-23 07:37:16 | I |         sum  error  = [    0.2001]
25-01-23 07:37:16 | I |         best error  = [    0.2001]
25-01-23 07:37:16 | I |     + error = [0.2001]
25-01-23 07:37:17 | I |       - range scale = [    1.0000]
25-01-23 07:37:17 | I |         sum  error  = [    3.9780]
25-01-23 07:37:17 | I |         best error  = [    3.9780]
25-01-23 07:37:17 | I |     + error = [3.9780]
25-01-23 07:37:18 | I |       - range scale = [    1.0000]
25-01-23 07:37:18 | I |         sum  error  = [    0.6505]
25-01-23 07:37:18 | I |         best error  = [    0.6505]
25-01-23 07:37:18 | I |     + error = [0.6505]
25-01-23 07:37:19 | I |       - range scale = [    1.0000]
25-01-23 07:37:19 | I |         sum  error  = [    5.9877]
25-01-23 07:37:19 | I |         best error  = [    5.9877]
25-01-23 07:37:19 | I |     + error = [5.9877]
25-01-23 07:37:20 | I |       - range scale = [    1.0000]
25-01-23 07:37:20 | I |         sum  error  = [    0.1532]
25-01-23 07:37:20 | I |         best error  = [    0.1532]
25-01-23 07:37:20 | I |     + error = [0.1532]
25-01-23 07:37:20 | I |       - range scale = [    1.0000]
25-01-23 07:37:20 | I |         sum  error  = [    1.2563]
25-01-23 07:37:20 | I |         best error  = [    1.2563]
25-01-23 07:37:20 | I |     + error = [1.2563]
25-01-23 07:37:21 | I |       - range scale = [    1.0000]
25-01-23 07:37:21 | I |         sum  error  = [    1.1022]
25-01-23 07:37:21 | I |         best error  = [    1.1022]
25-01-23 07:37:21 | I |     + error = [1.1022]
25-01-23 07:37:23 | I |       - range scale = [    1.0000]
25-01-23 07:37:23 | I |         sum  error  = [   11.7370]
25-01-23 07:37:23 | I |         best error  = [   11.7370]
25-01-23 07:37:23 | I |     + error = [11.7370]
25-01-23 07:37:23 | I |       - range scale = [    1.0000]
25-01-23 07:37:23 | I |         sum  error  = [    1.1651]
25-01-23 07:37:23 | I |         best error  = [    1.1651]
25-01-23 07:37:23 | I |     + error = [1.1651]
25-01-23 07:37:25 | I |       - range scale = [    1.0000]
25-01-23 07:37:25 | I |         sum  error  = [   12.1141]
25-01-23 07:37:25 | I |         best error  = [   12.1141]
25-01-23 07:37:25 | I |     + error = [12.1141]
25-01-23 07:37:26 | I |       - range scale = [    1.0000]
25-01-23 07:37:26 | I |         sum  error  = [    0.1852]
25-01-23 07:37:26 | I |         best error  = [    0.1852]
25-01-23 07:37:26 | I |     + error = [0.1852]
25-01-23 07:37:27 | I |       - range scale = [    1.0000]
25-01-23 07:37:27 | I |         sum  error  = [    1.8218]
25-01-23 07:37:27 | I |         best error  = [    1.8218]
25-01-23 07:37:27 | I |     + error = [1.8218]
25-01-23 07:37:52 | I | in valid, quantize current layer acts
25-01-23 07:37:54 | I | collecting calibration activations in model.layers.0
25-01-23 07:37:54 | I | collecting calibration activations in model.layers.0
25-01-23 07:37:54 | I | collecting calibration activations in model.layers.0
25-01-23 07:37:54 | I | collecting calibration activations in model.layers.0
25-01-23 07:37:54 | I | collecting calibration activations in model.layers.0
25-01-23 07:37:54 | I | collecting calibration activations in model.layers.0
25-01-23 07:37:54 | I | collecting calibration activations in model.layers.0
25-01-23 07:37:54 | I | collecting calibration activations in model.layers.0
25-01-23 07:37:54 | I | collecting calibration activations in model.layers.0
25-01-23 07:37:54 | I | collecting calibration activations in model.layers.0
25-01-23 07:37:54 | I | collecting calibration activations in model.layers.0
25-01-23 07:37:54 | I | collecting calibration activations in model.layers.0
25-01-23 07:37:54 | I | collecting calibration activations in model.layers.0
25-01-23 07:37:54 | I | collecting calibration activations in model.layers.0
25-01-23 07:37:54 | I | collecting calibration activations in model.layers.0
25-01-23 07:37:55 | I | collecting calibration activations in model.layers.0
25-01-23 07:37:55 | I | collecting calibration activations in model.layers.0
25-01-23 07:37:55 | I | collecting calibration activations in model.layers.0
25-01-23 07:37:55 | I | collecting calibration activations in model.layers.0
25-01-23 07:37:55 | I | collecting calibration activations in model.layers.0
25-01-23 07:37:55 | I | collecting calibration activations in model.layers.0
25-01-23 07:37:55 | I | collecting calibration activations in model.layers.0
25-01-23 07:37:55 | I | collecting calibration activations in model.layers.0
25-01-23 07:37:55 | I | collecting calibration activations in model.layers.0
25-01-23 07:37:55 | I | collecting calibration activations in model.layers.0
25-01-23 07:37:55 | I | collecting calibration activations in model.layers.0
25-01-23 07:37:55 | I | collecting calibration activations in model.layers.0
25-01-23 07:37:55 | I | collecting calibration activations in model.layers.0
25-01-23 07:37:55 | I | collecting calibration activations in model.layers.0
25-01-23 07:37:55 | I | collecting calibration activations in model.layers.0
25-01-23 07:37:56 | I | collecting calibration activations in model.layers.0
25-01-23 07:37:56 | I | collecting calibration activations in model.layers.0
25-01-23 07:37:56 | I | collecting calibration activations in model.layers.0
25-01-23 07:37:56 | I | collecting calibration activations in model.layers.0
25-01-23 07:37:56 | I | collecting calibration activations in model.layers.0
25-01-23 07:37:56 | I | collecting calibration activations in model.layers.0
25-01-23 07:37:56 | I | collecting calibration activations in model.layers.0
25-01-23 07:37:56 | I | collecting calibration activations in model.layers.0
25-01-23 07:37:56 | I | collecting calibration activations in model.layers.0
25-01-23 07:37:56 | I | collecting calibration activations in model.layers.0
25-01-23 07:37:56 | I | collecting calibration activations in model.layers.0
25-01-23 07:37:56 | I | collecting calibration activations in model.layers.0
25-01-23 07:37:56 | I | collecting calibration activations in model.layers.0
25-01-23 07:37:56 | I | collecting calibration activations in model.layers.0
25-01-23 07:37:56 | I | collecting calibration activations in model.layers.0
25-01-23 07:37:57 | I | collecting calibration activations in model.layers.0
25-01-23 07:37:57 | I | collecting calibration activations in model.layers.0
25-01-23 07:37:57 | I | collecting calibration activations in model.layers.0
25-01-23 07:37:57 | I | collecting calibration activations in model.layers.0
25-01-23 07:37:57 | I | collecting calibration activations in model.layers.0
25-01-23 07:37:57 | I | collecting calibration activations in model.layers.0
25-01-23 07:37:57 | I | collecting calibration activations in model.layers.0
25-01-23 07:37:57 | I | collecting calibration activations in model.layers.0
25-01-23 07:37:57 | I | collecting calibration activations in model.layers.0
25-01-23 07:37:57 | I | collecting calibration activations in model.layers.0
25-01-23 07:37:57 | I | collecting calibration activations in model.layers.0
25-01-23 07:37:57 | I | collecting calibration activations in model.layers.0
25-01-23 07:37:57 | I | collecting calibration activations in model.layers.0
25-01-23 07:37:57 | I | collecting calibration activations in model.layers.0
25-01-23 07:37:57 | I | collecting calibration activations in model.layers.0
25-01-23 07:37:58 | I | collecting calibration activations in model.layers.0
25-01-23 07:37:58 | I | collecting calibration activations in model.layers.0
25-01-23 07:37:58 | I | collecting calibration activations in model.layers.0
25-01-23 07:37:58 | I | collecting calibration activations in model.layers.0
25-01-23 07:37:58 | I | collecting calibration activations in model.layers.0
25-01-23 07:37:58 | I | collecting calibration activations in model.layers.0
25-01-23 07:37:58 | I | collecting calibration activations in model.layers.0
25-01-23 07:37:58 | I | collecting calibration activations in model.layers.0
25-01-23 07:37:58 | I | collecting calibration activations in model.layers.0
25-01-23 07:37:58 | I | collecting calibration activations in model.layers.0
25-01-23 07:37:58 | I | collecting calibration activations in model.layers.0
25-01-23 07:37:58 | I | collecting calibration activations in model.layers.0
25-01-23 07:37:58 | I | collecting calibration activations in model.layers.0
25-01-23 07:37:58 | I | collecting calibration activations in model.layers.0
25-01-23 07:37:58 | I | collecting calibration activations in model.layers.0
25-01-23 07:37:59 | I | collecting calibration activations in model.layers.0
25-01-23 07:37:59 | I | collecting calibration activations in model.layers.0
25-01-23 07:37:59 | I | collecting calibration activations in model.layers.0
25-01-23 07:37:59 | I | collecting calibration activations in model.layers.0
25-01-23 07:37:59 | I | collecting calibration activations in model.layers.0
25-01-23 07:37:59 | I | collecting calibration activations in model.layers.0
25-01-23 07:37:59 | I | collecting calibration activations in model.layers.0
25-01-23 07:37:59 | I | collecting calibration activations in model.layers.0
25-01-23 07:37:59 | I | collecting calibration activations in model.layers.0
25-01-23 07:37:59 | I | collecting calibration activations in model.layers.0
25-01-23 07:37:59 | I | collecting calibration activations in model.layers.0
25-01-23 07:37:59 | I | collecting calibration activations in model.layers.0
25-01-23 07:37:59 | I | collecting calibration activations in model.layers.0
25-01-23 07:37:59 | I | collecting calibration activations in model.layers.0
25-01-23 07:37:59 | I | collecting calibration activations in model.layers.0
25-01-23 07:37:59 | I | collecting calibration activations in model.layers.0
25-01-23 07:38:00 | I | collecting calibration activations in model.layers.0
25-01-23 07:38:00 | I | collecting calibration activations in model.layers.0
25-01-23 07:38:00 | I | collecting calibration activations in model.layers.0
25-01-23 07:38:00 | I | collecting calibration activations in model.layers.0
25-01-23 07:38:00 | I | collecting calibration activations in model.layers.0
25-01-23 07:38:00 | I | collecting calibration activations in model.layers.0
25-01-23 07:38:00 | I | collecting calibration activations in model.layers.0
25-01-23 07:38:00 | I | collecting calibration activations in model.layers.0
25-01-23 07:38:00 | I | collecting calibration activations in model.layers.0
25-01-23 07:38:00 | I | collecting calibration activations in model.layers.0
25-01-23 07:38:00 | I | collecting calibration activations in model.layers.0
25-01-23 07:38:00 | I | collecting calibration activations in model.layers.0
25-01-23 07:38:00 | I | collecting calibration activations in model.layers.0
25-01-23 07:38:00 | I | collecting calibration activations in model.layers.0
25-01-23 07:38:00 | I | collecting calibration activations in model.layers.0
25-01-23 07:38:01 | I | collecting calibration activations in model.layers.0
25-01-23 07:38:01 | I | collecting calibration activations in model.layers.0
25-01-23 07:38:01 | I | collecting calibration activations in model.layers.0
25-01-23 07:38:01 | I | collecting calibration activations in model.layers.0
25-01-23 07:38:01 | I | collecting calibration activations in model.layers.0
25-01-23 07:38:01 | I | collecting calibration activations in model.layers.0
25-01-23 07:38:01 | I | collecting calibration activations in model.layers.0
25-01-23 07:38:01 | I | collecting calibration activations in model.layers.0
25-01-23 07:38:01 | I | collecting calibration activations in model.layers.0
25-01-23 07:38:01 | I | collecting calibration activations in model.layers.0
25-01-23 07:38:01 | I | collecting calibration activations in model.layers.0
25-01-23 07:38:01 | I | collecting calibration activations in model.layers.0
25-01-23 07:38:01 | I | collecting calibration activations in model.layers.0
25-01-23 07:38:01 | I | collecting calibration activations in model.layers.0
25-01-23 07:38:01 | I | collecting calibration activations in model.layers.0
25-01-23 07:38:02 | I | collecting calibration activations in model.layers.0
25-01-23 07:38:02 | I | collecting calibration activations in model.layers.0
25-01-23 07:38:02 | I | collecting calibration activations in model.layers.0
25-01-23 07:38:02 | I | collecting calibration activations in model.layers.0
25-01-23 07:38:02 | I | collecting calibration activations in model.layers.0
25-01-23 07:38:02 | I | collecting calibration activations in model.layers.0
25-01-23 07:38:02 | I | collecting calibration activations in model.layers.0
25-01-23 07:38:04 | I | use gptq_eval(lmquant) to calculate ppl, partly quanted
25-01-23 07:38:04 | W |   `pretrained` model kwarg is not of type `str`. Many other model arguments may be ignored. Please do not launch via accelerate or use `parallelize=True` if passing an existing model this way.
25-01-23 07:38:04 | W |   Passed an already-initialized model through `pretrained`, assuming single-process call to evaluate() or custom distributed integration
25-01-23 07:38:04 | I | - Evaluator: gptq
25-01-23 07:38:04 | I | - Tasks: ['wikitext', 'val_valid']
25-01-23 07:38:04 | I | - Batch_size: 8
25-01-23 07:38:04 | I |   + Max_seq_length: 2048
25-01-23 07:39:42 | I |     - Results:
25-01-23 07:39:42 | I |       |  Task   |Version|    Metric     |Value |   |Stderr|
25-01-23 07:39:42 | I |       |---------|------:|---------------|-----:|---|-----:|
25-01-23 07:39:42 | I |       |wikitext |      1|word_perplexity|5.4914|±  |5.4914|
25-01-23 07:39:42 | I |       |val_valid|      1|word_perplexity|5.0343|±  |5.0343|
25-01-23 07:39:42 | I |       
25-01-23 07:39:42 | I |   + Max_seq_length: 4096
25-01-23 07:41:17 | I |     - Results:
25-01-23 07:41:17 | I |       |  Task   |Version|    Metric     |Value |   |Stderr|
25-01-23 07:41:17 | I |       |---------|------:|---------------|-----:|---|-----:|
25-01-23 07:41:17 | I |       |wikitext |      1|word_perplexity|5.1307|±  |5.1307|
25-01-23 07:41:17 | I |       |val_valid|      1|word_perplexity|4.8280|±  |4.8280|
25-01-23 07:41:17 | I |       
25-01-23 07:41:19 | W | Criterion.reduce_metrics did not log a 'loss_valid' value, which may break some functionality
25-01-23 07:42:17 | I | epoch 001 | valid on 'valid' subset:    102 / 819200000 loss_valid=2.149, lmquant_ppl_result_val=-1, lmquant_ppl_result_wikitext=-1, ppl=4.44, wps=7219.2, wpb=4058.2, bsz=2, num_updates=10, lmquant_ppl_wikitext_all_quanted=5.48528, lmquant_ppl_val_all_quanted=5.02862, lmquant_ppl_wikitext_partly_quanted=5.4914, lmquant_ppl_val_partly_quanted=5.03431
25-01-23 07:42:17 | I | epoch 001 | valid on 'valid' subset | loss_valid 2.149 | lmquant_ppl_result_val -1 | lmquant_ppl_result_wikitext -1 | ppl 4.44 | wps 7219.2 | wpb 4058.2 | bsz 2 | num_updates 10 | lmquant_ppl_wikitext_all_quanted 5.48528 | lmquant_ppl_val_all_quanted 5.02862 | lmquant_ppl_wikitext_partly_quanted 5.4914 | lmquant_ppl_val_partly_quanted 5.03431
25-01-23 07:42:17 | I | TRAIN CURRENT LAYER_IDX = 0
25-01-23 07:42:17 | I | in layer model.layers.0
25-01-23 07:42:17 | I | quantizing weights for layer model.layers.0
25-01-23 07:42:17 | I | collecting calibration activations in model.layers.0
25-01-23 07:42:17 | I | collecting calibration activations in model.layers.0
25-01-23 07:42:18 | I | - Quantizing decoder layer model.layers.0
25-01-23 07:42:18 | I |   - Calibrating model.layers.0.self_attn.q_proj.weight
25-01-23 07:42:18 | I |       - range scale = [    1.0000]
25-01-23 07:42:18 | I |         sum  error  = [    0.0483]
25-01-23 07:42:18 | I |         best error  = [    0.0483]
25-01-23 07:42:18 | I |     + error = [0.0483]
25-01-23 07:42:19 | I |       - range scale = [    1.0000]
25-01-23 07:42:19 | I |         sum  error  = [    0.7567]
25-01-23 07:42:19 | I |         best error  = [    0.7567]
25-01-23 07:42:19 | I |     + error = [0.7567]
25-01-23 07:42:19 | I |   - Calibrating model.layers.0.self_attn.k_proj.weight
25-01-23 07:42:20 | I |       - range scale = [    1.0000]
25-01-23 07:42:20 | I |         sum  error  = [    0.0613]
25-01-23 07:42:20 | I |         best error  = [    0.0613]
25-01-23 07:42:20 | I |     + error = [0.0613]
25-01-23 07:42:21 | I |       - range scale = [    1.0000]
25-01-23 07:42:21 | I |         sum  error  = [    1.0618]
25-01-23 07:42:21 | I |         best error  = [    1.0618]
25-01-23 07:42:21 | I |     + error = [1.0618]
25-01-23 07:42:21 | I |   - Calibrating model.layers.0.self_attn.v_proj.weight
25-01-23 07:42:21 | I |       - range scale = [    1.0000]
25-01-23 07:42:21 | I |         sum  error  = [    0.7213]
25-01-23 07:42:21 | I |         best error  = [    0.7213]
25-01-23 07:42:21 | I |     + error = [0.7213]
25-01-23 07:42:22 | I |       - range scale = [    1.0000]
25-01-23 07:42:22 | I |         sum  error  = [    6.6320]
25-01-23 07:42:22 | I |         best error  = [    6.6320]
25-01-23 07:42:22 | I |     + error = [6.6320]
25-01-23 07:42:22 | I |   - Calibrating model.layers.0.self_attn.o_proj.weight
25-01-23 07:42:23 | I |       - range scale = [    1.0000]
25-01-23 07:42:23 | I |         sum  error  = [    0.2128]
25-01-23 07:42:23 | I |         best error  = [    0.2128]
25-01-23 07:42:23 | I |     + error = [0.2128]
25-01-23 07:42:24 | I |       - range scale = [    1.0000]
25-01-23 07:42:24 | I |         sum  error  = [    1.6924]
25-01-23 07:42:24 | I |         best error  = [    1.6924]
25-01-23 07:42:24 | I |     + error = [1.6924]
25-01-23 07:42:24 | I |   - Calibrating model.layers.0.mlp.up_proj.weight
25-01-23 07:42:25 | I |       - range scale = [    1.0000]
25-01-23 07:42:25 | I |         sum  error  = [    1.0783]
25-01-23 07:42:25 | I |         best error  = [    1.0783]
25-01-23 07:42:25 | I |     + error = [1.0783]
25-01-23 07:42:26 | I |       - range scale = [    1.0000]
25-01-23 07:42:26 | I |         sum  error  = [   11.4569]
25-01-23 07:42:26 | I |         best error  = [   11.4569]
25-01-23 07:42:26 | I |     + error = [11.4569]
25-01-23 07:42:26 | I |   - Calibrating model.layers.0.mlp.gate_proj.weight
25-01-23 07:42:27 | I |       - range scale = [    1.0000]
25-01-23 07:42:27 | I |         sum  error  = [    1.1351]
25-01-23 07:42:27 | I |         best error  = [    1.1351]
25-01-23 07:42:27 | I |     + error = [1.1351]
25-01-23 07:42:28 | I |       - range scale = [    1.0000]
25-01-23 07:42:28 | I |         sum  error  = [   11.8452]
25-01-23 07:42:28 | I |         best error  = [   11.8452]
25-01-23 07:42:28 | I |     + error = [11.8452]
25-01-23 07:42:28 | I |   - Calibrating model.layers.0.mlp.down_proj.weight
25-01-23 07:42:29 | I |       - range scale = [    1.0000]
25-01-23 07:42:29 | I |         sum  error  = [    0.1713]
25-01-23 07:42:29 | I |         best error  = [    0.1713]
25-01-23 07:42:29 | I |     + error = [0.1713]
25-01-23 07:42:30 | I |       - range scale = [    1.0000]
25-01-23 07:42:30 | I |         sum  error  = [    1.6618]
25-01-23 07:42:30 | I |         best error  = [    1.6618]
25-01-23 07:42:30 | I |     + error = [1.6618]
25-01-23 07:42:30 | I |   - Quantizing model.layers.0.self_attn.q_proj.weight
25-01-23 07:42:33 | I |   - Quantizing model.layers.0.self_attn.k_proj.weight
25-01-23 07:42:36 | I |   - Quantizing model.layers.0.self_attn.v_proj.weight
25-01-23 07:42:39 | I |   - Quantizing model.layers.0.self_attn.o_proj.weight
25-01-23 07:42:42 | I |   - Quantizing model.layers.0.mlp.up_proj.weight
25-01-23 07:42:45 | I |   - Quantizing model.layers.0.mlp.gate_proj.weight
25-01-23 07:42:48 | I |   - Quantizing model.layers.0.mlp.down_proj.weight
25-01-23 07:42:56 | I | quantizing activations for layer model.layers.0
25-01-23 07:42:56 | I | collecting calibration activations in model.layers.0
25-01-23 07:42:56 | I | collecting calibration activations in model.layers.0
25-01-23 07:42:58 | I | forward this layer
25-01-23 07:42:58 | I | input_file: /data/gyy/lmquant-main/lmquant/data/data_without_preprocess_llama_7b_bs16/shard/val_input_args_layer_0/14.pt
25-01-23 07:42:58 | I | output_file: /data/gyy/lmquant-main/lmquant/data/data_without_preprocess_llama_7b_bs16/shard/val_teacher_output_layer_0/14.pt
25-01-23 07:42:59 | I | In layer model.layers.0, gradient of model.layers.0.self_attn.q_proj has nan: False
25-01-23 07:42:59 | I | In layer model.layers.0, gradient of model.layers.0.self_attn.q_proj has inf: False
25-01-23 07:42:59 | I | In layer model.layers.0, gradient of model.layers.0.self_attn.k_proj has nan: False
25-01-23 07:42:59 | I | In layer model.layers.0, gradient of model.layers.0.self_attn.k_proj has inf: False
25-01-23 07:42:59 | I | In layer model.layers.0, gradient of model.layers.0.self_attn.v_proj has nan: False
25-01-23 07:42:59 | I | In layer model.layers.0, gradient of model.layers.0.self_attn.v_proj has inf: False
25-01-23 07:42:59 | I | In layer model.layers.0, gradient of model.layers.0.self_attn.o_proj has nan: False
25-01-23 07:42:59 | I | In layer model.layers.0, gradient of model.layers.0.self_attn.o_proj has inf: False
25-01-23 07:42:59 | I | In layer model.layers.0, gradient of model.layers.0.mlp.up_proj has nan: False
25-01-23 07:42:59 | I | In layer model.layers.0, gradient of model.layers.0.mlp.up_proj has inf: False
25-01-23 07:42:59 | I | In layer model.layers.0, gradient of model.layers.0.mlp.gate_proj has nan: False
25-01-23 07:42:59 | I | In layer model.layers.0, gradient of model.layers.0.mlp.gate_proj has inf: False
25-01-23 07:42:59 | I | In layer model.layers.0, gradient of model.layers.0.mlp.down_proj has nan: False
25-01-23 07:42:59 | I | In layer model.layers.0, gradient of model.layers.0.mlp.down_proj has inf: False
25-01-23 07:42:59 | I | [10] done with optimizer step
25-01-23 07:42:59 | I | epoch 001:     15 / 819200000 loss=1.72955e-05, loss_per_token=0.0708425, loss_sum=580.342, wps=13.9, ups=0, wpb=8192, bsz=16, num_updates=11, lr=0.0011, gnorm=0.244, clip=0, loss_scale=8, train_wall=42, cuda_gb_allocated=16.8, cuda_gb_reserved=19.7, cuda_gb_free=6.9, wall=1126
25-01-23 07:42:59 | I | TRAIN CURRENT LAYER_IDX = 0
25-01-23 07:42:59 | I | in layer model.layers.0
25-01-23 07:42:59 | I | quantizing weights for layer model.layers.0
25-01-23 07:42:59 | I | collecting calibration activations in model.layers.0
25-01-23 07:42:59 | I | collecting calibration activations in model.layers.0
25-01-23 07:42:59 | I | - Quantizing decoder layer model.layers.0
25-01-23 07:42:59 | I |   - Calibrating model.layers.0.self_attn.q_proj.weight
25-01-23 07:43:00 | I |       - range scale = [    1.0000]
25-01-23 07:43:00 | I |         sum  error  = [    0.0423]
25-01-23 07:43:00 | I |         best error  = [    0.0423]
25-01-23 07:43:00 | I |     + error = [0.0423]
25-01-23 07:43:01 | I |       - range scale = [    1.0000]
25-01-23 07:43:01 | I |         sum  error  = [    0.6548]
25-01-23 07:43:01 | I |         best error  = [    0.6548]
25-01-23 07:43:01 | I |     + error = [0.6548]
25-01-23 07:43:01 | I |   - Calibrating model.layers.0.self_attn.k_proj.weight
25-01-23 07:43:02 | I |       - range scale = [    1.0000]
25-01-23 07:43:02 | I |         sum  error  = [    0.0530]
25-01-23 07:43:02 | I |         best error  = [    0.0530]
25-01-23 07:43:02 | I |     + error = [0.0530]
25-01-23 07:43:03 | I |       - range scale = [    1.0000]
25-01-23 07:43:03 | I |         sum  error  = [    1.0583]
25-01-23 07:43:03 | I |         best error  = [    1.0583]
25-01-23 07:43:03 | I |     + error = [1.0583]
25-01-23 07:43:03 | I |   - Calibrating model.layers.0.self_attn.v_proj.weight
25-01-23 07:43:04 | I |       - range scale = [    1.0000]
25-01-23 07:43:04 | I |         sum  error  = [    0.6664]
25-01-23 07:43:04 | I |         best error  = [    0.6664]
25-01-23 07:43:04 | I |     + error = [0.6664]
25-01-23 07:43:05 | I |       - range scale = [    1.0000]
25-01-23 07:43:05 | I |         sum  error  = [    6.1852]
25-01-23 07:43:05 | I |         best error  = [    6.1852]
25-01-23 07:43:05 | I |     + error = [6.1852]
25-01-23 07:43:05 | I |   - Calibrating model.layers.0.self_attn.o_proj.weight
25-01-23 07:43:05 | I |       - range scale = [    1.0000]
25-01-23 07:43:05 | I |         sum  error  = [    0.2053]
25-01-23 07:43:05 | I |         best error  = [    0.2053]
25-01-23 07:43:05 | I |     + error = [0.2053]
25-01-23 07:43:06 | I |       - range scale = [    1.0000]
25-01-23 07:43:06 | I |         sum  error  = [    1.6416]
25-01-23 07:43:06 | I |         best error  = [    1.6416]
25-01-23 07:43:06 | I |     + error = [1.6416]
25-01-23 07:43:06 | I |   - Calibrating model.layers.0.mlp.up_proj.weight
25-01-23 07:43:07 | I |       - range scale = [    1.0000]
25-01-23 07:43:07 | I |         sum  error  = [    1.1122]
25-01-23 07:43:07 | I |         best error  = [    1.1122]
25-01-23 07:43:07 | I |     + error = [1.1122]
25-01-23 07:43:08 | I |       - range scale = [    1.0000]
25-01-23 07:43:08 | I |         sum  error  = [   11.8417]
25-01-23 07:43:08 | I |         best error  = [   11.8417]
25-01-23 07:43:08 | I |     + error = [11.8417]
25-01-23 07:43:08 | I |   - Calibrating model.layers.0.mlp.gate_proj.weight
25-01-23 07:43:09 | I |       - range scale = [    1.0000]
25-01-23 07:43:09 | I |         sum  error  = [    1.1711]
25-01-23 07:43:09 | I |         best error  = [    1.1711]
25-01-23 07:43:09 | I |     + error = [1.1711]
25-01-23 07:43:10 | I |       - range scale = [    1.0000]
25-01-23 07:43:10 | I |         sum  error  = [   12.2339]
25-01-23 07:43:10 | I |         best error  = [   12.2339]
25-01-23 07:43:10 | I |     + error = [12.2339]
25-01-23 07:43:11 | I |   - Calibrating model.layers.0.mlp.down_proj.weight
25-01-23 07:43:11 | I |       - range scale = [    1.0000]
25-01-23 07:43:11 | I |         sum  error  = [    0.1725]
25-01-23 07:43:11 | I |         best error  = [    0.1725]
25-01-23 07:43:11 | I |     + error = [0.1725]
25-01-23 07:43:13 | I |       - range scale = [    1.0000]
25-01-23 07:43:13 | I |         sum  error  = [    1.6713]
25-01-23 07:43:13 | I |         best error  = [    1.6713]
25-01-23 07:43:13 | I |     + error = [1.6713]
25-01-23 07:43:13 | I |   - Quantizing model.layers.0.self_attn.q_proj.weight
25-01-23 07:43:16 | I |   - Quantizing model.layers.0.self_attn.k_proj.weight
25-01-23 07:43:18 | I |   - Quantizing model.layers.0.self_attn.v_proj.weight
25-01-23 07:43:21 | I |   - Quantizing model.layers.0.self_attn.o_proj.weight
25-01-23 07:43:24 | I |   - Quantizing model.layers.0.mlp.up_proj.weight
25-01-23 07:43:27 | I |   - Quantizing model.layers.0.mlp.gate_proj.weight
25-01-23 07:43:30 | I |   - Quantizing model.layers.0.mlp.down_proj.weight
25-01-23 07:43:38 | I | quantizing activations for layer model.layers.0
25-01-23 07:43:38 | I | collecting calibration activations in model.layers.0
25-01-23 07:43:38 | I | collecting calibration activations in model.layers.0
25-01-23 07:43:40 | I | forward this layer
25-01-23 07:43:40 | I | input_file: /data/gyy/lmquant-main/lmquant/data/data_without_preprocess_llama_7b_bs16/shard/val_input_args_layer_0/15.pt
25-01-23 07:43:40 | I | output_file: /data/gyy/lmquant-main/lmquant/data/data_without_preprocess_llama_7b_bs16/shard/val_teacher_output_layer_0/15.pt
25-01-23 07:43:41 | I | In layer model.layers.0, gradient of model.layers.0.self_attn.q_proj has nan: False
25-01-23 07:43:41 | I | In layer model.layers.0, gradient of model.layers.0.self_attn.q_proj has inf: False
25-01-23 07:43:41 | I | In layer model.layers.0, gradient of model.layers.0.self_attn.k_proj has nan: False
25-01-23 07:43:41 | I | In layer model.layers.0, gradient of model.layers.0.self_attn.k_proj has inf: False
25-01-23 07:43:41 | I | In layer model.layers.0, gradient of model.layers.0.self_attn.v_proj has nan: False
25-01-23 07:43:41 | I | In layer model.layers.0, gradient of model.layers.0.self_attn.v_proj has inf: False
25-01-23 07:43:41 | I | In layer model.layers.0, gradient of model.layers.0.self_attn.o_proj has nan: False
25-01-23 07:43:41 | I | In layer model.layers.0, gradient of model.layers.0.self_attn.o_proj has inf: False
25-01-23 07:43:41 | I | In layer model.layers.0, gradient of model.layers.0.mlp.up_proj has nan: False
25-01-23 07:43:41 | I | In layer model.layers.0, gradient of model.layers.0.mlp.up_proj has inf: False
25-01-23 07:43:41 | I | In layer model.layers.0, gradient of model.layers.0.mlp.gate_proj has nan: False
25-01-23 07:43:41 | I | In layer model.layers.0, gradient of model.layers.0.mlp.gate_proj has inf: False
25-01-23 07:43:41 | I | In layer model.layers.0, gradient of model.layers.0.mlp.down_proj has nan: False
25-01-23 07:43:41 | I | In layer model.layers.0, gradient of model.layers.0.mlp.down_proj has inf: False
25-01-23 07:43:41 | I | [11] done with optimizer step
25-01-23 07:43:41 | I | epoch 001:     16 / 819200000 loss=1.76229e-05, loss_per_token=0.0721834, loss_sum=591.326, wps=194.5, ups=0.02, wpb=8192, bsz=16, num_updates=12, lr=0.0012, gnorm=0.503, clip=0, loss_scale=8, train_wall=42, cuda_gb_allocated=17.1, cuda_gb_reserved=18.6, cuda_gb_free=6.6, wall=1168
25-01-23 07:43:41 | I | TRAIN CURRENT LAYER_IDX = 0
25-01-23 07:43:41 | I | in layer model.layers.0
25-01-23 07:43:41 | I | quantizing weights for layer model.layers.0
25-01-23 07:43:41 | I | collecting calibration activations in model.layers.0
25-01-23 07:43:41 | I | collecting calibration activations in model.layers.0
25-01-23 07:43:42 | I | - Quantizing decoder layer model.layers.0
25-01-23 07:43:42 | I |   - Calibrating model.layers.0.self_attn.q_proj.weight
25-01-23 07:43:42 | I |       - range scale = [    1.0000]
25-01-23 07:43:42 | I |         sum  error  = [    0.0448]
25-01-23 07:43:42 | I |         best error  = [    0.0448]
25-01-23 07:43:42 | I |     + error = [0.0448]
25-01-23 07:43:43 | I |       - range scale = [    1.0000]
25-01-23 07:43:43 | I |         sum  error  = [    0.6996]
25-01-23 07:43:43 | I |         best error  = [    0.6996]
25-01-23 07:43:43 | I |     + error = [0.6996]
25-01-23 07:43:43 | I |   - Calibrating model.layers.0.self_attn.k_proj.weight
25-01-23 07:43:44 | I |       - range scale = [    1.0000]
25-01-23 07:43:44 | I |         sum  error  = [    0.0557]
25-01-23 07:43:44 | I |         best error  = [    0.0557]
25-01-23 07:43:44 | I |     + error = [0.0557]
25-01-23 07:43:45 | I |       - range scale = [    1.0000]
25-01-23 07:43:45 | I |         sum  error  = [    0.9489]
25-01-23 07:43:45 | I |         best error  = [    0.9489]
25-01-23 07:43:45 | I |     + error = [0.9489]
25-01-23 07:43:45 | I |   - Calibrating model.layers.0.self_attn.v_proj.weight
25-01-23 07:43:46 | I |       - range scale = [    1.0000]
25-01-23 07:43:46 | I |         sum  error  = [    0.6908]
25-01-23 07:43:46 | I |         best error  = [    0.6908]
25-01-23 07:43:46 | I |     + error = [0.6908]
25-01-23 07:43:46 | I |       - range scale = [    1.0000]
25-01-23 07:43:46 | I |         sum  error  = [    6.4560]
25-01-23 07:43:46 | I |         best error  = [    6.4560]
25-01-23 07:43:46 | I |     + error = [6.4560]
25-01-23 07:43:47 | I |   - Calibrating model.layers.0.self_attn.o_proj.weight
25-01-23 07:43:47 | I |       - range scale = [    1.0000]
25-01-23 07:43:47 | I |         sum  error  = [    0.2205]
25-01-23 07:43:47 | I |         best error  = [    0.2205]
25-01-23 07:43:47 | I |     + error = [0.2205]
25-01-23 07:43:48 | I |       - range scale = [    1.0000]
25-01-23 07:43:48 | I |         sum  error  = [    1.7446]
25-01-23 07:43:48 | I |         best error  = [    1.7446]
25-01-23 07:43:48 | I |     + error = [1.7446]
25-01-23 07:43:48 | I |   - Calibrating model.layers.0.mlp.up_proj.weight
25-01-23 07:43:49 | I |       - range scale = [    1.0000]
25-01-23 07:43:49 | I |         sum  error  = [    1.1277]
25-01-23 07:43:49 | I |         best error  = [    1.1277]
25-01-23 07:43:49 | I |     + error = [1.1277]
25-01-23 07:43:50 | I |       - range scale = [    1.0000]
25-01-23 07:43:50 | I |         sum  error  = [   12.0116]
25-01-23 07:43:50 | I |         best error  = [   12.0116]
25-01-23 07:43:50 | I |     + error = [12.0116]
25-01-23 07:43:50 | I |   - Calibrating model.layers.0.mlp.gate_proj.weight
25-01-23 07:43:51 | I |       - range scale = [    1.0000]
25-01-23 07:43:51 | I |         sum  error  = [    1.1954]
25-01-23 07:43:51 | I |         best error  = [    1.1954]
25-01-23 07:43:51 | I |     + error = [1.1954]
25-01-23 07:43:52 | I |       - range scale = [    1.0000]
25-01-23 07:43:52 | I |         sum  error  = [   12.4046]
25-01-23 07:43:52 | I |         best error  = [   12.4046]
25-01-23 07:43:52 | I |     + error = [12.4046]
25-01-23 07:43:52 | I |   - Calibrating model.layers.0.mlp.down_proj.weight
25-01-23 07:43:53 | I |       - range scale = [    1.0000]
25-01-23 07:43:53 | I |         sum  error  = [    0.1798]
25-01-23 07:43:53 | I |         best error  = [    0.1798]
25-01-23 07:43:53 | I |     + error = [0.1798]
25-01-23 07:43:54 | I |       - range scale = [    1.0000]
25-01-23 07:43:54 | I |         sum  error  = [    1.7447]
25-01-23 07:43:54 | I |         best error  = [    1.7447]
25-01-23 07:43:54 | I |     + error = [1.7447]
25-01-23 07:43:54 | I |   - Quantizing model.layers.0.self_attn.q_proj.weight
25-01-23 07:43:57 | I |   - Quantizing model.layers.0.self_attn.k_proj.weight
25-01-23 07:44:00 | I |   - Quantizing model.layers.0.self_attn.v_proj.weight
25-01-23 07:44:03 | I |   - Quantizing model.layers.0.self_attn.o_proj.weight
25-01-23 07:44:05 | I |   - Quantizing model.layers.0.mlp.up_proj.weight
25-01-23 07:44:07 | I |   - Quantizing model.layers.0.mlp.gate_proj.weight
25-01-23 07:44:10 | I |   - Quantizing model.layers.0.mlp.down_proj.weight
25-01-23 07:44:16 | I | quantizing activations for layer model.layers.0
25-01-23 07:44:16 | I | collecting calibration activations in model.layers.0
25-01-23 07:44:16 | I | collecting calibration activations in model.layers.0
25-01-23 07:44:18 | I | forward this layer
25-01-23 07:44:18 | I | input_file: /data/gyy/lmquant-main/lmquant/data/data_without_preprocess_llama_7b_bs16/shard/val_input_args_layer_0/16.pt
25-01-23 07:44:18 | I | output_file: /data/gyy/lmquant-main/lmquant/data/data_without_preprocess_llama_7b_bs16/shard/val_teacher_output_layer_0/16.pt
25-01-23 07:44:18 | I | In layer model.layers.0, gradient of model.layers.0.self_attn.q_proj has nan: False
25-01-23 07:44:18 | I | In layer model.layers.0, gradient of model.layers.0.self_attn.q_proj has inf: False
25-01-23 07:44:18 | I | In layer model.layers.0, gradient of model.layers.0.self_attn.k_proj has nan: False
25-01-23 07:44:18 | I | In layer model.layers.0, gradient of model.layers.0.self_attn.k_proj has inf: False
25-01-23 07:44:18 | I | In layer model.layers.0, gradient of model.layers.0.self_attn.v_proj has nan: False
25-01-23 07:44:18 | I | In layer model.layers.0, gradient of model.layers.0.self_attn.v_proj has inf: False
25-01-23 07:44:18 | I | In layer model.layers.0, gradient of model.layers.0.self_attn.o_proj has nan: False
25-01-23 07:44:18 | I | In layer model.layers.0, gradient of model.layers.0.self_attn.o_proj has inf: False
25-01-23 07:44:18 | I | In layer model.layers.0, gradient of model.layers.0.mlp.up_proj has nan: False
25-01-23 07:44:18 | I | In layer model.layers.0, gradient of model.layers.0.mlp.up_proj has inf: False
25-01-23 07:44:18 | I | In layer model.layers.0, gradient of model.layers.0.mlp.gate_proj has nan: False
25-01-23 07:44:18 | I | In layer model.layers.0, gradient of model.layers.0.mlp.gate_proj has inf: False
25-01-23 07:44:18 | I | In layer model.layers.0, gradient of model.layers.0.mlp.down_proj has nan: False
25-01-23 07:44:18 | I | In layer model.layers.0, gradient of model.layers.0.mlp.down_proj has inf: False
25-01-23 07:44:18 | I | [12] done with optimizer step
25-01-23 07:44:19 | I | epoch 001:     17 / 819200000 loss=1.75908e-05, loss_per_token=0.0720521, loss_sum=590.251, wps=216.7, ups=0.03, wpb=8192, bsz=16, num_updates=13, lr=0.0013, gnorm=0.35, clip=0, loss_scale=8, train_wall=38, cuda_gb_allocated=17.1, cuda_gb_reserved=18.3, cuda_gb_free=6.6, wall=1206
25-01-23 07:44:19 | I | TRAIN CURRENT LAYER_IDX = 0
25-01-23 07:44:19 | I | in layer model.layers.0
25-01-23 07:44:19 | I | quantizing weights for layer model.layers.0
25-01-23 07:44:19 | I | collecting calibration activations in model.layers.0
25-01-23 07:44:19 | I | collecting calibration activations in model.layers.0
25-01-23 07:44:19 | I | - Quantizing decoder layer model.layers.0
25-01-23 07:44:19 | I |   - Calibrating model.layers.0.self_attn.q_proj.weight
25-01-23 07:44:20 | I |       - range scale = [    1.0000]
25-01-23 07:44:20 | I |         sum  error  = [    0.0429]
25-01-23 07:44:20 | I |         best error  = [    0.0429]
25-01-23 07:44:20 | I |     + error = [0.0429]
25-01-23 07:44:21 | I |       - range scale = [    1.0000]
25-01-23 07:44:21 | I |         sum  error  = [    0.6836]
25-01-23 07:44:21 | I |         best error  = [    0.6836]
25-01-23 07:44:21 | I |     + error = [0.6836]
25-01-23 07:44:21 | I |   - Calibrating model.layers.0.self_attn.k_proj.weight
25-01-23 07:44:22 | I |       - range scale = [    1.0000]
25-01-23 07:44:22 | I |         sum  error  = [    0.0550]
25-01-23 07:44:22 | I |         best error  = [    0.0550]
25-01-23 07:44:22 | I |     + error = [0.0550]
25-01-23 07:44:22 | I |       - range scale = [    1.0000]
25-01-23 07:44:22 | I |         sum  error  = [    0.9921]
25-01-23 07:44:22 | I |         best error  = [    0.9921]
25-01-23 07:44:22 | I |     + error = [0.9921]
25-01-23 07:44:23 | I |   - Calibrating model.layers.0.self_attn.v_proj.weight
25-01-23 07:44:23 | I |       - range scale = [    1.0000]
25-01-23 07:44:23 | I |         sum  error  = [    0.6813]
25-01-23 07:44:23 | I |         best error  = [    0.6813]
25-01-23 07:44:23 | I |     + error = [0.6813]
25-01-23 07:44:24 | I |       - range scale = [    1.0000]
25-01-23 07:44:24 | I |         sum  error  = [    6.3189]
25-01-23 07:44:24 | I |         best error  = [    6.3189]
25-01-23 07:44:24 | I |     + error = [6.3189]
25-01-23 07:44:24 | I |   - Calibrating model.layers.0.self_attn.o_proj.weight
25-01-23 07:44:25 | I |       - range scale = [    1.0000]
25-01-23 07:44:25 | I |         sum  error  = [    0.2045]
25-01-23 07:44:25 | I |         best error  = [    0.2045]
25-01-23 07:44:25 | I |     + error = [0.2045]
25-01-23 07:44:26 | I |       - range scale = [    1.0000]
25-01-23 07:44:26 | I |         sum  error  = [    1.6231]
25-01-23 07:44:26 | I |         best error  = [    1.6231]
25-01-23 07:44:26 | I |     + error = [1.6231]
25-01-23 07:44:26 | I |   - Calibrating model.layers.0.mlp.up_proj.weight
25-01-23 07:44:27 | I |       - range scale = [    1.0000]
25-01-23 07:44:27 | I |         sum  error  = [    1.0935]
25-01-23 07:44:27 | I |         best error  = [    1.0935]
25-01-23 07:44:27 | I |     + error = [1.0935]
25-01-23 07:44:28 | I |       - range scale = [    1.0000]
25-01-23 07:44:28 | I |         sum  error  = [   11.6246]
25-01-23 07:44:28 | I |         best error  = [   11.6246]
25-01-23 07:44:28 | I |     + error = [11.6246]
25-01-23 07:44:28 | I |   - Calibrating model.layers.0.mlp.gate_proj.weight
25-01-23 07:44:29 | I |       - range scale = [    1.0000]
25-01-23 07:44:29 | I |         sum  error  = [    1.1510]
25-01-23 07:44:29 | I |         best error  = [    1.1510]
25-01-23 07:44:29 | I |     + error = [1.1510]
25-01-23 07:44:30 | I |       - range scale = [    1.0000]
25-01-23 07:44:30 | I |         sum  error  = [   12.0115]
25-01-23 07:44:30 | I |         best error  = [   12.0115]
25-01-23 07:44:30 | I |     + error = [12.0115]
25-01-23 07:44:30 | I |   - Calibrating model.layers.0.mlp.down_proj.weight
25-01-23 07:44:31 | I |       - range scale = [    1.0000]
25-01-23 07:44:31 | I |         sum  error  = [    0.1695]
25-01-23 07:44:31 | I |         best error  = [    0.1695]
25-01-23 07:44:31 | I |     + error = [0.1695]
25-01-23 07:44:32 | I |       - range scale = [    1.0000]
25-01-23 07:44:32 | I |         sum  error  = [    1.6477]
25-01-23 07:44:32 | I |         best error  = [    1.6477]
25-01-23 07:44:32 | I |     + error = [1.6477]
25-01-23 07:44:32 | I |   - Quantizing model.layers.0.self_attn.q_proj.weight
25-01-23 07:44:35 | I |   - Quantizing model.layers.0.self_attn.k_proj.weight
25-01-23 07:44:37 | I |   - Quantizing model.layers.0.self_attn.v_proj.weight
25-01-23 07:44:39 | I |   - Quantizing model.layers.0.self_attn.o_proj.weight
25-01-23 07:44:42 | I |   - Quantizing model.layers.0.mlp.up_proj.weight
25-01-23 07:44:44 | I |   - Quantizing model.layers.0.mlp.gate_proj.weight
25-01-23 07:44:46 | I |   - Quantizing model.layers.0.mlp.down_proj.weight
25-01-23 07:44:52 | I | quantizing activations for layer model.layers.0
25-01-23 07:44:53 | I | collecting calibration activations in model.layers.0
25-01-23 07:44:53 | I | collecting calibration activations in model.layers.0
25-01-23 07:44:55 | I | forward this layer
25-01-23 07:44:55 | I | input_file: /data/gyy/lmquant-main/lmquant/data/data_without_preprocess_llama_7b_bs16/shard/val_input_args_layer_0/17.pt
25-01-23 07:44:55 | I | output_file: /data/gyy/lmquant-main/lmquant/data/data_without_preprocess_llama_7b_bs16/shard/val_teacher_output_layer_0/17.pt
25-01-23 07:44:55 | I | In layer model.layers.0, gradient of model.layers.0.self_attn.q_proj has nan: False
25-01-23 07:44:55 | I | In layer model.layers.0, gradient of model.layers.0.self_attn.q_proj has inf: False
25-01-23 07:44:55 | I | In layer model.layers.0, gradient of model.layers.0.self_attn.k_proj has nan: False
25-01-23 07:44:55 | I | In layer model.layers.0, gradient of model.layers.0.self_attn.k_proj has inf: False
25-01-23 07:44:55 | I | In layer model.layers.0, gradient of model.layers.0.self_attn.v_proj has nan: False
25-01-23 07:44:55 | I | In layer model.layers.0, gradient of model.layers.0.self_attn.v_proj has inf: False
25-01-23 07:44:55 | I | In layer model.layers.0, gradient of model.layers.0.self_attn.o_proj has nan: False
25-01-23 07:44:55 | I | In layer model.layers.0, gradient of model.layers.0.self_attn.o_proj has inf: False
25-01-23 07:44:55 | I | In layer model.layers.0, gradient of model.layers.0.mlp.up_proj has nan: False
25-01-23 07:44:55 | I | In layer model.layers.0, gradient of model.layers.0.mlp.up_proj has inf: False
25-01-23 07:44:55 | I | In layer model.layers.0, gradient of model.layers.0.mlp.gate_proj has nan: False
25-01-23 07:44:55 | I | In layer model.layers.0, gradient of model.layers.0.mlp.gate_proj has inf: False
25-01-23 07:44:55 | I | In layer model.layers.0, gradient of model.layers.0.mlp.down_proj has nan: False
25-01-23 07:44:55 | I | In layer model.layers.0, gradient of model.layers.0.mlp.down_proj has inf: False
25-01-23 07:44:55 | I | [13] done with optimizer step
25-01-23 07:44:55 | I | epoch 001:     18 / 819200000 loss=1.4023e-05, loss_per_token=0.0574384, loss_sum=470.535, wps=223.5, ups=0.03, wpb=8192, bsz=16, num_updates=14, lr=0.0014, gnorm=0.248, clip=0, loss_scale=8, train_wall=37, cuda_gb_allocated=17.1, cuda_gb_reserved=18.6, cuda_gb_free=6.6, wall=1243
25-01-23 07:44:55 | I | TRAIN CURRENT LAYER_IDX = 0
25-01-23 07:44:55 | I | in layer model.layers.0
25-01-23 07:44:55 | I | quantizing weights for layer model.layers.0
25-01-23 07:44:56 | I | collecting calibration activations in model.layers.0
25-01-23 07:44:56 | I | collecting calibration activations in model.layers.0
25-01-23 07:44:56 | I | - Quantizing decoder layer model.layers.0
25-01-23 07:44:56 | I |   - Calibrating model.layers.0.self_attn.q_proj.weight
25-01-23 07:44:57 | I |       - range scale = [    1.0000]
25-01-23 07:44:57 | I |         sum  error  = [    0.0475]
25-01-23 07:44:57 | I |         best error  = [    0.0475]
25-01-23 07:44:57 | I |     + error = [0.0475]
25-01-23 07:44:57 | I |       - range scale = [    1.0000]
25-01-23 07:44:57 | I |         sum  error  = [    0.7437]
25-01-23 07:44:57 | I |         best error  = [    0.7437]
25-01-23 07:44:57 | I |     + error = [0.7437]
25-01-23 07:44:58 | I |   - Calibrating model.layers.0.self_attn.k_proj.weight
25-01-23 07:44:58 | I |       - range scale = [    1.0000]
25-01-23 07:44:58 | I |         sum  error  = [    0.0689]
25-01-23 07:44:58 | I |         best error  = [    0.0689]
25-01-23 07:44:58 | I |     + error = [0.0689]
25-01-23 07:44:59 | I |       - range scale = [    1.0000]
25-01-23 07:44:59 | I |         sum  error  = [    0.9751]
25-01-23 07:44:59 | I |         best error  = [    0.9751]
25-01-23 07:44:59 | I |     + error = [0.9751]
25-01-23 07:44:59 | I |   - Calibrating model.layers.0.self_attn.v_proj.weight
25-01-23 07:45:00 | I |       - range scale = [    1.0000]
25-01-23 07:45:00 | I |         sum  error  = [    0.7107]
25-01-23 07:45:00 | I |         best error  = [    0.7107]
25-01-23 07:45:00 | I |     + error = [0.7107]
25-01-23 07:45:01 | I |       - range scale = [    1.0000]
25-01-23 07:45:01 | I |         sum  error  = [    6.6127]
25-01-23 07:45:01 | I |         best error  = [    6.6127]
25-01-23 07:45:01 | I |     + error = [6.6127]
25-01-23 07:45:01 | I |   - Calibrating model.layers.0.self_attn.o_proj.weight
25-01-23 07:45:02 | I |       - range scale = [    1.0000]
25-01-23 07:45:02 | I |         sum  error  = [    0.2147]
25-01-23 07:45:02 | I |         best error  = [    0.2147]
25-01-23 07:45:02 | I |     + error = [0.2147]
25-01-23 07:45:03 | I |       - range scale = [    1.0000]
25-01-23 07:45:03 | I |         sum  error  = [    1.7082]
25-01-23 07:45:03 | I |         best error  = [    1.7082]
25-01-23 07:45:03 | I |     + error = [1.7082]
25-01-23 07:45:03 | I |   - Calibrating model.layers.0.mlp.up_proj.weight
25-01-23 07:45:04 | I |       - range scale = [    1.0000]
25-01-23 07:45:04 | I |         sum  error  = [    1.1232]
25-01-23 07:45:04 | I |         best error  = [    1.1232]
25-01-23 07:45:04 | I |     + error = [1.1232]
25-01-23 07:45:05 | I |       - range scale = [    1.0000]
25-01-23 07:45:05 | I |         sum  error  = [   11.9508]
25-01-23 07:45:05 | I |         best error  = [   11.9508]
25-01-23 07:45:05 | I |     + error = [11.9508]
25-01-23 07:45:05 | I |   - Calibrating model.layers.0.mlp.gate_proj.weight
25-01-23 07:45:06 | I |       - range scale = [    1.0000]
25-01-23 07:45:06 | I |         sum  error  = [    1.1864]
25-01-23 07:45:06 | I |         best error  = [    1.1864]
25-01-23 07:45:06 | I |     + error = [1.1864]
25-01-23 07:45:07 | I |       - range scale = [    1.0000]
25-01-23 07:45:07 | I |         sum  error  = [   12.3488]
25-01-23 07:45:07 | I |         best error  = [   12.3488]
25-01-23 07:45:07 | I |     + error = [12.3488]
25-01-23 07:45:07 | I |   - Calibrating model.layers.0.mlp.down_proj.weight
25-01-23 07:45:08 | I |       - range scale = [    1.0000]
25-01-23 07:45:08 | I |         sum  error  = [    0.1720]
25-01-23 07:45:08 | I |         best error  = [    0.1720]
25-01-23 07:45:08 | I |     + error = [0.1720]
25-01-23 07:45:09 | I |       - range scale = [    1.0000]
25-01-23 07:45:09 | I |         sum  error  = [    1.6666]
25-01-23 07:45:09 | I |         best error  = [    1.6666]
25-01-23 07:45:09 | I |     + error = [1.6666]
25-01-23 07:45:09 | I |   - Quantizing model.layers.0.self_attn.q_proj.weight
25-01-23 07:45:12 | I |   - Quantizing model.layers.0.self_attn.k_proj.weight
25-01-23 07:45:14 | I |   - Quantizing model.layers.0.self_attn.v_proj.weight
25-01-23 07:45:16 | I |   - Quantizing model.layers.0.self_attn.o_proj.weight
25-01-23 07:45:18 | I |   - Quantizing model.layers.0.mlp.up_proj.weight
25-01-23 07:45:21 | I |   - Quantizing model.layers.0.mlp.gate_proj.weight
25-01-23 07:45:23 | I |   - Quantizing model.layers.0.mlp.down_proj.weight
25-01-23 07:45:29 | I | quantizing activations for layer model.layers.0
25-01-23 07:45:30 | I | collecting calibration activations in model.layers.0
25-01-23 07:45:30 | I | collecting calibration activations in model.layers.0
25-01-23 07:45:32 | I | forward this layer
25-01-23 07:45:32 | I | input_file: /data/gyy/lmquant-main/lmquant/data/data_without_preprocess_llama_7b_bs16/shard/val_input_args_layer_0/18.pt
25-01-23 07:45:32 | I | output_file: /data/gyy/lmquant-main/lmquant/data/data_without_preprocess_llama_7b_bs16/shard/val_teacher_output_layer_0/18.pt
25-01-23 07:45:32 | I | In layer model.layers.0, gradient of model.layers.0.self_attn.q_proj has nan: False
25-01-23 07:45:32 | I | In layer model.layers.0, gradient of model.layers.0.self_attn.q_proj has inf: False
25-01-23 07:45:32 | I | In layer model.layers.0, gradient of model.layers.0.self_attn.k_proj has nan: False
25-01-23 07:45:32 | I | In layer model.layers.0, gradient of model.layers.0.self_attn.k_proj has inf: False
25-01-23 07:45:32 | I | In layer model.layers.0, gradient of model.layers.0.self_attn.v_proj has nan: False
25-01-23 07:45:32 | I | In layer model.layers.0, gradient of model.layers.0.self_attn.v_proj has inf: False
25-01-23 07:45:32 | I | In layer model.layers.0, gradient of model.layers.0.self_attn.o_proj has nan: False
25-01-23 07:45:32 | I | In layer model.layers.0, gradient of model.layers.0.self_attn.o_proj has inf: False
25-01-23 07:45:32 | I | In layer model.layers.0, gradient of model.layers.0.mlp.up_proj has nan: False
25-01-23 07:45:32 | I | In layer model.layers.0, gradient of model.layers.0.mlp.up_proj has inf: False
25-01-23 07:45:32 | I | In layer model.layers.0, gradient of model.layers.0.mlp.gate_proj has nan: False
25-01-23 07:45:32 | I | In layer model.layers.0, gradient of model.layers.0.mlp.gate_proj has inf: False
25-01-23 07:45:32 | I | In layer model.layers.0, gradient of model.layers.0.mlp.down_proj has nan: False
25-01-23 07:45:32 | I | In layer model.layers.0, gradient of model.layers.0.mlp.down_proj has inf: False
25-01-23 07:45:32 | I | [14] done with optimizer step
25-01-23 07:45:32 | I | epoch 001:     19 / 819200000 loss=1.49609e-05, loss_per_token=0.0612798, loss_sum=502.004, wps=222.3, ups=0.03, wpb=8192, bsz=16, num_updates=15, lr=0.0015, gnorm=0.239, clip=0, loss_scale=8, train_wall=37, cuda_gb_allocated=17.1, cuda_gb_reserved=18.3, cuda_gb_free=6.6, wall=1280
25-01-23 07:45:32 | I | TRAIN CURRENT LAYER_IDX = 0
25-01-23 07:45:32 | I | in layer model.layers.0
25-01-23 07:45:32 | I | quantizing weights for layer model.layers.0
25-01-23 07:45:33 | I | collecting calibration activations in model.layers.0
25-01-23 07:45:33 | I | collecting calibration activations in model.layers.0
25-01-23 07:45:33 | I | - Quantizing decoder layer model.layers.0
25-01-23 07:45:33 | I |   - Calibrating model.layers.0.self_attn.q_proj.weight
25-01-23 07:45:34 | I |       - range scale = [    1.0000]
25-01-23 07:45:34 | I |         sum  error  = [    0.0407]
25-01-23 07:45:34 | I |         best error  = [    0.0407]
25-01-23 07:45:34 | I |     + error = [0.0407]
25-01-23 07:45:34 | I |       - range scale = [    1.0000]
25-01-23 07:45:34 | I |         sum  error  = [    0.6461]
25-01-23 07:45:34 | I |         best error  = [    0.6461]
25-01-23 07:45:34 | I |     + error = [0.6461]
25-01-23 07:45:35 | I |   - Calibrating model.layers.0.self_attn.k_proj.weight
25-01-23 07:45:35 | I |       - range scale = [    1.0000]
25-01-23 07:45:35 | I |         sum  error  = [    0.0506]
25-01-23 07:45:35 | I |         best error  = [    0.0506]
25-01-23 07:45:35 | I |     + error = [0.0506]
25-01-23 07:45:36 | I |       - range scale = [    1.0000]
25-01-23 07:45:36 | I |         sum  error  = [    0.9293]
25-01-23 07:45:36 | I |         best error  = [    0.9293]
25-01-23 07:45:36 | I |     + error = [0.9293]
25-01-23 07:45:36 | I |   - Calibrating model.layers.0.self_attn.v_proj.weight
25-01-23 07:45:37 | I |       - range scale = [    1.0000]
25-01-23 07:45:37 | I |         sum  error  = [    0.6699]
25-01-23 07:45:37 | I |         best error  = [    0.6699]
25-01-23 07:45:37 | I |     + error = [0.6699]
25-01-23 07:45:38 | I |       - range scale = [    1.0000]
25-01-23 07:45:38 | I |         sum  error  = [    6.2073]
25-01-23 07:45:38 | I |         best error  = [    6.2073]
25-01-23 07:45:38 | I |     + error = [6.2073]
25-01-23 07:45:38 | I |   - Calibrating model.layers.0.self_attn.o_proj.weight
25-01-23 07:45:39 | I |       - range scale = [    1.0000]
25-01-23 07:45:39 | I |         sum  error  = [    0.1997]
25-01-23 07:45:39 | I |         best error  = [    0.1997]
25-01-23 07:45:39 | I |     + error = [0.1997]
25-01-23 07:45:40 | I |       - range scale = [    1.0000]
25-01-23 07:45:40 | I |         sum  error  = [    1.5822]
25-01-23 07:45:40 | I |         best error  = [    1.5822]
25-01-23 07:45:40 | I |     + error = [1.5822]
25-01-23 07:45:40 | I |   - Calibrating model.layers.0.mlp.up_proj.weight
25-01-23 07:45:40 | I |       - range scale = [    1.0000]
25-01-23 07:45:40 | I |         sum  error  = [    1.0802]
25-01-23 07:45:40 | I |         best error  = [    1.0802]
25-01-23 07:45:40 | I |     + error = [1.0802]
25-01-23 07:45:42 | I |       - range scale = [    1.0000]
25-01-23 07:45:42 | I |         sum  error  = [   11.4878]
25-01-23 07:45:42 | I |         best error  = [   11.4878]
25-01-23 07:45:42 | I |     + error = [11.4878]
25-01-23 07:45:42 | I |   - Calibrating model.layers.0.mlp.gate_proj.weight
25-01-23 07:45:43 | I |       - range scale = [    1.0000]
25-01-23 07:45:43 | I |         sum  error  = [    1.1396]
25-01-23 07:45:43 | I |         best error  = [    1.1396]
25-01-23 07:45:43 | I |     + error = [1.1396]
25-01-23 07:45:44 | I |       - range scale = [    1.0000]
25-01-23 07:45:44 | I |         sum  error  = [   11.8679]
25-01-23 07:45:44 | I |         best error  = [   11.8679]
25-01-23 07:45:44 | I |     + error = [11.8679]
25-01-23 07:45:44 | I |   - Calibrating model.layers.0.mlp.down_proj.weight
25-01-23 07:45:45 | I |       - range scale = [    1.0000]
25-01-23 07:45:45 | I |         sum  error  = [    0.1669]
25-01-23 07:45:45 | I |         best error  = [    0.1669]
25-01-23 07:45:45 | I |     + error = [0.1669]
25-01-23 07:45:46 | I |       - range scale = [    1.0000]
25-01-23 07:45:46 | I |         sum  error  = [    1.6269]
25-01-23 07:45:46 | I |         best error  = [    1.6269]
25-01-23 07:45:46 | I |     + error = [1.6269]
25-01-23 07:45:46 | I |   - Quantizing model.layers.0.self_attn.q_proj.weight
25-01-23 07:45:48 | I |   - Quantizing model.layers.0.self_attn.k_proj.weight
25-01-23 07:45:51 | I |   - Quantizing model.layers.0.self_attn.v_proj.weight
25-01-23 07:45:53 | I |   - Quantizing model.layers.0.self_attn.o_proj.weight
25-01-23 07:45:55 | I |   - Quantizing model.layers.0.mlp.up_proj.weight
25-01-23 07:45:58 | I |   - Quantizing model.layers.0.mlp.gate_proj.weight
25-01-23 07:46:00 | I |   - Quantizing model.layers.0.mlp.down_proj.weight
25-01-23 07:46:06 | I | quantizing activations for layer model.layers.0
25-01-23 07:46:06 | I | collecting calibration activations in model.layers.0
25-01-23 07:46:06 | I | collecting calibration activations in model.layers.0
25-01-23 07:46:08 | I | forward this layer
25-01-23 07:46:08 | I | input_file: /data/gyy/lmquant-main/lmquant/data/data_without_preprocess_llama_7b_bs16/shard/val_input_args_layer_0/19.pt
25-01-23 07:46:08 | I | output_file: /data/gyy/lmquant-main/lmquant/data/data_without_preprocess_llama_7b_bs16/shard/val_teacher_output_layer_0/19.pt
25-01-23 07:46:08 | I | In layer model.layers.0, gradient of model.layers.0.self_attn.q_proj has nan: False
25-01-23 07:46:08 | I | In layer model.layers.0, gradient of model.layers.0.self_attn.q_proj has inf: False
25-01-23 07:46:08 | I | In layer model.layers.0, gradient of model.layers.0.self_attn.k_proj has nan: False
25-01-23 07:46:08 | I | In layer model.layers.0, gradient of model.layers.0.self_attn.k_proj has inf: False
25-01-23 07:46:08 | I | In layer model.layers.0, gradient of model.layers.0.self_attn.v_proj has nan: False
25-01-23 07:46:08 | I | In layer model.layers.0, gradient of model.layers.0.self_attn.v_proj has inf: False
25-01-23 07:46:08 | I | In layer model.layers.0, gradient of model.layers.0.self_attn.o_proj has nan: False
25-01-23 07:46:08 | I | In layer model.layers.0, gradient of model.layers.0.self_attn.o_proj has inf: False
25-01-23 07:46:08 | I | In layer model.layers.0, gradient of model.layers.0.mlp.up_proj has nan: False
25-01-23 07:46:08 | I | In layer model.layers.0, gradient of model.layers.0.mlp.up_proj has inf: False
25-01-23 07:46:08 | I | In layer model.layers.0, gradient of model.layers.0.mlp.gate_proj has nan: False
25-01-23 07:46:08 | I | In layer model.layers.0, gradient of model.layers.0.mlp.gate_proj has inf: False
25-01-23 07:46:08 | I | In layer model.layers.0, gradient of model.layers.0.mlp.down_proj has nan: False
25-01-23 07:46:08 | I | In layer model.layers.0, gradient of model.layers.0.mlp.down_proj has inf: False
25-01-23 07:46:09 | I | [15] done with optimizer step
25-01-23 07:46:09 | I | epoch 001:     20 / 819200000 loss=1.54153e-05, loss_per_token=0.0631409, loss_sum=517.25, wps=224.6, ups=0.03, wpb=8192, bsz=16, num_updates=16, lr=0.0016, gnorm=0.408, clip=0, loss_scale=8, train_wall=36, cuda_gb_allocated=17.1, cuda_gb_reserved=18.3, cuda_gb_free=6.6, wall=1316
25-01-23 07:46:09 | I | TRAIN CURRENT LAYER_IDX = 0
25-01-23 07:46:09 | I | in layer model.layers.0
25-01-23 07:46:09 | I | quantizing weights for layer model.layers.0
25-01-23 07:46:09 | I | collecting calibration activations in model.layers.0
25-01-23 07:46:09 | I | collecting calibration activations in model.layers.0
25-01-23 07:46:09 | I | - Quantizing decoder layer model.layers.0
25-01-23 07:46:09 | I |   - Calibrating model.layers.0.self_attn.q_proj.weight
25-01-23 07:46:10 | I |       - range scale = [    1.0000]
25-01-23 07:46:10 | I |         sum  error  = [    0.0449]
25-01-23 07:46:10 | I |         best error  = [    0.0449]
25-01-23 07:46:10 | I |     + error = [0.0449]
25-01-23 07:46:11 | I |       - range scale = [    1.0000]
25-01-23 07:46:11 | I |         sum  error  = [    0.7011]
25-01-23 07:46:11 | I |         best error  = [    0.7011]
25-01-23 07:46:11 | I |     + error = [0.7011]
25-01-23 07:46:11 | I |   - Calibrating model.layers.0.self_attn.k_proj.weight
25-01-23 07:46:12 | I |       - range scale = [    1.0000]
25-01-23 07:46:12 | I |         sum  error  = [    0.0589]
25-01-23 07:46:12 | I |         best error  = [    0.0589]
25-01-23 07:46:12 | I |     + error = [0.0589]
25-01-23 07:46:12 | I |       - range scale = [    1.0000]
25-01-23 07:46:12 | I |         sum  error  = [    0.9868]
25-01-23 07:46:12 | I |         best error  = [    0.9868]
25-01-23 07:46:12 | I |     + error = [0.9868]
25-01-23 07:46:13 | I |   - Calibrating model.layers.0.self_attn.v_proj.weight
25-01-23 07:46:13 | I |       - range scale = [    1.0000]
25-01-23 07:46:13 | I |         sum  error  = [    0.6988]
25-01-23 07:46:13 | I |         best error  = [    0.6988]
25-01-23 07:46:13 | I |     + error = [0.6988]
25-01-23 07:46:14 | I |       - range scale = [    1.0000]
25-01-23 07:46:14 | I |         sum  error  = [    6.5090]
25-01-23 07:46:14 | I |         best error  = [    6.5090]
25-01-23 07:46:14 | I |     + error = [6.5090]
25-01-23 07:46:14 | I |   - Calibrating model.layers.0.self_attn.o_proj.weight
25-01-23 07:46:15 | I |       - range scale = [    1.0000]
25-01-23 07:46:15 | I |         sum  error  = [    0.2140]
25-01-23 07:46:15 | I |         best error  = [    0.2140]
25-01-23 07:46:15 | I |     + error = [0.2140]
25-01-23 07:46:16 | I |       - range scale = [    1.0000]
25-01-23 07:46:16 | I |         sum  error  = [    1.7052]
25-01-23 07:46:16 | I |         best error  = [    1.7052]
25-01-23 07:46:16 | I |     + error = [1.7052]
25-01-23 07:46:16 | I |   - Calibrating model.layers.0.mlp.up_proj.weight
25-01-23 07:46:17 | I |       - range scale = [    1.0000]
25-01-23 07:46:17 | I |         sum  error  = [    1.1355]
25-01-23 07:46:17 | I |         best error  = [    1.1355]
25-01-23 07:46:17 | I |     + error = [1.1355]
25-01-23 07:46:18 | I |       - range scale = [    1.0000]
25-01-23 07:46:18 | I |         sum  error  = [   12.0672]
25-01-23 07:46:18 | I |         best error  = [   12.0672]
25-01-23 07:46:18 | I |     + error = [12.0672]
25-01-23 07:46:18 | I |   - Calibrating model.layers.0.mlp.gate_proj.weight
25-01-23 07:46:19 | I |       - range scale = [    1.0000]
25-01-23 07:46:19 | I |         sum  error  = [    1.1968]
25-01-23 07:46:19 | I |         best error  = [    1.1968]
25-01-23 07:46:19 | I |     + error = [1.1968]
25-01-23 07:46:20 | I |       - range scale = [    1.0000]
25-01-23 07:46:20 | I |         sum  error  = [   12.4689]
25-01-23 07:46:20 | I |         best error  = [   12.4689]
25-01-23 07:46:20 | I |     + error = [12.4689]
25-01-23 07:46:20 | I |   - Calibrating model.layers.0.mlp.down_proj.weight
25-01-23 07:46:21 | I |       - range scale = [    1.0000]
25-01-23 07:46:21 | I |         sum  error  = [    0.1774]
25-01-23 07:46:21 | I |         best error  = [    0.1774]
25-01-23 07:46:21 | I |     + error = [0.1774]
25-01-23 07:46:22 | I |       - range scale = [    1.0000]
25-01-23 07:46:22 | I |         sum  error  = [    1.7216]
25-01-23 07:46:22 | I |         best error  = [    1.7216]
25-01-23 07:46:22 | I |     + error = [1.7216]
25-01-23 07:46:22 | I |   - Quantizing model.layers.0.self_attn.q_proj.weight
25-01-23 07:46:25 | I |   - Quantizing model.layers.0.self_attn.k_proj.weight
25-01-23 07:46:27 | I |   - Quantizing model.layers.0.self_attn.v_proj.weight
25-01-23 07:46:29 | I |   - Quantizing model.layers.0.self_attn.o_proj.weight
25-01-23 07:46:32 | I |   - Quantizing model.layers.0.mlp.up_proj.weight
25-01-23 07:46:34 | I |   - Quantizing model.layers.0.mlp.gate_proj.weight
25-01-23 07:46:36 | I |   - Quantizing model.layers.0.mlp.down_proj.weight
25-01-23 07:46:42 | I | quantizing activations for layer model.layers.0
25-01-23 07:46:43 | I | collecting calibration activations in model.layers.0
25-01-23 07:46:43 | I | collecting calibration activations in model.layers.0
25-01-23 07:46:45 | I | forward this layer
25-01-23 07:46:45 | I | input_file: /data/gyy/lmquant-main/lmquant/data/data_without_preprocess_llama_7b_bs16/shard/val_input_args_layer_0/20.pt
25-01-23 07:46:45 | I | output_file: /data/gyy/lmquant-main/lmquant/data/data_without_preprocess_llama_7b_bs16/shard/val_teacher_output_layer_0/20.pt
25-01-23 07:46:45 | I | In layer model.layers.0, gradient of model.layers.0.self_attn.q_proj has nan: False
25-01-23 07:46:45 | I | In layer model.layers.0, gradient of model.layers.0.self_attn.q_proj has inf: False
25-01-23 07:46:45 | I | In layer model.layers.0, gradient of model.layers.0.self_attn.k_proj has nan: False
25-01-23 07:46:45 | I | In layer model.layers.0, gradient of model.layers.0.self_attn.k_proj has inf: False
25-01-23 07:46:45 | I | In layer model.layers.0, gradient of model.layers.0.self_attn.v_proj has nan: False
25-01-23 07:46:45 | I | In layer model.layers.0, gradient of model.layers.0.self_attn.v_proj has inf: False
25-01-23 07:46:45 | I | In layer model.layers.0, gradient of model.layers.0.self_attn.o_proj has nan: False
25-01-23 07:46:45 | I | In layer model.layers.0, gradient of model.layers.0.self_attn.o_proj has inf: False
25-01-23 07:46:45 | I | In layer model.layers.0, gradient of model.layers.0.mlp.up_proj has nan: False
25-01-23 07:46:45 | I | In layer model.layers.0, gradient of model.layers.0.mlp.up_proj has inf: False
25-01-23 07:46:45 | I | In layer model.layers.0, gradient of model.layers.0.mlp.gate_proj has nan: False
25-01-23 07:46:45 | I | In layer model.layers.0, gradient of model.layers.0.mlp.gate_proj has inf: False
25-01-23 07:46:45 | I | In layer model.layers.0, gradient of model.layers.0.mlp.down_proj has nan: False
25-01-23 07:46:45 | I | In layer model.layers.0, gradient of model.layers.0.mlp.down_proj has inf: False
25-01-23 07:46:45 | I | [16] done with optimizer step
25-01-23 07:46:45 | I | epoch 001:     21 / 819200000 loss=1.29301e-05, loss_per_token=0.0529615, loss_sum=433.861, wps=224, ups=0.03, wpb=8192, bsz=16, num_updates=17, lr=0.0017, gnorm=0.196, clip=0, loss_scale=8, train_wall=36, cuda_gb_allocated=17.1, cuda_gb_reserved=18.3, cuda_gb_free=6.6, wall=1353
25-01-23 07:46:45 | I | TRAIN CURRENT LAYER_IDX = 0
25-01-23 07:46:45 | I | in layer model.layers.0
25-01-23 07:46:45 | I | quantizing weights for layer model.layers.0
25-01-23 07:46:46 | I | collecting calibration activations in model.layers.0
25-01-23 07:46:46 | I | collecting calibration activations in model.layers.0
25-01-23 07:46:46 | I | - Quantizing decoder layer model.layers.0
25-01-23 07:46:46 | I |   - Calibrating model.layers.0.self_attn.q_proj.weight
25-01-23 07:46:47 | I |       - range scale = [    1.0000]
25-01-23 07:46:47 | I |         sum  error  = [    0.0440]
25-01-23 07:46:47 | I |         best error  = [    0.0440]
25-01-23 07:46:47 | I |     + error = [0.0440]
25-01-23 07:46:47 | I |       - range scale = [    1.0000]
25-01-23 07:46:47 | I |         sum  error  = [    0.6960]
25-01-23 07:46:47 | I |         best error  = [    0.6960]
25-01-23 07:46:47 | I |     + error = [0.6960]
25-01-23 07:46:48 | I |   - Calibrating model.layers.0.self_attn.k_proj.weight
25-01-23 07:46:48 | I |       - range scale = [    1.0000]
25-01-23 07:46:48 | I |         sum  error  = [    0.0594]
25-01-23 07:46:48 | I |         best error  = [    0.0594]
25-01-23 07:46:48 | I |     + error = [0.0594]
25-01-23 07:46:49 | I |       - range scale = [    1.0000]
25-01-23 07:46:49 | I |         sum  error  = [    0.9747]
25-01-23 07:46:49 | I |         best error  = [    0.9747]
25-01-23 07:46:49 | I |     + error = [0.9747]
25-01-23 07:46:49 | I |   - Calibrating model.layers.0.self_attn.v_proj.weight
25-01-23 07:46:50 | I |       - range scale = [    1.0000]
25-01-23 07:46:50 | I |         sum  error  = [    0.6969]
25-01-23 07:46:50 | I |         best error  = [    0.6969]
25-01-23 07:46:50 | I |     + error = [0.6969]
25-01-23 07:46:51 | I |       - range scale = [    1.0000]
25-01-23 07:46:51 | I |         sum  error  = [    6.4682]
25-01-23 07:46:51 | I |         best error  = [    6.4682]
25-01-23 07:46:51 | I |     + error = [6.4682]
25-01-23 07:46:51 | I |   - Calibrating model.layers.0.self_attn.o_proj.weight
25-01-23 07:46:52 | I |       - range scale = [    1.0000]
25-01-23 07:46:52 | I |         sum  error  = [    0.2019]
25-01-23 07:46:52 | I |         best error  = [    0.2019]
25-01-23 07:46:52 | I |     + error = [0.2019]
25-01-23 07:46:52 | I |       - range scale = [    1.0000]
25-01-23 07:46:52 | I |         sum  error  = [    1.6091]
25-01-23 07:46:52 | I |         best error  = [    1.6091]
25-01-23 07:46:52 | I |     + error = [1.6091]
25-01-23 07:46:53 | I |   - Calibrating model.layers.0.mlp.up_proj.weight
25-01-23 07:46:53 | I |       - range scale = [    1.0000]
25-01-23 07:46:53 | I |         sum  error  = [    1.1306]
25-01-23 07:46:53 | I |         best error  = [    1.1306]
25-01-23 07:46:53 | I |     + error = [1.1306]
25-01-23 07:46:55 | I |       - range scale = [    1.0000]
25-01-23 07:46:55 | I |         sum  error  = [   12.0330]
25-01-23 07:46:55 | I |         best error  = [   12.0330]
25-01-23 07:46:55 | I |     + error = [12.0330]
25-01-23 07:46:55 | I |   - Calibrating model.layers.0.mlp.gate_proj.weight
25-01-23 07:46:55 | I |       - range scale = [    1.0000]
25-01-23 07:46:55 | I |         sum  error  = [    1.1951]
25-01-23 07:46:55 | I |         best error  = [    1.1951]
25-01-23 07:46:55 | I |     + error = [1.1951]
25-01-23 07:46:57 | I |       - range scale = [    1.0000]
25-01-23 07:46:57 | I |         sum  error  = [   12.4378]
25-01-23 07:46:57 | I |         best error  = [   12.4378]
25-01-23 07:46:57 | I |     + error = [12.4378]
25-01-23 07:46:57 | I |   - Calibrating model.layers.0.mlp.down_proj.weight
25-01-23 07:46:58 | I |       - range scale = [    1.0000]
25-01-23 07:46:58 | I |         sum  error  = [    0.1715]
25-01-23 07:46:58 | I |         best error  = [    0.1715]
25-01-23 07:46:58 | I |     + error = [0.1715]
25-01-23 07:46:59 | I |       - range scale = [    1.0000]
25-01-23 07:46:59 | I |         sum  error  = [    1.6651]
25-01-23 07:46:59 | I |         best error  = [    1.6651]
25-01-23 07:46:59 | I |     + error = [1.6651]
25-01-23 07:46:59 | I |   - Quantizing model.layers.0.self_attn.q_proj.weight
25-01-23 07:47:01 | I |   - Quantizing model.layers.0.self_attn.k_proj.weight
25-01-23 07:47:04 | I |   - Quantizing model.layers.0.self_attn.v_proj.weight
25-01-23 07:47:06 | I |   - Quantizing model.layers.0.self_attn.o_proj.weight
25-01-23 07:47:08 | I |   - Quantizing model.layers.0.mlp.up_proj.weight
25-01-23 07:47:10 | I |   - Quantizing model.layers.0.mlp.gate_proj.weight
25-01-23 07:47:13 | I |   - Quantizing model.layers.0.mlp.down_proj.weight
25-01-23 07:47:19 | I | quantizing activations for layer model.layers.0
25-01-23 07:47:19 | I | collecting calibration activations in model.layers.0
25-01-23 07:47:19 | I | collecting calibration activations in model.layers.0
25-01-23 07:47:21 | I | forward this layer
25-01-23 07:47:21 | I | input_file: /data/gyy/lmquant-main/lmquant/data/data_without_preprocess_llama_7b_bs16/shard/val_input_args_layer_0/21.pt
25-01-23 07:47:21 | I | output_file: /data/gyy/lmquant-main/lmquant/data/data_without_preprocess_llama_7b_bs16/shard/val_teacher_output_layer_0/21.pt
25-01-23 07:47:21 | I | In layer model.layers.0, gradient of model.layers.0.self_attn.q_proj has nan: False
25-01-23 07:47:21 | I | In layer model.layers.0, gradient of model.layers.0.self_attn.q_proj has inf: False
25-01-23 07:47:21 | I | In layer model.layers.0, gradient of model.layers.0.self_attn.k_proj has nan: False
25-01-23 07:47:21 | I | In layer model.layers.0, gradient of model.layers.0.self_attn.k_proj has inf: False
25-01-23 07:47:21 | I | In layer model.layers.0, gradient of model.layers.0.self_attn.v_proj has nan: False
25-01-23 07:47:21 | I | In layer model.layers.0, gradient of model.layers.0.self_attn.v_proj has inf: False
25-01-23 07:47:21 | I | In layer model.layers.0, gradient of model.layers.0.self_attn.o_proj has nan: False
25-01-23 07:47:21 | I | In layer model.layers.0, gradient of model.layers.0.self_attn.o_proj has inf: False
25-01-23 07:47:21 | I | In layer model.layers.0, gradient of model.layers.0.mlp.up_proj has nan: False
25-01-23 07:47:21 | I | In layer model.layers.0, gradient of model.layers.0.mlp.up_proj has inf: False
25-01-23 07:47:21 | I | In layer model.layers.0, gradient of model.layers.0.mlp.gate_proj has nan: False
25-01-23 07:47:21 | I | In layer model.layers.0, gradient of model.layers.0.mlp.gate_proj has inf: False
25-01-23 07:47:21 | I | In layer model.layers.0, gradient of model.layers.0.mlp.down_proj has nan: False
25-01-23 07:47:21 | I | In layer model.layers.0, gradient of model.layers.0.mlp.down_proj has inf: False
25-01-23 07:47:21 | I | [17] done with optimizer step
25-01-23 07:47:21 | I | epoch 001:     22 / 819200000 loss=1.25375e-05, loss_per_token=0.0513535, loss_sum=420.688, wps=225.5, ups=0.03, wpb=8192, bsz=16, num_updates=18, lr=0.0018, gnorm=0.219, clip=0, loss_scale=8, train_wall=36, cuda_gb_allocated=17.1, cuda_gb_reserved=18.7, cuda_gb_free=6.6, wall=1389
25-01-23 07:47:22 | I | TRAIN CURRENT LAYER_IDX = 0
25-01-23 07:47:22 | I | in layer model.layers.0
25-01-23 07:47:22 | I | quantizing weights for layer model.layers.0
25-01-23 07:47:22 | I | collecting calibration activations in model.layers.0
25-01-23 07:47:22 | I | collecting calibration activations in model.layers.0
25-01-23 07:47:22 | I | - Quantizing decoder layer model.layers.0
25-01-23 07:47:22 | I |   - Calibrating model.layers.0.self_attn.q_proj.weight
25-01-23 07:47:23 | I |       - range scale = [    1.0000]
25-01-23 07:47:23 | I |         sum  error  = [    0.0415]
25-01-23 07:47:23 | I |         best error  = [    0.0415]
25-01-23 07:47:23 | I |     + error = [0.0415]
25-01-23 07:47:24 | I |       - range scale = [    1.0000]
25-01-23 07:47:24 | I |         sum  error  = [    0.6632]
25-01-23 07:47:24 | I |         best error  = [    0.6632]
25-01-23 07:47:24 | I |     + error = [0.6632]
25-01-23 07:47:24 | I |   - Calibrating model.layers.0.self_attn.k_proj.weight
25-01-23 07:47:25 | I |       - range scale = [    1.0000]
25-01-23 07:47:25 | I |         sum  error  = [    0.0525]
25-01-23 07:47:25 | I |         best error  = [    0.0525]
25-01-23 07:47:25 | I |     + error = [0.0525]
25-01-23 07:47:25 | I |       - range scale = [    1.0000]
25-01-23 07:47:25 | I |         sum  error  = [    0.9330]
25-01-23 07:47:25 | I |         best error  = [    0.9330]
25-01-23 07:47:25 | I |     + error = [0.9330]
25-01-23 07:47:26 | I |   - Calibrating model.layers.0.self_attn.v_proj.weight
25-01-23 07:47:26 | I |       - range scale = [    1.0000]
25-01-23 07:47:26 | I |         sum  error  = [    0.6451]
25-01-23 07:47:26 | I |         best error  = [    0.6451]
25-01-23 07:47:26 | I |     + error = [0.6451]
25-01-23 07:47:27 | I |       - range scale = [    1.0000]
25-01-23 07:47:27 | I |         sum  error  = [    5.9688]
25-01-23 07:47:27 | I |         best error  = [    5.9688]
25-01-23 07:47:27 | I |     + error = [5.9688]
25-01-23 07:47:27 | I |   - Calibrating model.layers.0.self_attn.o_proj.weight
25-01-23 07:47:28 | I |       - range scale = [    1.0000]
25-01-23 07:47:28 | I |         sum  error  = [    0.1908]
25-01-23 07:47:28 | I |         best error  = [    0.1908]
25-01-23 07:47:28 | I |     + error = [0.1908]
25-01-23 07:47:29 | I |       - range scale = [    1.0000]
25-01-23 07:47:29 | I |         sum  error  = [    1.5231]
25-01-23 07:47:29 | I |         best error  = [    1.5231]
25-01-23 07:47:29 | I |     + error = [1.5231]
25-01-23 07:47:29 | I |   - Calibrating model.layers.0.mlp.up_proj.weight
25-01-23 07:47:30 | I |       - range scale = [    1.0000]
25-01-23 07:47:30 | I |         sum  error  = [    1.1079]
25-01-23 07:47:30 | I |         best error  = [    1.1079]
25-01-23 07:47:30 | I |     + error = [1.1079]
25-01-23 07:47:31 | I |       - range scale = [    1.0000]
25-01-23 07:47:31 | I |         sum  error  = [   11.7762]
25-01-23 07:47:31 | I |         best error  = [   11.7762]
25-01-23 07:47:31 | I |     + error = [11.7762]
25-01-23 07:47:31 | I |   - Calibrating model.layers.0.mlp.gate_proj.weight
25-01-23 07:47:32 | I |       - range scale = [    1.0000]
25-01-23 07:47:32 | I |         sum  error  = [    1.1697]
25-01-23 07:47:32 | I |         best error  = [    1.1697]
25-01-23 07:47:32 | I |     + error = [1.1697]
25-01-23 07:47:33 | I |       - range scale = [    1.0000]
25-01-23 07:47:33 | I |         sum  error  = [   12.1703]
25-01-23 07:47:33 | I |         best error  = [   12.1703]
25-01-23 07:47:33 | I |     + error = [12.1703]
25-01-23 07:47:33 | I |   - Calibrating model.layers.0.mlp.down_proj.weight
25-01-23 07:47:34 | I |       - range scale = [    1.0000]
25-01-23 07:47:34 | I |         sum  error  = [    0.1676]
25-01-23 07:47:34 | I |         best error  = [    0.1676]
25-01-23 07:47:34 | I |     + error = [0.1676]
25-01-23 07:47:35 | I |       - range scale = [    1.0000]
25-01-23 07:47:35 | I |         sum  error  = [    1.6313]
25-01-23 07:47:35 | I |         best error  = [    1.6313]
25-01-23 07:47:35 | I |     + error = [1.6313]
25-01-23 07:47:35 | I |   - Quantizing model.layers.0.self_attn.q_proj.weight
25-01-23 07:47:38 | I |   - Quantizing model.layers.0.self_attn.k_proj.weight
25-01-23 07:47:40 | I |   - Quantizing model.layers.0.self_attn.v_proj.weight
25-01-23 07:47:42 | I |   - Quantizing model.layers.0.self_attn.o_proj.weight
25-01-23 07:47:44 | I |   - Quantizing model.layers.0.mlp.up_proj.weight
25-01-23 07:47:47 | I |   - Quantizing model.layers.0.mlp.gate_proj.weight
25-01-23 07:47:49 | I |   - Quantizing model.layers.0.mlp.down_proj.weight
25-01-23 07:47:57 | I | quantizing activations for layer model.layers.0
25-01-23 07:47:57 | I | collecting calibration activations in model.layers.0
25-01-23 07:47:57 | I | collecting calibration activations in model.layers.0
25-01-23 07:47:59 | I | forward this layer
25-01-23 07:47:59 | I | input_file: /data/gyy/lmquant-main/lmquant/data/data_without_preprocess_llama_7b_bs16/shard/val_input_args_layer_0/22.pt
25-01-23 07:47:59 | I | output_file: /data/gyy/lmquant-main/lmquant/data/data_without_preprocess_llama_7b_bs16/shard/val_teacher_output_layer_0/22.pt
25-01-23 07:48:00 | I | In layer model.layers.0, gradient of model.layers.0.self_attn.q_proj has nan: False
25-01-23 07:48:00 | I | In layer model.layers.0, gradient of model.layers.0.self_attn.q_proj has inf: False
25-01-23 07:48:00 | I | In layer model.layers.0, gradient of model.layers.0.self_attn.k_proj has nan: False
25-01-23 07:48:00 | I | In layer model.layers.0, gradient of model.layers.0.self_attn.k_proj has inf: False
25-01-23 07:48:00 | I | In layer model.layers.0, gradient of model.layers.0.self_attn.v_proj has nan: False
25-01-23 07:48:00 | I | In layer model.layers.0, gradient of model.layers.0.self_attn.v_proj has inf: False
25-01-23 07:48:00 | I | In layer model.layers.0, gradient of model.layers.0.self_attn.o_proj has nan: False
25-01-23 07:48:00 | I | In layer model.layers.0, gradient of model.layers.0.self_attn.o_proj has inf: False
25-01-23 07:48:00 | I | In layer model.layers.0, gradient of model.layers.0.mlp.up_proj has nan: False
25-01-23 07:48:00 | I | In layer model.layers.0, gradient of model.layers.0.mlp.up_proj has inf: False
25-01-23 07:48:00 | I | In layer model.layers.0, gradient of model.layers.0.mlp.gate_proj has nan: False
25-01-23 07:48:00 | I | In layer model.layers.0, gradient of model.layers.0.mlp.gate_proj has inf: False
25-01-23 07:48:00 | I | In layer model.layers.0, gradient of model.layers.0.mlp.down_proj has nan: False
25-01-23 07:48:00 | I | In layer model.layers.0, gradient of model.layers.0.mlp.down_proj has inf: False
25-01-23 07:48:00 | I | [18] done with optimizer step
25-01-23 07:48:00 | I | epoch 001:     23 / 819200000 loss=1.20405e-05, loss_per_token=0.049318, loss_sum=404.013, wps=214.7, ups=0.03, wpb=8192, bsz=16, num_updates=19, lr=0.0019, gnorm=0.146, clip=0, loss_scale=8, train_wall=38, cuda_gb_allocated=17.1, cuda_gb_reserved=18.4, cuda_gb_free=6.6, wall=1427
25-01-23 07:48:00 | I | TRAIN CURRENT LAYER_IDX = 0
25-01-23 07:48:00 | I | in layer model.layers.0
25-01-23 07:48:00 | I | quantizing weights for layer model.layers.0
25-01-23 07:48:00 | I | collecting calibration activations in model.layers.0
25-01-23 07:48:00 | I | collecting calibration activations in model.layers.0
25-01-23 07:48:01 | I | - Quantizing decoder layer model.layers.0
25-01-23 07:48:01 | I |   - Calibrating model.layers.0.self_attn.q_proj.weight
25-01-23 07:48:01 | I |       - range scale = [    1.0000]
25-01-23 07:48:01 | I |         sum  error  = [    0.0457]
25-01-23 07:48:01 | I |         best error  = [    0.0457]
25-01-23 07:48:01 | I |     + error = [0.0457]
25-01-23 07:48:02 | I |       - range scale = [    1.0000]
25-01-23 07:48:02 | I |         sum  error  = [    0.7270]
25-01-23 07:48:02 | I |         best error  = [    0.7270]
25-01-23 07:48:02 | I |     + error = [0.7270]
25-01-23 07:48:02 | I |   - Calibrating model.layers.0.self_attn.k_proj.weight
25-01-23 07:48:03 | I |       - range scale = [    1.0000]
25-01-23 07:48:03 | I |         sum  error  = [    0.0587]
25-01-23 07:48:03 | I |         best error  = [    0.0587]
25-01-23 07:48:03 | I |     + error = [0.0587]
25-01-23 07:48:04 | I |       - range scale = [    1.0000]
25-01-23 07:48:04 | I |         sum  error  = [    1.0053]
25-01-23 07:48:04 | I |         best error  = [    1.0053]
25-01-23 07:48:04 | I |     + error = [1.0053]
25-01-23 07:48:04 | I |   - Calibrating model.layers.0.self_attn.v_proj.weight
25-01-23 07:48:04 | I |       - range scale = [    1.0000]
25-01-23 07:48:04 | I |         sum  error  = [    0.7034]
25-01-23 07:48:04 | I |         best error  = [    0.7034]
25-01-23 07:48:04 | I |     + error = [0.7034]
25-01-23 07:48:05 | I |       - range scale = [    1.0000]
25-01-23 07:48:05 | I |         sum  error  = [    6.4847]
25-01-23 07:48:05 | I |         best error  = [    6.4847]
25-01-23 07:48:05 | I |     + error = [6.4847]
25-01-23 07:48:05 | I |   - Calibrating model.layers.0.self_attn.o_proj.weight
25-01-23 07:48:06 | I |       - range scale = [    1.0000]
25-01-23 07:48:06 | I |         sum  error  = [    0.1950]
25-01-23 07:48:06 | I |         best error  = [    0.1950]
25-01-23 07:48:06 | I |     + error = [0.1950]
25-01-23 07:48:07 | I |       - range scale = [    1.0000]
25-01-23 07:48:07 | I |         sum  error  = [    1.5330]
25-01-23 07:48:07 | I |         best error  = [    1.5330]
25-01-23 07:48:07 | I |     + error = [1.5330]
25-01-23 07:48:07 | I |   - Calibrating model.layers.0.mlp.up_proj.weight
25-01-23 07:48:08 | I |       - range scale = [    1.0000]
25-01-23 07:48:08 | I |         sum  error  = [    1.0703]
25-01-23 07:48:08 | I |         best error  = [    1.0703]
25-01-23 07:48:08 | I |     + error = [1.0703]
25-01-23 07:48:09 | I |       - range scale = [    1.0000]
25-01-23 07:48:09 | I |         sum  error  = [   11.3769]
25-01-23 07:48:09 | I |         best error  = [   11.3769]
25-01-23 07:48:09 | I |     + error = [11.3769]
25-01-23 07:48:09 | I |   - Calibrating model.layers.0.mlp.gate_proj.weight
25-01-23 07:48:10 | I |       - range scale = [    1.0000]
25-01-23 07:48:10 | I |         sum  error  = [    1.1330]
25-01-23 07:48:10 | I |         best error  = [    1.1330]
25-01-23 07:48:10 | I |     + error = [1.1330]
25-01-23 07:48:11 | I |       - range scale = [    1.0000]
25-01-23 07:48:11 | I |         sum  error  = [   11.7629]
25-01-23 07:48:11 | I |         best error  = [   11.7629]
25-01-23 07:48:11 | I |     + error = [11.7629]
25-01-23 07:48:11 | I |   - Calibrating model.layers.0.mlp.down_proj.weight
25-01-23 07:48:12 | I |       - range scale = [    1.0000]
25-01-23 07:48:12 | I |         sum  error  = [    0.1585]
25-01-23 07:48:12 | I |         best error  = [    0.1585]
25-01-23 07:48:12 | I |     + error = [0.1585]
25-01-23 07:48:13 | I |       - range scale = [    1.0000]
25-01-23 07:48:13 | I |         sum  error  = [    1.5355]
25-01-23 07:48:13 | I |         best error  = [    1.5355]
25-01-23 07:48:13 | I |     + error = [1.5355]
25-01-23 07:48:13 | I |   - Quantizing model.layers.0.self_attn.q_proj.weight
25-01-23 07:48:16 | I |   - Quantizing model.layers.0.self_attn.k_proj.weight
25-01-23 07:48:18 | I |   - Quantizing model.layers.0.self_attn.v_proj.weight
25-01-23 07:48:20 | I |   - Quantizing model.layers.0.self_attn.o_proj.weight
25-01-23 07:48:22 | I |   - Quantizing model.layers.0.mlp.up_proj.weight
25-01-23 07:48:25 | I |   - Quantizing model.layers.0.mlp.gate_proj.weight
25-01-23 07:48:27 | I |   - Quantizing model.layers.0.mlp.down_proj.weight
25-01-23 07:48:33 | I | quantizing activations for layer model.layers.0
25-01-23 07:48:34 | I | collecting calibration activations in model.layers.0
25-01-23 07:48:34 | I | collecting calibration activations in model.layers.0
25-01-23 07:48:35 | I | forward this layer
25-01-23 07:48:35 | I | input_file: /data/gyy/lmquant-main/lmquant/data/data_without_preprocess_llama_7b_bs16/shard/val_input_args_layer_0/23.pt
25-01-23 07:48:35 | I | output_file: /data/gyy/lmquant-main/lmquant/data/data_without_preprocess_llama_7b_bs16/shard/val_teacher_output_layer_0/23.pt
25-01-23 07:48:36 | I | In layer model.layers.0, gradient of model.layers.0.self_attn.q_proj has nan: False
25-01-23 07:48:36 | I | In layer model.layers.0, gradient of model.layers.0.self_attn.q_proj has inf: False
25-01-23 07:48:36 | I | In layer model.layers.0, gradient of model.layers.0.self_attn.k_proj has nan: False
25-01-23 07:48:36 | I | In layer model.layers.0, gradient of model.layers.0.self_attn.k_proj has inf: False
25-01-23 07:48:36 | I | In layer model.layers.0, gradient of model.layers.0.self_attn.v_proj has nan: False
25-01-23 07:48:36 | I | In layer model.layers.0, gradient of model.layers.0.self_attn.v_proj has inf: False
25-01-23 07:48:36 | I | In layer model.layers.0, gradient of model.layers.0.self_attn.o_proj has nan: False
25-01-23 07:48:36 | I | In layer model.layers.0, gradient of model.layers.0.self_attn.o_proj has inf: False
25-01-23 07:48:36 | I | In layer model.layers.0, gradient of model.layers.0.mlp.up_proj has nan: False
25-01-23 07:48:36 | I | In layer model.layers.0, gradient of model.layers.0.mlp.up_proj has inf: False
25-01-23 07:48:36 | I | In layer model.layers.0, gradient of model.layers.0.mlp.gate_proj has nan: False
25-01-23 07:48:36 | I | In layer model.layers.0, gradient of model.layers.0.mlp.gate_proj has inf: False
25-01-23 07:48:36 | I | In layer model.layers.0, gradient of model.layers.0.mlp.down_proj has nan: False
25-01-23 07:48:36 | I | In layer model.layers.0, gradient of model.layers.0.mlp.down_proj has inf: False
25-01-23 07:48:36 | I | [19] done with optimizer step
25-01-23 07:48:36 | I | epoch 001:     24 / 819200000 loss=1.26161e-05, loss_per_token=0.0516753, loss_sum=423.324, wps=226.1, ups=0.03, wpb=8192, bsz=16, num_updates=20, lr=0.002, gnorm=0.284, clip=0, loss_scale=8, train_wall=36, cuda_gb_allocated=17.1, cuda_gb_reserved=18.7, cuda_gb_free=6.6, wall=1463
25-01-23 07:48:36 | I | begin validation on "valid" subset on rank 0
25-01-23 07:48:36 | I | got valid iterator on "valid" subset on rank 0
25-01-23 07:48:36 | I | Valid: Start iterating over samples
25-01-23 07:48:36 | W |   `pretrained` model kwarg is not of type `str`. Many other model arguments may be ignored. Please do not launch via accelerate or use `parallelize=True` if passing an existing model this way.
25-01-23 07:48:36 | W |   Passed an already-initialized model through `pretrained`, assuming single-process call to evaluate() or custom distributed integration
25-01-23 07:48:36 | I | - Evaluator: gptq
25-01-23 07:48:36 | I | - Tasks: ['wikitext', 'val_valid']
25-01-23 07:48:36 | I | - Batch_size: 8
25-01-23 07:48:36 | I |   + Max_seq_length: 2048
25-01-23 07:50:11 | I |     - Results:
25-01-23 07:50:11 | I |       |  Task   |Version|    Metric     |Value |   |Stderr|
25-01-23 07:50:11 | I |       |---------|------:|---------------|-----:|---|-----:|
25-01-23 07:50:11 | I |       |wikitext |      1|word_perplexity|5.4860|±  |5.4860|
25-01-23 07:50:11 | I |       |val_valid|      1|word_perplexity|5.0225|±  |5.0225|
25-01-23 07:50:11 | I |       
25-01-23 07:50:11 | I |   + Max_seq_length: 4096
25-01-23 07:51:44 | I |     - Results:
25-01-23 07:51:44 | I |       |  Task   |Version|    Metric     |Value |   |Stderr|
25-01-23 07:51:44 | I |       |---------|------:|---------------|-----:|---|-----:|
25-01-23 07:51:44 | I |       |wikitext |      1|word_perplexity|5.1263|±  |5.1263|
25-01-23 07:51:44 | I |       |val_valid|      1|word_perplexity|4.8165|±  |4.8165|
25-01-23 07:51:44 | I |       
25-01-23 07:51:44 | I | in valid, quantize current layer weights
25-01-23 07:51:45 | W | Repo card metadata block was not found. Setting CardData to empty.
25-01-23 07:52:30 | I | collecting calibration activations in model.layers.0
25-01-23 07:52:30 | I | collecting calibration activations in model.layers.0
25-01-23 07:52:30 | I | collecting calibration activations in model.layers.0
25-01-23 07:52:30 | I | collecting calibration activations in model.layers.0
25-01-23 07:52:30 | I | collecting calibration activations in model.layers.0
25-01-23 07:52:30 | I | collecting calibration activations in model.layers.0
25-01-23 07:52:30 | I | collecting calibration activations in model.layers.0
25-01-23 07:52:30 | I | collecting calibration activations in model.layers.0
25-01-23 07:52:30 | I | collecting calibration activations in model.layers.0
25-01-23 07:52:30 | I | collecting calibration activations in model.layers.0
25-01-23 07:52:30 | I | collecting calibration activations in model.layers.0
25-01-23 07:52:30 | I | collecting calibration activations in model.layers.0
25-01-23 07:52:30 | I | collecting calibration activations in model.layers.0
25-01-23 07:52:30 | I | collecting calibration activations in model.layers.0
25-01-23 07:52:30 | I | collecting calibration activations in model.layers.0
25-01-23 07:52:30 | I | collecting calibration activations in model.layers.0
25-01-23 07:52:30 | I | collecting calibration activations in model.layers.0
25-01-23 07:52:31 | I | collecting calibration activations in model.layers.0
25-01-23 07:52:31 | I | collecting calibration activations in model.layers.0
25-01-23 07:52:31 | I | collecting calibration activations in model.layers.0
25-01-23 07:52:31 | I | collecting calibration activations in model.layers.0
25-01-23 07:52:31 | I | collecting calibration activations in model.layers.0
25-01-23 07:52:31 | I | collecting calibration activations in model.layers.0
25-01-23 07:52:31 | I | collecting calibration activations in model.layers.0
25-01-23 07:52:31 | I | collecting calibration activations in model.layers.0
25-01-23 07:52:31 | I | collecting calibration activations in model.layers.0
25-01-23 07:52:31 | I | collecting calibration activations in model.layers.0
25-01-23 07:52:31 | I | collecting calibration activations in model.layers.0
25-01-23 07:52:31 | I | collecting calibration activations in model.layers.0
25-01-23 07:52:31 | I | collecting calibration activations in model.layers.0
25-01-23 07:52:31 | I | collecting calibration activations in model.layers.0
25-01-23 07:52:31 | I | collecting calibration activations in model.layers.0
25-01-23 07:52:31 | I | collecting calibration activations in model.layers.0
25-01-23 07:52:31 | I | collecting calibration activations in model.layers.0
25-01-23 07:52:31 | I | collecting calibration activations in model.layers.0
25-01-23 07:52:31 | I | collecting calibration activations in model.layers.0
25-01-23 07:52:32 | I | collecting calibration activations in model.layers.0
25-01-23 07:52:32 | I | collecting calibration activations in model.layers.0
25-01-23 07:52:32 | I | collecting calibration activations in model.layers.0
25-01-23 07:52:32 | I | collecting calibration activations in model.layers.0
25-01-23 07:52:32 | I | collecting calibration activations in model.layers.0
25-01-23 07:52:32 | I | collecting calibration activations in model.layers.0
25-01-23 07:52:32 | I | collecting calibration activations in model.layers.0
25-01-23 07:52:32 | I | collecting calibration activations in model.layers.0
25-01-23 07:52:32 | I | collecting calibration activations in model.layers.0
25-01-23 07:52:32 | I | collecting calibration activations in model.layers.0
25-01-23 07:52:32 | I | collecting calibration activations in model.layers.0
25-01-23 07:52:32 | I | collecting calibration activations in model.layers.0
25-01-23 07:52:32 | I | collecting calibration activations in model.layers.0
25-01-23 07:52:32 | I | collecting calibration activations in model.layers.0
25-01-23 07:52:32 | I | collecting calibration activations in model.layers.0
25-01-23 07:52:32 | I | collecting calibration activations in model.layers.0
25-01-23 07:52:32 | I | collecting calibration activations in model.layers.0
25-01-23 07:52:32 | I | collecting calibration activations in model.layers.0
25-01-23 07:52:32 | I | collecting calibration activations in model.layers.0
25-01-23 07:52:33 | I | collecting calibration activations in model.layers.0
25-01-23 07:52:33 | I | collecting calibration activations in model.layers.0
25-01-23 07:52:33 | I | collecting calibration activations in model.layers.0
25-01-23 07:52:33 | I | collecting calibration activations in model.layers.0
25-01-23 07:52:33 | I | collecting calibration activations in model.layers.0
25-01-23 07:52:33 | I | collecting calibration activations in model.layers.0
25-01-23 07:52:33 | I | collecting calibration activations in model.layers.0
25-01-23 07:52:33 | I | collecting calibration activations in model.layers.0
25-01-23 07:52:33 | I | collecting calibration activations in model.layers.0
25-01-23 07:52:33 | I | collecting calibration activations in model.layers.0
25-01-23 07:52:33 | I | collecting calibration activations in model.layers.0
25-01-23 07:52:33 | I | collecting calibration activations in model.layers.0
25-01-23 07:52:33 | I | collecting calibration activations in model.layers.0
25-01-23 07:52:33 | I | collecting calibration activations in model.layers.0
25-01-23 07:52:33 | I | collecting calibration activations in model.layers.0
25-01-23 07:52:33 | I | collecting calibration activations in model.layers.0
25-01-23 07:52:33 | I | collecting calibration activations in model.layers.0
25-01-23 07:52:33 | I | collecting calibration activations in model.layers.0
25-01-23 07:52:33 | I | collecting calibration activations in model.layers.0
25-01-23 07:52:34 | I | collecting calibration activations in model.layers.0
25-01-23 07:52:34 | I | collecting calibration activations in model.layers.0
25-01-23 07:52:34 | I | collecting calibration activations in model.layers.0
25-01-23 07:52:34 | I | collecting calibration activations in model.layers.0
25-01-23 07:52:34 | I | collecting calibration activations in model.layers.0
25-01-23 07:52:34 | I | collecting calibration activations in model.layers.0
25-01-23 07:52:34 | I | collecting calibration activations in model.layers.0
25-01-23 07:52:34 | I | collecting calibration activations in model.layers.0
25-01-23 07:52:34 | I | collecting calibration activations in model.layers.0
25-01-23 07:52:34 | I | collecting calibration activations in model.layers.0
25-01-23 07:52:34 | I | collecting calibration activations in model.layers.0
25-01-23 07:52:34 | I | collecting calibration activations in model.layers.0
25-01-23 07:52:34 | I | collecting calibration activations in model.layers.0
25-01-23 07:52:34 | I | collecting calibration activations in model.layers.0
25-01-23 07:52:34 | I | collecting calibration activations in model.layers.0
25-01-23 07:52:34 | I | collecting calibration activations in model.layers.0
25-01-23 07:52:34 | I | collecting calibration activations in model.layers.0
25-01-23 07:52:34 | I | collecting calibration activations in model.layers.0
25-01-23 07:52:34 | I | collecting calibration activations in model.layers.0
25-01-23 07:52:35 | I | collecting calibration activations in model.layers.0
25-01-23 07:52:35 | I | collecting calibration activations in model.layers.0
25-01-23 07:52:35 | I | collecting calibration activations in model.layers.0
25-01-23 07:52:35 | I | collecting calibration activations in model.layers.0
25-01-23 07:52:35 | I | collecting calibration activations in model.layers.0
25-01-23 07:52:35 | I | collecting calibration activations in model.layers.0
25-01-23 07:52:35 | I | collecting calibration activations in model.layers.0
25-01-23 07:52:35 | I | collecting calibration activations in model.layers.0
25-01-23 07:52:35 | I | collecting calibration activations in model.layers.0
25-01-23 07:52:35 | I | collecting calibration activations in model.layers.0
25-01-23 07:52:35 | I | collecting calibration activations in model.layers.0
25-01-23 07:52:35 | I | collecting calibration activations in model.layers.0
25-01-23 07:52:35 | I | collecting calibration activations in model.layers.0
25-01-23 07:52:35 | I | collecting calibration activations in model.layers.0
25-01-23 07:52:35 | I | collecting calibration activations in model.layers.0
25-01-23 07:52:35 | I | collecting calibration activations in model.layers.0
25-01-23 07:52:35 | I | collecting calibration activations in model.layers.0
25-01-23 07:52:35 | I | collecting calibration activations in model.layers.0
25-01-23 07:52:35 | I | collecting calibration activations in model.layers.0
25-01-23 07:52:36 | I | collecting calibration activations in model.layers.0
25-01-23 07:52:36 | I | collecting calibration activations in model.layers.0
25-01-23 07:52:36 | I | collecting calibration activations in model.layers.0
25-01-23 07:52:36 | I | collecting calibration activations in model.layers.0
25-01-23 07:52:36 | I | collecting calibration activations in model.layers.0
25-01-23 07:52:36 | I | collecting calibration activations in model.layers.0
25-01-23 07:52:36 | I | collecting calibration activations in model.layers.0
25-01-23 07:52:36 | I | collecting calibration activations in model.layers.0
25-01-23 07:52:36 | I | collecting calibration activations in model.layers.0
25-01-23 07:52:36 | I | collecting calibration activations in model.layers.0
25-01-23 07:52:36 | I | collecting calibration activations in model.layers.0
25-01-23 07:52:36 | I | collecting calibration activations in model.layers.0
25-01-23 07:52:36 | I | collecting calibration activations in model.layers.0
25-01-23 07:52:36 | I | collecting calibration activations in model.layers.0
25-01-23 07:52:36 | I | collecting calibration activations in model.layers.0
25-01-23 07:52:36 | I | collecting calibration activations in model.layers.0
25-01-23 07:52:38 | I |       - range scale = [    1.0000]
25-01-23 07:52:38 | I |         sum  error  = [    0.1618]
25-01-23 07:52:38 | I |         best error  = [    0.1618]
25-01-23 07:52:38 | I |     + error = [0.1618]
25-01-23 07:52:40 | I |       - range scale = [    1.0000]
25-01-23 07:52:40 | I |         sum  error  = [    2.8439]
25-01-23 07:52:40 | I |         best error  = [    2.8439]
25-01-23 07:52:40 | I |     + error = [2.8439]
25-01-23 07:52:41 | I |       - range scale = [    1.0000]
25-01-23 07:52:41 | I |         sum  error  = [    0.2010]
25-01-23 07:52:41 | I |         best error  = [    0.2010]
25-01-23 07:52:41 | I |     + error = [0.2010]
25-01-23 07:52:43 | I |       - range scale = [    1.0000]
25-01-23 07:52:43 | I |         sum  error  = [    3.9974]
25-01-23 07:52:43 | I |         best error  = [    3.9974]
25-01-23 07:52:43 | I |     + error = [3.9974]
25-01-23 07:52:44 | I |       - range scale = [    1.0000]
25-01-23 07:52:44 | I |         sum  error  = [    0.6510]
25-01-23 07:52:44 | I |         best error  = [    0.6510]
25-01-23 07:52:44 | I |     + error = [0.6510]
25-01-23 07:52:44 | I |       - range scale = [    1.0000]
25-01-23 07:52:44 | I |         sum  error  = [    5.9889]
25-01-23 07:52:44 | I |         best error  = [    5.9889]
25-01-23 07:52:44 | I |     + error = [5.9889]
25-01-23 07:52:45 | I |       - range scale = [    1.0000]
25-01-23 07:52:45 | I |         sum  error  = [    0.1519]
25-01-23 07:52:45 | I |         best error  = [    0.1519]
25-01-23 07:52:45 | I |     + error = [0.1519]
25-01-23 07:52:46 | I |       - range scale = [    1.0000]
25-01-23 07:52:46 | I |         sum  error  = [    1.2476]
25-01-23 07:52:46 | I |         best error  = [    1.2476]
25-01-23 07:52:46 | I |     + error = [1.2476]
25-01-23 07:52:47 | I |       - range scale = [    1.0000]
25-01-23 07:52:47 | I |         sum  error  = [    1.1101]
25-01-23 07:52:47 | I |         best error  = [    1.1101]
25-01-23 07:52:47 | I |     + error = [1.1101]
25-01-23 07:52:48 | I |       - range scale = [    1.0000]
25-01-23 07:52:48 | I |         sum  error  = [   11.8207]
25-01-23 07:52:48 | I |         best error  = [   11.8207]
25-01-23 07:52:48 | I |     + error = [11.8207]
25-01-23 07:52:49 | I |       - range scale = [    1.0000]
25-01-23 07:52:49 | I |         sum  error  = [    1.1757]
25-01-23 07:52:49 | I |         best error  = [    1.1757]
25-01-23 07:52:49 | I |     + error = [1.1757]
25-01-23 07:52:50 | I |       - range scale = [    1.0000]
25-01-23 07:52:50 | I |         sum  error  = [   12.2040]
25-01-23 07:52:50 | I |         best error  = [   12.2040]
25-01-23 07:52:50 | I |     + error = [12.2040]
25-01-23 07:52:51 | I |       - range scale = [    1.0000]
25-01-23 07:52:51 | I |         sum  error  = [    0.1952]
25-01-23 07:52:51 | I |         best error  = [    0.1952]
25-01-23 07:52:51 | I |     + error = [0.1952]
25-01-23 07:52:53 | I |       - range scale = [    1.0000]
25-01-23 07:52:53 | I |         sum  error  = [    1.9268]
25-01-23 07:52:53 | I |         best error  = [    1.9268]
25-01-23 07:52:53 | I |     + error = [1.9268]
25-01-23 07:53:19 | I | in valid, quantize current layer acts
25-01-23 07:53:20 | I | collecting calibration activations in model.layers.0
25-01-23 07:53:20 | I | collecting calibration activations in model.layers.0
25-01-23 07:53:20 | I | collecting calibration activations in model.layers.0
25-01-23 07:53:21 | I | collecting calibration activations in model.layers.0
25-01-23 07:53:21 | I | collecting calibration activations in model.layers.0
25-01-23 07:53:21 | I | collecting calibration activations in model.layers.0
25-01-23 07:53:21 | I | collecting calibration activations in model.layers.0
25-01-23 07:53:21 | I | collecting calibration activations in model.layers.0
25-01-23 07:53:21 | I | collecting calibration activations in model.layers.0
25-01-23 07:53:21 | I | collecting calibration activations in model.layers.0
25-01-23 07:53:21 | I | collecting calibration activations in model.layers.0
25-01-23 07:53:21 | I | collecting calibration activations in model.layers.0
25-01-23 07:53:21 | I | collecting calibration activations in model.layers.0
25-01-23 07:53:21 | I | collecting calibration activations in model.layers.0
25-01-23 07:53:21 | I | collecting calibration activations in model.layers.0
25-01-23 07:53:21 | I | collecting calibration activations in model.layers.0
25-01-23 07:53:21 | I | collecting calibration activations in model.layers.0
25-01-23 07:53:21 | I | collecting calibration activations in model.layers.0
25-01-23 07:53:22 | I | collecting calibration activations in model.layers.0
25-01-23 07:53:22 | I | collecting calibration activations in model.layers.0
25-01-23 07:53:22 | I | collecting calibration activations in model.layers.0
25-01-23 07:53:22 | I | collecting calibration activations in model.layers.0
25-01-23 07:53:22 | I | collecting calibration activations in model.layers.0
25-01-23 07:53:22 | I | collecting calibration activations in model.layers.0
25-01-23 07:53:22 | I | collecting calibration activations in model.layers.0
25-01-23 07:53:22 | I | collecting calibration activations in model.layers.0
25-01-23 07:53:22 | I | collecting calibration activations in model.layers.0
25-01-23 07:53:22 | I | collecting calibration activations in model.layers.0
25-01-23 07:53:22 | I | collecting calibration activations in model.layers.0
25-01-23 07:53:22 | I | collecting calibration activations in model.layers.0
25-01-23 07:53:22 | I | collecting calibration activations in model.layers.0
25-01-23 07:53:22 | I | collecting calibration activations in model.layers.0
25-01-23 07:53:22 | I | collecting calibration activations in model.layers.0
25-01-23 07:53:22 | I | collecting calibration activations in model.layers.0
25-01-23 07:53:23 | I | collecting calibration activations in model.layers.0
25-01-23 07:53:23 | I | collecting calibration activations in model.layers.0
25-01-23 07:53:23 | I | collecting calibration activations in model.layers.0
25-01-23 07:53:23 | I | collecting calibration activations in model.layers.0
25-01-23 07:53:23 | I | collecting calibration activations in model.layers.0
25-01-23 07:53:23 | I | collecting calibration activations in model.layers.0
25-01-23 07:53:23 | I | collecting calibration activations in model.layers.0
25-01-23 07:53:23 | I | collecting calibration activations in model.layers.0
25-01-23 07:53:23 | I | collecting calibration activations in model.layers.0
25-01-23 07:53:23 | I | collecting calibration activations in model.layers.0
25-01-23 07:53:23 | I | collecting calibration activations in model.layers.0
25-01-23 07:53:23 | I | collecting calibration activations in model.layers.0
25-01-23 07:53:23 | I | collecting calibration activations in model.layers.0
25-01-23 07:53:23 | I | collecting calibration activations in model.layers.0
25-01-23 07:53:23 | I | collecting calibration activations in model.layers.0
25-01-23 07:53:23 | I | collecting calibration activations in model.layers.0
25-01-23 07:53:24 | I | collecting calibration activations in model.layers.0
25-01-23 07:53:24 | I | collecting calibration activations in model.layers.0
25-01-23 07:53:24 | I | collecting calibration activations in model.layers.0
25-01-23 07:53:24 | I | collecting calibration activations in model.layers.0
25-01-23 07:53:24 | I | collecting calibration activations in model.layers.0
25-01-23 07:53:24 | I | collecting calibration activations in model.layers.0
25-01-23 07:53:24 | I | collecting calibration activations in model.layers.0
25-01-23 07:53:24 | I | collecting calibration activations in model.layers.0
25-01-23 07:53:24 | I | collecting calibration activations in model.layers.0
25-01-23 07:53:24 | I | collecting calibration activations in model.layers.0
25-01-23 07:53:24 | I | collecting calibration activations in model.layers.0
25-01-23 07:53:24 | I | collecting calibration activations in model.layers.0
25-01-23 07:53:24 | I | collecting calibration activations in model.layers.0
25-01-23 07:53:24 | I | collecting calibration activations in model.layers.0
25-01-23 07:53:24 | I | collecting calibration activations in model.layers.0
25-01-23 07:53:25 | I | collecting calibration activations in model.layers.0
25-01-23 07:53:25 | I | collecting calibration activations in model.layers.0
25-01-23 07:53:25 | I | collecting calibration activations in model.layers.0
25-01-23 07:53:25 | I | collecting calibration activations in model.layers.0
25-01-23 07:53:25 | I | collecting calibration activations in model.layers.0
25-01-23 07:53:25 | I | collecting calibration activations in model.layers.0
25-01-23 07:53:25 | I | collecting calibration activations in model.layers.0
25-01-23 07:53:25 | I | collecting calibration activations in model.layers.0
25-01-23 07:53:25 | I | collecting calibration activations in model.layers.0
25-01-23 07:53:25 | I | collecting calibration activations in model.layers.0
25-01-23 07:53:25 | I | collecting calibration activations in model.layers.0
25-01-23 07:53:25 | I | collecting calibration activations in model.layers.0
25-01-23 07:53:25 | I | collecting calibration activations in model.layers.0
25-01-23 07:53:25 | I | collecting calibration activations in model.layers.0
25-01-23 07:53:25 | I | collecting calibration activations in model.layers.0
25-01-23 07:53:25 | I | collecting calibration activations in model.layers.0
25-01-23 07:53:26 | I | collecting calibration activations in model.layers.0
25-01-23 07:53:26 | I | collecting calibration activations in model.layers.0
25-01-23 07:53:26 | I | collecting calibration activations in model.layers.0
25-01-23 07:53:26 | I | collecting calibration activations in model.layers.0
25-01-23 07:53:26 | I | collecting calibration activations in model.layers.0
25-01-23 07:53:26 | I | collecting calibration activations in model.layers.0
25-01-23 07:53:26 | I | collecting calibration activations in model.layers.0
25-01-23 07:53:26 | I | collecting calibration activations in model.layers.0
25-01-23 07:53:26 | I | collecting calibration activations in model.layers.0
25-01-23 07:53:26 | I | collecting calibration activations in model.layers.0
25-01-23 07:53:26 | I | collecting calibration activations in model.layers.0
25-01-23 07:53:26 | I | collecting calibration activations in model.layers.0
25-01-23 07:53:26 | I | collecting calibration activations in model.layers.0
25-01-23 07:53:26 | I | collecting calibration activations in model.layers.0
25-01-23 07:53:26 | I | collecting calibration activations in model.layers.0
25-01-23 07:53:27 | I | collecting calibration activations in model.layers.0
25-01-23 07:53:27 | I | collecting calibration activations in model.layers.0
25-01-23 07:53:27 | I | collecting calibration activations in model.layers.0
25-01-23 07:53:27 | I | collecting calibration activations in model.layers.0
25-01-23 07:53:27 | I | collecting calibration activations in model.layers.0
25-01-23 07:53:27 | I | collecting calibration activations in model.layers.0
25-01-23 07:53:27 | I | collecting calibration activations in model.layers.0
25-01-23 07:53:27 | I | collecting calibration activations in model.layers.0
25-01-23 07:53:27 | I | collecting calibration activations in model.layers.0
25-01-23 07:53:27 | I | collecting calibration activations in model.layers.0
25-01-23 07:53:27 | I | collecting calibration activations in model.layers.0
25-01-23 07:53:27 | I | collecting calibration activations in model.layers.0
25-01-23 07:53:27 | I | collecting calibration activations in model.layers.0
25-01-23 07:53:27 | I | collecting calibration activations in model.layers.0
25-01-23 07:53:27 | I | collecting calibration activations in model.layers.0
25-01-23 07:53:27 | I | collecting calibration activations in model.layers.0
25-01-23 07:53:28 | I | collecting calibration activations in model.layers.0
25-01-23 07:53:28 | I | collecting calibration activations in model.layers.0
25-01-23 07:53:28 | I | collecting calibration activations in model.layers.0
25-01-23 07:53:28 | I | collecting calibration activations in model.layers.0
25-01-23 07:53:28 | I | collecting calibration activations in model.layers.0
25-01-23 07:53:28 | I | collecting calibration activations in model.layers.0
25-01-23 07:53:28 | I | collecting calibration activations in model.layers.0
25-01-23 07:53:28 | I | collecting calibration activations in model.layers.0
25-01-23 07:53:28 | I | collecting calibration activations in model.layers.0
25-01-23 07:53:28 | I | collecting calibration activations in model.layers.0
25-01-23 07:53:28 | I | collecting calibration activations in model.layers.0
25-01-23 07:53:28 | I | collecting calibration activations in model.layers.0
25-01-23 07:53:28 | I | collecting calibration activations in model.layers.0
25-01-23 07:53:28 | I | collecting calibration activations in model.layers.0
25-01-23 07:53:28 | I | collecting calibration activations in model.layers.0
25-01-23 07:53:28 | I | collecting calibration activations in model.layers.0
25-01-23 07:53:31 | I | use gptq_eval(lmquant) to calculate ppl, partly quanted
25-01-23 07:53:31 | W |   `pretrained` model kwarg is not of type `str`. Many other model arguments may be ignored. Please do not launch via accelerate or use `parallelize=True` if passing an existing model this way.
25-01-23 07:53:31 | W |   Passed an already-initialized model through `pretrained`, assuming single-process call to evaluate() or custom distributed integration
25-01-23 07:53:31 | I | - Evaluator: gptq
25-01-23 07:53:31 | I | - Tasks: ['wikitext', 'val_valid']
25-01-23 07:53:31 | I | - Batch_size: 8
25-01-23 07:53:31 | I |   + Max_seq_length: 2048
25-01-23 07:55:08 | I |     - Results:
25-01-23 07:55:08 | I |       |  Task   |Version|    Metric     |Value |   |Stderr|
25-01-23 07:55:08 | I |       |---------|------:|---------------|-----:|---|-----:|
25-01-23 07:55:08 | I |       |wikitext |      1|word_perplexity|5.4902|±  |5.4902|
25-01-23 07:55:08 | I |       |val_valid|      1|word_perplexity|5.0316|±  |5.0316|
25-01-23 07:55:08 | I |       
25-01-23 07:55:08 | I |   + Max_seq_length: 4096
25-01-23 07:56:43 | I |     - Results:
25-01-23 07:56:43 | I |       |  Task   |Version|    Metric     |Value |   |Stderr|
25-01-23 07:56:43 | I |       |---------|------:|---------------|-----:|---|-----:|
25-01-23 07:56:43 | I |       |wikitext |      1|word_perplexity|5.1271|±  |5.1271|
25-01-23 07:56:43 | I |       |val_valid|      1|word_perplexity|4.8263|±  |4.8263|
25-01-23 07:56:43 | I |       
25-01-23 07:57:43 | I | epoch 001 | valid on 'valid' subset:    102 / 819200000 loss_valid=2.149, lmquant_ppl_result_val=-1, lmquant_ppl_result_wikitext=-1, ppl=4.44, wps=7240.2, wpb=4058.2, bsz=2, num_updates=20, lmquant_ppl_wikitext_all_quanted=5.48603, lmquant_ppl_val_all_quanted=5.02248, lmquant_ppl_wikitext_partly_quanted=5.49016, lmquant_ppl_val_partly_quanted=5.03161
25-01-23 07:57:43 | I | epoch 001 | valid on 'valid' subset | loss_valid 2.149 | lmquant_ppl_result_val -1 | lmquant_ppl_result_wikitext -1 | ppl 4.44 | wps 7240.2 | wpb 4058.2 | bsz 2 | num_updates 20 | lmquant_ppl_wikitext_all_quanted 5.48603 | lmquant_ppl_val_all_quanted 5.02248 | lmquant_ppl_wikitext_partly_quanted 5.49016 | lmquant_ppl_val_partly_quanted 5.03161
25-01-23 07:57:43 | I | TRAIN CURRENT LAYER_IDX = 0
25-01-23 07:57:43 | I | in layer model.layers.0
25-01-23 07:57:43 | I | quantizing weights for layer model.layers.0
25-01-23 07:57:43 | I | collecting calibration activations in model.layers.0
25-01-23 07:57:43 | I | collecting calibration activations in model.layers.0
25-01-23 07:57:44 | I | - Quantizing decoder layer model.layers.0
25-01-23 07:57:44 | I |   - Calibrating model.layers.0.self_attn.q_proj.weight
25-01-23 07:57:44 | I |       - range scale = [    1.0000]
25-01-23 07:57:44 | I |         sum  error  = [    0.0486]
25-01-23 07:57:44 | I |         best error  = [    0.0486]
25-01-23 07:57:44 | I |     + error = [0.0486]
25-01-23 07:57:45 | I |       - range scale = [    1.0000]
25-01-23 07:57:45 | I |         sum  error  = [    0.7615]
25-01-23 07:57:45 | I |         best error  = [    0.7615]
25-01-23 07:57:45 | I |     + error = [0.7615]
25-01-23 07:57:45 | I |   - Calibrating model.layers.0.self_attn.k_proj.weight
25-01-23 07:57:46 | I |       - range scale = [    1.0000]
25-01-23 07:57:46 | I |         sum  error  = [    0.0636]
25-01-23 07:57:46 | I |         best error  = [    0.0636]
25-01-23 07:57:46 | I |     + error = [0.0636]
25-01-23 07:57:47 | I |       - range scale = [    1.0000]
25-01-23 07:57:47 | I |         sum  error  = [    1.0193]
25-01-23 07:57:47 | I |         best error  = [    1.0193]
25-01-23 07:57:47 | I |     + error = [1.0193]
25-01-23 07:57:47 | I |   - Calibrating model.layers.0.self_attn.v_proj.weight
25-01-23 07:57:48 | I |       - range scale = [    1.0000]
25-01-23 07:57:48 | I |         sum  error  = [    0.7172]
25-01-23 07:57:48 | I |         best error  = [    0.7172]
25-01-23 07:57:48 | I |     + error = [0.7172]
25-01-23 07:57:48 | I |       - range scale = [    1.0000]
25-01-23 07:57:48 | I |         sum  error  = [    6.6699]
25-01-23 07:57:48 | I |         best error  = [    6.6699]
25-01-23 07:57:48 | I |     + error = [6.6699]
25-01-23 07:57:49 | I |   - Calibrating model.layers.0.self_attn.o_proj.weight
25-01-23 07:57:49 | I |       - range scale = [    1.0000]
25-01-23 07:57:49 | I |         sum  error  = [    0.2064]
25-01-23 07:57:49 | I |         best error  = [    0.2064]
25-01-23 07:57:49 | I |     + error = [0.2064]
25-01-23 07:57:50 | I |       - range scale = [    1.0000]
25-01-23 07:57:50 | I |         sum  error  = [    1.6140]
25-01-23 07:57:50 | I |         best error  = [    1.6140]
25-01-23 07:57:50 | I |     + error = [1.6140]
25-01-23 07:57:50 | I |   - Calibrating model.layers.0.mlp.up_proj.weight
25-01-23 07:57:51 | I |       - range scale = [    1.0000]
25-01-23 07:57:51 | I |         sum  error  = [    1.1123]
25-01-23 07:57:51 | I |         best error  = [    1.1123]
25-01-23 07:57:51 | I |     + error = [1.1123]
25-01-23 07:57:52 | I |       - range scale = [    1.0000]
25-01-23 07:57:52 | I |         sum  error  = [   11.8025]
25-01-23 07:57:52 | I |         best error  = [   11.8025]
25-01-23 07:57:52 | I |     + error = [11.8025]
25-01-23 07:57:52 | I |   - Calibrating model.layers.0.mlp.gate_proj.weight
25-01-23 07:57:53 | I |       - range scale = [    1.0000]
25-01-23 07:57:53 | I |         sum  error  = [    1.1731]
25-01-23 07:57:53 | I |         best error  = [    1.1731]
25-01-23 07:57:53 | I |     + error = [1.1731]
25-01-23 07:57:54 | I |       - range scale = [    1.0000]
25-01-23 07:57:54 | I |         sum  error  = [   12.1952]
25-01-23 07:57:54 | I |         best error  = [   12.1952]
25-01-23 07:57:54 | I |     + error = [12.1952]
25-01-23 07:57:54 | I |   - Calibrating model.layers.0.mlp.down_proj.weight
25-01-23 07:57:55 | I |       - range scale = [    1.0000]
25-01-23 07:57:55 | I |         sum  error  = [    0.1676]
25-01-23 07:57:55 | I |         best error  = [    0.1676]
25-01-23 07:57:55 | I |     + error = [0.1676]
25-01-23 07:57:56 | I |       - range scale = [    1.0000]
25-01-23 07:57:56 | I |         sum  error  = [    1.6264]
25-01-23 07:57:56 | I |         best error  = [    1.6264]
25-01-23 07:57:56 | I |     + error = [1.6264]
25-01-23 07:57:57 | I |   - Quantizing model.layers.0.self_attn.q_proj.weight
25-01-23 07:57:59 | I |   - Quantizing model.layers.0.self_attn.k_proj.weight
25-01-23 07:58:01 | I |   - Quantizing model.layers.0.self_attn.v_proj.weight
25-01-23 07:58:03 | I |   - Quantizing model.layers.0.self_attn.o_proj.weight
25-01-23 07:58:06 | I |   - Quantizing model.layers.0.mlp.up_proj.weight
25-01-23 07:58:08 | I |   - Quantizing model.layers.0.mlp.gate_proj.weight
25-01-23 07:58:11 | I |   - Quantizing model.layers.0.mlp.down_proj.weight
25-01-23 07:58:18 | I | quantizing activations for layer model.layers.0
25-01-23 07:58:19 | I | collecting calibration activations in model.layers.0
25-01-23 07:58:19 | I | collecting calibration activations in model.layers.0
25-01-23 07:58:21 | I | forward this layer
25-01-23 07:58:21 | I | input_file: /data/gyy/lmquant-main/lmquant/data/data_without_preprocess_llama_7b_bs16/shard/val_input_args_layer_0/24.pt
25-01-23 07:58:21 | I | output_file: /data/gyy/lmquant-main/lmquant/data/data_without_preprocess_llama_7b_bs16/shard/val_teacher_output_layer_0/24.pt
25-01-23 07:58:21 | I | In layer model.layers.0, gradient of model.layers.0.self_attn.q_proj has nan: False
25-01-23 07:58:21 | I | In layer model.layers.0, gradient of model.layers.0.self_attn.q_proj has inf: False
25-01-23 07:58:21 | I | In layer model.layers.0, gradient of model.layers.0.self_attn.k_proj has nan: False
25-01-23 07:58:21 | I | In layer model.layers.0, gradient of model.layers.0.self_attn.k_proj has inf: False
25-01-23 07:58:21 | I | In layer model.layers.0, gradient of model.layers.0.self_attn.v_proj has nan: False
25-01-23 07:58:21 | I | In layer model.layers.0, gradient of model.layers.0.self_attn.v_proj has inf: False
25-01-23 07:58:21 | I | In layer model.layers.0, gradient of model.layers.0.self_attn.o_proj has nan: False
25-01-23 07:58:21 | I | In layer model.layers.0, gradient of model.layers.0.self_attn.o_proj has inf: False
25-01-23 07:58:21 | I | In layer model.layers.0, gradient of model.layers.0.mlp.up_proj has nan: False
25-01-23 07:58:21 | I | In layer model.layers.0, gradient of model.layers.0.mlp.up_proj has inf: False
25-01-23 07:58:21 | I | In layer model.layers.0, gradient of model.layers.0.mlp.gate_proj has nan: False
25-01-23 07:58:21 | I | In layer model.layers.0, gradient of model.layers.0.mlp.gate_proj has inf: False
25-01-23 07:58:21 | I | In layer model.layers.0, gradient of model.layers.0.mlp.down_proj has nan: False
25-01-23 07:58:21 | I | In layer model.layers.0, gradient of model.layers.0.mlp.down_proj has inf: False
25-01-23 07:58:21 | I | [20] done with optimizer step
25-01-23 07:58:21 | I | epoch 001:     25 / 819200000 loss=1.31898e-05, loss_per_token=0.0540253, loss_sum=442.575, wps=14, ups=0, wpb=8192, bsz=16, num_updates=21, lr=0.0021, gnorm=0.257, clip=0, loss_scale=8, train_wall=39, cuda_gb_allocated=17.2, cuda_gb_reserved=21.2, cuda_gb_free=6.5, wall=2049
25-01-23 07:58:21 | I | TRAIN CURRENT LAYER_IDX = 0
25-01-23 07:58:21 | I | in layer model.layers.0
25-01-23 07:58:21 | I | quantizing weights for layer model.layers.0
25-01-23 07:58:22 | I | collecting calibration activations in model.layers.0
25-01-23 07:58:22 | I | collecting calibration activations in model.layers.0
25-01-23 07:58:22 | I | - Quantizing decoder layer model.layers.0
25-01-23 07:58:22 | I |   - Calibrating model.layers.0.self_attn.q_proj.weight
25-01-23 07:58:23 | I |       - range scale = [    1.0000]
25-01-23 07:58:23 | I |         sum  error  = [    0.0512]
25-01-23 07:58:23 | I |         best error  = [    0.0512]
25-01-23 07:58:23 | I |     + error = [0.0512]
25-01-23 07:58:24 | I |       - range scale = [    1.0000]
25-01-23 07:58:24 | I |         sum  error  = [    0.8023]
25-01-23 07:58:24 | I |         best error  = [    0.8023]
25-01-23 07:58:24 | I |     + error = [0.8023]
25-01-23 07:58:24 | I |   - Calibrating model.layers.0.self_attn.k_proj.weight
25-01-23 07:58:25 | I |       - range scale = [    1.0000]
25-01-23 07:58:25 | I |         sum  error  = [    0.0701]
25-01-23 07:58:25 | I |         best error  = [    0.0701]
25-01-23 07:58:25 | I |     + error = [0.0701]
25-01-23 07:58:25 | I |       - range scale = [    1.0000]
25-01-23 07:58:25 | I |         sum  error  = [    1.0250]
25-01-23 07:58:25 | I |         best error  = [    1.0250]
25-01-23 07:58:25 | I |     + error = [1.0250]
25-01-23 07:58:26 | I |   - Calibrating model.layers.0.self_attn.v_proj.weight
25-01-23 07:58:26 | I |       - range scale = [    1.0000]
25-01-23 07:58:26 | I |         sum  error  = [    0.7226]
25-01-23 07:58:26 | I |         best error  = [    0.7226]
25-01-23 07:58:26 | I |     + error = [0.7226]
25-01-23 07:58:27 | I |       - range scale = [    1.0000]
25-01-23 07:58:27 | I |         sum  error  = [    6.7381]
25-01-23 07:58:27 | I |         best error  = [    6.7381]
25-01-23 07:58:27 | I |     + error = [6.7381]
25-01-23 07:58:27 | I |   - Calibrating model.layers.0.self_attn.o_proj.weight
25-01-23 07:58:28 | I |       - range scale = [    1.0000]
25-01-23 07:58:28 | I |         sum  error  = [    0.2105]
25-01-23 07:58:28 | I |         best error  = [    0.2105]
25-01-23 07:58:28 | I |     + error = [0.2105]
25-01-23 07:58:29 | I |       - range scale = [    1.0000]
25-01-23 07:58:29 | I |         sum  error  = [    1.6391]
25-01-23 07:58:29 | I |         best error  = [    1.6391]
25-01-23 07:58:29 | I |     + error = [1.6391]
25-01-23 07:58:29 | I |   - Calibrating model.layers.0.mlp.up_proj.weight
25-01-23 07:58:30 | I |       - range scale = [    1.0000]
25-01-23 07:58:30 | I |         sum  error  = [    1.1186]
25-01-23 07:58:30 | I |         best error  = [    1.1186]
25-01-23 07:58:30 | I |     + error = [1.1186]
25-01-23 07:58:31 | I |       - range scale = [    1.0000]
25-01-23 07:58:31 | I |         sum  error  = [   11.8695]
25-01-23 07:58:31 | I |         best error  = [   11.8695]
25-01-23 07:58:31 | I |     + error = [11.8695]
25-01-23 07:58:31 | I |   - Calibrating model.layers.0.mlp.gate_proj.weight
25-01-23 07:58:32 | I |       - range scale = [    1.0000]
25-01-23 07:58:32 | I |         sum  error  = [    1.1810]
25-01-23 07:58:32 | I |         best error  = [    1.1810]
25-01-23 07:58:32 | I |     + error = [1.1810]
25-01-23 07:58:33 | I |       - range scale = [    1.0000]
25-01-23 07:58:33 | I |         sum  error  = [   12.2670]
25-01-23 07:58:33 | I |         best error  = [   12.2670]
25-01-23 07:58:33 | I |     + error = [12.2670]
25-01-23 07:58:33 | I |   - Calibrating model.layers.0.mlp.down_proj.weight
25-01-23 07:58:34 | I |       - range scale = [    1.0000]
25-01-23 07:58:34 | I |         sum  error  = [    0.1678]
25-01-23 07:58:34 | I |         best error  = [    0.1678]
25-01-23 07:58:34 | I |     + error = [0.1678]
25-01-23 07:58:35 | I |       - range scale = [    1.0000]
25-01-23 07:58:35 | I |         sum  error  = [    1.6259]
25-01-23 07:58:35 | I |         best error  = [    1.6259]
25-01-23 07:58:35 | I |     + error = [1.6259]
25-01-23 07:58:35 | I |   - Quantizing model.layers.0.self_attn.q_proj.weight
25-01-23 07:58:38 | I |   - Quantizing model.layers.0.self_attn.k_proj.weight
25-01-23 07:58:40 | I |   - Quantizing model.layers.0.self_attn.v_proj.weight
25-01-23 07:58:42 | I |   - Quantizing model.layers.0.self_attn.o_proj.weight
25-01-23 07:58:44 | I |   - Quantizing model.layers.0.mlp.up_proj.weight
25-01-23 07:58:47 | I |   - Quantizing model.layers.0.mlp.gate_proj.weight
25-01-23 07:58:49 | I |   - Quantizing model.layers.0.mlp.down_proj.weight
25-01-23 07:58:55 | I | quantizing activations for layer model.layers.0
25-01-23 07:58:55 | I | collecting calibration activations in model.layers.0
25-01-23 07:58:55 | I | collecting calibration activations in model.layers.0
25-01-23 07:58:57 | I | forward this layer
25-01-23 07:58:57 | I | input_file: /data/gyy/lmquant-main/lmquant/data/data_without_preprocess_llama_7b_bs16/shard/val_input_args_layer_0/25.pt
25-01-23 07:58:57 | I | output_file: /data/gyy/lmquant-main/lmquant/data/data_without_preprocess_llama_7b_bs16/shard/val_teacher_output_layer_0/25.pt
25-01-23 07:58:57 | I | In layer model.layers.0, gradient of model.layers.0.self_attn.q_proj has nan: False
25-01-23 07:58:57 | I | In layer model.layers.0, gradient of model.layers.0.self_attn.q_proj has inf: False
25-01-23 07:58:57 | I | In layer model.layers.0, gradient of model.layers.0.self_attn.k_proj has nan: False
25-01-23 07:58:57 | I | In layer model.layers.0, gradient of model.layers.0.self_attn.k_proj has inf: False
25-01-23 07:58:57 | I | In layer model.layers.0, gradient of model.layers.0.self_attn.v_proj has nan: False
25-01-23 07:58:57 | I | In layer model.layers.0, gradient of model.layers.0.self_attn.v_proj has inf: False
25-01-23 07:58:57 | I | In layer model.layers.0, gradient of model.layers.0.self_attn.o_proj has nan: False
25-01-23 07:58:57 | I | In layer model.layers.0, gradient of model.layers.0.self_attn.o_proj has inf: False
25-01-23 07:58:57 | I | In layer model.layers.0, gradient of model.layers.0.mlp.up_proj has nan: False
25-01-23 07:58:57 | I | In layer model.layers.0, gradient of model.layers.0.mlp.up_proj has inf: False
25-01-23 07:58:57 | I | In layer model.layers.0, gradient of model.layers.0.mlp.gate_proj has nan: False
25-01-23 07:58:57 | I | In layer model.layers.0, gradient of model.layers.0.mlp.gate_proj has inf: False
25-01-23 07:58:57 | I | In layer model.layers.0, gradient of model.layers.0.mlp.down_proj has nan: False
25-01-23 07:58:57 | I | In layer model.layers.0, gradient of model.layers.0.mlp.down_proj has inf: False
25-01-23 07:58:57 | I | [21] done with optimizer step
25-01-23 07:58:57 | I | epoch 001:     26 / 819200000 loss=1.28173e-05, loss_per_token=0.0524996, loss_sum=430.077, wps=227, ups=0.03, wpb=8192, bsz=16, num_updates=22, lr=0.0022, gnorm=0.265, clip=0, loss_scale=8, train_wall=36, cuda_gb_allocated=17.1, cuda_gb_reserved=18.3, cuda_gb_free=6.6, wall=2085
25-01-23 07:58:58 | I | TRAIN CURRENT LAYER_IDX = 0
25-01-23 07:58:58 | I | in layer model.layers.0
25-01-23 07:58:58 | I | quantizing weights for layer model.layers.0
25-01-23 07:58:58 | I | collecting calibration activations in model.layers.0
25-01-23 07:58:58 | I | collecting calibration activations in model.layers.0
25-01-23 07:58:58 | I | - Quantizing decoder layer model.layers.0
25-01-23 07:58:58 | I |   - Calibrating model.layers.0.self_attn.q_proj.weight
25-01-23 07:58:59 | I |       - range scale = [    1.0000]
25-01-23 07:58:59 | I |         sum  error  = [    0.0530]
25-01-23 07:58:59 | I |         best error  = [    0.0530]
25-01-23 07:58:59 | I |     + error = [0.0530]
25-01-23 07:59:00 | I |       - range scale = [    1.0000]
25-01-23 07:59:00 | I |         sum  error  = [    0.8116]
25-01-23 07:59:00 | I |         best error  = [    0.8116]
25-01-23 07:59:00 | I |     + error = [0.8116]
25-01-23 07:59:00 | I |   - Calibrating model.layers.0.self_attn.k_proj.weight
25-01-23 07:59:01 | I |       - range scale = [    1.0000]
25-01-23 07:59:01 | I |         sum  error  = [    0.0680]
25-01-23 07:59:01 | I |         best error  = [    0.0680]
25-01-23 07:59:01 | I |     + error = [0.0680]
25-01-23 07:59:01 | I |       - range scale = [    1.0000]
25-01-23 07:59:01 | I |         sum  error  = [    1.0402]
25-01-23 07:59:01 | I |         best error  = [    1.0402]
25-01-23 07:59:01 | I |     + error = [1.0402]
25-01-23 07:59:02 | I |   - Calibrating model.layers.0.self_attn.v_proj.weight
25-01-23 07:59:02 | I |       - range scale = [    1.0000]
25-01-23 07:59:02 | I |         sum  error  = [    0.7220]
25-01-23 07:59:02 | I |         best error  = [    0.7220]
25-01-23 07:59:02 | I |     + error = [0.7220]
25-01-23 07:59:03 | I |       - range scale = [    1.0000]
25-01-23 07:59:03 | I |         sum  error  = [    6.7882]
25-01-23 07:59:03 | I |         best error  = [    6.7882]
25-01-23 07:59:03 | I |     + error = [6.7882]
25-01-23 07:59:03 | I |   - Calibrating model.layers.0.self_attn.o_proj.weight
25-01-23 07:59:04 | I |       - range scale = [    1.0000]
25-01-23 07:59:04 | I |         sum  error  = [    0.2094]
25-01-23 07:59:04 | I |         best error  = [    0.2094]
25-01-23 07:59:04 | I |     + error = [0.2094]
25-01-23 07:59:05 | I |       - range scale = [    1.0000]
25-01-23 07:59:05 | I |         sum  error  = [    1.6360]
25-01-23 07:59:05 | I |         best error  = [    1.6360]
25-01-23 07:59:05 | I |     + error = [1.6360]
25-01-23 07:59:05 | I |   - Calibrating model.layers.0.mlp.up_proj.weight
25-01-23 07:59:06 | I |       - range scale = [    1.0000]
25-01-23 07:59:06 | I |         sum  error  = [    1.1207]
25-01-23 07:59:06 | I |         best error  = [    1.1207]
25-01-23 07:59:06 | I |     + error = [1.1207]
25-01-23 07:59:07 | I |       - range scale = [    1.0000]
25-01-23 07:59:07 | I |         sum  error  = [   11.8800]
25-01-23 07:59:07 | I |         best error  = [   11.8800]
25-01-23 07:59:07 | I |     + error = [11.8800]
25-01-23 07:59:07 | I |   - Calibrating model.layers.0.mlp.gate_proj.weight
25-01-23 07:59:08 | I |       - range scale = [    1.0000]
25-01-23 07:59:08 | I |         sum  error  = [    1.1788]
25-01-23 07:59:08 | I |         best error  = [    1.1788]
25-01-23 07:59:08 | I |     + error = [1.1788]
25-01-23 07:59:09 | I |       - range scale = [    1.0000]
25-01-23 07:59:09 | I |         sum  error  = [   12.2732]
25-01-23 07:59:09 | I |         best error  = [   12.2732]
25-01-23 07:59:09 | I |     + error = [12.2732]
25-01-23 07:59:09 | I |   - Calibrating model.layers.0.mlp.down_proj.weight
25-01-23 07:59:10 | I |       - range scale = [    1.0000]
25-01-23 07:59:10 | I |         sum  error  = [    0.1673]
25-01-23 07:59:10 | I |         best error  = [    0.1673]
25-01-23 07:59:10 | I |     + error = [0.1673]
25-01-23 07:59:11 | I |       - range scale = [    1.0000]
25-01-23 07:59:11 | I |         sum  error  = [    1.6238]
25-01-23 07:59:11 | I |         best error  = [    1.6238]
25-01-23 07:59:11 | I |     + error = [1.6238]
25-01-23 07:59:11 | I |   - Quantizing model.layers.0.self_attn.q_proj.weight
25-01-23 07:59:14 | I |   - Quantizing model.layers.0.self_attn.k_proj.weight
25-01-23 07:59:16 | I |   - Quantizing model.layers.0.self_attn.v_proj.weight
25-01-23 07:59:18 | I |   - Quantizing model.layers.0.self_attn.o_proj.weight
25-01-23 07:59:20 | I |   - Quantizing model.layers.0.mlp.up_proj.weight
25-01-23 07:59:23 | I |   - Quantizing model.layers.0.mlp.gate_proj.weight
25-01-23 07:59:25 | I |   - Quantizing model.layers.0.mlp.down_proj.weight
25-01-23 07:59:31 | I | quantizing activations for layer model.layers.0
25-01-23 07:59:31 | I | collecting calibration activations in model.layers.0
25-01-23 07:59:31 | I | collecting calibration activations in model.layers.0
25-01-23 07:59:33 | I | forward this layer
25-01-23 07:59:33 | I | input_file: /data/gyy/lmquant-main/lmquant/data/data_without_preprocess_llama_7b_bs16/shard/val_input_args_layer_0/26.pt
25-01-23 07:59:33 | I | output_file: /data/gyy/lmquant-main/lmquant/data/data_without_preprocess_llama_7b_bs16/shard/val_teacher_output_layer_0/26.pt
25-01-23 07:59:33 | I | In layer model.layers.0, gradient of model.layers.0.self_attn.q_proj has nan: False
25-01-23 07:59:33 | I | In layer model.layers.0, gradient of model.layers.0.self_attn.q_proj has inf: False
25-01-23 07:59:33 | I | In layer model.layers.0, gradient of model.layers.0.self_attn.k_proj has nan: False
25-01-23 07:59:33 | I | In layer model.layers.0, gradient of model.layers.0.self_attn.k_proj has inf: False
25-01-23 07:59:33 | I | In layer model.layers.0, gradient of model.layers.0.self_attn.v_proj has nan: False
25-01-23 07:59:33 | I | In layer model.layers.0, gradient of model.layers.0.self_attn.v_proj has inf: False
25-01-23 07:59:33 | I | In layer model.layers.0, gradient of model.layers.0.self_attn.o_proj has nan: False
25-01-23 07:59:33 | I | In layer model.layers.0, gradient of model.layers.0.self_attn.o_proj has inf: False
25-01-23 07:59:33 | I | In layer model.layers.0, gradient of model.layers.0.mlp.up_proj has nan: False
25-01-23 07:59:33 | I | In layer model.layers.0, gradient of model.layers.0.mlp.up_proj has inf: False
25-01-23 07:59:33 | I | In layer model.layers.0, gradient of model.layers.0.mlp.gate_proj has nan: False
25-01-23 07:59:33 | I | In layer model.layers.0, gradient of model.layers.0.mlp.gate_proj has inf: False
25-01-23 07:59:33 | I | In layer model.layers.0, gradient of model.layers.0.mlp.down_proj has nan: False
25-01-23 07:59:33 | I | In layer model.layers.0, gradient of model.layers.0.mlp.down_proj has inf: False
25-01-23 07:59:33 | I | [22] done with optimizer step
25-01-23 07:59:33 | I | epoch 001:     27 / 819200000 loss=1.25752e-05, loss_per_token=0.051508, loss_sum=421.954, wps=227.8, ups=0.03, wpb=8192, bsz=16, num_updates=23, lr=0.0023, gnorm=0.233, clip=0, loss_scale=8, train_wall=36, cuda_gb_allocated=17.1, cuda_gb_reserved=18.3, cuda_gb_free=6.6, wall=2121
25-01-23 07:59:34 | I | TRAIN CURRENT LAYER_IDX = 0
25-01-23 07:59:34 | I | in layer model.layers.0
25-01-23 07:59:34 | I | quantizing weights for layer model.layers.0
25-01-23 07:59:34 | I | collecting calibration activations in model.layers.0
25-01-23 07:59:34 | I | collecting calibration activations in model.layers.0
25-01-23 07:59:34 | I | - Quantizing decoder layer model.layers.0
25-01-23 07:59:34 | I |   - Calibrating model.layers.0.self_attn.q_proj.weight
25-01-23 07:59:35 | I |       - range scale = [    1.0000]
25-01-23 07:59:35 | I |         sum  error  = [    0.0495]
25-01-23 07:59:35 | I |         best error  = [    0.0495]
25-01-23 07:59:35 | I |     + error = [0.0495]
25-01-23 07:59:36 | I |       - range scale = [    1.0000]
25-01-23 07:59:36 | I |         sum  error  = [    0.7686]
25-01-23 07:59:36 | I |         best error  = [    0.7686]
25-01-23 07:59:36 | I |     + error = [0.7686]
25-01-23 07:59:36 | I |   - Calibrating model.layers.0.self_attn.k_proj.weight
25-01-23 07:59:37 | I |       - range scale = [    1.0000]
25-01-23 07:59:37 | I |         sum  error  = [    0.0644]
25-01-23 07:59:37 | I |         best error  = [    0.0644]
25-01-23 07:59:37 | I |     + error = [0.0644]
25-01-23 07:59:37 | I |       - range scale = [    1.0000]
25-01-23 07:59:37 | I |         sum  error  = [    1.0385]
25-01-23 07:59:37 | I |         best error  = [    1.0385]
25-01-23 07:59:37 | I |     + error = [1.0385]
25-01-23 07:59:37 | I |   - Calibrating model.layers.0.self_attn.v_proj.weight
25-01-23 07:59:38 | I |       - range scale = [    1.0000]
25-01-23 07:59:38 | I |         sum  error  = [    0.7198]
25-01-23 07:59:38 | I |         best error  = [    0.7198]
25-01-23 07:59:38 | I |     + error = [0.7198]
25-01-23 07:59:39 | I |       - range scale = [    1.0000]
25-01-23 07:59:39 | I |         sum  error  = [    6.6743]
25-01-23 07:59:39 | I |         best error  = [    6.6743]
25-01-23 07:59:39 | I |     + error = [6.6743]
25-01-23 07:59:39 | I |   - Calibrating model.layers.0.self_attn.o_proj.weight
25-01-23 07:59:40 | I |       - range scale = [    1.0000]
25-01-23 07:59:40 | I |         sum  error  = [    0.2033]
25-01-23 07:59:40 | I |         best error  = [    0.2033]
25-01-23 07:59:40 | I |     + error = [0.2033]
25-01-23 07:59:41 | I |       - range scale = [    1.0000]
25-01-23 07:59:41 | I |         sum  error  = [    1.5989]
25-01-23 07:59:41 | I |         best error  = [    1.5989]
25-01-23 07:59:41 | I |     + error = [1.5989]
25-01-23 07:59:41 | I |   - Calibrating model.layers.0.mlp.up_proj.weight
25-01-23 07:59:42 | I |       - range scale = [    1.0000]
25-01-23 07:59:42 | I |         sum  error  = [    1.0904]
25-01-23 07:59:42 | I |         best error  = [    1.0904]
25-01-23 07:59:42 | I |     + error = [1.0904]
25-01-23 07:59:43 | I |       - range scale = [    1.0000]
25-01-23 07:59:43 | I |         sum  error  = [   11.5634]
25-01-23 07:59:43 | I |         best error  = [   11.5634]
25-01-23 07:59:43 | I |     + error = [11.5634]
25-01-23 07:59:43 | I |   - Calibrating model.layers.0.mlp.gate_proj.weight
25-01-23 07:59:44 | I |       - range scale = [    1.0000]
25-01-23 07:59:44 | I |         sum  error  = [    1.1502]
25-01-23 07:59:44 | I |         best error  = [    1.1502]
25-01-23 07:59:44 | I |     + error = [1.1502]
25-01-23 07:59:45 | I |       - range scale = [    1.0000]
25-01-23 07:59:45 | I |         sum  error  = [   11.9549]
25-01-23 07:59:45 | I |         best error  = [   11.9549]
25-01-23 07:59:45 | I |     + error = [11.9549]
25-01-23 07:59:45 | I |   - Calibrating model.layers.0.mlp.down_proj.weight
25-01-23 07:59:46 | I |       - range scale = [    1.0000]
25-01-23 07:59:46 | I |         sum  error  = [    0.1612]
25-01-23 07:59:46 | I |         best error  = [    0.1612]
25-01-23 07:59:46 | I |     + error = [0.1612]
25-01-23 07:59:47 | I |       - range scale = [    1.0000]
25-01-23 07:59:47 | I |         sum  error  = [    1.5678]
25-01-23 07:59:47 | I |         best error  = [    1.5678]
25-01-23 07:59:47 | I |     + error = [1.5678]
25-01-23 07:59:47 | I |   - Quantizing model.layers.0.self_attn.q_proj.weight
25-01-23 07:59:49 | I |   - Quantizing model.layers.0.self_attn.k_proj.weight
25-01-23 07:59:52 | I |   - Quantizing model.layers.0.self_attn.v_proj.weight
25-01-23 07:59:54 | I |   - Quantizing model.layers.0.self_attn.o_proj.weight
25-01-23 07:59:56 | I |   - Quantizing model.layers.0.mlp.up_proj.weight
25-01-23 07:59:58 | I |   - Quantizing model.layers.0.mlp.gate_proj.weight
25-01-23 08:00:01 | I |   - Quantizing model.layers.0.mlp.down_proj.weight
25-01-23 08:00:06 | I | quantizing activations for layer model.layers.0
25-01-23 08:00:07 | I | collecting calibration activations in model.layers.0
25-01-23 08:00:07 | I | collecting calibration activations in model.layers.0
25-01-23 08:00:09 | I | forward this layer
25-01-23 08:00:09 | I | input_file: /data/gyy/lmquant-main/lmquant/data/data_without_preprocess_llama_7b_bs16/shard/val_input_args_layer_0/27.pt
25-01-23 08:00:09 | I | output_file: /data/gyy/lmquant-main/lmquant/data/data_without_preprocess_llama_7b_bs16/shard/val_teacher_output_layer_0/27.pt
25-01-23 08:00:09 | I | In layer model.layers.0, gradient of model.layers.0.self_attn.q_proj has nan: False
25-01-23 08:00:09 | I | In layer model.layers.0, gradient of model.layers.0.self_attn.q_proj has inf: False
25-01-23 08:00:09 | I | In layer model.layers.0, gradient of model.layers.0.self_attn.k_proj has nan: False
25-01-23 08:00:09 | I | In layer model.layers.0, gradient of model.layers.0.self_attn.k_proj has inf: False
25-01-23 08:00:09 | I | In layer model.layers.0, gradient of model.layers.0.self_attn.v_proj has nan: False
25-01-23 08:00:09 | I | In layer model.layers.0, gradient of model.layers.0.self_attn.v_proj has inf: False
25-01-23 08:00:09 | I | In layer model.layers.0, gradient of model.layers.0.self_attn.o_proj has nan: False
25-01-23 08:00:09 | I | In layer model.layers.0, gradient of model.layers.0.self_attn.o_proj has inf: False
25-01-23 08:00:09 | I | In layer model.layers.0, gradient of model.layers.0.mlp.up_proj has nan: False
25-01-23 08:00:09 | I | In layer model.layers.0, gradient of model.layers.0.mlp.up_proj has inf: False
25-01-23 08:00:09 | I | In layer model.layers.0, gradient of model.layers.0.mlp.gate_proj has nan: False
25-01-23 08:00:09 | I | In layer model.layers.0, gradient of model.layers.0.mlp.gate_proj has inf: False
25-01-23 08:00:09 | I | In layer model.layers.0, gradient of model.layers.0.mlp.down_proj has nan: False
25-01-23 08:00:09 | I | In layer model.layers.0, gradient of model.layers.0.mlp.down_proj has inf: False
25-01-23 08:00:09 | I | [23] done with optimizer step
25-01-23 08:00:09 | I | epoch 001:     28 / 819200000 loss=1.2296e-05, loss_per_token=0.0503646, loss_sum=412.587, wps=229.5, ups=0.03, wpb=8192, bsz=16, num_updates=24, lr=0.0024, gnorm=0.179, clip=0, loss_scale=8, train_wall=36, cuda_gb_allocated=17.1, cuda_gb_reserved=18.3, cuda_gb_free=6.6, wall=2157
25-01-23 08:00:09 | I | TRAIN CURRENT LAYER_IDX = 0
25-01-23 08:00:09 | I | in layer model.layers.0
25-01-23 08:00:09 | I | quantizing weights for layer model.layers.0
25-01-23 08:00:10 | I | collecting calibration activations in model.layers.0
25-01-23 08:00:10 | I | collecting calibration activations in model.layers.0
25-01-23 08:00:10 | I | - Quantizing decoder layer model.layers.0
25-01-23 08:00:10 | I |   - Calibrating model.layers.0.self_attn.q_proj.weight
25-01-23 08:00:11 | I |       - range scale = [    1.0000]
25-01-23 08:00:11 | I |         sum  error  = [    0.0512]
25-01-23 08:00:11 | I |         best error  = [    0.0512]
25-01-23 08:00:11 | I |     + error = [0.0512]
25-01-23 08:00:11 | I |       - range scale = [    1.0000]
25-01-23 08:00:11 | I |         sum  error  = [    0.7989]
25-01-23 08:00:11 | I |         best error  = [    0.7989]
25-01-23 08:00:11 | I |     + error = [0.7989]
25-01-23 08:00:12 | I |   - Calibrating model.layers.0.self_attn.k_proj.weight
25-01-23 08:00:12 | I |       - range scale = [    1.0000]
25-01-23 08:00:12 | I |         sum  error  = [    0.0684]
25-01-23 08:00:12 | I |         best error  = [    0.0684]
25-01-23 08:00:12 | I |     + error = [0.0684]
25-01-23 08:00:13 | I |       - range scale = [    1.0000]
25-01-23 08:00:13 | I |         sum  error  = [    1.0288]
25-01-23 08:00:13 | I |         best error  = [    1.0288]
25-01-23 08:00:13 | I |     + error = [1.0288]
25-01-23 08:00:13 | I |   - Calibrating model.layers.0.self_attn.v_proj.weight
25-01-23 08:00:14 | I |       - range scale = [    1.0000]
25-01-23 08:00:14 | I |         sum  error  = [    0.7260]
25-01-23 08:00:14 | I |         best error  = [    0.7260]
25-01-23 08:00:14 | I |     + error = [0.7260]
25-01-23 08:00:15 | I |       - range scale = [    1.0000]
25-01-23 08:00:15 | I |         sum  error  = [    6.7456]
25-01-23 08:00:15 | I |         best error  = [    6.7456]
25-01-23 08:00:15 | I |     + error = [6.7456]
25-01-23 08:00:15 | I |   - Calibrating model.layers.0.self_attn.o_proj.weight
25-01-23 08:00:16 | I |       - range scale = [    1.0000]
25-01-23 08:00:16 | I |         sum  error  = [    0.2070]
25-01-23 08:00:16 | I |         best error  = [    0.2070]
25-01-23 08:00:16 | I |     + error = [0.2070]
25-01-23 08:00:16 | I |       - range scale = [    1.0000]
25-01-23 08:00:16 | I |         sum  error  = [    1.6204]
25-01-23 08:00:16 | I |         best error  = [    1.6204]
25-01-23 08:00:16 | I |     + error = [1.6204]
25-01-23 08:00:17 | I |   - Calibrating model.layers.0.mlp.up_proj.weight
25-01-23 08:00:17 | I |       - range scale = [    1.0000]
25-01-23 08:00:17 | I |         sum  error  = [    1.0873]
25-01-23 08:00:17 | I |         best error  = [    1.0873]
25-01-23 08:00:17 | I |     + error = [1.0873]
25-01-23 08:00:19 | I |       - range scale = [    1.0000]
25-01-23 08:00:19 | I |         sum  error  = [   11.5211]
25-01-23 08:00:19 | I |         best error  = [   11.5211]
25-01-23 08:00:19 | I |     + error = [11.5211]
25-01-23 08:00:19 | I |   - Calibrating model.layers.0.mlp.gate_proj.weight
25-01-23 08:00:19 | I |       - range scale = [    1.0000]
25-01-23 08:00:19 | I |         sum  error  = [    1.1466]
25-01-23 08:00:19 | I |         best error  = [    1.1466]
25-01-23 08:00:19 | I |     + error = [1.1466]
25-01-23 08:00:21 | I |       - range scale = [    1.0000]
25-01-23 08:00:21 | I |         sum  error  = [   11.9089]
25-01-23 08:00:21 | I |         best error  = [   11.9089]
25-01-23 08:00:21 | I |     + error = [11.9089]
25-01-23 08:00:21 | I |   - Calibrating model.layers.0.mlp.down_proj.weight
25-01-23 08:00:22 | I |       - range scale = [    1.0000]
25-01-23 08:00:22 | I |         sum  error  = [    0.1654]
25-01-23 08:00:22 | I |         best error  = [    0.1654]
25-01-23 08:00:22 | I |     + error = [0.1654]
25-01-23 08:00:23 | I |       - range scale = [    1.0000]
25-01-23 08:00:23 | I |         sum  error  = [    1.6062]
25-01-23 08:00:23 | I |         best error  = [    1.6062]
25-01-23 08:00:23 | I |     + error = [1.6062]
25-01-23 08:00:23 | I |   - Quantizing model.layers.0.self_attn.q_proj.weight
25-01-23 08:00:25 | I |   - Quantizing model.layers.0.self_attn.k_proj.weight
25-01-23 08:00:27 | I |   - Quantizing model.layers.0.self_attn.v_proj.weight
25-01-23 08:00:30 | I |   - Quantizing model.layers.0.self_attn.o_proj.weight
25-01-23 08:00:32 | I |   - Quantizing model.layers.0.mlp.up_proj.weight
25-01-23 08:00:34 | I |   - Quantizing model.layers.0.mlp.gate_proj.weight
25-01-23 08:00:37 | I |   - Quantizing model.layers.0.mlp.down_proj.weight
25-01-23 08:00:42 | I | quantizing activations for layer model.layers.0
25-01-23 08:00:43 | I | collecting calibration activations in model.layers.0
25-01-23 08:00:43 | I | collecting calibration activations in model.layers.0
25-01-23 08:00:45 | I | forward this layer
25-01-23 08:00:45 | I | input_file: /data/gyy/lmquant-main/lmquant/data/data_without_preprocess_llama_7b_bs16/shard/val_input_args_layer_0/28.pt
25-01-23 08:00:45 | I | output_file: /data/gyy/lmquant-main/lmquant/data/data_without_preprocess_llama_7b_bs16/shard/val_teacher_output_layer_0/28.pt
25-01-23 08:00:45 | I | In layer model.layers.0, gradient of model.layers.0.self_attn.q_proj has nan: False
25-01-23 08:00:45 | I | In layer model.layers.0, gradient of model.layers.0.self_attn.q_proj has inf: False
25-01-23 08:00:45 | I | In layer model.layers.0, gradient of model.layers.0.self_attn.k_proj has nan: False
25-01-23 08:00:45 | I | In layer model.layers.0, gradient of model.layers.0.self_attn.k_proj has inf: False
25-01-23 08:00:45 | I | In layer model.layers.0, gradient of model.layers.0.self_attn.v_proj has nan: False
25-01-23 08:00:45 | I | In layer model.layers.0, gradient of model.layers.0.self_attn.v_proj has inf: False
25-01-23 08:00:45 | I | In layer model.layers.0, gradient of model.layers.0.self_attn.o_proj has nan: False
25-01-23 08:00:45 | I | In layer model.layers.0, gradient of model.layers.0.self_attn.o_proj has inf: False
25-01-23 08:00:45 | I | In layer model.layers.0, gradient of model.layers.0.mlp.up_proj has nan: False
25-01-23 08:00:45 | I | In layer model.layers.0, gradient of model.layers.0.mlp.up_proj has inf: False
25-01-23 08:00:45 | I | In layer model.layers.0, gradient of model.layers.0.mlp.gate_proj has nan: False
25-01-23 08:00:45 | I | In layer model.layers.0, gradient of model.layers.0.mlp.gate_proj has inf: False
25-01-23 08:00:45 | I | In layer model.layers.0, gradient of model.layers.0.mlp.down_proj has nan: False
25-01-23 08:00:45 | I | In layer model.layers.0, gradient of model.layers.0.mlp.down_proj has inf: False
25-01-23 08:00:45 | I | [24] done with optimizer step
25-01-23 08:00:45 | I | epoch 001:     29 / 819200000 loss=1.2239e-05, loss_per_token=0.0501308, loss_sum=410.671, wps=227.5, ups=0.03, wpb=8192, bsz=16, num_updates=25, lr=0.0025, gnorm=0.194, clip=0, loss_scale=8, train_wall=36, cuda_gb_allocated=17.1, cuda_gb_reserved=18.4, cuda_gb_free=6.6, wall=2193
25-01-23 08:00:45 | I | TRAIN CURRENT LAYER_IDX = 0
25-01-23 08:00:45 | I | in layer model.layers.0
25-01-23 08:00:45 | I | quantizing weights for layer model.layers.0
25-01-23 08:00:46 | I | collecting calibration activations in model.layers.0
25-01-23 08:00:46 | I | collecting calibration activations in model.layers.0
25-01-23 08:00:46 | I | - Quantizing decoder layer model.layers.0
25-01-23 08:00:46 | I |   - Calibrating model.layers.0.self_attn.q_proj.weight
25-01-23 08:00:47 | I |       - range scale = [    1.0000]
25-01-23 08:00:47 | I |         sum  error  = [    0.0511]
25-01-23 08:00:47 | I |         best error  = [    0.0511]
25-01-23 08:00:47 | I |     + error = [0.0511]
25-01-23 08:00:47 | I |       - range scale = [    1.0000]
25-01-23 08:00:47 | I |         sum  error  = [    0.7814]
25-01-23 08:00:47 | I |         best error  = [    0.7814]
25-01-23 08:00:47 | I |     + error = [0.7814]
25-01-23 08:00:48 | I |   - Calibrating model.layers.0.self_attn.k_proj.weight
25-01-23 08:00:48 | I |       - range scale = [    1.0000]
25-01-23 08:00:48 | I |         sum  error  = [    0.0675]
25-01-23 08:00:48 | I |         best error  = [    0.0675]
25-01-23 08:00:48 | I |     + error = [0.0675]
25-01-23 08:00:49 | I |       - range scale = [    1.0000]
25-01-23 08:00:49 | I |         sum  error  = [    1.0239]
25-01-23 08:00:49 | I |         best error  = [    1.0239]
25-01-23 08:00:49 | I |     + error = [1.0239]
25-01-23 08:00:49 | I |   - Calibrating model.layers.0.self_attn.v_proj.weight
25-01-23 08:00:50 | I |       - range scale = [    1.0000]
25-01-23 08:00:50 | I |         sum  error  = [    0.7271]
25-01-23 08:00:50 | I |         best error  = [    0.7271]
25-01-23 08:00:50 | I |     + error = [0.7271]
25-01-23 08:00:51 | I |       - range scale = [    1.0000]
25-01-23 08:00:51 | I |         sum  error  = [    6.7889]
25-01-23 08:00:51 | I |         best error  = [    6.7889]
25-01-23 08:00:51 | I |     + error = [6.7889]
25-01-23 08:00:51 | I |   - Calibrating model.layers.0.self_attn.o_proj.weight
25-01-23 08:00:52 | I |       - range scale = [    1.0000]
25-01-23 08:00:52 | I |         sum  error  = [    0.2078]
25-01-23 08:00:52 | I |         best error  = [    0.2078]
25-01-23 08:00:52 | I |     + error = [0.2078]
25-01-23 08:00:52 | I |       - range scale = [    1.0000]
25-01-23 08:00:52 | I |         sum  error  = [    1.6291]
25-01-23 08:00:52 | I |         best error  = [    1.6291]
25-01-23 08:00:52 | I |     + error = [1.6291]
25-01-23 08:00:53 | I |   - Calibrating model.layers.0.mlp.up_proj.weight
25-01-23 08:00:53 | I |       - range scale = [    1.0000]
25-01-23 08:00:53 | I |         sum  error  = [    1.0968]
25-01-23 08:00:53 | I |         best error  = [    1.0968]
25-01-23 08:00:53 | I |     + error = [1.0968]
25-01-23 08:00:54 | I |       - range scale = [    1.0000]
25-01-23 08:00:54 | I |         sum  error  = [   11.6412]
25-01-23 08:00:54 | I |         best error  = [   11.6412]
25-01-23 08:00:54 | I |     + error = [11.6412]
25-01-23 08:00:55 | I |   - Calibrating model.layers.0.mlp.gate_proj.weight
25-01-23 08:00:55 | I |       - range scale = [    1.0000]
25-01-23 08:00:55 | I |         sum  error  = [    1.1586]
25-01-23 08:00:55 | I |         best error  = [    1.1586]
25-01-23 08:00:55 | I |     + error = [1.1586]
25-01-23 08:00:57 | I |       - range scale = [    1.0000]
25-01-23 08:00:57 | I |         sum  error  = [   12.0274]
25-01-23 08:00:57 | I |         best error  = [   12.0274]
25-01-23 08:00:57 | I |     + error = [12.0274]
25-01-23 08:00:57 | I |   - Calibrating model.layers.0.mlp.down_proj.weight
25-01-23 08:00:57 | I |       - range scale = [    1.0000]
25-01-23 08:00:57 | I |         sum  error  = [    0.1678]
25-01-23 08:00:57 | I |         best error  = [    0.1678]
25-01-23 08:00:57 | I |     + error = [0.1678]
25-01-23 08:00:59 | I |       - range scale = [    1.0000]
25-01-23 08:00:59 | I |         sum  error  = [    1.6348]
25-01-23 08:00:59 | I |         best error  = [    1.6348]
25-01-23 08:00:59 | I |     + error = [1.6348]
25-01-23 08:00:59 | I |   - Quantizing model.layers.0.self_attn.q_proj.weight
25-01-23 08:01:01 | I |   - Quantizing model.layers.0.self_attn.k_proj.weight
25-01-23 08:01:03 | I |   - Quantizing model.layers.0.self_attn.v_proj.weight
25-01-23 08:01:06 | I |   - Quantizing model.layers.0.self_attn.o_proj.weight
25-01-23 08:01:08 | I |   - Quantizing model.layers.0.mlp.up_proj.weight
25-01-23 08:01:10 | I |   - Quantizing model.layers.0.mlp.gate_proj.weight
25-01-23 08:01:12 | I |   - Quantizing model.layers.0.mlp.down_proj.weight
25-01-23 08:01:18 | I | quantizing activations for layer model.layers.0
25-01-23 08:01:19 | I | collecting calibration activations in model.layers.0
25-01-23 08:01:19 | I | collecting calibration activations in model.layers.0
25-01-23 08:01:20 | I | forward this layer
25-01-23 08:01:20 | I | input_file: /data/gyy/lmquant-main/lmquant/data/data_without_preprocess_llama_7b_bs16/shard/val_input_args_layer_0/29.pt
25-01-23 08:01:20 | I | output_file: /data/gyy/lmquant-main/lmquant/data/data_without_preprocess_llama_7b_bs16/shard/val_teacher_output_layer_0/29.pt
25-01-23 08:01:21 | I | In layer model.layers.0, gradient of model.layers.0.self_attn.q_proj has nan: False
25-01-23 08:01:21 | I | In layer model.layers.0, gradient of model.layers.0.self_attn.q_proj has inf: False
25-01-23 08:01:21 | I | In layer model.layers.0, gradient of model.layers.0.self_attn.k_proj has nan: False
25-01-23 08:01:21 | I | In layer model.layers.0, gradient of model.layers.0.self_attn.k_proj has inf: False
25-01-23 08:01:21 | I | In layer model.layers.0, gradient of model.layers.0.self_attn.v_proj has nan: False
25-01-23 08:01:21 | I | In layer model.layers.0, gradient of model.layers.0.self_attn.v_proj has inf: False
25-01-23 08:01:21 | I | In layer model.layers.0, gradient of model.layers.0.self_attn.o_proj has nan: False
25-01-23 08:01:21 | I | In layer model.layers.0, gradient of model.layers.0.self_attn.o_proj has inf: False
25-01-23 08:01:21 | I | In layer model.layers.0, gradient of model.layers.0.mlp.up_proj has nan: False
25-01-23 08:01:21 | I | In layer model.layers.0, gradient of model.layers.0.mlp.up_proj has inf: False
25-01-23 08:01:21 | I | In layer model.layers.0, gradient of model.layers.0.mlp.gate_proj has nan: False
25-01-23 08:01:21 | I | In layer model.layers.0, gradient of model.layers.0.mlp.gate_proj has inf: False
25-01-23 08:01:21 | I | In layer model.layers.0, gradient of model.layers.0.mlp.down_proj has nan: False
25-01-23 08:01:21 | I | In layer model.layers.0, gradient of model.layers.0.mlp.down_proj has inf: False
25-01-23 08:01:21 | I | [25] done with optimizer step
25-01-23 08:01:21 | I | epoch 001:     30 / 819200000 loss=1.21309e-05, loss_per_token=0.049688, loss_sum=407.044, wps=229.5, ups=0.03, wpb=8192, bsz=16, num_updates=26, lr=0.0026, gnorm=0.136, clip=0, loss_scale=8, train_wall=36, cuda_gb_allocated=17.1, cuda_gb_reserved=18.3, cuda_gb_free=6.6, wall=2228
25-01-23 08:01:21 | I | TRAIN CURRENT LAYER_IDX = 0
25-01-23 08:01:21 | I | in layer model.layers.0
25-01-23 08:01:21 | I | quantizing weights for layer model.layers.0
25-01-23 08:01:21 | I | collecting calibration activations in model.layers.0
25-01-23 08:01:21 | I | collecting calibration activations in model.layers.0
25-01-23 08:01:22 | I | - Quantizing decoder layer model.layers.0
25-01-23 08:01:22 | I |   - Calibrating model.layers.0.self_attn.q_proj.weight
25-01-23 08:01:22 | I |       - range scale = [    1.0000]
25-01-23 08:01:22 | I |         sum  error  = [    0.0467]
25-01-23 08:01:22 | I |         best error  = [    0.0467]
25-01-23 08:01:22 | I |     + error = [0.0467]
25-01-23 08:01:23 | I |       - range scale = [    1.0000]
25-01-23 08:01:23 | I |         sum  error  = [    0.7324]
25-01-23 08:01:23 | I |         best error  = [    0.7324]
25-01-23 08:01:23 | I |     + error = [0.7324]
25-01-23 08:01:23 | I |   - Calibrating model.layers.0.self_attn.k_proj.weight
25-01-23 08:01:24 | I |       - range scale = [    1.0000]
25-01-23 08:01:24 | I |         sum  error  = [    0.0597]
25-01-23 08:01:24 | I |         best error  = [    0.0597]
25-01-23 08:01:24 | I |     + error = [0.0597]
25-01-23 08:01:25 | I |       - range scale = [    1.0000]
25-01-23 08:01:25 | I |         sum  error  = [    1.0070]
25-01-23 08:01:25 | I |         best error  = [    1.0070]
25-01-23 08:01:25 | I |     + error = [1.0070]
25-01-23 08:01:25 | I |   - Calibrating model.layers.0.self_attn.v_proj.weight
25-01-23 08:01:26 | I |       - range scale = [    1.0000]
25-01-23 08:01:26 | I |         sum  error  = [    0.7142]
25-01-23 08:01:26 | I |         best error  = [    0.7142]
25-01-23 08:01:26 | I |     + error = [0.7142]
25-01-23 08:01:26 | I |       - range scale = [    1.0000]
25-01-23 08:01:26 | I |         sum  error  = [    6.6227]
25-01-23 08:01:26 | I |         best error  = [    6.6227]
25-01-23 08:01:26 | I |     + error = [6.6227]
25-01-23 08:01:27 | I |   - Calibrating model.layers.0.self_attn.o_proj.weight
25-01-23 08:01:27 | I |       - range scale = [    1.0000]
25-01-23 08:01:27 | I |         sum  error  = [    0.2023]
25-01-23 08:01:27 | I |         best error  = [    0.2023]
25-01-23 08:01:27 | I |     + error = [0.2023]
25-01-23 08:01:28 | I |       - range scale = [    1.0000]
25-01-23 08:01:28 | I |         sum  error  = [    1.6011]
25-01-23 08:01:28 | I |         best error  = [    1.6011]
25-01-23 08:01:28 | I |     + error = [1.6011]
25-01-23 08:01:28 | I |   - Calibrating model.layers.0.mlp.up_proj.weight
25-01-23 08:01:29 | I |       - range scale = [    1.0000]
25-01-23 08:01:29 | I |         sum  error  = [    1.0802]
25-01-23 08:01:29 | I |         best error  = [    1.0802]
25-01-23 08:01:29 | I |     + error = [1.0802]
25-01-23 08:01:30 | I |       - range scale = [    1.0000]
25-01-23 08:01:30 | I |         sum  error  = [   11.4588]
25-01-23 08:01:30 | I |         best error  = [   11.4588]
25-01-23 08:01:30 | I |     + error = [11.4588]
25-01-23 08:01:30 | I |   - Calibrating model.layers.0.mlp.gate_proj.weight
25-01-23 08:01:31 | I |       - range scale = [    1.0000]
25-01-23 08:01:31 | I |         sum  error  = [    1.1389]
25-01-23 08:01:31 | I |         best error  = [    1.1389]
25-01-23 08:01:31 | I |     + error = [1.1389]
25-01-23 08:01:32 | I |       - range scale = [    1.0000]
25-01-23 08:01:32 | I |         sum  error  = [   11.8390]
25-01-23 08:01:32 | I |         best error  = [   11.8390]
25-01-23 08:01:32 | I |     + error = [11.8390]
25-01-23 08:01:32 | I |   - Calibrating model.layers.0.mlp.down_proj.weight
25-01-23 08:01:33 | I |       - range scale = [    1.0000]
25-01-23 08:01:33 | I |         sum  error  = [    0.1601]
25-01-23 08:01:33 | I |         best error  = [    0.1601]
25-01-23 08:01:33 | I |     + error = [0.1601]
25-01-23 08:01:34 | I |       - range scale = [    1.0000]
25-01-23 08:01:34 | I |         sum  error  = [    1.5581]
25-01-23 08:01:34 | I |         best error  = [    1.5581]
25-01-23 08:01:34 | I |     + error = [1.5581]
25-01-23 08:01:35 | I |   - Quantizing model.layers.0.self_attn.q_proj.weight
25-01-23 08:01:37 | I |   - Quantizing model.layers.0.self_attn.k_proj.weight
25-01-23 08:01:39 | I |   - Quantizing model.layers.0.self_attn.v_proj.weight
25-01-23 08:01:41 | I |   - Quantizing model.layers.0.self_attn.o_proj.weight
25-01-23 08:01:44 | I |   - Quantizing model.layers.0.mlp.up_proj.weight
25-01-23 08:01:46 | I |   - Quantizing model.layers.0.mlp.gate_proj.weight
25-01-23 08:01:48 | I |   - Quantizing model.layers.0.mlp.down_proj.weight
25-01-23 08:01:54 | I | quantizing activations for layer model.layers.0
25-01-23 08:01:54 | I | collecting calibration activations in model.layers.0
25-01-23 08:01:54 | I | collecting calibration activations in model.layers.0
25-01-23 08:01:56 | I | forward this layer
25-01-23 08:01:56 | I | input_file: /data/gyy/lmquant-main/lmquant/data/data_without_preprocess_llama_7b_bs16/shard/val_input_args_layer_0/30.pt
25-01-23 08:01:56 | I | output_file: /data/gyy/lmquant-main/lmquant/data/data_without_preprocess_llama_7b_bs16/shard/val_teacher_output_layer_0/30.pt
25-01-23 08:01:57 | I | In layer model.layers.0, gradient of model.layers.0.self_attn.q_proj has nan: False
25-01-23 08:01:57 | I | In layer model.layers.0, gradient of model.layers.0.self_attn.q_proj has inf: False
25-01-23 08:01:57 | I | In layer model.layers.0, gradient of model.layers.0.self_attn.k_proj has nan: False
25-01-23 08:01:57 | I | In layer model.layers.0, gradient of model.layers.0.self_attn.k_proj has inf: False
25-01-23 08:01:57 | I | In layer model.layers.0, gradient of model.layers.0.self_attn.v_proj has nan: False
25-01-23 08:01:57 | I | In layer model.layers.0, gradient of model.layers.0.self_attn.v_proj has inf: False
25-01-23 08:01:57 | I | In layer model.layers.0, gradient of model.layers.0.self_attn.o_proj has nan: False
25-01-23 08:01:57 | I | In layer model.layers.0, gradient of model.layers.0.self_attn.o_proj has inf: False
25-01-23 08:01:57 | I | In layer model.layers.0, gradient of model.layers.0.mlp.up_proj has nan: False
25-01-23 08:01:57 | I | In layer model.layers.0, gradient of model.layers.0.mlp.up_proj has inf: False
25-01-23 08:01:57 | I | In layer model.layers.0, gradient of model.layers.0.mlp.gate_proj has nan: False
25-01-23 08:01:57 | I | In layer model.layers.0, gradient of model.layers.0.mlp.gate_proj has inf: False
25-01-23 08:01:57 | I | In layer model.layers.0, gradient of model.layers.0.mlp.down_proj has nan: False
25-01-23 08:01:57 | I | In layer model.layers.0, gradient of model.layers.0.mlp.down_proj has inf: False
25-01-23 08:01:57 | I | [26] done with optimizer step
25-01-23 08:01:57 | I | epoch 001:     31 / 819200000 loss=1.14338e-05, loss_per_token=0.0468328, loss_sum=383.654, wps=228.6, ups=0.03, wpb=8192, bsz=16, num_updates=27, lr=0.0027, gnorm=0.158, clip=0, loss_scale=8, train_wall=36, cuda_gb_allocated=17.1, cuda_gb_reserved=18.3, cuda_gb_free=6.6, wall=2264
25-01-23 08:01:57 | I | TRAIN CURRENT LAYER_IDX = 0
25-01-23 08:01:57 | I | in layer model.layers.0
25-01-23 08:01:57 | I | quantizing weights for layer model.layers.0
25-01-23 08:01:57 | I | collecting calibration activations in model.layers.0
25-01-23 08:01:57 | I | collecting calibration activations in model.layers.0
25-01-23 08:01:58 | I | - Quantizing decoder layer model.layers.0
25-01-23 08:01:58 | I |   - Calibrating model.layers.0.self_attn.q_proj.weight
25-01-23 08:01:58 | I |       - range scale = [    1.0000]
25-01-23 08:01:58 | I |         sum  error  = [    0.0485]
25-01-23 08:01:58 | I |         best error  = [    0.0485]
25-01-23 08:01:58 | I |     + error = [0.0485]
25-01-23 08:01:59 | I |       - range scale = [    1.0000]
25-01-23 08:01:59 | I |         sum  error  = [    0.7611]
25-01-23 08:01:59 | I |         best error  = [    0.7611]
25-01-23 08:01:59 | I |     + error = [0.7611]
25-01-23 08:01:59 | I |   - Calibrating model.layers.0.self_attn.k_proj.weight
25-01-23 08:02:00 | I |       - range scale = [    1.0000]
25-01-23 08:02:00 | I |         sum  error  = [    0.0641]
25-01-23 08:02:00 | I |         best error  = [    0.0641]
25-01-23 08:02:00 | I |     + error = [0.0641]
25-01-23 08:02:01 | I |       - range scale = [    1.0000]
25-01-23 08:02:01 | I |         sum  error  = [    0.9681]
25-01-23 08:02:01 | I |         best error  = [    0.9681]
25-01-23 08:02:01 | I |     + error = [0.9681]
25-01-23 08:02:01 | I |   - Calibrating model.layers.0.self_attn.v_proj.weight
25-01-23 08:02:01 | I |       - range scale = [    1.0000]
25-01-23 08:02:01 | I |         sum  error  = [    0.7132]
25-01-23 08:02:01 | I |         best error  = [    0.7132]
25-01-23 08:02:01 | I |     + error = [0.7132]
25-01-23 08:02:02 | I |       - range scale = [    1.0000]
25-01-23 08:02:02 | I |         sum  error  = [    6.6546]
25-01-23 08:02:02 | I |         best error  = [    6.6546]
25-01-23 08:02:02 | I |     + error = [6.6546]
25-01-23 08:02:03 | I |   - Calibrating model.layers.0.self_attn.o_proj.weight
25-01-23 08:02:03 | I |       - range scale = [    1.0000]
25-01-23 08:02:03 | I |         sum  error  = [    0.1994]
25-01-23 08:02:03 | I |         best error  = [    0.1994]
25-01-23 08:02:03 | I |     + error = [0.1994]
25-01-23 08:02:04 | I |       - range scale = [    1.0000]
25-01-23 08:02:04 | I |         sum  error  = [    1.5663]
25-01-23 08:02:04 | I |         best error  = [    1.5663]
25-01-23 08:02:04 | I |     + error = [1.5663]
25-01-23 08:02:04 | I |   - Calibrating model.layers.0.mlp.up_proj.weight
25-01-23 08:02:05 | I |       - range scale = [    1.0000]
25-01-23 08:02:05 | I |         sum  error  = [    1.0804]
25-01-23 08:02:05 | I |         best error  = [    1.0804]
25-01-23 08:02:05 | I |     + error = [1.0804]
25-01-23 08:02:06 | I |       - range scale = [    1.0000]
25-01-23 08:02:06 | I |         sum  error  = [   11.4675]
25-01-23 08:02:06 | I |         best error  = [   11.4675]
25-01-23 08:02:06 | I |     + error = [11.4675]
25-01-23 08:02:06 | I |   - Calibrating model.layers.0.mlp.gate_proj.weight
25-01-23 08:02:07 | I |       - range scale = [    1.0000]
25-01-23 08:02:07 | I |         sum  error  = [    1.1428]
25-01-23 08:02:07 | I |         best error  = [    1.1428]
25-01-23 08:02:07 | I |     + error = [1.1428]
25-01-23 08:02:08 | I |       - range scale = [    1.0000]
25-01-23 08:02:08 | I |         sum  error  = [   11.8461]
25-01-23 08:02:08 | I |         best error  = [   11.8461]
25-01-23 08:02:08 | I |     + error = [11.8461]
25-01-23 08:02:08 | I |   - Calibrating model.layers.0.mlp.down_proj.weight
25-01-23 08:02:09 | I |       - range scale = [    1.0000]
25-01-23 08:02:09 | I |         sum  error  = [    0.1659]
25-01-23 08:02:09 | I |         best error  = [    0.1659]
25-01-23 08:02:09 | I |     + error = [0.1659]
25-01-23 08:02:10 | I |       - range scale = [    1.0000]
25-01-23 08:02:10 | I |         sum  error  = [    1.6120]
25-01-23 08:02:10 | I |         best error  = [    1.6120]
25-01-23 08:02:10 | I |     + error = [1.6120]
25-01-23 08:02:10 | I |   - Quantizing model.layers.0.self_attn.q_proj.weight
25-01-23 08:02:13 | I |   - Quantizing model.layers.0.self_attn.k_proj.weight
25-01-23 08:02:15 | I |   - Quantizing model.layers.0.self_attn.v_proj.weight
25-01-23 08:02:17 | I |   - Quantizing model.layers.0.self_attn.o_proj.weight
25-01-23 08:02:19 | I |   - Quantizing model.layers.0.mlp.up_proj.weight
25-01-23 08:02:22 | I |   - Quantizing model.layers.0.mlp.gate_proj.weight
25-01-23 08:02:24 | I |   - Quantizing model.layers.0.mlp.down_proj.weight
25-01-23 08:02:30 | I | quantizing activations for layer model.layers.0
25-01-23 08:02:30 | I | collecting calibration activations in model.layers.0
25-01-23 08:02:30 | I | collecting calibration activations in model.layers.0
25-01-23 08:02:32 | I | forward this layer
25-01-23 08:02:32 | I | input_file: /data/gyy/lmquant-main/lmquant/data/data_without_preprocess_llama_7b_bs16/shard/val_input_args_layer_0/31.pt
25-01-23 08:02:32 | I | output_file: /data/gyy/lmquant-main/lmquant/data/data_without_preprocess_llama_7b_bs16/shard/val_teacher_output_layer_0/31.pt
25-01-23 08:02:33 | I | In layer model.layers.0, gradient of model.layers.0.self_attn.q_proj has nan: False
25-01-23 08:02:33 | I | In layer model.layers.0, gradient of model.layers.0.self_attn.q_proj has inf: False
25-01-23 08:02:33 | I | In layer model.layers.0, gradient of model.layers.0.self_attn.k_proj has nan: False
25-01-23 08:02:33 | I | In layer model.layers.0, gradient of model.layers.0.self_attn.k_proj has inf: False
25-01-23 08:02:33 | I | In layer model.layers.0, gradient of model.layers.0.self_attn.v_proj has nan: False
25-01-23 08:02:33 | I | In layer model.layers.0, gradient of model.layers.0.self_attn.v_proj has inf: False
25-01-23 08:02:33 | I | In layer model.layers.0, gradient of model.layers.0.self_attn.o_proj has nan: False
25-01-23 08:02:33 | I | In layer model.layers.0, gradient of model.layers.0.self_attn.o_proj has inf: False
25-01-23 08:02:33 | I | In layer model.layers.0, gradient of model.layers.0.mlp.up_proj has nan: False
25-01-23 08:02:33 | I | In layer model.layers.0, gradient of model.layers.0.mlp.up_proj has inf: False
25-01-23 08:02:33 | I | In layer model.layers.0, gradient of model.layers.0.mlp.gate_proj has nan: False
25-01-23 08:02:33 | I | In layer model.layers.0, gradient of model.layers.0.mlp.gate_proj has inf: False
25-01-23 08:02:33 | I | In layer model.layers.0, gradient of model.layers.0.mlp.down_proj has nan: False
25-01-23 08:02:33 | I | In layer model.layers.0, gradient of model.layers.0.mlp.down_proj has inf: False
25-01-23 08:02:33 | I | [27] done with optimizer step
25-01-23 08:02:33 | I | epoch 001:     32 / 819200000 loss=1.15829e-05, loss_per_token=0.0474434, loss_sum=388.656, wps=228.3, ups=0.03, wpb=8192, bsz=16, num_updates=28, lr=0.0028, gnorm=0.132, clip=0, loss_scale=8, train_wall=36, cuda_gb_allocated=17.1, cuda_gb_reserved=18.3, cuda_gb_free=6.6, wall=2300
25-01-23 08:02:33 | I | TRAIN CURRENT LAYER_IDX = 0
25-01-23 08:02:33 | I | in layer model.layers.0
25-01-23 08:02:33 | I | quantizing weights for layer model.layers.0
25-01-23 08:02:33 | I | collecting calibration activations in model.layers.0
25-01-23 08:02:33 | I | collecting calibration activations in model.layers.0
25-01-23 08:02:33 | I | - Quantizing decoder layer model.layers.0
25-01-23 08:02:33 | I |   - Calibrating model.layers.0.self_attn.q_proj.weight
25-01-23 08:02:34 | I |       - range scale = [    1.0000]
25-01-23 08:02:34 | I |         sum  error  = [    0.0478]
25-01-23 08:02:34 | I |         best error  = [    0.0478]
25-01-23 08:02:34 | I |     + error = [0.0478]
25-01-23 08:02:35 | I |       - range scale = [    1.0000]
25-01-23 08:02:35 | I |         sum  error  = [    0.7442]
25-01-23 08:02:35 | I |         best error  = [    0.7442]
25-01-23 08:02:35 | I |     + error = [0.7442]
25-01-23 08:02:35 | I |   - Calibrating model.layers.0.self_attn.k_proj.weight
25-01-23 08:02:36 | I |       - range scale = [    1.0000]
25-01-23 08:02:36 | I |         sum  error  = [    0.0640]
25-01-23 08:02:36 | I |         best error  = [    0.0640]
25-01-23 08:02:36 | I |     + error = [0.0640]
25-01-23 08:02:36 | I |       - range scale = [    1.0000]
25-01-23 08:02:36 | I |         sum  error  = [    0.9819]
25-01-23 08:02:36 | I |         best error  = [    0.9819]
25-01-23 08:02:36 | I |     + error = [0.9819]
25-01-23 08:02:37 | I |   - Calibrating model.layers.0.self_attn.v_proj.weight
25-01-23 08:02:37 | I |       - range scale = [    1.0000]
25-01-23 08:02:37 | I |         sum  error  = [    0.7285]
25-01-23 08:02:37 | I |         best error  = [    0.7285]
25-01-23 08:02:37 | I |     + error = [0.7285]
25-01-23 08:02:38 | I |       - range scale = [    1.0000]
25-01-23 08:02:38 | I |         sum  error  = [    6.7401]
25-01-23 08:02:38 | I |         best error  = [    6.7401]
25-01-23 08:02:38 | I |     + error = [6.7401]
25-01-23 08:02:38 | I |   - Calibrating model.layers.0.self_attn.o_proj.weight
25-01-23 08:02:39 | I |       - range scale = [    1.0000]
25-01-23 08:02:39 | I |         sum  error  = [    0.1958]
25-01-23 08:02:39 | I |         best error  = [    0.1958]
25-01-23 08:02:39 | I |     + error = [0.1958]
25-01-23 08:02:40 | I |       - range scale = [    1.0000]
25-01-23 08:02:40 | I |         sum  error  = [    1.5421]
25-01-23 08:02:40 | I |         best error  = [    1.5421]
25-01-23 08:02:40 | I |     + error = [1.5421]
25-01-23 08:02:40 | I |   - Calibrating model.layers.0.mlp.up_proj.weight
25-01-23 08:02:41 | I |       - range scale = [    1.0000]
25-01-23 08:02:41 | I |         sum  error  = [    1.0678]
25-01-23 08:02:41 | I |         best error  = [    1.0678]
25-01-23 08:02:41 | I |     + error = [1.0678]
25-01-23 08:02:42 | I |       - range scale = [    1.0000]
25-01-23 08:02:42 | I |         sum  error  = [   11.3397]
25-01-23 08:02:42 | I |         best error  = [   11.3397]
25-01-23 08:02:42 | I |     + error = [11.3397]
25-01-23 08:02:42 | I |   - Calibrating model.layers.0.mlp.gate_proj.weight
25-01-23 08:02:43 | I |       - range scale = [    1.0000]
25-01-23 08:02:43 | I |         sum  error  = [    1.1277]
25-01-23 08:02:43 | I |         best error  = [    1.1277]
25-01-23 08:02:43 | I |     + error = [1.1277]
25-01-23 08:02:44 | I |       - range scale = [    1.0000]
25-01-23 08:02:44 | I |         sum  error  = [   11.7115]
25-01-23 08:02:44 | I |         best error  = [   11.7115]
25-01-23 08:02:44 | I |     + error = [11.7115]
25-01-23 08:02:44 | I |   - Calibrating model.layers.0.mlp.down_proj.weight
25-01-23 08:02:45 | I |       - range scale = [    1.0000]
25-01-23 08:02:45 | I |         sum  error  = [    0.1591]
25-01-23 08:02:45 | I |         best error  = [    0.1591]
25-01-23 08:02:45 | I |     + error = [0.1591]
25-01-23 08:02:46 | I |       - range scale = [    1.0000]
25-01-23 08:02:46 | I |         sum  error  = [    1.5438]
25-01-23 08:02:46 | I |         best error  = [    1.5438]
25-01-23 08:02:46 | I |     + error = [1.5438]
25-01-23 08:02:46 | I |   - Quantizing model.layers.0.self_attn.q_proj.weight
25-01-23 08:02:49 | I |   - Quantizing model.layers.0.self_attn.k_proj.weight
25-01-23 08:02:51 | I |   - Quantizing model.layers.0.self_attn.v_proj.weight
25-01-23 08:02:53 | I |   - Quantizing model.layers.0.self_attn.o_proj.weight
25-01-23 08:02:55 | I |   - Quantizing model.layers.0.mlp.up_proj.weight
25-01-23 08:02:58 | I |   - Quantizing model.layers.0.mlp.gate_proj.weight
25-01-23 08:03:00 | I |   - Quantizing model.layers.0.mlp.down_proj.weight
25-01-23 08:03:06 | I | quantizing activations for layer model.layers.0
25-01-23 08:03:06 | I | collecting calibration activations in model.layers.0
25-01-23 08:03:06 | I | collecting calibration activations in model.layers.0
25-01-23 08:03:08 | I | forward this layer
25-01-23 08:03:08 | I | input_file: /data/gyy/lmquant-main/lmquant/data/data_without_preprocess_llama_7b_bs16/shard/val_input_args_layer_0/32.pt
25-01-23 08:03:08 | I | output_file: /data/gyy/lmquant-main/lmquant/data/data_without_preprocess_llama_7b_bs16/shard/val_teacher_output_layer_0/32.pt
25-01-23 08:03:09 | I | In layer model.layers.0, gradient of model.layers.0.self_attn.q_proj has nan: False
25-01-23 08:03:09 | I | In layer model.layers.0, gradient of model.layers.0.self_attn.q_proj has inf: False
25-01-23 08:03:09 | I | In layer model.layers.0, gradient of model.layers.0.self_attn.k_proj has nan: False
25-01-23 08:03:09 | I | In layer model.layers.0, gradient of model.layers.0.self_attn.k_proj has inf: False
25-01-23 08:03:09 | I | In layer model.layers.0, gradient of model.layers.0.self_attn.v_proj has nan: False
25-01-23 08:03:09 | I | In layer model.layers.0, gradient of model.layers.0.self_attn.v_proj has inf: False
25-01-23 08:03:09 | I | In layer model.layers.0, gradient of model.layers.0.self_attn.o_proj has nan: False
25-01-23 08:03:09 | I | In layer model.layers.0, gradient of model.layers.0.self_attn.o_proj has inf: False
25-01-23 08:03:09 | I | In layer model.layers.0, gradient of model.layers.0.mlp.up_proj has nan: False
25-01-23 08:03:09 | I | In layer model.layers.0, gradient of model.layers.0.mlp.up_proj has inf: False
25-01-23 08:03:09 | I | In layer model.layers.0, gradient of model.layers.0.mlp.gate_proj has nan: False
25-01-23 08:03:09 | I | In layer model.layers.0, gradient of model.layers.0.mlp.gate_proj has inf: False
25-01-23 08:03:09 | I | In layer model.layers.0, gradient of model.layers.0.mlp.down_proj has nan: False
25-01-23 08:03:09 | I | In layer model.layers.0, gradient of model.layers.0.mlp.down_proj has inf: False
25-01-23 08:03:09 | I | [28] done with optimizer step
25-01-23 08:03:09 | I | epoch 001:     33 / 819200000 loss=1.21602e-05, loss_per_token=0.0498082, loss_sum=408.029, wps=227.3, ups=0.03, wpb=8192, bsz=16, num_updates=29, lr=0.0029, gnorm=0.19, clip=0, loss_scale=8, train_wall=36, cuda_gb_allocated=17.1, cuda_gb_reserved=18.4, cuda_gb_free=6.6, wall=2336
25-01-23 08:03:09 | I | TRAIN CURRENT LAYER_IDX = 0
25-01-23 08:03:09 | I | in layer model.layers.0
25-01-23 08:03:09 | I | quantizing weights for layer model.layers.0
25-01-23 08:03:09 | I | collecting calibration activations in model.layers.0
25-01-23 08:03:09 | I | collecting calibration activations in model.layers.0
25-01-23 08:03:10 | I | - Quantizing decoder layer model.layers.0
25-01-23 08:03:10 | I |   - Calibrating model.layers.0.self_attn.q_proj.weight
25-01-23 08:03:10 | I |       - range scale = [    1.0000]
25-01-23 08:03:10 | I |         sum  error  = [    0.0461]
25-01-23 08:03:10 | I |         best error  = [    0.0461]
25-01-23 08:03:10 | I |     + error = [0.0461]
25-01-23 08:03:11 | I |       - range scale = [    1.0000]
25-01-23 08:03:11 | I |         sum  error  = [    0.7182]
25-01-23 08:03:11 | I |         best error  = [    0.7182]
25-01-23 08:03:11 | I |     + error = [0.7182]
25-01-23 08:03:11 | I |   - Calibrating model.layers.0.self_attn.k_proj.weight
25-01-23 08:03:12 | I |       - range scale = [    1.0000]
25-01-23 08:03:12 | I |         sum  error  = [    0.0596]
25-01-23 08:03:12 | I |         best error  = [    0.0596]
25-01-23 08:03:12 | I |     + error = [0.0596]
25-01-23 08:03:12 | I |       - range scale = [    1.0000]
25-01-23 08:03:12 | I |         sum  error  = [    0.9470]
25-01-23 08:03:12 | I |         best error  = [    0.9470]
25-01-23 08:03:12 | I |     + error = [0.9470]
25-01-23 08:03:13 | I |   - Calibrating model.layers.0.self_attn.v_proj.weight
25-01-23 08:03:13 | I |       - range scale = [    1.0000]
25-01-23 08:03:13 | I |         sum  error  = [    0.7163]
25-01-23 08:03:13 | I |         best error  = [    0.7163]
25-01-23 08:03:13 | I |     + error = [0.7163]
25-01-23 08:03:14 | I |       - range scale = [    1.0000]
25-01-23 08:03:14 | I |         sum  error  = [    6.6376]
25-01-23 08:03:14 | I |         best error  = [    6.6376]
25-01-23 08:03:14 | I |     + error = [6.6376]
25-01-23 08:03:14 | I |   - Calibrating model.layers.0.self_attn.o_proj.weight
25-01-23 08:03:15 | I |       - range scale = [    1.0000]
25-01-23 08:03:15 | I |         sum  error  = [    0.1981]
25-01-23 08:03:15 | I |         best error  = [    0.1981]
25-01-23 08:03:15 | I |     + error = [0.1981]
25-01-23 08:03:16 | I |       - range scale = [    1.0000]
25-01-23 08:03:16 | I |         sum  error  = [    1.5644]
25-01-23 08:03:16 | I |         best error  = [    1.5644]
25-01-23 08:03:16 | I |     + error = [1.5644]
25-01-23 08:03:16 | I |   - Calibrating model.layers.0.mlp.up_proj.weight
25-01-23 08:03:17 | I |       - range scale = [    1.0000]
25-01-23 08:03:17 | I |         sum  error  = [    1.0699]
25-01-23 08:03:17 | I |         best error  = [    1.0699]
25-01-23 08:03:17 | I |     + error = [1.0699]
25-01-23 08:03:18 | I |       - range scale = [    1.0000]
25-01-23 08:03:18 | I |         sum  error  = [   11.3653]
25-01-23 08:03:18 | I |         best error  = [   11.3653]
25-01-23 08:03:18 | I |     + error = [11.3653]
25-01-23 08:03:18 | I |   - Calibrating model.layers.0.mlp.gate_proj.weight
25-01-23 08:03:19 | I |       - range scale = [    1.0000]
25-01-23 08:03:19 | I |         sum  error  = [    1.1329]
25-01-23 08:03:19 | I |         best error  = [    1.1329]
25-01-23 08:03:19 | I |     + error = [1.1329]
25-01-23 08:03:20 | I |       - range scale = [    1.0000]
25-01-23 08:03:20 | I |         sum  error  = [   11.7411]
25-01-23 08:03:20 | I |         best error  = [   11.7411]
25-01-23 08:03:20 | I |     + error = [11.7411]
25-01-23 08:03:20 | I |   - Calibrating model.layers.0.mlp.down_proj.weight
25-01-23 08:03:21 | I |       - range scale = [    1.0000]
25-01-23 08:03:21 | I |         sum  error  = [    0.1653]
25-01-23 08:03:21 | I |         best error  = [    0.1653]
25-01-23 08:03:21 | I |     + error = [0.1653]
25-01-23 08:03:22 | I |       - range scale = [    1.0000]
25-01-23 08:03:22 | I |         sum  error  = [    1.6069]
25-01-23 08:03:22 | I |         best error  = [    1.6069]
25-01-23 08:03:22 | I |     + error = [1.6069]
25-01-23 08:03:22 | I |   - Quantizing model.layers.0.self_attn.q_proj.weight
25-01-23 08:03:25 | I |   - Quantizing model.layers.0.self_attn.k_proj.weight
25-01-23 08:03:28 | I |   - Quantizing model.layers.0.self_attn.v_proj.weight
25-01-23 08:03:31 | I |   - Quantizing model.layers.0.self_attn.o_proj.weight
25-01-23 08:03:34 | I |   - Quantizing model.layers.0.mlp.up_proj.weight
25-01-23 08:03:37 | I |   - Quantizing model.layers.0.mlp.gate_proj.weight
25-01-23 08:03:40 | I |   - Quantizing model.layers.0.mlp.down_proj.weight
25-01-23 08:03:47 | I | quantizing activations for layer model.layers.0
25-01-23 08:03:48 | I | collecting calibration activations in model.layers.0
25-01-23 08:03:48 | I | collecting calibration activations in model.layers.0
25-01-23 08:03:49 | I | forward this layer
25-01-23 08:03:49 | I | input_file: /data/gyy/lmquant-main/lmquant/data/data_without_preprocess_llama_7b_bs16/shard/val_input_args_layer_0/33.pt
25-01-23 08:03:49 | I | output_file: /data/gyy/lmquant-main/lmquant/data/data_without_preprocess_llama_7b_bs16/shard/val_teacher_output_layer_0/33.pt
25-01-23 08:03:50 | I | In layer model.layers.0, gradient of model.layers.0.self_attn.q_proj has nan: False
25-01-23 08:03:50 | I | In layer model.layers.0, gradient of model.layers.0.self_attn.q_proj has inf: False
25-01-23 08:03:50 | I | In layer model.layers.0, gradient of model.layers.0.self_attn.k_proj has nan: False
25-01-23 08:03:50 | I | In layer model.layers.0, gradient of model.layers.0.self_attn.k_proj has inf: False
25-01-23 08:03:50 | I | In layer model.layers.0, gradient of model.layers.0.self_attn.v_proj has nan: False
25-01-23 08:03:50 | I | In layer model.layers.0, gradient of model.layers.0.self_attn.v_proj has inf: False
25-01-23 08:03:50 | I | In layer model.layers.0, gradient of model.layers.0.self_attn.o_proj has nan: False
25-01-23 08:03:50 | I | In layer model.layers.0, gradient of model.layers.0.self_attn.o_proj has inf: False
25-01-23 08:03:50 | I | In layer model.layers.0, gradient of model.layers.0.mlp.up_proj has nan: False
25-01-23 08:03:50 | I | In layer model.layers.0, gradient of model.layers.0.mlp.up_proj has inf: False
25-01-23 08:03:50 | I | In layer model.layers.0, gradient of model.layers.0.mlp.gate_proj has nan: False
25-01-23 08:03:50 | I | In layer model.layers.0, gradient of model.layers.0.mlp.gate_proj has inf: False
25-01-23 08:03:50 | I | In layer model.layers.0, gradient of model.layers.0.mlp.down_proj has nan: False
25-01-23 08:03:50 | I | In layer model.layers.0, gradient of model.layers.0.mlp.down_proj has inf: False
25-01-23 08:03:50 | I | [29] done with optimizer step
25-01-23 08:03:50 | I | epoch 001:     34 / 819200000 loss=1.19512e-05, loss_per_token=0.0489522, loss_sum=401.017, wps=198.7, ups=0.02, wpb=8192, bsz=16, num_updates=30, lr=0.003, gnorm=0.234, clip=0, loss_scale=8, train_wall=41, cuda_gb_allocated=17.1, cuda_gb_reserved=18.3, cuda_gb_free=6.6, wall=2377
25-01-23 08:03:50 | I | begin validation on "valid" subset on rank 0
25-01-23 08:03:50 | I | got valid iterator on "valid" subset on rank 0
25-01-23 08:03:50 | I | Valid: Start iterating over samples
25-01-23 08:03:50 | W |   `pretrained` model kwarg is not of type `str`. Many other model arguments may be ignored. Please do not launch via accelerate or use `parallelize=True` if passing an existing model this way.
25-01-23 08:03:50 | W |   Passed an already-initialized model through `pretrained`, assuming single-process call to evaluate() or custom distributed integration
25-01-23 08:03:50 | I | - Evaluator: gptq
25-01-23 08:03:50 | I | - Tasks: ['wikitext', 'val_valid']
25-01-23 08:03:50 | I | - Batch_size: 8
25-01-23 08:03:50 | I |   + Max_seq_length: 2048
25-01-23 08:05:25 | I |     - Results:
25-01-23 08:05:25 | I |       |  Task   |Version|    Metric     |Value |   |Stderr|
25-01-23 08:05:25 | I |       |---------|------:|---------------|-----:|---|-----:|
25-01-23 08:05:25 | I |       |wikitext |      1|word_perplexity|5.4855|±  |5.4855|
25-01-23 08:05:25 | I |       |val_valid|      1|word_perplexity|5.0938|±  |5.0938|
25-01-23 08:05:25 | I |       
25-01-23 08:05:25 | I |   + Max_seq_length: 4096
25-01-23 08:06:58 | I |     - Results:
25-01-23 08:06:58 | I |       |  Task   |Version|    Metric     |Value |   |Stderr|
25-01-23 08:06:58 | I |       |---------|------:|---------------|-----:|---|-----:|
25-01-23 08:06:58 | I |       |wikitext |      1|word_perplexity|5.1261|±  |5.1261|
25-01-23 08:06:58 | I |       |val_valid|      1|word_perplexity|4.8882|±  |4.8882|
25-01-23 08:06:58 | I |       
25-01-23 08:06:58 | I | in valid, quantize current layer weights
25-01-23 08:07:00 | W | Repo card metadata block was not found. Setting CardData to empty.
25-01-23 08:07:46 | I | collecting calibration activations in model.layers.0
25-01-23 08:07:46 | I | collecting calibration activations in model.layers.0
25-01-23 08:07:46 | I | collecting calibration activations in model.layers.0
25-01-23 08:07:46 | I | collecting calibration activations in model.layers.0
25-01-23 08:07:46 | I | collecting calibration activations in model.layers.0
25-01-23 08:07:46 | I | collecting calibration activations in model.layers.0
25-01-23 08:07:46 | I | collecting calibration activations in model.layers.0
25-01-23 08:07:46 | I | collecting calibration activations in model.layers.0
25-01-23 08:07:46 | I | collecting calibration activations in model.layers.0
25-01-23 08:07:46 | I | collecting calibration activations in model.layers.0
25-01-23 08:07:46 | I | collecting calibration activations in model.layers.0
25-01-23 08:07:46 | I | collecting calibration activations in model.layers.0
25-01-23 08:07:46 | I | collecting calibration activations in model.layers.0
25-01-23 08:07:46 | I | collecting calibration activations in model.layers.0
25-01-23 08:07:46 | I | collecting calibration activations in model.layers.0
25-01-23 08:07:46 | I | collecting calibration activations in model.layers.0
25-01-23 08:07:46 | I | collecting calibration activations in model.layers.0
25-01-23 08:07:47 | I | collecting calibration activations in model.layers.0
25-01-23 08:07:47 | I | collecting calibration activations in model.layers.0
25-01-23 08:07:47 | I | collecting calibration activations in model.layers.0
25-01-23 08:07:47 | I | collecting calibration activations in model.layers.0
25-01-23 08:07:47 | I | collecting calibration activations in model.layers.0
25-01-23 08:07:47 | I | collecting calibration activations in model.layers.0
25-01-23 08:07:47 | I | collecting calibration activations in model.layers.0
25-01-23 08:07:47 | I | collecting calibration activations in model.layers.0
25-01-23 08:07:47 | I | collecting calibration activations in model.layers.0
25-01-23 08:07:47 | I | collecting calibration activations in model.layers.0
25-01-23 08:07:47 | I | collecting calibration activations in model.layers.0
25-01-23 08:07:47 | I | collecting calibration activations in model.layers.0
25-01-23 08:07:47 | I | collecting calibration activations in model.layers.0
25-01-23 08:07:47 | I | collecting calibration activations in model.layers.0
25-01-23 08:07:47 | I | collecting calibration activations in model.layers.0
25-01-23 08:07:47 | I | collecting calibration activations in model.layers.0
25-01-23 08:07:47 | I | collecting calibration activations in model.layers.0
25-01-23 08:07:47 | I | collecting calibration activations in model.layers.0
25-01-23 08:07:47 | I | collecting calibration activations in model.layers.0
25-01-23 08:07:47 | I | collecting calibration activations in model.layers.0
25-01-23 08:07:48 | I | collecting calibration activations in model.layers.0
25-01-23 08:07:48 | I | collecting calibration activations in model.layers.0
25-01-23 08:07:48 | I | collecting calibration activations in model.layers.0
25-01-23 08:07:48 | I | collecting calibration activations in model.layers.0
25-01-23 08:07:48 | I | collecting calibration activations in model.layers.0
25-01-23 08:07:48 | I | collecting calibration activations in model.layers.0
25-01-23 08:07:48 | I | collecting calibration activations in model.layers.0
25-01-23 08:07:48 | I | collecting calibration activations in model.layers.0
25-01-23 08:07:48 | I | collecting calibration activations in model.layers.0
25-01-23 08:07:48 | I | collecting calibration activations in model.layers.0
25-01-23 08:07:48 | I | collecting calibration activations in model.layers.0
25-01-23 08:07:48 | I | collecting calibration activations in model.layers.0
25-01-23 08:07:48 | I | collecting calibration activations in model.layers.0
25-01-23 08:07:48 | I | collecting calibration activations in model.layers.0
25-01-23 08:07:48 | I | collecting calibration activations in model.layers.0
25-01-23 08:07:48 | I | collecting calibration activations in model.layers.0
25-01-23 08:07:48 | I | collecting calibration activations in model.layers.0
25-01-23 08:07:48 | I | collecting calibration activations in model.layers.0
25-01-23 08:07:48 | I | collecting calibration activations in model.layers.0
25-01-23 08:07:49 | I | collecting calibration activations in model.layers.0
25-01-23 08:07:49 | I | collecting calibration activations in model.layers.0
25-01-23 08:07:49 | I | collecting calibration activations in model.layers.0
25-01-23 08:07:49 | I | collecting calibration activations in model.layers.0
25-01-23 08:07:49 | I | collecting calibration activations in model.layers.0
25-01-23 08:07:49 | I | collecting calibration activations in model.layers.0
25-01-23 08:07:49 | I | collecting calibration activations in model.layers.0
25-01-23 08:07:49 | I | collecting calibration activations in model.layers.0
25-01-23 08:07:49 | I | collecting calibration activations in model.layers.0
25-01-23 08:07:49 | I | collecting calibration activations in model.layers.0
25-01-23 08:07:49 | I | collecting calibration activations in model.layers.0
25-01-23 08:07:49 | I | collecting calibration activations in model.layers.0
25-01-23 08:07:49 | I | collecting calibration activations in model.layers.0
25-01-23 08:07:49 | I | collecting calibration activations in model.layers.0
25-01-23 08:07:49 | I | collecting calibration activations in model.layers.0
25-01-23 08:07:49 | I | collecting calibration activations in model.layers.0
25-01-23 08:07:49 | I | collecting calibration activations in model.layers.0
25-01-23 08:07:49 | I | collecting calibration activations in model.layers.0
25-01-23 08:07:49 | I | collecting calibration activations in model.layers.0
25-01-23 08:07:50 | I | collecting calibration activations in model.layers.0
25-01-23 08:07:50 | I | collecting calibration activations in model.layers.0
25-01-23 08:07:50 | I | collecting calibration activations in model.layers.0
25-01-23 08:07:50 | I | collecting calibration activations in model.layers.0
25-01-23 08:07:50 | I | collecting calibration activations in model.layers.0
25-01-23 08:07:50 | I | collecting calibration activations in model.layers.0
25-01-23 08:07:50 | I | collecting calibration activations in model.layers.0
25-01-23 08:07:50 | I | collecting calibration activations in model.layers.0
25-01-23 08:07:50 | I | collecting calibration activations in model.layers.0
25-01-23 08:07:50 | I | collecting calibration activations in model.layers.0
25-01-23 08:07:50 | I | collecting calibration activations in model.layers.0
25-01-23 08:07:50 | I | collecting calibration activations in model.layers.0
25-01-23 08:07:50 | I | collecting calibration activations in model.layers.0
25-01-23 08:07:50 | I | collecting calibration activations in model.layers.0
25-01-23 08:07:50 | I | collecting calibration activations in model.layers.0
25-01-23 08:07:50 | I | collecting calibration activations in model.layers.0
25-01-23 08:07:50 | I | collecting calibration activations in model.layers.0
25-01-23 08:07:50 | I | collecting calibration activations in model.layers.0
25-01-23 08:07:50 | I | collecting calibration activations in model.layers.0
25-01-23 08:07:51 | I | collecting calibration activations in model.layers.0
25-01-23 08:07:51 | I | collecting calibration activations in model.layers.0
25-01-23 08:07:51 | I | collecting calibration activations in model.layers.0
25-01-23 08:07:51 | I | collecting calibration activations in model.layers.0
25-01-23 08:07:51 | I | collecting calibration activations in model.layers.0
25-01-23 08:07:51 | I | collecting calibration activations in model.layers.0
25-01-23 08:07:51 | I | collecting calibration activations in model.layers.0
25-01-23 08:07:51 | I | collecting calibration activations in model.layers.0
25-01-23 08:07:51 | I | collecting calibration activations in model.layers.0
25-01-23 08:07:51 | I | collecting calibration activations in model.layers.0
25-01-23 08:07:51 | I | collecting calibration activations in model.layers.0
25-01-23 08:07:51 | I | collecting calibration activations in model.layers.0
25-01-23 08:07:51 | I | collecting calibration activations in model.layers.0
25-01-23 08:07:51 | I | collecting calibration activations in model.layers.0
25-01-23 08:07:51 | I | collecting calibration activations in model.layers.0
25-01-23 08:07:51 | I | collecting calibration activations in model.layers.0
25-01-23 08:07:51 | I | collecting calibration activations in model.layers.0
25-01-23 08:07:51 | I | collecting calibration activations in model.layers.0
25-01-23 08:07:51 | I | collecting calibration activations in model.layers.0
25-01-23 08:07:52 | I | collecting calibration activations in model.layers.0
25-01-23 08:07:52 | I | collecting calibration activations in model.layers.0
25-01-23 08:07:52 | I | collecting calibration activations in model.layers.0
25-01-23 08:07:52 | I | collecting calibration activations in model.layers.0
25-01-23 08:07:52 | I | collecting calibration activations in model.layers.0
25-01-23 08:07:52 | I | collecting calibration activations in model.layers.0
25-01-23 08:07:52 | I | collecting calibration activations in model.layers.0
25-01-23 08:07:52 | I | collecting calibration activations in model.layers.0
25-01-23 08:07:52 | I | collecting calibration activations in model.layers.0
25-01-23 08:07:52 | I | collecting calibration activations in model.layers.0
25-01-23 08:07:52 | I | collecting calibration activations in model.layers.0
25-01-23 08:07:52 | I | collecting calibration activations in model.layers.0
25-01-23 08:07:52 | I | collecting calibration activations in model.layers.0
25-01-23 08:07:52 | I | collecting calibration activations in model.layers.0
25-01-23 08:07:52 | I | collecting calibration activations in model.layers.0
25-01-23 08:07:54 | I |       - range scale = [    1.0000]
25-01-23 08:07:54 | I |         sum  error  = [    0.1555]
25-01-23 08:07:54 | I |         best error  = [    0.1555]
25-01-23 08:07:54 | I |     + error = [0.1555]
25-01-23 08:07:56 | I |       - range scale = [    1.0000]
25-01-23 08:07:56 | I |         sum  error  = [    2.6797]
25-01-23 08:07:56 | I |         best error  = [    2.6797]
25-01-23 08:07:56 | I |     + error = [2.6797]
25-01-23 08:07:58 | I |       - range scale = [    1.0000]
25-01-23 08:07:58 | I |         sum  error  = [    0.1881]
25-01-23 08:07:58 | I |         best error  = [    0.1881]
25-01-23 08:07:58 | I |     + error = [0.1881]
25-01-23 08:07:59 | I |       - range scale = [    1.0000]
25-01-23 08:07:59 | I |         sum  error  = [    3.7838]
25-01-23 08:07:59 | I |         best error  = [    3.7838]
25-01-23 08:07:59 | I |     + error = [3.7838]
25-01-23 08:08:00 | I |       - range scale = [    1.0000]
25-01-23 08:08:00 | I |         sum  error  = [    0.6480]
25-01-23 08:08:00 | I |         best error  = [    0.6480]
25-01-23 08:08:00 | I |     + error = [0.6480]
25-01-23 08:08:01 | I |       - range scale = [    1.0000]
25-01-23 08:08:01 | I |         sum  error  = [    5.9722]
25-01-23 08:08:01 | I |         best error  = [    5.9722]
25-01-23 08:08:01 | I |     + error = [5.9722]
25-01-23 08:08:02 | I |       - range scale = [    1.0000]
25-01-23 08:08:02 | I |         sum  error  = [    0.1471]
25-01-23 08:08:02 | I |         best error  = [    0.1471]
25-01-23 08:08:02 | I |     + error = [0.1471]
25-01-23 08:08:03 | I |       - range scale = [    1.0000]
25-01-23 08:08:03 | I |         sum  error  = [    1.2074]
25-01-23 08:08:03 | I |         best error  = [    1.2074]
25-01-23 08:08:03 | I |     + error = [1.2074]
25-01-23 08:08:03 | I |       - range scale = [    1.0000]
25-01-23 08:08:03 | I |         sum  error  = [    1.0906]
25-01-23 08:08:03 | I |         best error  = [    1.0906]
25-01-23 08:08:03 | I |     + error = [1.0906]
25-01-23 08:08:05 | I |       - range scale = [    1.0000]
25-01-23 08:08:05 | I |         sum  error  = [   11.6138]
25-01-23 08:08:05 | I |         best error  = [   11.6138]
25-01-23 08:08:05 | I |     + error = [11.6138]
25-01-23 08:08:06 | I |       - range scale = [    1.0000]
25-01-23 08:08:06 | I |         sum  error  = [    1.1560]
25-01-23 08:08:06 | I |         best error  = [    1.1560]
25-01-23 08:08:06 | I |     + error = [1.1560]
25-01-23 08:08:07 | I |       - range scale = [    1.0000]
25-01-23 08:08:07 | I |         sum  error  = [   11.9804]
25-01-23 08:08:07 | I |         best error  = [   11.9804]
25-01-23 08:08:07 | I |     + error = [11.9804]
25-01-23 08:08:08 | I |       - range scale = [    1.0000]
25-01-23 08:08:08 | I |         sum  error  = [    0.1934]
25-01-23 08:08:08 | I |         best error  = [    0.1934]
25-01-23 08:08:08 | I |     + error = [0.1934]
25-01-23 08:08:09 | I |       - range scale = [    1.0000]
25-01-23 08:08:09 | I |         sum  error  = [    1.9132]
25-01-23 08:08:09 | I |         best error  = [    1.9132]
25-01-23 08:08:09 | I |     + error = [1.9132]
25-01-23 08:08:33 | I | in valid, quantize current layer acts
25-01-23 08:08:35 | I | collecting calibration activations in model.layers.0
25-01-23 08:08:35 | I | collecting calibration activations in model.layers.0
25-01-23 08:08:35 | I | collecting calibration activations in model.layers.0
25-01-23 08:08:35 | I | collecting calibration activations in model.layers.0
25-01-23 08:08:35 | I | collecting calibration activations in model.layers.0
25-01-23 08:08:35 | I | collecting calibration activations in model.layers.0
25-01-23 08:08:35 | I | collecting calibration activations in model.layers.0
25-01-23 08:08:35 | I | collecting calibration activations in model.layers.0
25-01-23 08:08:35 | I | collecting calibration activations in model.layers.0
25-01-23 08:08:35 | I | collecting calibration activations in model.layers.0
25-01-23 08:08:36 | I | collecting calibration activations in model.layers.0
25-01-23 08:08:36 | I | collecting calibration activations in model.layers.0
25-01-23 08:08:36 | I | collecting calibration activations in model.layers.0
25-01-23 08:08:36 | I | collecting calibration activations in model.layers.0
25-01-23 08:08:36 | I | collecting calibration activations in model.layers.0
25-01-23 08:08:36 | I | collecting calibration activations in model.layers.0
25-01-23 08:08:36 | I | collecting calibration activations in model.layers.0
25-01-23 08:08:36 | I | collecting calibration activations in model.layers.0
25-01-23 08:08:36 | I | collecting calibration activations in model.layers.0
25-01-23 08:08:36 | I | collecting calibration activations in model.layers.0
25-01-23 08:08:36 | I | collecting calibration activations in model.layers.0
25-01-23 08:08:36 | I | collecting calibration activations in model.layers.0
25-01-23 08:08:36 | I | collecting calibration activations in model.layers.0
25-01-23 08:08:36 | I | collecting calibration activations in model.layers.0
25-01-23 08:08:36 | I | collecting calibration activations in model.layers.0
25-01-23 08:08:36 | I | collecting calibration activations in model.layers.0
25-01-23 08:08:37 | I | collecting calibration activations in model.layers.0
25-01-23 08:08:37 | I | collecting calibration activations in model.layers.0
25-01-23 08:08:37 | I | collecting calibration activations in model.layers.0
25-01-23 08:08:37 | I | collecting calibration activations in model.layers.0
25-01-23 08:08:37 | I | collecting calibration activations in model.layers.0
25-01-23 08:08:37 | I | collecting calibration activations in model.layers.0
25-01-23 08:08:37 | I | collecting calibration activations in model.layers.0
25-01-23 08:08:37 | I | collecting calibration activations in model.layers.0
25-01-23 08:08:37 | I | collecting calibration activations in model.layers.0
25-01-23 08:08:37 | I | collecting calibration activations in model.layers.0
25-01-23 08:08:37 | I | collecting calibration activations in model.layers.0
25-01-23 08:08:37 | I | collecting calibration activations in model.layers.0
25-01-23 08:08:37 | I | collecting calibration activations in model.layers.0
25-01-23 08:08:37 | I | collecting calibration activations in model.layers.0
25-01-23 08:08:37 | I | collecting calibration activations in model.layers.0
25-01-23 08:08:37 | I | collecting calibration activations in model.layers.0
25-01-23 08:08:38 | I | collecting calibration activations in model.layers.0
25-01-23 08:08:38 | I | collecting calibration activations in model.layers.0
25-01-23 08:08:38 | I | collecting calibration activations in model.layers.0
25-01-23 08:08:38 | I | collecting calibration activations in model.layers.0
25-01-23 08:08:38 | I | collecting calibration activations in model.layers.0
25-01-23 08:08:38 | I | collecting calibration activations in model.layers.0
25-01-23 08:08:38 | I | collecting calibration activations in model.layers.0
25-01-23 08:08:38 | I | collecting calibration activations in model.layers.0
25-01-23 08:08:38 | I | collecting calibration activations in model.layers.0
25-01-23 08:08:38 | I | collecting calibration activations in model.layers.0
25-01-23 08:08:38 | I | collecting calibration activations in model.layers.0
25-01-23 08:08:38 | I | collecting calibration activations in model.layers.0
25-01-23 08:08:38 | I | collecting calibration activations in model.layers.0
25-01-23 08:08:38 | I | collecting calibration activations in model.layers.0
25-01-23 08:08:38 | I | collecting calibration activations in model.layers.0
25-01-23 08:08:38 | I | collecting calibration activations in model.layers.0
25-01-23 08:08:39 | I | collecting calibration activations in model.layers.0
25-01-23 08:08:39 | I | collecting calibration activations in model.layers.0
25-01-23 08:08:39 | I | collecting calibration activations in model.layers.0
25-01-23 08:08:39 | I | collecting calibration activations in model.layers.0
25-01-23 08:08:39 | I | collecting calibration activations in model.layers.0
25-01-23 08:08:39 | I | collecting calibration activations in model.layers.0
25-01-23 08:08:39 | I | collecting calibration activations in model.layers.0
25-01-23 08:08:39 | I | collecting calibration activations in model.layers.0
25-01-23 08:08:39 | I | collecting calibration activations in model.layers.0
25-01-23 08:08:39 | I | collecting calibration activations in model.layers.0
25-01-23 08:08:39 | I | collecting calibration activations in model.layers.0
25-01-23 08:08:39 | I | collecting calibration activations in model.layers.0
25-01-23 08:08:39 | I | collecting calibration activations in model.layers.0
25-01-23 08:08:39 | I | collecting calibration activations in model.layers.0
25-01-23 08:08:39 | I | collecting calibration activations in model.layers.0
25-01-23 08:08:40 | I | collecting calibration activations in model.layers.0
25-01-23 08:08:40 | I | collecting calibration activations in model.layers.0
25-01-23 08:08:40 | I | collecting calibration activations in model.layers.0
25-01-23 08:08:40 | I | collecting calibration activations in model.layers.0
25-01-23 08:08:40 | I | collecting calibration activations in model.layers.0
25-01-23 08:08:40 | I | collecting calibration activations in model.layers.0
25-01-23 08:08:40 | I | collecting calibration activations in model.layers.0
25-01-23 08:08:40 | I | collecting calibration activations in model.layers.0
25-01-23 08:08:40 | I | collecting calibration activations in model.layers.0
25-01-23 08:08:40 | I | collecting calibration activations in model.layers.0
25-01-23 08:08:40 | I | collecting calibration activations in model.layers.0
25-01-23 08:08:40 | I | collecting calibration activations in model.layers.0
25-01-23 08:08:40 | I | collecting calibration activations in model.layers.0
25-01-23 08:08:40 | I | collecting calibration activations in model.layers.0
25-01-23 08:08:40 | I | collecting calibration activations in model.layers.0
25-01-23 08:08:40 | I | collecting calibration activations in model.layers.0
25-01-23 08:08:41 | I | collecting calibration activations in model.layers.0
25-01-23 08:08:41 | I | collecting calibration activations in model.layers.0
25-01-23 08:08:41 | I | collecting calibration activations in model.layers.0
25-01-23 08:08:41 | I | collecting calibration activations in model.layers.0
25-01-23 08:08:41 | I | collecting calibration activations in model.layers.0
25-01-23 08:08:41 | I | collecting calibration activations in model.layers.0
25-01-23 08:08:41 | I | collecting calibration activations in model.layers.0
25-01-23 08:08:41 | I | collecting calibration activations in model.layers.0
25-01-23 08:08:41 | I | collecting calibration activations in model.layers.0
25-01-23 08:08:41 | I | collecting calibration activations in model.layers.0
25-01-23 08:08:41 | I | collecting calibration activations in model.layers.0
25-01-23 08:08:41 | I | collecting calibration activations in model.layers.0
25-01-23 08:08:41 | I | collecting calibration activations in model.layers.0
25-01-23 08:08:41 | I | collecting calibration activations in model.layers.0
25-01-23 08:08:41 | I | collecting calibration activations in model.layers.0
25-01-23 08:08:41 | I | collecting calibration activations in model.layers.0
25-01-23 08:08:42 | I | collecting calibration activations in model.layers.0
25-01-23 08:08:42 | I | collecting calibration activations in model.layers.0
25-01-23 08:08:42 | I | collecting calibration activations in model.layers.0
25-01-23 08:08:42 | I | collecting calibration activations in model.layers.0
25-01-23 08:08:42 | I | collecting calibration activations in model.layers.0
25-01-23 08:08:42 | I | collecting calibration activations in model.layers.0
25-01-23 08:08:42 | I | collecting calibration activations in model.layers.0
25-01-23 08:08:42 | I | collecting calibration activations in model.layers.0
25-01-23 08:08:42 | I | collecting calibration activations in model.layers.0
25-01-23 08:08:42 | I | collecting calibration activations in model.layers.0
25-01-23 08:08:42 | I | collecting calibration activations in model.layers.0
25-01-23 08:08:42 | I | collecting calibration activations in model.layers.0
25-01-23 08:08:42 | I | collecting calibration activations in model.layers.0
25-01-23 08:08:42 | I | collecting calibration activations in model.layers.0
25-01-23 08:08:42 | I | collecting calibration activations in model.layers.0
25-01-23 08:08:43 | I | collecting calibration activations in model.layers.0
25-01-23 08:08:43 | I | collecting calibration activations in model.layers.0
25-01-23 08:08:43 | I | collecting calibration activations in model.layers.0
25-01-23 08:08:43 | I | collecting calibration activations in model.layers.0
25-01-23 08:08:43 | I | collecting calibration activations in model.layers.0
25-01-23 08:08:43 | I | collecting calibration activations in model.layers.0
25-01-23 08:08:43 | I | collecting calibration activations in model.layers.0
25-01-23 08:08:43 | I | collecting calibration activations in model.layers.0
25-01-23 08:08:45 | I | use gptq_eval(lmquant) to calculate ppl, partly quanted
25-01-23 08:08:45 | W |   `pretrained` model kwarg is not of type `str`. Many other model arguments may be ignored. Please do not launch via accelerate or use `parallelize=True` if passing an existing model this way.
25-01-23 08:08:45 | W |   Passed an already-initialized model through `pretrained`, assuming single-process call to evaluate() or custom distributed integration
25-01-23 08:08:45 | I | - Evaluator: gptq
25-01-23 08:08:45 | I | - Tasks: ['wikitext', 'val_valid']
25-01-23 08:08:45 | I | - Batch_size: 8
25-01-23 08:08:45 | I |   + Max_seq_length: 2048
25-01-23 08:10:23 | I |     - Results:
25-01-23 08:10:23 | I |       |  Task   |Version|    Metric     |Value |   |Stderr|
25-01-23 08:10:23 | I |       |---------|------:|---------------|-----:|---|-----:|
25-01-23 08:10:23 | I |       |wikitext |      1|word_perplexity|5.4934|±  |5.4934|
25-01-23 08:10:23 | I |       |val_valid|      1|word_perplexity|5.1194|±  |5.1194|
25-01-23 08:10:23 | I |       
25-01-23 08:10:23 | I |   + Max_seq_length: 4096
25-01-23 08:11:58 | I |     - Results:
25-01-23 08:11:58 | I |       |  Task   |Version|    Metric     |Value |   |Stderr|
25-01-23 08:11:58 | I |       |---------|------:|---------------|-----:|---|-----:|
25-01-23 08:11:58 | I |       |wikitext |      1|word_perplexity|5.1324|±  |5.1324|
25-01-23 08:11:58 | I |       |val_valid|      1|word_perplexity|4.9128|±  |4.9128|
25-01-23 08:11:58 | I |       
25-01-23 08:12:57 | I | epoch 001 | valid on 'valid' subset:    102 / 819200000 loss_valid=2.15, lmquant_ppl_result_val=-1, lmquant_ppl_result_wikitext=-1, ppl=4.44, wps=7239.9, wpb=4058.2, bsz=2, num_updates=30, lmquant_ppl_wikitext_all_quanted=5.48553, lmquant_ppl_val_all_quanted=5.09379, lmquant_ppl_wikitext_partly_quanted=5.49345, lmquant_ppl_val_partly_quanted=5.11937
25-01-23 08:12:57 | I | epoch 001 | valid on 'valid' subset | loss_valid 2.15 | lmquant_ppl_result_val -1 | lmquant_ppl_result_wikitext -1 | ppl 4.44 | wps 7239.9 | wpb 4058.2 | bsz 2 | num_updates 30 | lmquant_ppl_wikitext_all_quanted 5.48553 | lmquant_ppl_val_all_quanted 5.09379 | lmquant_ppl_wikitext_partly_quanted 5.49345 | lmquant_ppl_val_partly_quanted 5.11937
25-01-23 08:12:57 | I | TRAIN CURRENT LAYER_IDX = 0
25-01-23 08:12:57 | I | in layer model.layers.0
25-01-23 08:12:57 | I | quantizing weights for layer model.layers.0
25-01-23 08:12:58 | I | collecting calibration activations in model.layers.0
25-01-23 08:12:58 | I | collecting calibration activations in model.layers.0
25-01-23 08:12:58 | I | - Quantizing decoder layer model.layers.0
25-01-23 08:12:58 | I |   - Calibrating model.layers.0.self_attn.q_proj.weight
25-01-23 08:12:59 | I |       - range scale = [    1.0000]
25-01-23 08:12:59 | I |         sum  error  = [    0.0482]
25-01-23 08:12:59 | I |         best error  = [    0.0482]
25-01-23 08:12:59 | I |     + error = [0.0482]
25-01-23 08:12:59 | I |       - range scale = [    1.0000]
25-01-23 08:12:59 | I |         sum  error  = [    0.7285]
25-01-23 08:12:59 | I |         best error  = [    0.7285]
25-01-23 08:12:59 | I |     + error = [0.7285]
25-01-23 08:13:00 | I |   - Calibrating model.layers.0.self_attn.k_proj.weight
25-01-23 08:13:00 | I |       - range scale = [    1.0000]
25-01-23 08:13:00 | I |         sum  error  = [    0.0619]
25-01-23 08:13:00 | I |         best error  = [    0.0619]
25-01-23 08:13:00 | I |     + error = [0.0619]
25-01-23 08:13:01 | I |       - range scale = [    1.0000]
25-01-23 08:13:01 | I |         sum  error  = [    0.9700]
25-01-23 08:13:01 | I |         best error  = [    0.9700]
25-01-23 08:13:01 | I |     + error = [0.9700]
25-01-23 08:13:01 | I |   - Calibrating model.layers.0.self_attn.v_proj.weight
25-01-23 08:13:02 | I |       - range scale = [    1.0000]
25-01-23 08:13:02 | I |         sum  error  = [    0.7212]
25-01-23 08:13:02 | I |         best error  = [    0.7212]
25-01-23 08:13:02 | I |     + error = [0.7212]
25-01-23 08:13:03 | I |       - range scale = [    1.0000]
25-01-23 08:13:03 | I |         sum  error  = [    6.7433]
25-01-23 08:13:03 | I |         best error  = [    6.7433]
25-01-23 08:13:03 | I |     + error = [6.7433]
25-01-23 08:13:03 | I |   - Calibrating model.layers.0.self_attn.o_proj.weight
25-01-23 08:13:03 | I |       - range scale = [    1.0000]
25-01-23 08:13:03 | I |         sum  error  = [    0.1980]
25-01-23 08:13:03 | I |         best error  = [    0.1980]
25-01-23 08:13:03 | I |     + error = [0.1980]
25-01-23 08:13:04 | I |       - range scale = [    1.0000]
25-01-23 08:13:04 | I |         sum  error  = [    1.5430]
25-01-23 08:13:04 | I |         best error  = [    1.5430]
25-01-23 08:13:04 | I |     + error = [1.5430]
25-01-23 08:13:04 | I |   - Calibrating model.layers.0.mlp.up_proj.weight
25-01-23 08:13:05 | I |       - range scale = [    1.0000]
25-01-23 08:13:05 | I |         sum  error  = [    1.0993]
25-01-23 08:13:05 | I |         best error  = [    1.0993]
25-01-23 08:13:05 | I |     + error = [1.0993]
25-01-23 08:13:06 | I |       - range scale = [    1.0000]
25-01-23 08:13:06 | I |         sum  error  = [   11.6624]
25-01-23 08:13:06 | I |         best error  = [   11.6624]
25-01-23 08:13:06 | I |     + error = [11.6624]
25-01-23 08:13:06 | I |   - Calibrating model.layers.0.mlp.gate_proj.weight
25-01-23 08:13:07 | I |       - range scale = [    1.0000]
25-01-23 08:13:07 | I |         sum  error  = [    1.1609]
25-01-23 08:13:07 | I |         best error  = [    1.1609]
25-01-23 08:13:07 | I |     + error = [1.1609]
25-01-23 08:13:08 | I |       - range scale = [    1.0000]
25-01-23 08:13:08 | I |         sum  error  = [   12.0394]
25-01-23 08:13:08 | I |         best error  = [   12.0394]
25-01-23 08:13:08 | I |     + error = [12.0394]
25-01-23 08:13:08 | I |   - Calibrating model.layers.0.mlp.down_proj.weight
25-01-23 08:13:09 | I |       - range scale = [    1.0000]
25-01-23 08:13:09 | I |         sum  error  = [    0.1681]
25-01-23 08:13:09 | I |         best error  = [    0.1681]
25-01-23 08:13:09 | I |     + error = [0.1681]
25-01-23 08:13:10 | I |       - range scale = [    1.0000]
25-01-23 08:13:10 | I |         sum  error  = [    1.6353]
25-01-23 08:13:10 | I |         best error  = [    1.6353]
25-01-23 08:13:10 | I |     + error = [1.6353]
25-01-23 08:13:11 | I |   - Quantizing model.layers.0.self_attn.q_proj.weight
25-01-23 08:13:13 | I |   - Quantizing model.layers.0.self_attn.k_proj.weight
25-01-23 08:13:16 | I |   - Quantizing model.layers.0.self_attn.v_proj.weight
25-01-23 08:13:19 | I |   - Quantizing model.layers.0.self_attn.o_proj.weight
25-01-23 08:13:22 | I |   - Quantizing model.layers.0.mlp.up_proj.weight
25-01-23 08:13:25 | I |   - Quantizing model.layers.0.mlp.gate_proj.weight
25-01-23 08:13:28 | I |   - Quantizing model.layers.0.mlp.down_proj.weight
25-01-23 08:13:35 | I | quantizing activations for layer model.layers.0
25-01-23 08:13:36 | I | collecting calibration activations in model.layers.0
25-01-23 08:13:36 | I | collecting calibration activations in model.layers.0
25-01-23 08:13:38 | I | forward this layer
25-01-23 08:13:38 | I | input_file: /data/gyy/lmquant-main/lmquant/data/data_without_preprocess_llama_7b_bs16/shard/val_input_args_layer_0/34.pt
25-01-23 08:13:38 | I | output_file: /data/gyy/lmquant-main/lmquant/data/data_without_preprocess_llama_7b_bs16/shard/val_teacher_output_layer_0/34.pt
25-01-23 08:13:38 | I | In layer model.layers.0, gradient of model.layers.0.self_attn.q_proj has nan: False
25-01-23 08:13:38 | I | In layer model.layers.0, gradient of model.layers.0.self_attn.q_proj has inf: False
25-01-23 08:13:38 | I | In layer model.layers.0, gradient of model.layers.0.self_attn.k_proj has nan: False
25-01-23 08:13:38 | I | In layer model.layers.0, gradient of model.layers.0.self_attn.k_proj has inf: False
25-01-23 08:13:38 | I | In layer model.layers.0, gradient of model.layers.0.self_attn.v_proj has nan: False
25-01-23 08:13:38 | I | In layer model.layers.0, gradient of model.layers.0.self_attn.v_proj has inf: False
25-01-23 08:13:38 | I | In layer model.layers.0, gradient of model.layers.0.self_attn.o_proj has nan: False
25-01-23 08:13:38 | I | In layer model.layers.0, gradient of model.layers.0.self_attn.o_proj has inf: False
25-01-23 08:13:38 | I | In layer model.layers.0, gradient of model.layers.0.mlp.up_proj has nan: False
25-01-23 08:13:38 | I | In layer model.layers.0, gradient of model.layers.0.mlp.up_proj has inf: False
25-01-23 08:13:38 | I | In layer model.layers.0, gradient of model.layers.0.mlp.gate_proj has nan: False
25-01-23 08:13:38 | I | In layer model.layers.0, gradient of model.layers.0.mlp.gate_proj has inf: False
25-01-23 08:13:38 | I | In layer model.layers.0, gradient of model.layers.0.mlp.down_proj has nan: False
25-01-23 08:13:38 | I | In layer model.layers.0, gradient of model.layers.0.mlp.down_proj has inf: False
25-01-23 08:13:38 | I | [30] done with optimizer step
25-01-23 08:13:38 | I | epoch 001:     35 / 819200000 loss=1.19089e-05, loss_per_token=0.0487788, loss_sum=399.596, wps=13.9, ups=0, wpb=8192, bsz=16, num_updates=31, lr=0.0031, gnorm=0.214, clip=0, loss_scale=8, train_wall=41, cuda_gb_allocated=17.2, cuda_gb_reserved=20.2, cuda_gb_free=6.5, wall=2966
25-01-23 08:13:38 | I | TRAIN CURRENT LAYER_IDX = 0
25-01-23 08:13:38 | I | in layer model.layers.0
25-01-23 08:13:38 | I | quantizing weights for layer model.layers.0
25-01-23 08:13:39 | I | collecting calibration activations in model.layers.0
25-01-23 08:13:39 | I | collecting calibration activations in model.layers.0
25-01-23 08:13:39 | I | - Quantizing decoder layer model.layers.0
25-01-23 08:13:39 | I |   - Calibrating model.layers.0.self_attn.q_proj.weight
25-01-23 08:13:40 | I |       - range scale = [    1.0000]
25-01-23 08:13:40 | I |         sum  error  = [    0.0472]
25-01-23 08:13:40 | I |         best error  = [    0.0472]
25-01-23 08:13:40 | I |     + error = [0.0472]
25-01-23 08:13:40 | I |       - range scale = [    1.0000]
25-01-23 08:13:40 | I |         sum  error  = [    0.7191]
25-01-23 08:13:40 | I |         best error  = [    0.7191]
25-01-23 08:13:40 | I |     + error = [0.7191]
25-01-23 08:13:41 | I |   - Calibrating model.layers.0.self_attn.k_proj.weight
25-01-23 08:13:41 | I |       - range scale = [    1.0000]
25-01-23 08:13:41 | I |         sum  error  = [    0.0601]
25-01-23 08:13:41 | I |         best error  = [    0.0601]
25-01-23 08:13:41 | I |     + error = [0.0601]
25-01-23 08:13:42 | I |       - range scale = [    1.0000]
25-01-23 08:13:42 | I |         sum  error  = [    0.9592]
25-01-23 08:13:42 | I |         best error  = [    0.9592]
25-01-23 08:13:42 | I |     + error = [0.9592]
25-01-23 08:13:42 | I |   - Calibrating model.layers.0.self_attn.v_proj.weight
25-01-23 08:13:43 | I |       - range scale = [    1.0000]
25-01-23 08:13:43 | I |         sum  error  = [    0.6987]
25-01-23 08:13:43 | I |         best error  = [    0.6987]
25-01-23 08:13:43 | I |     + error = [0.6987]
25-01-23 08:13:44 | I |       - range scale = [    1.0000]
25-01-23 08:13:44 | I |         sum  error  = [    6.5650]
25-01-23 08:13:44 | I |         best error  = [    6.5650]
25-01-23 08:13:44 | I |     + error = [6.5650]
25-01-23 08:13:44 | I |   - Calibrating model.layers.0.self_attn.o_proj.weight
25-01-23 08:13:44 | I |       - range scale = [    1.0000]
25-01-23 08:13:44 | I |         sum  error  = [    0.1973]
25-01-23 08:13:44 | I |         best error  = [    0.1973]
25-01-23 08:13:44 | I |     + error = [0.1973]
25-01-23 08:13:45 | I |       - range scale = [    1.0000]
25-01-23 08:13:45 | I |         sum  error  = [    1.5517]
25-01-23 08:13:45 | I |         best error  = [    1.5517]
25-01-23 08:13:45 | I |     + error = [1.5517]
25-01-23 08:13:45 | I |   - Calibrating model.layers.0.mlp.up_proj.weight
25-01-23 08:13:46 | I |       - range scale = [    1.0000]
25-01-23 08:13:46 | I |         sum  error  = [    1.1003]
25-01-23 08:13:46 | I |         best error  = [    1.1003]
25-01-23 08:13:46 | I |     + error = [1.1003]
25-01-23 08:13:47 | I |       - range scale = [    1.0000]
25-01-23 08:13:47 | I |         sum  error  = [   11.6809]
25-01-23 08:13:47 | I |         best error  = [   11.6809]
25-01-23 08:13:47 | I |     + error = [11.6809]
25-01-23 08:13:47 | I |   - Calibrating model.layers.0.mlp.gate_proj.weight
25-01-23 08:13:48 | I |       - range scale = [    1.0000]
25-01-23 08:13:48 | I |         sum  error  = [    1.1621]
25-01-23 08:13:48 | I |         best error  = [    1.1621]
25-01-23 08:13:48 | I |     + error = [1.1621]
25-01-23 08:13:49 | I |       - range scale = [    1.0000]
25-01-23 08:13:49 | I |         sum  error  = [   12.0624]
25-01-23 08:13:49 | I |         best error  = [   12.0624]
25-01-23 08:13:49 | I |     + error = [12.0624]
25-01-23 08:13:49 | I |   - Calibrating model.layers.0.mlp.down_proj.weight
25-01-23 08:13:50 | I |       - range scale = [    1.0000]
25-01-23 08:13:50 | I |         sum  error  = [    0.1725]
25-01-23 08:13:50 | I |         best error  = [    0.1725]
25-01-23 08:13:50 | I |     + error = [0.1725]
25-01-23 08:13:51 | I |       - range scale = [    1.0000]
25-01-23 08:13:51 | I |         sum  error  = [    1.6784]
25-01-23 08:13:51 | I |         best error  = [    1.6784]
25-01-23 08:13:51 | I |     + error = [1.6784]
25-01-23 08:13:52 | I |   - Quantizing model.layers.0.self_attn.q_proj.weight
25-01-23 08:13:54 | I |   - Quantizing model.layers.0.self_attn.k_proj.weight
25-01-23 08:13:56 | I |   - Quantizing model.layers.0.self_attn.v_proj.weight
25-01-23 08:13:58 | I |   - Quantizing model.layers.0.self_attn.o_proj.weight
25-01-23 08:14:01 | I |   - Quantizing model.layers.0.mlp.up_proj.weight
25-01-23 08:14:03 | I |   - Quantizing model.layers.0.mlp.gate_proj.weight
25-01-23 08:14:05 | I |   - Quantizing model.layers.0.mlp.down_proj.weight
25-01-23 08:14:11 | I | quantizing activations for layer model.layers.0
25-01-23 08:14:12 | I | collecting calibration activations in model.layers.0
25-01-23 08:14:12 | I | collecting calibration activations in model.layers.0
25-01-23 08:14:14 | I | forward this layer
25-01-23 08:14:14 | I | input_file: /data/gyy/lmquant-main/lmquant/data/data_without_preprocess_llama_7b_bs16/shard/val_input_args_layer_0/35.pt
25-01-23 08:14:14 | I | output_file: /data/gyy/lmquant-main/lmquant/data/data_without_preprocess_llama_7b_bs16/shard/val_teacher_output_layer_0/35.pt
25-01-23 08:14:14 | I | In layer model.layers.0, gradient of model.layers.0.self_attn.q_proj has nan: False
25-01-23 08:14:14 | I | In layer model.layers.0, gradient of model.layers.0.self_attn.q_proj has inf: False
25-01-23 08:14:14 | I | In layer model.layers.0, gradient of model.layers.0.self_attn.k_proj has nan: False
25-01-23 08:14:14 | I | In layer model.layers.0, gradient of model.layers.0.self_attn.k_proj has inf: False
25-01-23 08:14:14 | I | In layer model.layers.0, gradient of model.layers.0.self_attn.v_proj has nan: False
25-01-23 08:14:14 | I | In layer model.layers.0, gradient of model.layers.0.self_attn.v_proj has inf: False
25-01-23 08:14:14 | I | In layer model.layers.0, gradient of model.layers.0.self_attn.o_proj has nan: False
25-01-23 08:14:14 | I | In layer model.layers.0, gradient of model.layers.0.self_attn.o_proj has inf: False
25-01-23 08:14:14 | I | In layer model.layers.0, gradient of model.layers.0.mlp.up_proj has nan: False
25-01-23 08:14:14 | I | In layer model.layers.0, gradient of model.layers.0.mlp.up_proj has inf: False
25-01-23 08:14:14 | I | In layer model.layers.0, gradient of model.layers.0.mlp.gate_proj has nan: False
25-01-23 08:14:14 | I | In layer model.layers.0, gradient of model.layers.0.mlp.gate_proj has inf: False
25-01-23 08:14:14 | I | In layer model.layers.0, gradient of model.layers.0.mlp.down_proj has nan: False
25-01-23 08:14:14 | I | In layer model.layers.0, gradient of model.layers.0.mlp.down_proj has inf: False
25-01-23 08:14:14 | I | [31] done with optimizer step
25-01-23 08:14:14 | I | epoch 001:     36 / 819200000 loss=1.15375e-05, loss_per_token=0.0472576, loss_sum=387.134, wps=228.2, ups=0.03, wpb=8192, bsz=16, num_updates=32, lr=0.0032, gnorm=0.14, clip=0, loss_scale=8, train_wall=36, cuda_gb_allocated=17.1, cuda_gb_reserved=18.3, cuda_gb_free=6.6, wall=3002
25-01-23 08:14:14 | I | TRAIN CURRENT LAYER_IDX = 0
25-01-23 08:14:14 | I | in layer model.layers.0
25-01-23 08:14:14 | I | quantizing weights for layer model.layers.0
25-01-23 08:14:15 | I | collecting calibration activations in model.layers.0
25-01-23 08:14:15 | I | collecting calibration activations in model.layers.0
25-01-23 08:14:15 | I | - Quantizing decoder layer model.layers.0
25-01-23 08:14:15 | I |   - Calibrating model.layers.0.self_attn.q_proj.weight
25-01-23 08:14:16 | I |       - range scale = [    1.0000]
25-01-23 08:14:16 | I |         sum  error  = [    0.0416]
25-01-23 08:14:16 | I |         best error  = [    0.0416]
25-01-23 08:14:16 | I |     + error = [0.0416]
25-01-23 08:14:16 | I |       - range scale = [    1.0000]
25-01-23 08:14:16 | I |         sum  error  = [    0.6133]
25-01-23 08:14:16 | I |         best error  = [    0.6133]
25-01-23 08:14:16 | I |     + error = [0.6133]
25-01-23 08:14:16 | I |   - Calibrating model.layers.0.self_attn.k_proj.weight
25-01-23 08:14:17 | I |       - range scale = [    1.0000]
25-01-23 08:14:17 | I |         sum  error  = [    0.0493]
25-01-23 08:14:17 | I |         best error  = [    0.0493]
25-01-23 08:14:17 | I |     + error = [0.0493]
25-01-23 08:14:18 | I |       - range scale = [    1.0000]
25-01-23 08:14:18 | I |         sum  error  = [    0.9619]
25-01-23 08:14:18 | I |         best error  = [    0.9619]
25-01-23 08:14:18 | I |     + error = [0.9619]
25-01-23 08:14:18 | I |   - Calibrating model.layers.0.self_attn.v_proj.weight
25-01-23 08:14:19 | I |       - range scale = [    1.0000]
25-01-23 08:14:19 | I |         sum  error  = [    0.6519]
25-01-23 08:14:19 | I |         best error  = [    0.6519]
25-01-23 08:14:19 | I |     + error = [0.6519]
25-01-23 08:14:19 | I |       - range scale = [    1.0000]
25-01-23 08:14:19 | I |         sum  error  = [    6.0411]
25-01-23 08:14:19 | I |         best error  = [    6.0411]
25-01-23 08:14:19 | I |     + error = [6.0411]
25-01-23 08:14:20 | I |   - Calibrating model.layers.0.self_attn.o_proj.weight
25-01-23 08:14:20 | I |       - range scale = [    1.0000]
25-01-23 08:14:20 | I |         sum  error  = [    0.1850]
25-01-23 08:14:20 | I |         best error  = [    0.1850]
25-01-23 08:14:20 | I |     + error = [0.1850]
25-01-23 08:14:21 | I |       - range scale = [    1.0000]
25-01-23 08:14:21 | I |         sum  error  = [    1.4748]
25-01-23 08:14:21 | I |         best error  = [    1.4748]
25-01-23 08:14:21 | I |     + error = [1.4748]
25-01-23 08:14:21 | I |   - Calibrating model.layers.0.mlp.up_proj.weight
25-01-23 08:14:22 | I |       - range scale = [    1.0000]
25-01-23 08:14:22 | I |         sum  error  = [    1.0497]
25-01-23 08:14:22 | I |         best error  = [    1.0497]
25-01-23 08:14:22 | I |     + error = [1.0497]
25-01-23 08:14:23 | I |       - range scale = [    1.0000]
25-01-23 08:14:23 | I |         sum  error  = [   11.1552]
25-01-23 08:14:23 | I |         best error  = [   11.1552]
25-01-23 08:14:23 | I |     + error = [11.1552]
25-01-23 08:14:23 | I |   - Calibrating model.layers.0.mlp.gate_proj.weight
25-01-23 08:14:24 | I |       - range scale = [    1.0000]
25-01-23 08:14:24 | I |         sum  error  = [    1.1082]
25-01-23 08:14:24 | I |         best error  = [    1.1082]
25-01-23 08:14:24 | I |     + error = [1.1082]
25-01-23 08:14:25 | I |       - range scale = [    1.0000]
25-01-23 08:14:25 | I |         sum  error  = [   11.5190]
25-01-23 08:14:25 | I |         best error  = [   11.5190]
25-01-23 08:14:25 | I |     + error = [11.5190]
25-01-23 08:14:25 | I |   - Calibrating model.layers.0.mlp.down_proj.weight
25-01-23 08:14:26 | I |       - range scale = [    1.0000]
25-01-23 08:14:26 | I |         sum  error  = [    0.1657]
25-01-23 08:14:26 | I |         best error  = [    0.1657]
25-01-23 08:14:26 | I |     + error = [0.1657]
25-01-23 08:14:27 | I |       - range scale = [    1.0000]
25-01-23 08:14:27 | I |         sum  error  = [    1.6242]
25-01-23 08:14:27 | I |         best error  = [    1.6242]
25-01-23 08:14:27 | I |     + error = [1.6242]
25-01-23 08:14:27 | I |   - Quantizing model.layers.0.self_attn.q_proj.weight
25-01-23 08:14:30 | I |   - Quantizing model.layers.0.self_attn.k_proj.weight
25-01-23 08:14:32 | I |   - Quantizing model.layers.0.self_attn.v_proj.weight
25-01-23 08:14:34 | I |   - Quantizing model.layers.0.self_attn.o_proj.weight
25-01-23 08:14:37 | I |   - Quantizing model.layers.0.mlp.up_proj.weight
25-01-23 08:14:39 | I |   - Quantizing model.layers.0.mlp.gate_proj.weight
25-01-23 08:14:41 | I |   - Quantizing model.layers.0.mlp.down_proj.weight
25-01-23 08:14:47 | I | quantizing activations for layer model.layers.0
25-01-23 08:14:48 | I | collecting calibration activations in model.layers.0
25-01-23 08:14:48 | I | collecting calibration activations in model.layers.0
25-01-23 08:14:49 | I | forward this layer
25-01-23 08:14:49 | I | input_file: /data/gyy/lmquant-main/lmquant/data/data_without_preprocess_llama_7b_bs16/shard/val_input_args_layer_0/36.pt
25-01-23 08:14:49 | I | output_file: /data/gyy/lmquant-main/lmquant/data/data_without_preprocess_llama_7b_bs16/shard/val_teacher_output_layer_0/36.pt
25-01-23 08:14:50 | I | In layer model.layers.0, gradient of model.layers.0.self_attn.q_proj has nan: False
25-01-23 08:14:50 | I | In layer model.layers.0, gradient of model.layers.0.self_attn.q_proj has inf: False
25-01-23 08:14:50 | I | In layer model.layers.0, gradient of model.layers.0.self_attn.k_proj has nan: False
25-01-23 08:14:50 | I | In layer model.layers.0, gradient of model.layers.0.self_attn.k_proj has inf: False
25-01-23 08:14:50 | I | In layer model.layers.0, gradient of model.layers.0.self_attn.v_proj has nan: False
25-01-23 08:14:50 | I | In layer model.layers.0, gradient of model.layers.0.self_attn.v_proj has inf: False
25-01-23 08:14:50 | I | In layer model.layers.0, gradient of model.layers.0.self_attn.o_proj has nan: False
25-01-23 08:14:50 | I | In layer model.layers.0, gradient of model.layers.0.self_attn.o_proj has inf: False
25-01-23 08:14:50 | I | In layer model.layers.0, gradient of model.layers.0.mlp.up_proj has nan: False
25-01-23 08:14:50 | I | In layer model.layers.0, gradient of model.layers.0.mlp.up_proj has inf: False
25-01-23 08:14:50 | I | In layer model.layers.0, gradient of model.layers.0.mlp.gate_proj has nan: False
25-01-23 08:14:50 | I | In layer model.layers.0, gradient of model.layers.0.mlp.gate_proj has inf: False
25-01-23 08:14:50 | I | In layer model.layers.0, gradient of model.layers.0.mlp.down_proj has nan: False
25-01-23 08:14:50 | I | In layer model.layers.0, gradient of model.layers.0.mlp.down_proj has inf: False
25-01-23 08:14:50 | I | [32] done with optimizer step
25-01-23 08:14:50 | I | epoch 001:     37 / 819200000 loss=1.14747e-05, loss_per_token=0.0470002, loss_sum=385.026, wps=229.6, ups=0.03, wpb=8192, bsz=16, num_updates=33, lr=0.0033, gnorm=0.188, clip=0, loss_scale=8, train_wall=36, cuda_gb_allocated=17.1, cuda_gb_reserved=18.4, cuda_gb_free=6.6, wall=3037
25-01-23 08:14:50 | I | TRAIN CURRENT LAYER_IDX = 0
25-01-23 08:14:50 | I | in layer model.layers.0
25-01-23 08:14:50 | I | quantizing weights for layer model.layers.0
25-01-23 08:14:50 | I | collecting calibration activations in model.layers.0
25-01-23 08:14:50 | I | collecting calibration activations in model.layers.0
25-01-23 08:14:51 | I | - Quantizing decoder layer model.layers.0
25-01-23 08:14:51 | I |   - Calibrating model.layers.0.self_attn.q_proj.weight
25-01-23 08:14:51 | I |       - range scale = [    1.0000]
25-01-23 08:14:51 | I |         sum  error  = [    0.0435]
25-01-23 08:14:51 | I |         best error  = [    0.0435]
25-01-23 08:14:51 | I |     + error = [0.0435]
25-01-23 08:14:52 | I |       - range scale = [    1.0000]
25-01-23 08:14:52 | I |         sum  error  = [    0.6794]
25-01-23 08:14:52 | I |         best error  = [    0.6794]
25-01-23 08:14:52 | I |     + error = [0.6794]
25-01-23 08:14:52 | I |   - Calibrating model.layers.0.self_attn.k_proj.weight
25-01-23 08:14:53 | I |       - range scale = [    1.0000]
25-01-23 08:14:53 | I |         sum  error  = [    0.0552]
25-01-23 08:14:53 | I |         best error  = [    0.0552]
25-01-23 08:14:53 | I |     + error = [0.0552]
25-01-23 08:14:54 | I |       - range scale = [    1.0000]
25-01-23 08:14:54 | I |         sum  error  = [    0.9405]
25-01-23 08:14:54 | I |         best error  = [    0.9405]
25-01-23 08:14:54 | I |     + error = [0.9405]
25-01-23 08:14:54 | I |   - Calibrating model.layers.0.self_attn.v_proj.weight
25-01-23 08:14:54 | I |       - range scale = [    1.0000]
25-01-23 08:14:54 | I |         sum  error  = [    0.6782]
25-01-23 08:14:54 | I |         best error  = [    0.6782]
25-01-23 08:14:54 | I |     + error = [0.6782]
25-01-23 08:14:55 | I |       - range scale = [    1.0000]
25-01-23 08:14:55 | I |         sum  error  = [    6.2450]
25-01-23 08:14:55 | I |         best error  = [    6.2450]
25-01-23 08:14:55 | I |     + error = [6.2450]
25-01-23 08:14:55 | I |   - Calibrating model.layers.0.self_attn.o_proj.weight
25-01-23 08:14:56 | I |       - range scale = [    1.0000]
25-01-23 08:14:56 | I |         sum  error  = [    0.1966]
25-01-23 08:14:56 | I |         best error  = [    0.1966]
25-01-23 08:14:56 | I |     + error = [0.1966]
25-01-23 08:14:57 | I |       - range scale = [    1.0000]
25-01-23 08:14:57 | I |         sum  error  = [    1.5598]
25-01-23 08:14:57 | I |         best error  = [    1.5598]
25-01-23 08:14:57 | I |     + error = [1.5598]
25-01-23 08:14:57 | I |   - Calibrating model.layers.0.mlp.up_proj.weight
25-01-23 08:14:58 | I |       - range scale = [    1.0000]
25-01-23 08:14:58 | I |         sum  error  = [    1.0457]
25-01-23 08:14:58 | I |         best error  = [    1.0457]
25-01-23 08:14:58 | I |     + error = [1.0457]
25-01-23 08:14:59 | I |       - range scale = [    1.0000]
25-01-23 08:14:59 | I |         sum  error  = [   11.1044]
25-01-23 08:14:59 | I |         best error  = [   11.1044]
25-01-23 08:14:59 | I |     + error = [11.1044]
25-01-23 08:14:59 | I |   - Calibrating model.layers.0.mlp.gate_proj.weight
25-01-23 08:15:00 | I |       - range scale = [    1.0000]
25-01-23 08:15:00 | I |         sum  error  = [    1.1023]
25-01-23 08:15:00 | I |         best error  = [    1.1023]
25-01-23 08:15:00 | I |     + error = [1.1023]
25-01-23 08:15:01 | I |       - range scale = [    1.0000]
25-01-23 08:15:01 | I |         sum  error  = [   11.4773]
25-01-23 08:15:01 | I |         best error  = [   11.4773]
25-01-23 08:15:01 | I |     + error = [11.4773]
25-01-23 08:15:01 | I |   - Calibrating model.layers.0.mlp.down_proj.weight
25-01-23 08:15:02 | I |       - range scale = [    1.0000]
25-01-23 08:15:02 | I |         sum  error  = [    0.1639]
25-01-23 08:15:02 | I |         best error  = [    0.1639]
25-01-23 08:15:02 | I |     + error = [0.1639]
25-01-23 08:15:03 | I |       - range scale = [    1.0000]
25-01-23 08:15:03 | I |         sum  error  = [    1.6006]
25-01-23 08:15:03 | I |         best error  = [    1.6006]
25-01-23 08:15:03 | I |     + error = [1.6006]
25-01-23 08:15:03 | I |   - Quantizing model.layers.0.self_attn.q_proj.weight
25-01-23 08:15:06 | I |   - Quantizing model.layers.0.self_attn.k_proj.weight
25-01-23 08:15:08 | I |   - Quantizing model.layers.0.self_attn.v_proj.weight
25-01-23 08:15:10 | I |   - Quantizing model.layers.0.self_attn.o_proj.weight
25-01-23 08:15:12 | I |   - Quantizing model.layers.0.mlp.up_proj.weight
25-01-23 08:15:15 | I |   - Quantizing model.layers.0.mlp.gate_proj.weight
25-01-23 08:15:17 | I |   - Quantizing model.layers.0.mlp.down_proj.weight
25-01-23 08:15:23 | I | quantizing activations for layer model.layers.0
25-01-23 08:15:23 | I | collecting calibration activations in model.layers.0
25-01-23 08:15:23 | I | collecting calibration activations in model.layers.0
25-01-23 08:15:25 | I | forward this layer
25-01-23 08:15:25 | I | input_file: /data/gyy/lmquant-main/lmquant/data/data_without_preprocess_llama_7b_bs16/shard/val_input_args_layer_0/37.pt
25-01-23 08:15:25 | I | output_file: /data/gyy/lmquant-main/lmquant/data/data_without_preprocess_llama_7b_bs16/shard/val_teacher_output_layer_0/37.pt
25-01-23 08:15:26 | I | In layer model.layers.0, gradient of model.layers.0.self_attn.q_proj has nan: False
25-01-23 08:15:26 | I | In layer model.layers.0, gradient of model.layers.0.self_attn.q_proj has inf: False
25-01-23 08:15:26 | I | In layer model.layers.0, gradient of model.layers.0.self_attn.k_proj has nan: False
25-01-23 08:15:26 | I | In layer model.layers.0, gradient of model.layers.0.self_attn.k_proj has inf: False
25-01-23 08:15:26 | I | In layer model.layers.0, gradient of model.layers.0.self_attn.v_proj has nan: False
25-01-23 08:15:26 | I | In layer model.layers.0, gradient of model.layers.0.self_attn.v_proj has inf: False
25-01-23 08:15:26 | I | In layer model.layers.0, gradient of model.layers.0.self_attn.o_proj has nan: False
25-01-23 08:15:26 | I | In layer model.layers.0, gradient of model.layers.0.self_attn.o_proj has inf: False
25-01-23 08:15:26 | I | In layer model.layers.0, gradient of model.layers.0.mlp.up_proj has nan: False
25-01-23 08:15:26 | I | In layer model.layers.0, gradient of model.layers.0.mlp.up_proj has inf: False
25-01-23 08:15:26 | I | In layer model.layers.0, gradient of model.layers.0.mlp.gate_proj has nan: False
25-01-23 08:15:26 | I | In layer model.layers.0, gradient of model.layers.0.mlp.gate_proj has inf: False
25-01-23 08:15:26 | I | In layer model.layers.0, gradient of model.layers.0.mlp.down_proj has nan: False
25-01-23 08:15:26 | I | In layer model.layers.0, gradient of model.layers.0.mlp.down_proj has inf: False
25-01-23 08:15:26 | I | [33] done with optimizer step
25-01-23 08:15:26 | I | epoch 001:     38 / 819200000 loss=1.34185e-05, loss_per_token=0.0549621, loss_sum=450.25, wps=228.4, ups=0.03, wpb=8192, bsz=16, num_updates=34, lr=0.0034, gnorm=0.219, clip=0, loss_scale=8, train_wall=36, cuda_gb_allocated=17.1, cuda_gb_reserved=18.7, cuda_gb_free=6.6, wall=3073
25-01-23 08:15:26 | I | TRAIN CURRENT LAYER_IDX = 0
25-01-23 08:15:26 | I | in layer model.layers.0
25-01-23 08:15:26 | I | quantizing weights for layer model.layers.0
25-01-23 08:15:26 | I | collecting calibration activations in model.layers.0
25-01-23 08:15:26 | I | collecting calibration activations in model.layers.0
25-01-23 08:15:26 | I | - Quantizing decoder layer model.layers.0
25-01-23 08:15:26 | I |   - Calibrating model.layers.0.self_attn.q_proj.weight
25-01-23 08:15:27 | I |       - range scale = [    1.0000]
25-01-23 08:15:27 | I |         sum  error  = [    0.0441]
25-01-23 08:15:27 | I |         best error  = [    0.0441]
25-01-23 08:15:27 | I |     + error = [0.0441]
25-01-23 08:15:28 | I |       - range scale = [    1.0000]
25-01-23 08:15:28 | I |         sum  error  = [    0.6838]
25-01-23 08:15:28 | I |         best error  = [    0.6838]
25-01-23 08:15:28 | I |     + error = [0.6838]
25-01-23 08:15:28 | I |   - Calibrating model.layers.0.self_attn.k_proj.weight
25-01-23 08:15:29 | I |       - range scale = [    1.0000]
25-01-23 08:15:29 | I |         sum  error  = [    0.0576]
25-01-23 08:15:29 | I |         best error  = [    0.0576]
25-01-23 08:15:29 | I |     + error = [0.0576]
25-01-23 08:15:29 | I |       - range scale = [    1.0000]
25-01-23 08:15:29 | I |         sum  error  = [    0.9558]
25-01-23 08:15:29 | I |         best error  = [    0.9558]
25-01-23 08:15:29 | I |     + error = [0.9558]
25-01-23 08:15:30 | I |   - Calibrating model.layers.0.self_attn.v_proj.weight
25-01-23 08:15:30 | I |       - range scale = [    1.0000]
25-01-23 08:15:30 | I |         sum  error  = [    0.6851]
25-01-23 08:15:30 | I |         best error  = [    0.6851]
25-01-23 08:15:30 | I |     + error = [0.6851]
25-01-23 08:15:31 | I |       - range scale = [    1.0000]
25-01-23 08:15:31 | I |         sum  error  = [    6.0940]
25-01-23 08:15:31 | I |         best error  = [    6.0940]
25-01-23 08:15:31 | I |     + error = [6.0940]
25-01-23 08:15:31 | I |   - Calibrating model.layers.0.self_attn.o_proj.weight
25-01-23 08:15:32 | I |       - range scale = [    1.0000]
25-01-23 08:15:32 | I |         sum  error  = [    0.1978]
25-01-23 08:15:32 | I |         best error  = [    0.1978]
25-01-23 08:15:32 | I |     + error = [0.1978]
25-01-23 08:15:33 | I |       - range scale = [    1.0000]
25-01-23 08:15:33 | I |         sum  error  = [    1.5739]
25-01-23 08:15:33 | I |         best error  = [    1.5739]
25-01-23 08:15:33 | I |     + error = [1.5739]
25-01-23 08:15:33 | I |   - Calibrating model.layers.0.mlp.up_proj.weight
25-01-23 08:15:34 | I |       - range scale = [    1.0000]
25-01-23 08:15:34 | I |         sum  error  = [    0.9772]
25-01-23 08:15:34 | I |         best error  = [    0.9772]
25-01-23 08:15:34 | I |     + error = [0.9772]
25-01-23 08:15:35 | I |       - range scale = [    1.0000]
25-01-23 08:15:35 | I |         sum  error  = [   10.3243]
25-01-23 08:15:35 | I |         best error  = [   10.3243]
25-01-23 08:15:35 | I |     + error = [10.3243]
25-01-23 08:15:35 | I |   - Calibrating model.layers.0.mlp.gate_proj.weight
25-01-23 08:15:36 | I |       - range scale = [    1.0000]
25-01-23 08:15:36 | I |         sum  error  = [    1.0238]
25-01-23 08:15:36 | I |         best error  = [    1.0238]
25-01-23 08:15:36 | I |     + error = [1.0238]
25-01-23 08:15:37 | I |       - range scale = [    1.0000]
25-01-23 08:15:37 | I |         sum  error  = [   10.6949]
25-01-23 08:15:37 | I |         best error  = [   10.6949]
25-01-23 08:15:37 | I |     + error = [10.6949]
25-01-23 08:15:37 | I |   - Calibrating model.layers.0.mlp.down_proj.weight
25-01-23 08:15:38 | I |       - range scale = [    1.0000]
25-01-23 08:15:38 | I |         sum  error  = [    0.1489]
25-01-23 08:15:38 | I |         best error  = [    0.1489]
25-01-23 08:15:38 | I |     + error = [0.1489]
25-01-23 08:15:39 | I |       - range scale = [    1.0000]
25-01-23 08:15:39 | I |         sum  error  = [    1.4498]
25-01-23 08:15:39 | I |         best error  = [    1.4498]
25-01-23 08:15:39 | I |     + error = [1.4498]
25-01-23 08:15:39 | I |   - Quantizing model.layers.0.self_attn.q_proj.weight
25-01-23 08:15:42 | I |   - Quantizing model.layers.0.self_attn.k_proj.weight
25-01-23 08:15:45 | I |   - Quantizing model.layers.0.self_attn.v_proj.weight
25-01-23 08:15:48 | I |   - Quantizing model.layers.0.self_attn.o_proj.weight
25-01-23 08:15:51 | I |   - Quantizing model.layers.0.mlp.up_proj.weight
25-01-23 08:15:54 | I |   - Quantizing model.layers.0.mlp.gate_proj.weight
25-01-23 08:15:57 | I |   - Quantizing model.layers.0.mlp.down_proj.weight
25-01-23 08:16:04 | I | quantizing activations for layer model.layers.0
25-01-23 08:16:05 | I | collecting calibration activations in model.layers.0
25-01-23 08:16:05 | I | collecting calibration activations in model.layers.0
25-01-23 08:16:07 | I | forward this layer
25-01-23 08:16:07 | I | input_file: /data/gyy/lmquant-main/lmquant/data/data_without_preprocess_llama_7b_bs16/shard/val_input_args_layer_0/38.pt
25-01-23 08:16:07 | I | output_file: /data/gyy/lmquant-main/lmquant/data/data_without_preprocess_llama_7b_bs16/shard/val_teacher_output_layer_0/38.pt
25-01-23 08:16:07 | I | In layer model.layers.0, gradient of model.layers.0.self_attn.q_proj has nan: False
25-01-23 08:16:07 | I | In layer model.layers.0, gradient of model.layers.0.self_attn.q_proj has inf: False
25-01-23 08:16:07 | I | In layer model.layers.0, gradient of model.layers.0.self_attn.k_proj has nan: False
25-01-23 08:16:07 | I | In layer model.layers.0, gradient of model.layers.0.self_attn.k_proj has inf: False
25-01-23 08:16:07 | I | In layer model.layers.0, gradient of model.layers.0.self_attn.v_proj has nan: False
25-01-23 08:16:07 | I | In layer model.layers.0, gradient of model.layers.0.self_attn.v_proj has inf: False
25-01-23 08:16:07 | I | In layer model.layers.0, gradient of model.layers.0.self_attn.o_proj has nan: False
25-01-23 08:16:07 | I | In layer model.layers.0, gradient of model.layers.0.self_attn.o_proj has inf: False
25-01-23 08:16:07 | I | In layer model.layers.0, gradient of model.layers.0.mlp.up_proj has nan: False
25-01-23 08:16:07 | I | In layer model.layers.0, gradient of model.layers.0.mlp.up_proj has inf: False
25-01-23 08:16:07 | I | In layer model.layers.0, gradient of model.layers.0.mlp.gate_proj has nan: False
25-01-23 08:16:07 | I | In layer model.layers.0, gradient of model.layers.0.mlp.gate_proj has inf: False
25-01-23 08:16:07 | I | In layer model.layers.0, gradient of model.layers.0.mlp.down_proj has nan: False
25-01-23 08:16:07 | I | In layer model.layers.0, gradient of model.layers.0.mlp.down_proj has inf: False
25-01-23 08:16:07 | I | [34] done with optimizer step
25-01-23 08:16:07 | I | epoch 001:     39 / 819200000 loss=1.29269e-05, loss_per_token=0.0529487, loss_sum=433.756, wps=197, ups=0.02, wpb=8192, bsz=16, num_updates=35, lr=0.0035, gnorm=0.413, clip=0, loss_scale=8, train_wall=41, cuda_gb_allocated=17.1, cuda_gb_reserved=18.4, cuda_gb_free=6.6, wall=3115
25-01-23 08:16:07 | I | TRAIN CURRENT LAYER_IDX = 0
25-01-23 08:16:07 | I | in layer model.layers.0
25-01-23 08:16:07 | I | quantizing weights for layer model.layers.0
25-01-23 08:16:08 | I | collecting calibration activations in model.layers.0
25-01-23 08:16:08 | I | collecting calibration activations in model.layers.0
25-01-23 08:16:08 | I | - Quantizing decoder layer model.layers.0
25-01-23 08:16:08 | I |   - Calibrating model.layers.0.self_attn.q_proj.weight
25-01-23 08:16:09 | I |       - range scale = [    1.0000]
25-01-23 08:16:09 | I |         sum  error  = [    0.0449]
25-01-23 08:16:09 | I |         best error  = [    0.0449]
25-01-23 08:16:09 | I |     + error = [0.0449]
25-01-23 08:16:09 | I |       - range scale = [    1.0000]
25-01-23 08:16:09 | I |         sum  error  = [    0.6982]
25-01-23 08:16:09 | I |         best error  = [    0.6982]
25-01-23 08:16:09 | I |     + error = [0.6982]
25-01-23 08:16:10 | I |   - Calibrating model.layers.0.self_attn.k_proj.weight
25-01-23 08:16:10 | I |       - range scale = [    1.0000]
25-01-23 08:16:10 | I |         sum  error  = [    0.0573]
25-01-23 08:16:10 | I |         best error  = [    0.0573]
25-01-23 08:16:10 | I |     + error = [0.0573]
25-01-23 08:16:11 | I |       - range scale = [    1.0000]
25-01-23 08:16:11 | I |         sum  error  = [    0.9347]
25-01-23 08:16:11 | I |         best error  = [    0.9347]
25-01-23 08:16:11 | I |     + error = [0.9347]
25-01-23 08:16:11 | I |   - Calibrating model.layers.0.self_attn.v_proj.weight
25-01-23 08:16:12 | I |       - range scale = [    1.0000]
25-01-23 08:16:12 | I |         sum  error  = [    0.6868]
25-01-23 08:16:12 | I |         best error  = [    0.6868]
25-01-23 08:16:12 | I |     + error = [0.6868]
25-01-23 08:16:13 | I |       - range scale = [    1.0000]
25-01-23 08:16:13 | I |         sum  error  = [    6.3532]
25-01-23 08:16:13 | I |         best error  = [    6.3532]
25-01-23 08:16:13 | I |     + error = [6.3532]
25-01-23 08:16:13 | I |   - Calibrating model.layers.0.self_attn.o_proj.weight
25-01-23 08:16:14 | I |       - range scale = [    1.0000]
25-01-23 08:16:14 | I |         sum  error  = [    0.1898]
25-01-23 08:16:14 | I |         best error  = [    0.1898]
25-01-23 08:16:14 | I |     + error = [0.1898]
25-01-23 08:16:14 | I |       - range scale = [    1.0000]
25-01-23 08:16:14 | I |         sum  error  = [    1.5218]
25-01-23 08:16:14 | I |         best error  = [    1.5218]
25-01-23 08:16:14 | I |     + error = [1.5218]
25-01-23 08:16:15 | I |   - Calibrating model.layers.0.mlp.up_proj.weight
25-01-23 08:16:15 | I |       - range scale = [    1.0000]
25-01-23 08:16:15 | I |         sum  error  = [    1.0657]
25-01-23 08:16:15 | I |         best error  = [    1.0657]
25-01-23 08:16:15 | I |     + error = [1.0657]
25-01-23 08:16:16 | I |       - range scale = [    1.0000]
25-01-23 08:16:16 | I |         sum  error  = [   11.3200]
25-01-23 08:16:16 | I |         best error  = [   11.3200]
25-01-23 08:16:16 | I |     + error = [11.3200]
25-01-23 08:16:17 | I |   - Calibrating model.layers.0.mlp.gate_proj.weight
25-01-23 08:16:17 | I |       - range scale = [    1.0000]
25-01-23 08:16:17 | I |         sum  error  = [    1.1277]
25-01-23 08:16:17 | I |         best error  = [    1.1277]
25-01-23 08:16:17 | I |     + error = [1.1277]
25-01-23 08:16:18 | I |       - range scale = [    1.0000]
25-01-23 08:16:18 | I |         sum  error  = [   11.7030]
25-01-23 08:16:18 | I |         best error  = [   11.7030]
25-01-23 08:16:18 | I |     + error = [11.7030]
25-01-23 08:16:19 | I |   - Calibrating model.layers.0.mlp.down_proj.weight
25-01-23 08:16:19 | I |       - range scale = [    1.0000]
25-01-23 08:16:19 | I |         sum  error  = [    0.1693]
25-01-23 08:16:19 | I |         best error  = [    0.1693]
25-01-23 08:16:19 | I |     + error = [0.1693]
25-01-23 08:16:20 | I |       - range scale = [    1.0000]
25-01-23 08:16:20 | I |         sum  error  = [    1.6565]
25-01-23 08:16:20 | I |         best error  = [    1.6565]
25-01-23 08:16:20 | I |     + error = [1.6565]
25-01-23 08:16:21 | I |   - Quantizing model.layers.0.self_attn.q_proj.weight
25-01-23 08:16:23 | I |   - Quantizing model.layers.0.self_attn.k_proj.weight
25-01-23 08:16:25 | I |   - Quantizing model.layers.0.self_attn.v_proj.weight
25-01-23 08:16:28 | I |   - Quantizing model.layers.0.self_attn.o_proj.weight
25-01-23 08:16:30 | I |   - Quantizing model.layers.0.mlp.up_proj.weight
25-01-23 08:16:32 | I |   - Quantizing model.layers.0.mlp.gate_proj.weight
25-01-23 08:16:35 | I |   - Quantizing model.layers.0.mlp.down_proj.weight
25-01-23 08:16:41 | I | quantizing activations for layer model.layers.0
25-01-23 08:16:41 | I | collecting calibration activations in model.layers.0
25-01-23 08:16:41 | I | collecting calibration activations in model.layers.0
25-01-23 08:16:43 | I | forward this layer
25-01-23 08:16:43 | I | input_file: /data/gyy/lmquant-main/lmquant/data/data_without_preprocess_llama_7b_bs16/shard/val_input_args_layer_0/39.pt
25-01-23 08:16:43 | I | output_file: /data/gyy/lmquant-main/lmquant/data/data_without_preprocess_llama_7b_bs16/shard/val_teacher_output_layer_0/39.pt
25-01-23 08:16:43 | I | In layer model.layers.0, gradient of model.layers.0.self_attn.q_proj has nan: False
25-01-23 08:16:43 | I | In layer model.layers.0, gradient of model.layers.0.self_attn.q_proj has inf: False
25-01-23 08:16:43 | I | In layer model.layers.0, gradient of model.layers.0.self_attn.k_proj has nan: False
25-01-23 08:16:43 | I | In layer model.layers.0, gradient of model.layers.0.self_attn.k_proj has inf: False
25-01-23 08:16:43 | I | In layer model.layers.0, gradient of model.layers.0.self_attn.v_proj has nan: False
25-01-23 08:16:43 | I | In layer model.layers.0, gradient of model.layers.0.self_attn.v_proj has inf: False
25-01-23 08:16:43 | I | In layer model.layers.0, gradient of model.layers.0.self_attn.o_proj has nan: False
25-01-23 08:16:43 | I | In layer model.layers.0, gradient of model.layers.0.self_attn.o_proj has inf: False
25-01-23 08:16:43 | I | In layer model.layers.0, gradient of model.layers.0.mlp.up_proj has nan: False
25-01-23 08:16:43 | I | In layer model.layers.0, gradient of model.layers.0.mlp.up_proj has inf: False
25-01-23 08:16:43 | I | In layer model.layers.0, gradient of model.layers.0.mlp.gate_proj has nan: False
25-01-23 08:16:43 | I | In layer model.layers.0, gradient of model.layers.0.mlp.gate_proj has inf: False
25-01-23 08:16:43 | I | In layer model.layers.0, gradient of model.layers.0.mlp.down_proj has nan: False
25-01-23 08:16:43 | I | In layer model.layers.0, gradient of model.layers.0.mlp.down_proj has inf: False
25-01-23 08:16:43 | I | [35] done with optimizer step
25-01-23 08:16:43 | I | epoch 001:     40 / 819200000 loss=1.19109e-05, loss_per_token=0.0487872, loss_sum=399.664, wps=227.5, ups=0.03, wpb=8192, bsz=16, num_updates=36, lr=0.0036, gnorm=0.476, clip=0, loss_scale=8, train_wall=36, cuda_gb_allocated=17.1, cuda_gb_reserved=18.3, cuda_gb_free=6.6, wall=3151
25-01-23 08:16:43 | I | TRAIN CURRENT LAYER_IDX = 0
25-01-23 08:16:43 | I | in layer model.layers.0
25-01-23 08:16:43 | I | quantizing weights for layer model.layers.0
25-01-23 08:16:44 | I | collecting calibration activations in model.layers.0
25-01-23 08:16:44 | I | collecting calibration activations in model.layers.0
25-01-23 08:16:44 | I | - Quantizing decoder layer model.layers.0
25-01-23 08:16:44 | I |   - Calibrating model.layers.0.self_attn.q_proj.weight
25-01-23 08:16:45 | I |       - range scale = [    1.0000]
25-01-23 08:16:45 | I |         sum  error  = [    0.0451]
25-01-23 08:16:45 | I |         best error  = [    0.0451]
25-01-23 08:16:45 | I |     + error = [0.0451]
25-01-23 08:16:45 | I |       - range scale = [    1.0000]
25-01-23 08:16:45 | I |         sum  error  = [    0.6869]
25-01-23 08:16:45 | I |         best error  = [    0.6869]
25-01-23 08:16:45 | I |     + error = [0.6869]
25-01-23 08:16:46 | I |   - Calibrating model.layers.0.self_attn.k_proj.weight
25-01-23 08:16:46 | I |       - range scale = [    1.0000]
25-01-23 08:16:46 | I |         sum  error  = [    0.0555]
25-01-23 08:16:46 | I |         best error  = [    0.0555]
25-01-23 08:16:46 | I |     + error = [0.0555]
25-01-23 08:16:47 | I |       - range scale = [    1.0000]
25-01-23 08:16:47 | I |         sum  error  = [    0.8532]
25-01-23 08:16:47 | I |         best error  = [    0.8532]
25-01-23 08:16:47 | I |     + error = [0.8532]
25-01-23 08:16:47 | I |   - Calibrating model.layers.0.self_attn.v_proj.weight
25-01-23 08:16:48 | I |       - range scale = [    1.0000]
25-01-23 08:16:48 | I |         sum  error  = [    0.6868]
25-01-23 08:16:48 | I |         best error  = [    0.6868]
25-01-23 08:16:48 | I |     + error = [0.6868]
25-01-23 08:16:49 | I |       - range scale = [    1.0000]
25-01-23 08:16:49 | I |         sum  error  = [    6.3564]
25-01-23 08:16:49 | I |         best error  = [    6.3564]
25-01-23 08:16:49 | I |     + error = [6.3564]
25-01-23 08:16:49 | I |   - Calibrating model.layers.0.self_attn.o_proj.weight
25-01-23 08:16:50 | I |       - range scale = [    1.0000]
25-01-23 08:16:50 | I |         sum  error  = [    0.1988]
25-01-23 08:16:50 | I |         best error  = [    0.1988]
25-01-23 08:16:50 | I |     + error = [0.1988]
25-01-23 08:16:50 | I |       - range scale = [    1.0000]
25-01-23 08:16:50 | I |         sum  error  = [    1.5899]
25-01-23 08:16:50 | I |         best error  = [    1.5899]
25-01-23 08:16:50 | I |     + error = [1.5899]
25-01-23 08:16:51 | I |   - Calibrating model.layers.0.mlp.up_proj.weight
25-01-23 08:16:51 | I |       - range scale = [    1.0000]
25-01-23 08:16:51 | I |         sum  error  = [    1.0791]
25-01-23 08:16:51 | I |         best error  = [    1.0791]
25-01-23 08:16:51 | I |     + error = [1.0791]
25-01-23 08:16:52 | I |       - range scale = [    1.0000]
25-01-23 08:16:52 | I |         sum  error  = [   11.4410]
25-01-23 08:16:52 | I |         best error  = [   11.4410]
25-01-23 08:16:52 | I |     + error = [11.4410]
25-01-23 08:16:53 | I |   - Calibrating model.layers.0.mlp.gate_proj.weight
25-01-23 08:16:53 | I |       - range scale = [    1.0000]
25-01-23 08:16:53 | I |         sum  error  = [    1.1404]
25-01-23 08:16:53 | I |         best error  = [    1.1404]
25-01-23 08:16:53 | I |     + error = [1.1404]
25-01-23 08:16:54 | I |       - range scale = [    1.0000]
25-01-23 08:16:54 | I |         sum  error  = [   11.8347]
25-01-23 08:16:54 | I |         best error  = [   11.8347]
25-01-23 08:16:54 | I |     + error = [11.8347]
25-01-23 08:16:55 | I |   - Calibrating model.layers.0.mlp.down_proj.weight
25-01-23 08:16:55 | I |       - range scale = [    1.0000]
25-01-23 08:16:55 | I |         sum  error  = [    0.1709]
25-01-23 08:16:55 | I |         best error  = [    0.1709]
25-01-23 08:16:55 | I |     + error = [0.1709]
25-01-23 08:16:57 | I |       - range scale = [    1.0000]
25-01-23 08:16:57 | I |         sum  error  = [    1.6706]
25-01-23 08:16:57 | I |         best error  = [    1.6706]
25-01-23 08:16:57 | I |     + error = [1.6706]
25-01-23 08:16:57 | I |   - Quantizing model.layers.0.self_attn.q_proj.weight
25-01-23 08:16:59 | I |   - Quantizing model.layers.0.self_attn.k_proj.weight
25-01-23 08:17:01 | I |   - Quantizing model.layers.0.self_attn.v_proj.weight
25-01-23 08:17:04 | I |   - Quantizing model.layers.0.self_attn.o_proj.weight
25-01-23 08:17:06 | I |   - Quantizing model.layers.0.mlp.up_proj.weight
25-01-23 08:17:08 | I |   - Quantizing model.layers.0.mlp.gate_proj.weight
25-01-23 08:17:11 | I |   - Quantizing model.layers.0.mlp.down_proj.weight
25-01-23 08:17:17 | I | quantizing activations for layer model.layers.0
25-01-23 08:17:17 | I | collecting calibration activations in model.layers.0
25-01-23 08:17:17 | I | collecting calibration activations in model.layers.0
25-01-23 08:17:19 | I | forward this layer
25-01-23 08:17:19 | I | input_file: /data/gyy/lmquant-main/lmquant/data/data_without_preprocess_llama_7b_bs16/shard/val_input_args_layer_0/40.pt
25-01-23 08:17:19 | I | output_file: /data/gyy/lmquant-main/lmquant/data/data_without_preprocess_llama_7b_bs16/shard/val_teacher_output_layer_0/40.pt
25-01-23 08:17:19 | I | In layer model.layers.0, gradient of model.layers.0.self_attn.q_proj has nan: False
25-01-23 08:17:19 | I | In layer model.layers.0, gradient of model.layers.0.self_attn.q_proj has inf: False
25-01-23 08:17:19 | I | In layer model.layers.0, gradient of model.layers.0.self_attn.k_proj has nan: False
25-01-23 08:17:19 | I | In layer model.layers.0, gradient of model.layers.0.self_attn.k_proj has inf: False
25-01-23 08:17:19 | I | In layer model.layers.0, gradient of model.layers.0.self_attn.v_proj has nan: False
25-01-23 08:17:19 | I | In layer model.layers.0, gradient of model.layers.0.self_attn.v_proj has inf: False
25-01-23 08:17:19 | I | In layer model.layers.0, gradient of model.layers.0.self_attn.o_proj has nan: False
25-01-23 08:17:19 | I | In layer model.layers.0, gradient of model.layers.0.self_attn.o_proj has inf: False
25-01-23 08:17:19 | I | In layer model.layers.0, gradient of model.layers.0.mlp.up_proj has nan: False
25-01-23 08:17:19 | I | In layer model.layers.0, gradient of model.layers.0.mlp.up_proj has inf: False
25-01-23 08:17:19 | I | In layer model.layers.0, gradient of model.layers.0.mlp.gate_proj has nan: False
25-01-23 08:17:19 | I | In layer model.layers.0, gradient of model.layers.0.mlp.gate_proj has inf: False
25-01-23 08:17:19 | I | In layer model.layers.0, gradient of model.layers.0.mlp.down_proj has nan: False
25-01-23 08:17:19 | I | In layer model.layers.0, gradient of model.layers.0.mlp.down_proj has inf: False
25-01-23 08:17:19 | I | [36] done with optimizer step
25-01-23 08:17:19 | I | epoch 001:     41 / 819200000 loss=1.37118e-05, loss_per_token=0.0561637, loss_sum=460.093, wps=226.9, ups=0.03, wpb=8192, bsz=16, num_updates=37, lr=0.0037, gnorm=0.509, clip=0, loss_scale=8, train_wall=36, cuda_gb_allocated=17.1, cuda_gb_reserved=18.3, cuda_gb_free=6.6, wall=3187
25-01-23 08:17:19 | I | TRAIN CURRENT LAYER_IDX = 0
25-01-23 08:17:19 | I | in layer model.layers.0
25-01-23 08:17:19 | I | quantizing weights for layer model.layers.0
25-01-23 08:17:20 | I | collecting calibration activations in model.layers.0
25-01-23 08:17:20 | I | collecting calibration activations in model.layers.0
25-01-23 08:17:20 | I | - Quantizing decoder layer model.layers.0
25-01-23 08:17:20 | I |   - Calibrating model.layers.0.self_attn.q_proj.weight
25-01-23 08:17:21 | I |       - range scale = [    1.0000]
25-01-23 08:17:21 | I |         sum  error  = [    0.0443]
25-01-23 08:17:21 | I |         best error  = [    0.0443]
25-01-23 08:17:21 | I |     + error = [0.0443]
25-01-23 08:17:22 | I |       - range scale = [    1.0000]
25-01-23 08:17:22 | I |         sum  error  = [    0.6996]
25-01-23 08:17:22 | I |         best error  = [    0.6996]
25-01-23 08:17:22 | I |     + error = [0.6996]
25-01-23 08:17:22 | I |   - Calibrating model.layers.0.self_attn.k_proj.weight
25-01-23 08:17:22 | I |       - range scale = [    1.0000]
25-01-23 08:17:22 | I |         sum  error  = [    0.0558]
25-01-23 08:17:22 | I |         best error  = [    0.0558]
25-01-23 08:17:22 | I |     + error = [0.0558]
25-01-23 08:17:23 | I |       - range scale = [    1.0000]
25-01-23 08:17:23 | I |         sum  error  = [    0.9566]
25-01-23 08:17:23 | I |         best error  = [    0.9566]
25-01-23 08:17:23 | I |     + error = [0.9566]
25-01-23 08:17:23 | I |   - Calibrating model.layers.0.self_attn.v_proj.weight
25-01-23 08:17:24 | I |       - range scale = [    1.0000]
25-01-23 08:17:24 | I |         sum  error  = [    0.6919]
25-01-23 08:17:24 | I |         best error  = [    0.6919]
25-01-23 08:17:24 | I |     + error = [0.6919]
25-01-23 08:17:25 | I |       - range scale = [    1.0000]
25-01-23 08:17:25 | I |         sum  error  = [    6.4391]
25-01-23 08:17:25 | I |         best error  = [    6.4391]
25-01-23 08:17:25 | I |     + error = [6.4391]
25-01-23 08:17:25 | I |   - Calibrating model.layers.0.self_attn.o_proj.weight
25-01-23 08:17:26 | I |       - range scale = [    1.0000]
25-01-23 08:17:26 | I |         sum  error  = [    0.1929]
25-01-23 08:17:26 | I |         best error  = [    0.1929]
25-01-23 08:17:26 | I |     + error = [0.1929]
25-01-23 08:17:26 | I |       - range scale = [    1.0000]
25-01-23 08:17:26 | I |         sum  error  = [    1.5424]
25-01-23 08:17:26 | I |         best error  = [    1.5424]
25-01-23 08:17:26 | I |     + error = [1.5424]
25-01-23 08:17:27 | I |   - Calibrating model.layers.0.mlp.up_proj.weight
25-01-23 08:17:27 | I |       - range scale = [    1.0000]
25-01-23 08:17:27 | I |         sum  error  = [    1.0972]
25-01-23 08:17:27 | I |         best error  = [    1.0972]
25-01-23 08:17:27 | I |     + error = [1.0972]
25-01-23 08:17:28 | I |       - range scale = [    1.0000]
25-01-23 08:17:28 | I |         sum  error  = [   11.6606]
25-01-23 08:17:28 | I |         best error  = [   11.6606]
25-01-23 08:17:28 | I |     + error = [11.6606]
25-01-23 08:17:29 | I |   - Calibrating model.layers.0.mlp.gate_proj.weight
25-01-23 08:17:29 | I |       - range scale = [    1.0000]
25-01-23 08:17:29 | I |         sum  error  = [    1.1593]
25-01-23 08:17:29 | I |         best error  = [    1.1593]
25-01-23 08:17:29 | I |     + error = [1.1593]
25-01-23 08:17:30 | I |       - range scale = [    1.0000]
25-01-23 08:17:30 | I |         sum  error  = [   12.0553]
25-01-23 08:17:30 | I |         best error  = [   12.0553]
25-01-23 08:17:30 | I |     + error = [12.0553]
25-01-23 08:17:31 | I |   - Calibrating model.layers.0.mlp.down_proj.weight
25-01-23 08:17:31 | I |       - range scale = [    1.0000]
25-01-23 08:17:31 | I |         sum  error  = [    0.1736]
25-01-23 08:17:31 | I |         best error  = [    0.1736]
25-01-23 08:17:31 | I |     + error = [0.1736]
25-01-23 08:17:32 | I |       - range scale = [    1.0000]
25-01-23 08:17:32 | I |         sum  error  = [    1.6946]
25-01-23 08:17:32 | I |         best error  = [    1.6946]
25-01-23 08:17:32 | I |     + error = [1.6946]
25-01-23 08:17:33 | I |   - Quantizing model.layers.0.self_attn.q_proj.weight
25-01-23 08:17:35 | I |   - Quantizing model.layers.0.self_attn.k_proj.weight
25-01-23 08:17:37 | I |   - Quantizing model.layers.0.self_attn.v_proj.weight
25-01-23 08:17:40 | I |   - Quantizing model.layers.0.self_attn.o_proj.weight
25-01-23 08:17:42 | I |   - Quantizing model.layers.0.mlp.up_proj.weight
25-01-23 08:17:44 | I |   - Quantizing model.layers.0.mlp.gate_proj.weight
25-01-23 08:17:47 | I |   - Quantizing model.layers.0.mlp.down_proj.weight
25-01-23 08:17:53 | I | quantizing activations for layer model.layers.0
25-01-23 08:17:53 | I | collecting calibration activations in model.layers.0
25-01-23 08:17:53 | I | collecting calibration activations in model.layers.0
25-01-23 08:17:55 | I | forward this layer
25-01-23 08:17:55 | I | input_file: /data/gyy/lmquant-main/lmquant/data/data_without_preprocess_llama_7b_bs16/shard/val_input_args_layer_0/41.pt
25-01-23 08:17:55 | I | output_file: /data/gyy/lmquant-main/lmquant/data/data_without_preprocess_llama_7b_bs16/shard/val_teacher_output_layer_0/41.pt
25-01-23 08:17:55 | I | In layer model.layers.0, gradient of model.layers.0.self_attn.q_proj has nan: False
25-01-23 08:17:55 | I | In layer model.layers.0, gradient of model.layers.0.self_attn.q_proj has inf: False
25-01-23 08:17:55 | I | In layer model.layers.0, gradient of model.layers.0.self_attn.k_proj has nan: False
25-01-23 08:17:55 | I | In layer model.layers.0, gradient of model.layers.0.self_attn.k_proj has inf: False
25-01-23 08:17:55 | I | In layer model.layers.0, gradient of model.layers.0.self_attn.v_proj has nan: False
25-01-23 08:17:55 | I | In layer model.layers.0, gradient of model.layers.0.self_attn.v_proj has inf: False
25-01-23 08:17:55 | I | In layer model.layers.0, gradient of model.layers.0.self_attn.o_proj has nan: False
25-01-23 08:17:55 | I | In layer model.layers.0, gradient of model.layers.0.self_attn.o_proj has inf: False
25-01-23 08:17:55 | I | In layer model.layers.0, gradient of model.layers.0.mlp.up_proj has nan: False
25-01-23 08:17:55 | I | In layer model.layers.0, gradient of model.layers.0.mlp.up_proj has inf: False
25-01-23 08:17:55 | I | In layer model.layers.0, gradient of model.layers.0.mlp.gate_proj has nan: False
25-01-23 08:17:55 | I | In layer model.layers.0, gradient of model.layers.0.mlp.gate_proj has inf: False
25-01-23 08:17:55 | I | In layer model.layers.0, gradient of model.layers.0.mlp.down_proj has nan: False
25-01-23 08:17:55 | I | In layer model.layers.0, gradient of model.layers.0.mlp.down_proj has inf: False
25-01-23 08:17:55 | I | [37] done with optimizer step
25-01-23 08:17:55 | I | epoch 001:     42 / 819200000 loss=1.2584e-05, loss_per_token=0.0515441, loss_sum=422.25, wps=226.6, ups=0.03, wpb=8192, bsz=16, num_updates=38, lr=0.0038, gnorm=0.333, clip=0, loss_scale=8, train_wall=36, cuda_gb_allocated=17.1, cuda_gb_reserved=18.7, cuda_gb_free=6.6, wall=3223
25-01-23 08:17:56 | I | TRAIN CURRENT LAYER_IDX = 0
25-01-23 08:17:56 | I | in layer model.layers.0
25-01-23 08:17:56 | I | quantizing weights for layer model.layers.0
25-01-23 08:17:56 | I | collecting calibration activations in model.layers.0
25-01-23 08:17:56 | I | collecting calibration activations in model.layers.0
25-01-23 08:17:56 | I | - Quantizing decoder layer model.layers.0
25-01-23 08:17:56 | I |   - Calibrating model.layers.0.self_attn.q_proj.weight
25-01-23 08:17:57 | I |       - range scale = [    1.0000]
25-01-23 08:17:57 | I |         sum  error  = [    0.0454]
25-01-23 08:17:57 | I |         best error  = [    0.0454]
25-01-23 08:17:57 | I |     + error = [0.0454]
25-01-23 08:17:58 | I |       - range scale = [    1.0000]
25-01-23 08:17:58 | I |         sum  error  = [    0.6947]
25-01-23 08:17:58 | I |         best error  = [    0.6947]
25-01-23 08:17:58 | I |     + error = [0.6947]
25-01-23 08:17:58 | I |   - Calibrating model.layers.0.self_attn.k_proj.weight
25-01-23 08:17:59 | I |       - range scale = [    1.0000]
25-01-23 08:17:59 | I |         sum  error  = [    0.0579]
25-01-23 08:17:59 | I |         best error  = [    0.0579]
25-01-23 08:17:59 | I |     + error = [0.0579]
25-01-23 08:17:59 | I |       - range scale = [    1.0000]
25-01-23 08:17:59 | I |         sum  error  = [    0.9821]
25-01-23 08:17:59 | I |         best error  = [    0.9821]
25-01-23 08:17:59 | I |     + error = [0.9821]
25-01-23 08:17:59 | I |   - Calibrating model.layers.0.self_attn.v_proj.weight
25-01-23 08:18:00 | I |       - range scale = [    1.0000]
25-01-23 08:18:00 | I |         sum  error  = [    0.7072]
25-01-23 08:18:00 | I |         best error  = [    0.7072]
25-01-23 08:18:00 | I |     + error = [0.7072]
25-01-23 08:18:01 | I |       - range scale = [    1.0000]
25-01-23 08:18:01 | I |         sum  error  = [    6.5952]
25-01-23 08:18:01 | I |         best error  = [    6.5952]
25-01-23 08:18:01 | I |     + error = [6.5952]
25-01-23 08:18:01 | I |   - Calibrating model.layers.0.self_attn.o_proj.weight
25-01-23 08:18:02 | I |       - range scale = [    1.0000]
25-01-23 08:18:02 | I |         sum  error  = [    0.1919]
25-01-23 08:18:02 | I |         best error  = [    0.1919]
25-01-23 08:18:02 | I |     + error = [0.1919]
25-01-23 08:18:03 | I |       - range scale = [    1.0000]
25-01-23 08:18:03 | I |         sum  error  = [    1.5355]
25-01-23 08:18:03 | I |         best error  = [    1.5355]
25-01-23 08:18:03 | I |     + error = [1.5355]
25-01-23 08:18:03 | I |   - Calibrating model.layers.0.mlp.up_proj.weight
25-01-23 08:18:03 | I |       - range scale = [    1.0000]
25-01-23 08:18:03 | I |         sum  error  = [    1.0922]
25-01-23 08:18:03 | I |         best error  = [    1.0922]
25-01-23 08:18:03 | I |     + error = [1.0922]
25-01-23 08:18:05 | I |       - range scale = [    1.0000]
25-01-23 08:18:05 | I |         sum  error  = [   11.6364]
25-01-23 08:18:05 | I |         best error  = [   11.6364]
25-01-23 08:18:05 | I |     + error = [11.6364]
25-01-23 08:18:05 | I |   - Calibrating model.layers.0.mlp.gate_proj.weight
25-01-23 08:18:06 | I |       - range scale = [    1.0000]
25-01-23 08:18:06 | I |         sum  error  = [    1.1571]
25-01-23 08:18:06 | I |         best error  = [    1.1571]
25-01-23 08:18:06 | I |     + error = [1.1571]
25-01-23 08:18:07 | I |       - range scale = [    1.0000]
25-01-23 08:18:07 | I |         sum  error  = [   12.0310]
25-01-23 08:18:07 | I |         best error  = [   12.0310]
25-01-23 08:18:07 | I |     + error = [12.0310]
25-01-23 08:18:07 | I |   - Calibrating model.layers.0.mlp.down_proj.weight
25-01-23 08:18:08 | I |       - range scale = [    1.0000]
25-01-23 08:18:08 | I |         sum  error  = [    0.1653]
25-01-23 08:18:08 | I |         best error  = [    0.1653]
25-01-23 08:18:08 | I |     + error = [0.1653]
25-01-23 08:18:09 | I |       - range scale = [    1.0000]
25-01-23 08:18:09 | I |         sum  error  = [    1.6068]
25-01-23 08:18:09 | I |         best error  = [    1.6068]
25-01-23 08:18:09 | I |     + error = [1.6068]
25-01-23 08:18:09 | I |   - Quantizing model.layers.0.self_attn.q_proj.weight
25-01-23 08:18:11 | I |   - Quantizing model.layers.0.self_attn.k_proj.weight
25-01-23 08:18:14 | I |   - Quantizing model.layers.0.self_attn.v_proj.weight
25-01-23 08:18:16 | I |   - Quantizing model.layers.0.self_attn.o_proj.weight
25-01-23 08:18:18 | I |   - Quantizing model.layers.0.mlp.up_proj.weight
25-01-23 08:18:20 | I |   - Quantizing model.layers.0.mlp.gate_proj.weight
25-01-23 08:18:23 | I |   - Quantizing model.layers.0.mlp.down_proj.weight
25-01-23 08:18:29 | I | quantizing activations for layer model.layers.0
25-01-23 08:18:29 | I | collecting calibration activations in model.layers.0
25-01-23 08:18:29 | I | collecting calibration activations in model.layers.0
25-01-23 08:18:31 | I | forward this layer
25-01-23 08:18:31 | I | input_file: /data/gyy/lmquant-main/lmquant/data/data_without_preprocess_llama_7b_bs16/shard/val_input_args_layer_0/42.pt
25-01-23 08:18:31 | I | output_file: /data/gyy/lmquant-main/lmquant/data/data_without_preprocess_llama_7b_bs16/shard/val_teacher_output_layer_0/42.pt
25-01-23 08:18:31 | I | In layer model.layers.0, gradient of model.layers.0.self_attn.q_proj has nan: False
25-01-23 08:18:31 | I | In layer model.layers.0, gradient of model.layers.0.self_attn.q_proj has inf: False
25-01-23 08:18:31 | I | In layer model.layers.0, gradient of model.layers.0.self_attn.k_proj has nan: False
25-01-23 08:18:31 | I | In layer model.layers.0, gradient of model.layers.0.self_attn.k_proj has inf: False
25-01-23 08:18:31 | I | In layer model.layers.0, gradient of model.layers.0.self_attn.v_proj has nan: False
25-01-23 08:18:31 | I | In layer model.layers.0, gradient of model.layers.0.self_attn.v_proj has inf: False
25-01-23 08:18:31 | I | In layer model.layers.0, gradient of model.layers.0.self_attn.o_proj has nan: False
25-01-23 08:18:31 | I | In layer model.layers.0, gradient of model.layers.0.self_attn.o_proj has inf: False
25-01-23 08:18:31 | I | In layer model.layers.0, gradient of model.layers.0.mlp.up_proj has nan: False
25-01-23 08:18:31 | I | In layer model.layers.0, gradient of model.layers.0.mlp.up_proj has inf: False
25-01-23 08:18:31 | I | In layer model.layers.0, gradient of model.layers.0.mlp.gate_proj has nan: False
25-01-23 08:18:31 | I | In layer model.layers.0, gradient of model.layers.0.mlp.gate_proj has inf: False
25-01-23 08:18:31 | I | In layer model.layers.0, gradient of model.layers.0.mlp.down_proj has nan: False
25-01-23 08:18:32 | I | In layer model.layers.0, gradient of model.layers.0.mlp.down_proj has inf: False
25-01-23 08:18:32 | I | [38] done with optimizer step
25-01-23 08:18:32 | I | epoch 001:     43 / 819200000 loss=1.20244e-05, loss_per_token=0.049252, loss_sum=403.472, wps=227.5, ups=0.03, wpb=8192, bsz=16, num_updates=39, lr=0.0039, gnorm=0.172, clip=0, loss_scale=8, train_wall=36, cuda_gb_allocated=17.1, cuda_gb_reserved=18.7, cuda_gb_free=6.6, wall=3259
25-01-23 08:18:32 | I | TRAIN CURRENT LAYER_IDX = 0
25-01-23 08:18:32 | I | in layer model.layers.0
25-01-23 08:18:32 | I | quantizing weights for layer model.layers.0
25-01-23 08:18:32 | I | collecting calibration activations in model.layers.0
25-01-23 08:18:32 | I | collecting calibration activations in model.layers.0
25-01-23 08:18:32 | I | - Quantizing decoder layer model.layers.0
25-01-23 08:18:32 | I |   - Calibrating model.layers.0.self_attn.q_proj.weight
25-01-23 08:18:33 | I |       - range scale = [    1.0000]
25-01-23 08:18:33 | I |         sum  error  = [    0.0458]
25-01-23 08:18:33 | I |         best error  = [    0.0458]
25-01-23 08:18:33 | I |     + error = [0.0458]
25-01-23 08:18:34 | I |       - range scale = [    1.0000]
25-01-23 08:18:34 | I |         sum  error  = [    0.6774]
25-01-23 08:18:34 | I |         best error  = [    0.6774]
25-01-23 08:18:34 | I |     + error = [0.6774]
25-01-23 08:18:34 | I |   - Calibrating model.layers.0.self_attn.k_proj.weight
25-01-23 08:18:35 | I |       - range scale = [    1.0000]
25-01-23 08:18:35 | I |         sum  error  = [    0.0578]
25-01-23 08:18:35 | I |         best error  = [    0.0578]
25-01-23 08:18:35 | I |     + error = [0.0578]
25-01-23 08:18:35 | I |       - range scale = [    1.0000]
25-01-23 08:18:35 | I |         sum  error  = [    0.9496]
25-01-23 08:18:35 | I |         best error  = [    0.9496]
25-01-23 08:18:35 | I |     + error = [0.9496]
25-01-23 08:18:36 | I |   - Calibrating model.layers.0.self_attn.v_proj.weight
25-01-23 08:18:36 | I |       - range scale = [    1.0000]
25-01-23 08:18:36 | I |         sum  error  = [    0.7175]
25-01-23 08:18:36 | I |         best error  = [    0.7175]
25-01-23 08:18:36 | I |     + error = [0.7175]
25-01-23 08:18:37 | I |       - range scale = [    1.0000]
25-01-23 08:18:37 | I |         sum  error  = [    6.6661]
25-01-23 08:18:37 | I |         best error  = [    6.6661]
25-01-23 08:18:37 | I |     + error = [6.6661]
25-01-23 08:18:37 | I |   - Calibrating model.layers.0.self_attn.o_proj.weight
25-01-23 08:18:38 | I |       - range scale = [    1.0000]
25-01-23 08:18:38 | I |         sum  error  = [    0.1951]
25-01-23 08:18:38 | I |         best error  = [    0.1951]
25-01-23 08:18:38 | I |     + error = [0.1951]
25-01-23 08:18:39 | I |       - range scale = [    1.0000]
25-01-23 08:18:39 | I |         sum  error  = [    1.5505]
25-01-23 08:18:39 | I |         best error  = [    1.5505]
25-01-23 08:18:39 | I |     + error = [1.5505]
25-01-23 08:18:39 | I |   - Calibrating model.layers.0.mlp.up_proj.weight
25-01-23 08:18:40 | I |       - range scale = [    1.0000]
25-01-23 08:18:40 | I |         sum  error  = [    1.0896]
25-01-23 08:18:40 | I |         best error  = [    1.0896]
25-01-23 08:18:40 | I |     + error = [1.0896]
25-01-23 08:18:41 | I |       - range scale = [    1.0000]
25-01-23 08:18:41 | I |         sum  error  = [   11.5889]
25-01-23 08:18:41 | I |         best error  = [   11.5889]
25-01-23 08:18:41 | I |     + error = [11.5889]
25-01-23 08:18:41 | I |   - Calibrating model.layers.0.mlp.gate_proj.weight
25-01-23 08:18:42 | I |       - range scale = [    1.0000]
25-01-23 08:18:42 | I |         sum  error  = [    1.1515]
25-01-23 08:18:42 | I |         best error  = [    1.1515]
25-01-23 08:18:42 | I |     + error = [1.1515]
25-01-23 08:18:43 | I |       - range scale = [    1.0000]
25-01-23 08:18:43 | I |         sum  error  = [   11.9858]
25-01-23 08:18:43 | I |         best error  = [   11.9858]
25-01-23 08:18:43 | I |     + error = [11.9858]
25-01-23 08:18:43 | I |   - Calibrating model.layers.0.mlp.down_proj.weight
25-01-23 08:18:44 | I |       - range scale = [    1.0000]
25-01-23 08:18:44 | I |         sum  error  = [    0.1706]
25-01-23 08:18:44 | I |         best error  = [    0.1706]
25-01-23 08:18:44 | I |     + error = [0.1706]
25-01-23 08:18:45 | I |       - range scale = [    1.0000]
25-01-23 08:18:45 | I |         sum  error  = [    1.6627]
25-01-23 08:18:45 | I |         best error  = [    1.6627]
25-01-23 08:18:45 | I |     + error = [1.6627]
25-01-23 08:18:45 | I |   - Quantizing model.layers.0.self_attn.q_proj.weight
25-01-23 08:18:48 | I |   - Quantizing model.layers.0.self_attn.k_proj.weight
25-01-23 08:18:50 | I |   - Quantizing model.layers.0.self_attn.v_proj.weight
25-01-23 08:18:52 | I |   - Quantizing model.layers.0.self_attn.o_proj.weight
25-01-23 08:18:54 | I |   - Quantizing model.layers.0.mlp.up_proj.weight
25-01-23 08:18:57 | I |   - Quantizing model.layers.0.mlp.gate_proj.weight
25-01-23 08:18:59 | I |   - Quantizing model.layers.0.mlp.down_proj.weight
25-01-23 08:19:05 | I | quantizing activations for layer model.layers.0
25-01-23 08:19:06 | I | collecting calibration activations in model.layers.0
25-01-23 08:19:06 | I | collecting calibration activations in model.layers.0
25-01-23 08:19:08 | I | forward this layer
25-01-23 08:19:08 | I | input_file: /data/gyy/lmquant-main/lmquant/data/data_without_preprocess_llama_7b_bs16/shard/val_input_args_layer_0/43.pt
25-01-23 08:19:08 | I | output_file: /data/gyy/lmquant-main/lmquant/data/data_without_preprocess_llama_7b_bs16/shard/val_teacher_output_layer_0/43.pt
25-01-23 08:19:08 | I | In layer model.layers.0, gradient of model.layers.0.self_attn.q_proj has nan: False
25-01-23 08:19:08 | I | In layer model.layers.0, gradient of model.layers.0.self_attn.q_proj has inf: False
25-01-23 08:19:08 | I | In layer model.layers.0, gradient of model.layers.0.self_attn.k_proj has nan: False
25-01-23 08:19:08 | I | In layer model.layers.0, gradient of model.layers.0.self_attn.k_proj has inf: False
25-01-23 08:19:08 | I | In layer model.layers.0, gradient of model.layers.0.self_attn.v_proj has nan: False
25-01-23 08:19:08 | I | In layer model.layers.0, gradient of model.layers.0.self_attn.v_proj has inf: False
25-01-23 08:19:08 | I | In layer model.layers.0, gradient of model.layers.0.self_attn.o_proj has nan: False
25-01-23 08:19:08 | I | In layer model.layers.0, gradient of model.layers.0.self_attn.o_proj has inf: False
25-01-23 08:19:08 | I | In layer model.layers.0, gradient of model.layers.0.mlp.up_proj has nan: False
25-01-23 08:19:08 | I | In layer model.layers.0, gradient of model.layers.0.mlp.up_proj has inf: False
25-01-23 08:19:08 | I | In layer model.layers.0, gradient of model.layers.0.mlp.gate_proj has nan: False
25-01-23 08:19:08 | I | In layer model.layers.0, gradient of model.layers.0.mlp.gate_proj has inf: False
25-01-23 08:19:08 | I | In layer model.layers.0, gradient of model.layers.0.mlp.down_proj has nan: False
25-01-23 08:19:08 | I | In layer model.layers.0, gradient of model.layers.0.mlp.down_proj has inf: False
25-01-23 08:19:08 | I | [39] done with optimizer step
25-01-23 08:19:08 | I | epoch 001:     44 / 819200000 loss=1.28093e-05, loss_per_token=0.0524667, loss_sum=429.807, wps=224.4, ups=0.03, wpb=8192, bsz=16, num_updates=40, lr=0.004, gnorm=0.365, clip=0, loss_scale=8, train_wall=36, cuda_gb_allocated=17.1, cuda_gb_reserved=18.7, cuda_gb_free=6.6, wall=3296
25-01-23 08:19:08 | I | begin validation on "valid" subset on rank 0
25-01-23 08:19:08 | I | got valid iterator on "valid" subset on rank 0
25-01-23 08:19:08 | I | Valid: Start iterating over samples
25-01-23 08:19:08 | W |   `pretrained` model kwarg is not of type `str`. Many other model arguments may be ignored. Please do not launch via accelerate or use `parallelize=True` if passing an existing model this way.
25-01-23 08:19:08 | W |   Passed an already-initialized model through `pretrained`, assuming single-process call to evaluate() or custom distributed integration
25-01-23 08:19:08 | I | - Evaluator: gptq
25-01-23 08:19:08 | I | - Tasks: ['wikitext', 'val_valid']
25-01-23 08:19:08 | I | - Batch_size: 8
25-01-23 08:19:08 | I |   + Max_seq_length: 2048
25-01-23 08:20:43 | I |     - Results:
25-01-23 08:20:43 | I |       |  Task   |Version|    Metric     |Value |   |Stderr|
25-01-23 08:20:43 | I |       |---------|------:|---------------|-----:|---|-----:|
25-01-23 08:20:43 | I |       |wikitext |      1|word_perplexity|5.4825|±  |5.4825|
25-01-23 08:20:43 | I |       |val_valid|      1|word_perplexity|5.0259|±  |5.0259|
25-01-23 08:20:43 | I |       
25-01-23 08:20:43 | I |   + Max_seq_length: 4096
25-01-23 08:22:16 | I |     - Results:
25-01-23 08:22:16 | I |       |  Task   |Version|    Metric     |Value |   |Stderr|
25-01-23 08:22:16 | I |       |---------|------:|---------------|-----:|---|-----:|
25-01-23 08:22:16 | I |       |wikitext |      1|word_perplexity|5.1241|±  |5.1241|
25-01-23 08:22:16 | I |       |val_valid|      1|word_perplexity|4.8214|±  |4.8214|
25-01-23 08:22:16 | I |       
25-01-23 08:22:16 | I | in valid, quantize current layer weights
25-01-23 08:22:17 | W | Repo card metadata block was not found. Setting CardData to empty.
25-01-23 08:23:01 | I | collecting calibration activations in model.layers.0
25-01-23 08:23:01 | I | collecting calibration activations in model.layers.0
25-01-23 08:23:01 | I | collecting calibration activations in model.layers.0
25-01-23 08:23:01 | I | collecting calibration activations in model.layers.0
25-01-23 08:23:01 | I | collecting calibration activations in model.layers.0
25-01-23 08:23:01 | I | collecting calibration activations in model.layers.0
25-01-23 08:23:01 | I | collecting calibration activations in model.layers.0
25-01-23 08:23:02 | I | collecting calibration activations in model.layers.0
25-01-23 08:23:02 | I | collecting calibration activations in model.layers.0
25-01-23 08:23:02 | I | collecting calibration activations in model.layers.0
25-01-23 08:23:02 | I | collecting calibration activations in model.layers.0
25-01-23 08:23:02 | I | collecting calibration activations in model.layers.0
25-01-23 08:23:02 | I | collecting calibration activations in model.layers.0
25-01-23 08:23:02 | I | collecting calibration activations in model.layers.0
25-01-23 08:23:02 | I | collecting calibration activations in model.layers.0
25-01-23 08:23:02 | I | collecting calibration activations in model.layers.0
25-01-23 08:23:02 | I | collecting calibration activations in model.layers.0
25-01-23 08:23:02 | I | collecting calibration activations in model.layers.0
25-01-23 08:23:02 | I | collecting calibration activations in model.layers.0
25-01-23 08:23:02 | I | collecting calibration activations in model.layers.0
25-01-23 08:23:02 | I | collecting calibration activations in model.layers.0
25-01-23 08:23:02 | I | collecting calibration activations in model.layers.0
25-01-23 08:23:02 | I | collecting calibration activations in model.layers.0
25-01-23 08:23:02 | I | collecting calibration activations in model.layers.0
25-01-23 08:23:02 | I | collecting calibration activations in model.layers.0
25-01-23 08:23:02 | I | collecting calibration activations in model.layers.0
25-01-23 08:23:03 | I | collecting calibration activations in model.layers.0
25-01-23 08:23:03 | I | collecting calibration activations in model.layers.0
25-01-23 08:23:03 | I | collecting calibration activations in model.layers.0
25-01-23 08:23:03 | I | collecting calibration activations in model.layers.0
25-01-23 08:23:03 | I | collecting calibration activations in model.layers.0
25-01-23 08:23:03 | I | collecting calibration activations in model.layers.0
25-01-23 08:23:03 | I | collecting calibration activations in model.layers.0
25-01-23 08:23:03 | I | collecting calibration activations in model.layers.0
25-01-23 08:23:03 | I | collecting calibration activations in model.layers.0
25-01-23 08:23:03 | I | collecting calibration activations in model.layers.0
25-01-23 08:23:03 | I | collecting calibration activations in model.layers.0
25-01-23 08:23:03 | I | collecting calibration activations in model.layers.0
25-01-23 08:23:03 | I | collecting calibration activations in model.layers.0
25-01-23 08:23:03 | I | collecting calibration activations in model.layers.0
25-01-23 08:23:03 | I | collecting calibration activations in model.layers.0
25-01-23 08:23:03 | I | collecting calibration activations in model.layers.0
25-01-23 08:23:03 | I | collecting calibration activations in model.layers.0
25-01-23 08:23:03 | I | collecting calibration activations in model.layers.0
25-01-23 08:23:03 | I | collecting calibration activations in model.layers.0
25-01-23 08:23:04 | I | collecting calibration activations in model.layers.0
25-01-23 08:23:04 | I | collecting calibration activations in model.layers.0
25-01-23 08:23:04 | I | collecting calibration activations in model.layers.0
25-01-23 08:23:04 | I | collecting calibration activations in model.layers.0
25-01-23 08:23:04 | I | collecting calibration activations in model.layers.0
25-01-23 08:23:04 | I | collecting calibration activations in model.layers.0
25-01-23 08:23:04 | I | collecting calibration activations in model.layers.0
25-01-23 08:23:04 | I | collecting calibration activations in model.layers.0
25-01-23 08:23:04 | I | collecting calibration activations in model.layers.0
25-01-23 08:23:04 | I | collecting calibration activations in model.layers.0
25-01-23 08:23:04 | I | collecting calibration activations in model.layers.0
25-01-23 08:23:04 | I | collecting calibration activations in model.layers.0
25-01-23 08:23:04 | I | collecting calibration activations in model.layers.0
25-01-23 08:23:04 | I | collecting calibration activations in model.layers.0
25-01-23 08:23:04 | I | collecting calibration activations in model.layers.0
25-01-23 08:23:04 | I | collecting calibration activations in model.layers.0
25-01-23 08:23:04 | I | collecting calibration activations in model.layers.0
25-01-23 08:23:04 | I | collecting calibration activations in model.layers.0
25-01-23 08:23:04 | I | collecting calibration activations in model.layers.0
25-01-23 08:23:05 | I | collecting calibration activations in model.layers.0
25-01-23 08:23:05 | I | collecting calibration activations in model.layers.0
25-01-23 08:23:05 | I | collecting calibration activations in model.layers.0
25-01-23 08:23:05 | I | collecting calibration activations in model.layers.0
25-01-23 08:23:05 | I | collecting calibration activations in model.layers.0
25-01-23 08:23:05 | I | collecting calibration activations in model.layers.0
25-01-23 08:23:05 | I | collecting calibration activations in model.layers.0
25-01-23 08:23:05 | I | collecting calibration activations in model.layers.0
25-01-23 08:23:05 | I | collecting calibration activations in model.layers.0
25-01-23 08:23:05 | I | collecting calibration activations in model.layers.0
25-01-23 08:23:05 | I | collecting calibration activations in model.layers.0
25-01-23 08:23:05 | I | collecting calibration activations in model.layers.0
25-01-23 08:23:05 | I | collecting calibration activations in model.layers.0
25-01-23 08:23:05 | I | collecting calibration activations in model.layers.0
25-01-23 08:23:05 | I | collecting calibration activations in model.layers.0
25-01-23 08:23:05 | I | collecting calibration activations in model.layers.0
25-01-23 08:23:05 | I | collecting calibration activations in model.layers.0
25-01-23 08:23:05 | I | collecting calibration activations in model.layers.0
25-01-23 08:23:05 | I | collecting calibration activations in model.layers.0
25-01-23 08:23:06 | I | collecting calibration activations in model.layers.0
25-01-23 08:23:06 | I | collecting calibration activations in model.layers.0
25-01-23 08:23:06 | I | collecting calibration activations in model.layers.0
25-01-23 08:23:06 | I | collecting calibration activations in model.layers.0
25-01-23 08:23:06 | I | collecting calibration activations in model.layers.0
25-01-23 08:23:06 | I | collecting calibration activations in model.layers.0
25-01-23 08:23:06 | I | collecting calibration activations in model.layers.0
25-01-23 08:23:06 | I | collecting calibration activations in model.layers.0
25-01-23 08:23:06 | I | collecting calibration activations in model.layers.0
25-01-23 08:23:06 | I | collecting calibration activations in model.layers.0
25-01-23 08:23:06 | I | collecting calibration activations in model.layers.0
25-01-23 08:23:06 | I | collecting calibration activations in model.layers.0
25-01-23 08:23:06 | I | collecting calibration activations in model.layers.0
25-01-23 08:23:06 | I | collecting calibration activations in model.layers.0
25-01-23 08:23:06 | I | collecting calibration activations in model.layers.0
25-01-23 08:23:06 | I | collecting calibration activations in model.layers.0
25-01-23 08:23:06 | I | collecting calibration activations in model.layers.0
25-01-23 08:23:06 | I | collecting calibration activations in model.layers.0
25-01-23 08:23:06 | I | collecting calibration activations in model.layers.0
25-01-23 08:23:07 | I | collecting calibration activations in model.layers.0
25-01-23 08:23:07 | I | collecting calibration activations in model.layers.0
25-01-23 08:23:07 | I | collecting calibration activations in model.layers.0
25-01-23 08:23:07 | I | collecting calibration activations in model.layers.0
25-01-23 08:23:07 | I | collecting calibration activations in model.layers.0
25-01-23 08:23:07 | I | collecting calibration activations in model.layers.0
25-01-23 08:23:07 | I | collecting calibration activations in model.layers.0
25-01-23 08:23:07 | I | collecting calibration activations in model.layers.0
25-01-23 08:23:07 | I | collecting calibration activations in model.layers.0
25-01-23 08:23:07 | I | collecting calibration activations in model.layers.0
25-01-23 08:23:07 | I | collecting calibration activations in model.layers.0
25-01-23 08:23:07 | I | collecting calibration activations in model.layers.0
25-01-23 08:23:07 | I | collecting calibration activations in model.layers.0
25-01-23 08:23:07 | I | collecting calibration activations in model.layers.0
25-01-23 08:23:07 | I | collecting calibration activations in model.layers.0
25-01-23 08:23:07 | I | collecting calibration activations in model.layers.0
25-01-23 08:23:07 | I | collecting calibration activations in model.layers.0
25-01-23 08:23:07 | I | collecting calibration activations in model.layers.0
25-01-23 08:23:07 | I | collecting calibration activations in model.layers.0
25-01-23 08:23:08 | I | collecting calibration activations in model.layers.0
25-01-23 08:23:08 | I | collecting calibration activations in model.layers.0
25-01-23 08:23:08 | I | collecting calibration activations in model.layers.0
25-01-23 08:23:08 | I | collecting calibration activations in model.layers.0
25-01-23 08:23:08 | I | collecting calibration activations in model.layers.0
25-01-23 08:23:08 | I | collecting calibration activations in model.layers.0
25-01-23 08:23:08 | I | collecting calibration activations in model.layers.0
25-01-23 08:23:10 | I |       - range scale = [    1.0000]
25-01-23 08:23:10 | I |         sum  error  = [    0.1557]
25-01-23 08:23:10 | I |         best error  = [    0.1557]
25-01-23 08:23:10 | I |     + error = [0.1557]
25-01-23 08:23:11 | I |       - range scale = [    1.0000]
25-01-23 08:23:11 | I |         sum  error  = [    2.6770]
25-01-23 08:23:11 | I |         best error  = [    2.6770]
25-01-23 08:23:11 | I |     + error = [2.6770]
25-01-23 08:23:13 | I |       - range scale = [    1.0000]
25-01-23 08:23:13 | I |         sum  error  = [    0.1900]
25-01-23 08:23:13 | I |         best error  = [    0.1900]
25-01-23 08:23:13 | I |     + error = [0.1900]
25-01-23 08:23:14 | I |       - range scale = [    1.0000]
25-01-23 08:23:14 | I |         sum  error  = [    3.7688]
25-01-23 08:23:14 | I |         best error  = [    3.7688]
25-01-23 08:23:14 | I |     + error = [3.7688]
25-01-23 08:23:15 | I |       - range scale = [    1.0000]
25-01-23 08:23:15 | I |         sum  error  = [    0.6483]
25-01-23 08:23:15 | I |         best error  = [    0.6483]
25-01-23 08:23:15 | I |     + error = [0.6483]
25-01-23 08:23:16 | I |       - range scale = [    1.0000]
25-01-23 08:23:16 | I |         sum  error  = [    5.9763]
25-01-23 08:23:16 | I |         best error  = [    5.9763]
25-01-23 08:23:16 | I |     + error = [5.9763]
25-01-23 08:23:17 | I |       - range scale = [    1.0000]
25-01-23 08:23:17 | I |         sum  error  = [    0.1474]
25-01-23 08:23:17 | I |         best error  = [    0.1474]
25-01-23 08:23:17 | I |     + error = [0.1474]
25-01-23 08:23:18 | I |       - range scale = [    1.0000]
25-01-23 08:23:18 | I |         sum  error  = [    1.2098]
25-01-23 08:23:18 | I |         best error  = [    1.2098]
25-01-23 08:23:18 | I |     + error = [1.2098]
25-01-23 08:23:18 | I |       - range scale = [    1.0000]
25-01-23 08:23:18 | I |         sum  error  = [    1.0957]
25-01-23 08:23:18 | I |         best error  = [    1.0957]
25-01-23 08:23:18 | I |     + error = [1.0957]
25-01-23 08:23:20 | I |       - range scale = [    1.0000]
25-01-23 08:23:20 | I |         sum  error  = [   11.6706]
25-01-23 08:23:20 | I |         best error  = [   11.6706]
25-01-23 08:23:20 | I |     + error = [11.6706]
25-01-23 08:23:20 | I |       - range scale = [    1.0000]
25-01-23 08:23:20 | I |         sum  error  = [    1.1623]
25-01-23 08:23:20 | I |         best error  = [    1.1623]
25-01-23 08:23:20 | I |     + error = [1.1623]
25-01-23 08:23:22 | I |       - range scale = [    1.0000]
25-01-23 08:23:22 | I |         sum  error  = [   12.0503]
25-01-23 08:23:22 | I |         best error  = [   12.0503]
25-01-23 08:23:22 | I |     + error = [12.0503]
25-01-23 08:23:23 | I |       - range scale = [    1.0000]
25-01-23 08:23:23 | I |         sum  error  = [    0.1806]
25-01-23 08:23:23 | I |         best error  = [    0.1806]
25-01-23 08:23:23 | I |     + error = [0.1806]
25-01-23 08:23:24 | I |       - range scale = [    1.0000]
25-01-23 08:23:24 | I |         sum  error  = [    1.7803]
25-01-23 08:23:24 | I |         best error  = [    1.7803]
25-01-23 08:23:24 | I |     + error = [1.7803]
25-01-23 08:23:48 | I | in valid, quantize current layer acts
25-01-23 08:23:50 | I | collecting calibration activations in model.layers.0
25-01-23 08:23:50 | I | collecting calibration activations in model.layers.0
25-01-23 08:23:50 | I | collecting calibration activations in model.layers.0
25-01-23 08:23:50 | I | collecting calibration activations in model.layers.0
25-01-23 08:23:50 | I | collecting calibration activations in model.layers.0
25-01-23 08:23:50 | I | collecting calibration activations in model.layers.0
25-01-23 08:23:51 | I | collecting calibration activations in model.layers.0
25-01-23 08:23:51 | I | collecting calibration activations in model.layers.0
25-01-23 08:23:51 | I | collecting calibration activations in model.layers.0
25-01-23 08:23:51 | I | collecting calibration activations in model.layers.0
25-01-23 08:23:51 | I | collecting calibration activations in model.layers.0
25-01-23 08:23:51 | I | collecting calibration activations in model.layers.0
25-01-23 08:23:51 | I | collecting calibration activations in model.layers.0
25-01-23 08:23:51 | I | collecting calibration activations in model.layers.0
25-01-23 08:23:51 | I | collecting calibration activations in model.layers.0
25-01-23 08:23:51 | I | collecting calibration activations in model.layers.0
25-01-23 08:23:51 | I | collecting calibration activations in model.layers.0
25-01-23 08:23:51 | I | collecting calibration activations in model.layers.0
25-01-23 08:23:51 | I | collecting calibration activations in model.layers.0
25-01-23 08:23:51 | I | collecting calibration activations in model.layers.0
25-01-23 08:23:51 | I | collecting calibration activations in model.layers.0
25-01-23 08:23:52 | I | collecting calibration activations in model.layers.0
25-01-23 08:23:52 | I | collecting calibration activations in model.layers.0
25-01-23 08:23:52 | I | collecting calibration activations in model.layers.0
25-01-23 08:23:52 | I | collecting calibration activations in model.layers.0
25-01-23 08:23:52 | I | collecting calibration activations in model.layers.0
25-01-23 08:23:52 | I | collecting calibration activations in model.layers.0
25-01-23 08:23:52 | I | collecting calibration activations in model.layers.0
25-01-23 08:23:52 | I | collecting calibration activations in model.layers.0
25-01-23 08:23:52 | I | collecting calibration activations in model.layers.0
25-01-23 08:23:52 | I | collecting calibration activations in model.layers.0
25-01-23 08:23:52 | I | collecting calibration activations in model.layers.0
25-01-23 08:23:52 | I | collecting calibration activations in model.layers.0
25-01-23 08:23:52 | I | collecting calibration activations in model.layers.0
25-01-23 08:23:52 | I | collecting calibration activations in model.layers.0
25-01-23 08:23:52 | I | collecting calibration activations in model.layers.0
25-01-23 08:23:52 | I | collecting calibration activations in model.layers.0
25-01-23 08:23:53 | I | collecting calibration activations in model.layers.0
25-01-23 08:23:53 | I | collecting calibration activations in model.layers.0
25-01-23 08:23:53 | I | collecting calibration activations in model.layers.0
25-01-23 08:23:53 | I | collecting calibration activations in model.layers.0
25-01-23 08:23:53 | I | collecting calibration activations in model.layers.0
25-01-23 08:23:53 | I | collecting calibration activations in model.layers.0
25-01-23 08:23:53 | I | collecting calibration activations in model.layers.0
25-01-23 08:23:53 | I | collecting calibration activations in model.layers.0
25-01-23 08:23:53 | I | collecting calibration activations in model.layers.0
25-01-23 08:23:53 | I | collecting calibration activations in model.layers.0
25-01-23 08:23:53 | I | collecting calibration activations in model.layers.0
25-01-23 08:23:53 | I | collecting calibration activations in model.layers.0
25-01-23 08:23:53 | I | collecting calibration activations in model.layers.0
25-01-23 08:23:53 | I | collecting calibration activations in model.layers.0
25-01-23 08:23:53 | I | collecting calibration activations in model.layers.0
25-01-23 08:23:54 | I | collecting calibration activations in model.layers.0
25-01-23 08:23:54 | I | collecting calibration activations in model.layers.0
25-01-23 08:23:54 | I | collecting calibration activations in model.layers.0
25-01-23 08:23:54 | I | collecting calibration activations in model.layers.0
25-01-23 08:23:54 | I | collecting calibration activations in model.layers.0
25-01-23 08:23:54 | I | collecting calibration activations in model.layers.0
25-01-23 08:23:54 | I | collecting calibration activations in model.layers.0
25-01-23 08:23:54 | I | collecting calibration activations in model.layers.0
25-01-23 08:23:54 | I | collecting calibration activations in model.layers.0
25-01-23 08:23:54 | I | collecting calibration activations in model.layers.0
25-01-23 08:23:54 | I | collecting calibration activations in model.layers.0
25-01-23 08:23:54 | I | collecting calibration activations in model.layers.0
25-01-23 08:23:54 | I | collecting calibration activations in model.layers.0
25-01-23 08:23:54 | I | collecting calibration activations in model.layers.0
25-01-23 08:23:54 | I | collecting calibration activations in model.layers.0
25-01-23 08:23:54 | I | collecting calibration activations in model.layers.0
25-01-23 08:23:55 | I | collecting calibration activations in model.layers.0
25-01-23 08:23:55 | I | collecting calibration activations in model.layers.0
25-01-23 08:23:55 | I | collecting calibration activations in model.layers.0
25-01-23 08:23:55 | I | collecting calibration activations in model.layers.0
25-01-23 08:23:55 | I | collecting calibration activations in model.layers.0
25-01-23 08:23:55 | I | collecting calibration activations in model.layers.0
25-01-23 08:23:55 | I | collecting calibration activations in model.layers.0
25-01-23 08:23:55 | I | collecting calibration activations in model.layers.0
25-01-23 08:23:55 | I | collecting calibration activations in model.layers.0
25-01-23 08:23:55 | I | collecting calibration activations in model.layers.0
25-01-23 08:23:55 | I | collecting calibration activations in model.layers.0
25-01-23 08:23:55 | I | collecting calibration activations in model.layers.0
25-01-23 08:23:55 | I | collecting calibration activations in model.layers.0
25-01-23 08:23:55 | I | collecting calibration activations in model.layers.0
25-01-23 08:23:55 | I | collecting calibration activations in model.layers.0
25-01-23 08:23:55 | I | collecting calibration activations in model.layers.0
25-01-23 08:23:56 | I | collecting calibration activations in model.layers.0
25-01-23 08:23:56 | I | collecting calibration activations in model.layers.0
25-01-23 08:23:56 | I | collecting calibration activations in model.layers.0
25-01-23 08:23:56 | I | collecting calibration activations in model.layers.0
25-01-23 08:23:56 | I | collecting calibration activations in model.layers.0
25-01-23 08:23:56 | I | collecting calibration activations in model.layers.0
25-01-23 08:23:56 | I | collecting calibration activations in model.layers.0
25-01-23 08:23:56 | I | collecting calibration activations in model.layers.0
25-01-23 08:23:56 | I | collecting calibration activations in model.layers.0
25-01-23 08:23:56 | I | collecting calibration activations in model.layers.0
25-01-23 08:23:56 | I | collecting calibration activations in model.layers.0
25-01-23 08:23:56 | I | collecting calibration activations in model.layers.0
25-01-23 08:23:56 | I | collecting calibration activations in model.layers.0
25-01-23 08:23:56 | I | collecting calibration activations in model.layers.0
25-01-23 08:23:56 | I | collecting calibration activations in model.layers.0
25-01-23 08:23:57 | I | collecting calibration activations in model.layers.0
25-01-23 08:23:57 | I | collecting calibration activations in model.layers.0
25-01-23 08:23:57 | I | collecting calibration activations in model.layers.0
25-01-23 08:23:57 | I | collecting calibration activations in model.layers.0
25-01-23 08:23:57 | I | collecting calibration activations in model.layers.0
25-01-23 08:23:57 | I | collecting calibration activations in model.layers.0
25-01-23 08:23:57 | I | collecting calibration activations in model.layers.0
25-01-23 08:23:57 | I | collecting calibration activations in model.layers.0
25-01-23 08:23:57 | I | collecting calibration activations in model.layers.0
25-01-23 08:23:57 | I | collecting calibration activations in model.layers.0
25-01-23 08:23:57 | I | collecting calibration activations in model.layers.0
25-01-23 08:23:57 | I | collecting calibration activations in model.layers.0
25-01-23 08:23:57 | I | collecting calibration activations in model.layers.0
25-01-23 08:23:57 | I | collecting calibration activations in model.layers.0
25-01-23 08:23:57 | I | collecting calibration activations in model.layers.0
25-01-23 08:23:57 | I | collecting calibration activations in model.layers.0
25-01-23 08:23:58 | I | collecting calibration activations in model.layers.0
25-01-23 08:23:58 | I | collecting calibration activations in model.layers.0
25-01-23 08:23:58 | I | collecting calibration activations in model.layers.0
25-01-23 08:23:58 | I | collecting calibration activations in model.layers.0
25-01-23 08:23:58 | I | collecting calibration activations in model.layers.0
25-01-23 08:23:58 | I | collecting calibration activations in model.layers.0
25-01-23 08:23:58 | I | collecting calibration activations in model.layers.0
25-01-23 08:23:58 | I | collecting calibration activations in model.layers.0
25-01-23 08:23:58 | I | collecting calibration activations in model.layers.0
25-01-23 08:23:58 | I | collecting calibration activations in model.layers.0
25-01-23 08:23:58 | I | collecting calibration activations in model.layers.0
25-01-23 08:23:58 | I | collecting calibration activations in model.layers.0
25-01-23 08:23:58 | I | collecting calibration activations in model.layers.0
25-01-23 08:24:01 | I | use gptq_eval(lmquant) to calculate ppl, partly quanted
25-01-23 08:24:01 | W |   `pretrained` model kwarg is not of type `str`. Many other model arguments may be ignored. Please do not launch via accelerate or use `parallelize=True` if passing an existing model this way.
25-01-23 08:24:01 | W |   Passed an already-initialized model through `pretrained`, assuming single-process call to evaluate() or custom distributed integration
25-01-23 08:24:01 | I | - Evaluator: gptq
25-01-23 08:24:01 | I | - Tasks: ['wikitext', 'val_valid']
25-01-23 08:24:01 | I | - Batch_size: 8
25-01-23 08:24:01 | I |   + Max_seq_length: 2048
25-01-23 08:25:38 | I |     - Results:
25-01-23 08:25:38 | I |       |  Task   |Version|    Metric     |Value |   |Stderr|
25-01-23 08:25:38 | I |       |---------|------:|---------------|-----:|---|-----:|
25-01-23 08:25:38 | I |       |wikitext |      1|word_perplexity|5.4846|±  |5.4846|
25-01-23 08:25:38 | I |       |val_valid|      1|word_perplexity|5.0409|±  |5.0409|
25-01-23 08:25:38 | I |       
25-01-23 08:25:38 | I |   + Max_seq_length: 4096
25-01-23 08:27:14 | I |     - Results:
25-01-23 08:27:14 | I |       |  Task   |Version|    Metric     |Value |   |Stderr|
25-01-23 08:27:14 | I |       |---------|------:|---------------|-----:|---|-----:|
25-01-23 08:27:14 | I |       |wikitext |      1|word_perplexity|5.1246|±  |5.1246|
25-01-23 08:27:14 | I |       |val_valid|      1|word_perplexity|4.8351|±  |4.8351|
25-01-23 08:27:14 | I |       
25-01-23 08:28:13 | I | epoch 001 | valid on 'valid' subset:    102 / 819200000 loss_valid=2.149, lmquant_ppl_result_val=-1, lmquant_ppl_result_wikitext=-1, ppl=4.44, wps=7239.9, wpb=4058.2, bsz=2, num_updates=40, lmquant_ppl_wikitext_all_quanted=5.48251, lmquant_ppl_val_all_quanted=5.02586, lmquant_ppl_wikitext_partly_quanted=5.48462, lmquant_ppl_val_partly_quanted=5.04091
25-01-23 08:28:13 | I | epoch 001 | valid on 'valid' subset | loss_valid 2.149 | lmquant_ppl_result_val -1 | lmquant_ppl_result_wikitext -1 | ppl 4.44 | wps 7239.9 | wpb 4058.2 | bsz 2 | num_updates 40 | lmquant_ppl_wikitext_all_quanted 5.48251 | lmquant_ppl_val_all_quanted 5.02586 | lmquant_ppl_wikitext_partly_quanted 5.48462 | lmquant_ppl_val_partly_quanted 5.04091
25-01-23 08:28:13 | I | TRAIN CURRENT LAYER_IDX = 0
25-01-23 08:28:13 | I | in layer model.layers.0
25-01-23 08:28:13 | I | quantizing weights for layer model.layers.0
25-01-23 08:28:13 | I | collecting calibration activations in model.layers.0
25-01-23 08:28:14 | I | collecting calibration activations in model.layers.0
25-01-23 08:28:14 | I | - Quantizing decoder layer model.layers.0
25-01-23 08:28:14 | I |   - Calibrating model.layers.0.self_attn.q_proj.weight
25-01-23 08:28:15 | I |       - range scale = [    1.0000]
25-01-23 08:28:15 | I |         sum  error  = [    0.0428]
25-01-23 08:28:15 | I |         best error  = [    0.0428]
25-01-23 08:28:15 | I |     + error = [0.0428]
25-01-23 08:28:15 | I |       - range scale = [    1.0000]
25-01-23 08:28:15 | I |         sum  error  = [    0.6440]
25-01-23 08:28:15 | I |         best error  = [    0.6440]
25-01-23 08:28:15 | I |     + error = [0.6440]
25-01-23 08:28:15 | I |   - Calibrating model.layers.0.self_attn.k_proj.weight
25-01-23 08:28:16 | I |       - range scale = [    1.0000]
25-01-23 08:28:16 | I |         sum  error  = [    0.0531]
25-01-23 08:28:16 | I |         best error  = [    0.0531]
25-01-23 08:28:16 | I |     + error = [0.0531]
25-01-23 08:28:17 | I |       - range scale = [    1.0000]
25-01-23 08:28:17 | I |         sum  error  = [    0.9446]
25-01-23 08:28:17 | I |         best error  = [    0.9446]
25-01-23 08:28:17 | I |     + error = [0.9446]
25-01-23 08:28:17 | I |   - Calibrating model.layers.0.self_attn.v_proj.weight
25-01-23 08:28:18 | I |       - range scale = [    1.0000]
25-01-23 08:28:18 | I |         sum  error  = [    0.6771]
25-01-23 08:28:18 | I |         best error  = [    0.6771]
25-01-23 08:28:18 | I |     + error = [0.6771]
25-01-23 08:28:18 | I |       - range scale = [    1.0000]
25-01-23 08:28:18 | I |         sum  error  = [    6.2118]
25-01-23 08:28:18 | I |         best error  = [    6.2118]
25-01-23 08:28:18 | I |     + error = [6.2118]
25-01-23 08:28:19 | I |   - Calibrating model.layers.0.self_attn.o_proj.weight
25-01-23 08:28:19 | I |       - range scale = [    1.0000]
25-01-23 08:28:19 | I |         sum  error  = [    0.1874]
25-01-23 08:28:19 | I |         best error  = [    0.1874]
25-01-23 08:28:19 | I |     + error = [0.1874]
25-01-23 08:28:20 | I |       - range scale = [    1.0000]
25-01-23 08:28:20 | I |         sum  error  = [    1.4650]
25-01-23 08:28:20 | I |         best error  = [    1.4650]
25-01-23 08:28:20 | I |     + error = [1.4650]
25-01-23 08:28:20 | I |   - Calibrating model.layers.0.mlp.up_proj.weight
25-01-23 08:28:21 | I |       - range scale = [    1.0000]
25-01-23 08:28:21 | I |         sum  error  = [    1.0458]
25-01-23 08:28:21 | I |         best error  = [    1.0458]
25-01-23 08:28:21 | I |     + error = [1.0458]
25-01-23 08:28:22 | I |       - range scale = [    1.0000]
25-01-23 08:28:22 | I |         sum  error  = [   11.1245]
25-01-23 08:28:22 | I |         best error  = [   11.1245]
25-01-23 08:28:22 | I |     + error = [11.1245]
25-01-23 08:28:22 | I |   - Calibrating model.layers.0.mlp.gate_proj.weight
25-01-23 08:28:23 | I |       - range scale = [    1.0000]
25-01-23 08:28:23 | I |         sum  error  = [    1.1060]
25-01-23 08:28:23 | I |         best error  = [    1.1060]
25-01-23 08:28:23 | I |     + error = [1.1060]
25-01-23 08:28:24 | I |       - range scale = [    1.0000]
25-01-23 08:28:24 | I |         sum  error  = [   11.5093]
25-01-23 08:28:24 | I |         best error  = [   11.5093]
25-01-23 08:28:24 | I |     + error = [11.5093]
25-01-23 08:28:24 | I |   - Calibrating model.layers.0.mlp.down_proj.weight
25-01-23 08:28:25 | I |       - range scale = [    1.0000]
25-01-23 08:28:25 | I |         sum  error  = [    0.1524]
25-01-23 08:28:25 | I |         best error  = [    0.1524]
25-01-23 08:28:25 | I |     + error = [0.1524]
25-01-23 08:28:26 | I |       - range scale = [    1.0000]
25-01-23 08:28:26 | I |         sum  error  = [    1.4872]
25-01-23 08:28:26 | I |         best error  = [    1.4872]
25-01-23 08:28:26 | I |     + error = [1.4872]
25-01-23 08:28:27 | I |   - Quantizing model.layers.0.self_attn.q_proj.weight
25-01-23 08:28:29 | I |   - Quantizing model.layers.0.self_attn.k_proj.weight
25-01-23 08:28:31 | I |   - Quantizing model.layers.0.self_attn.v_proj.weight
25-01-23 08:28:33 | I |   - Quantizing model.layers.0.self_attn.o_proj.weight
25-01-23 08:28:36 | I |   - Quantizing model.layers.0.mlp.up_proj.weight
25-01-23 08:28:38 | I |   - Quantizing model.layers.0.mlp.gate_proj.weight
25-01-23 08:28:40 | I |   - Quantizing model.layers.0.mlp.down_proj.weight
25-01-23 08:28:46 | I | quantizing activations for layer model.layers.0
25-01-23 08:28:47 | I | collecting calibration activations in model.layers.0
25-01-23 08:28:47 | I | collecting calibration activations in model.layers.0
25-01-23 08:28:48 | I | forward this layer
25-01-23 08:28:48 | I | input_file: /data/gyy/lmquant-main/lmquant/data/data_without_preprocess_llama_7b_bs16/shard/val_input_args_layer_0/44.pt
25-01-23 08:28:48 | I | output_file: /data/gyy/lmquant-main/lmquant/data/data_without_preprocess_llama_7b_bs16/shard/val_teacher_output_layer_0/44.pt
25-01-23 08:28:49 | I | In layer model.layers.0, gradient of model.layers.0.self_attn.q_proj has nan: False
25-01-23 08:28:49 | I | In layer model.layers.0, gradient of model.layers.0.self_attn.q_proj has inf: False
25-01-23 08:28:49 | I | In layer model.layers.0, gradient of model.layers.0.self_attn.k_proj has nan: False
25-01-23 08:28:49 | I | In layer model.layers.0, gradient of model.layers.0.self_attn.k_proj has inf: False
25-01-23 08:28:49 | I | In layer model.layers.0, gradient of model.layers.0.self_attn.v_proj has nan: False
25-01-23 08:28:49 | I | In layer model.layers.0, gradient of model.layers.0.self_attn.v_proj has inf: False
25-01-23 08:28:49 | I | In layer model.layers.0, gradient of model.layers.0.self_attn.o_proj has nan: False
25-01-23 08:28:49 | I | In layer model.layers.0, gradient of model.layers.0.self_attn.o_proj has inf: False
25-01-23 08:28:49 | I | In layer model.layers.0, gradient of model.layers.0.mlp.up_proj has nan: False
25-01-23 08:28:49 | I | In layer model.layers.0, gradient of model.layers.0.mlp.up_proj has inf: False
25-01-23 08:28:49 | I | In layer model.layers.0, gradient of model.layers.0.mlp.gate_proj has nan: False
25-01-23 08:28:49 | I | In layer model.layers.0, gradient of model.layers.0.mlp.gate_proj has inf: False
25-01-23 08:28:49 | I | In layer model.layers.0, gradient of model.layers.0.mlp.down_proj has nan: False
25-01-23 08:28:49 | I | In layer model.layers.0, gradient of model.layers.0.mlp.down_proj has inf: False
25-01-23 08:28:49 | I | [40] done with optimizer step
25-01-23 08:28:49 | I | epoch 001:     45 / 819200000 loss=1.25353e-05, loss_per_token=0.0513445, loss_sum=420.614, wps=14.1, ups=0, wpb=8192, bsz=16, num_updates=41, lr=0.0041, gnorm=0.456, clip=0, loss_scale=8, train_wall=36, cuda_gb_allocated=17.2, cuda_gb_reserved=21.5, cuda_gb_free=6.5, wall=3876
25-01-23 08:28:49 | I | TRAIN CURRENT LAYER_IDX = 0
25-01-23 08:28:49 | I | in layer model.layers.0
25-01-23 08:28:49 | I | quantizing weights for layer model.layers.0
25-01-23 08:28:49 | I | collecting calibration activations in model.layers.0
25-01-23 08:28:49 | I | collecting calibration activations in model.layers.0
25-01-23 08:28:50 | I | - Quantizing decoder layer model.layers.0
25-01-23 08:28:50 | I |   - Calibrating model.layers.0.self_attn.q_proj.weight
25-01-23 08:28:50 | I |       - range scale = [    1.0000]
25-01-23 08:28:50 | I |         sum  error  = [    0.0429]
25-01-23 08:28:50 | I |         best error  = [    0.0429]
25-01-23 08:28:50 | I |     + error = [0.0429]
25-01-23 08:28:51 | I |       - range scale = [    1.0000]
25-01-23 08:28:51 | I |         sum  error  = [    0.6608]
25-01-23 08:28:51 | I |         best error  = [    0.6608]
25-01-23 08:28:51 | I |     + error = [0.6608]
25-01-23 08:28:51 | I |   - Calibrating model.layers.0.self_attn.k_proj.weight
25-01-23 08:28:52 | I |       - range scale = [    1.0000]
25-01-23 08:28:52 | I |         sum  error  = [    0.0526]
25-01-23 08:28:52 | I |         best error  = [    0.0526]
25-01-23 08:28:52 | I |     + error = [0.0526]
25-01-23 08:28:53 | I |       - range scale = [    1.0000]
25-01-23 08:28:53 | I |         sum  error  = [    0.9091]
25-01-23 08:28:53 | I |         best error  = [    0.9091]
25-01-23 08:28:53 | I |     + error = [0.9091]
25-01-23 08:28:53 | I |   - Calibrating model.layers.0.self_attn.v_proj.weight
25-01-23 08:28:54 | I |       - range scale = [    1.0000]
25-01-23 08:28:54 | I |         sum  error  = [    0.6911]
25-01-23 08:28:54 | I |         best error  = [    0.6911]
25-01-23 08:28:54 | I |     + error = [0.6911]
25-01-23 08:28:54 | I |       - range scale = [    1.0000]
25-01-23 08:28:54 | I |         sum  error  = [    6.3940]
25-01-23 08:28:54 | I |         best error  = [    6.3940]
25-01-23 08:28:54 | I |     + error = [6.3940]
25-01-23 08:28:55 | I |   - Calibrating model.layers.0.self_attn.o_proj.weight
25-01-23 08:28:55 | I |       - range scale = [    1.0000]
25-01-23 08:28:55 | I |         sum  error  = [    0.1983]
25-01-23 08:28:55 | I |         best error  = [    0.1983]
25-01-23 08:28:55 | I |     + error = [0.1983]
25-01-23 08:28:56 | I |       - range scale = [    1.0000]
25-01-23 08:28:56 | I |         sum  error  = [    1.5614]
25-01-23 08:28:56 | I |         best error  = [    1.5614]
25-01-23 08:28:56 | I |     + error = [1.5614]
25-01-23 08:28:56 | I |   - Calibrating model.layers.0.mlp.up_proj.weight
25-01-23 08:28:57 | I |       - range scale = [    1.0000]
25-01-23 08:28:57 | I |         sum  error  = [    1.0616]
25-01-23 08:28:57 | I |         best error  = [    1.0616]
25-01-23 08:28:57 | I |     + error = [1.0616]
25-01-23 08:28:58 | I |       - range scale = [    1.0000]
25-01-23 08:28:58 | I |         sum  error  = [   11.2904]
25-01-23 08:28:58 | I |         best error  = [   11.2904]
25-01-23 08:28:58 | I |     + error = [11.2904]
25-01-23 08:28:58 | I |   - Calibrating model.layers.0.mlp.gate_proj.weight
25-01-23 08:28:59 | I |       - range scale = [    1.0000]
25-01-23 08:28:59 | I |         sum  error  = [    1.1232]
25-01-23 08:28:59 | I |         best error  = [    1.1232]
25-01-23 08:28:59 | I |     + error = [1.1232]
25-01-23 08:29:00 | I |       - range scale = [    1.0000]
25-01-23 08:29:00 | I |         sum  error  = [   11.6831]
25-01-23 08:29:00 | I |         best error  = [   11.6831]
25-01-23 08:29:00 | I |     + error = [11.6831]
25-01-23 08:29:00 | I |   - Calibrating model.layers.0.mlp.down_proj.weight
25-01-23 08:29:01 | I |       - range scale = [    1.0000]
25-01-23 08:29:01 | I |         sum  error  = [    0.1595]
25-01-23 08:29:01 | I |         best error  = [    0.1595]
25-01-23 08:29:01 | I |     + error = [0.1595]
25-01-23 08:29:02 | I |       - range scale = [    1.0000]
25-01-23 08:29:02 | I |         sum  error  = [    1.5521]
25-01-23 08:29:02 | I |         best error  = [    1.5521]
25-01-23 08:29:02 | I |     + error = [1.5521]
25-01-23 08:29:02 | I |   - Quantizing model.layers.0.self_attn.q_proj.weight
25-01-23 08:29:05 | I |   - Quantizing model.layers.0.self_attn.k_proj.weight
25-01-23 08:29:07 | I |   - Quantizing model.layers.0.self_attn.v_proj.weight
25-01-23 08:29:09 | I |   - Quantizing model.layers.0.self_attn.o_proj.weight
25-01-23 08:29:12 | I |   - Quantizing model.layers.0.mlp.up_proj.weight
25-01-23 08:29:14 | I |   - Quantizing model.layers.0.mlp.gate_proj.weight
25-01-23 08:29:16 | I |   - Quantizing model.layers.0.mlp.down_proj.weight
25-01-23 08:29:22 | I | quantizing activations for layer model.layers.0
25-01-23 08:29:22 | I | collecting calibration activations in model.layers.0
25-01-23 08:29:22 | I | collecting calibration activations in model.layers.0
25-01-23 08:29:24 | I | forward this layer
25-01-23 08:29:24 | I | input_file: /data/gyy/lmquant-main/lmquant/data/data_without_preprocess_llama_7b_bs16/shard/val_input_args_layer_0/45.pt
25-01-23 08:29:24 | I | output_file: /data/gyy/lmquant-main/lmquant/data/data_without_preprocess_llama_7b_bs16/shard/val_teacher_output_layer_0/45.pt
25-01-23 08:29:25 | I | In layer model.layers.0, gradient of model.layers.0.self_attn.q_proj has nan: False
25-01-23 08:29:25 | I | In layer model.layers.0, gradient of model.layers.0.self_attn.q_proj has inf: False
25-01-23 08:29:25 | I | In layer model.layers.0, gradient of model.layers.0.self_attn.k_proj has nan: False
25-01-23 08:29:25 | I | In layer model.layers.0, gradient of model.layers.0.self_attn.k_proj has inf: False
25-01-23 08:29:25 | I | In layer model.layers.0, gradient of model.layers.0.self_attn.v_proj has nan: False
25-01-23 08:29:25 | I | In layer model.layers.0, gradient of model.layers.0.self_attn.v_proj has inf: False
25-01-23 08:29:25 | I | In layer model.layers.0, gradient of model.layers.0.self_attn.o_proj has nan: False
25-01-23 08:29:25 | I | In layer model.layers.0, gradient of model.layers.0.self_attn.o_proj has inf: False
25-01-23 08:29:25 | I | In layer model.layers.0, gradient of model.layers.0.mlp.up_proj has nan: False
25-01-23 08:29:25 | I | In layer model.layers.0, gradient of model.layers.0.mlp.up_proj has inf: False
25-01-23 08:29:25 | I | In layer model.layers.0, gradient of model.layers.0.mlp.gate_proj has nan: False
25-01-23 08:29:25 | I | In layer model.layers.0, gradient of model.layers.0.mlp.gate_proj has inf: False
25-01-23 08:29:25 | I | In layer model.layers.0, gradient of model.layers.0.mlp.down_proj has nan: False
25-01-23 08:29:25 | I | In layer model.layers.0, gradient of model.layers.0.mlp.down_proj has inf: False
25-01-23 08:29:25 | I | [41] done with optimizer step
25-01-23 08:29:25 | I | epoch 001:     46 / 819200000 loss=1.24675e-05, loss_per_token=0.0510669, loss_sum=418.34, wps=229.5, ups=0.03, wpb=8192, bsz=16, num_updates=42, lr=0.0042, gnorm=0.167, clip=0, loss_scale=8, train_wall=36, cuda_gb_allocated=17.1, cuda_gb_reserved=18.3, cuda_gb_free=6.6, wall=3912
25-01-23 08:29:25 | I | TRAIN CURRENT LAYER_IDX = 0
25-01-23 08:29:25 | I | in layer model.layers.0
25-01-23 08:29:25 | I | quantizing weights for layer model.layers.0
25-01-23 08:29:25 | I | collecting calibration activations in model.layers.0
25-01-23 08:29:25 | I | collecting calibration activations in model.layers.0
25-01-23 08:29:25 | I | - Quantizing decoder layer model.layers.0
25-01-23 08:29:25 | I |   - Calibrating model.layers.0.self_attn.q_proj.weight
25-01-23 08:29:26 | I |       - range scale = [    1.0000]
25-01-23 08:29:26 | I |         sum  error  = [    0.0427]
25-01-23 08:29:26 | I |         best error  = [    0.0427]
25-01-23 08:29:26 | I |     + error = [0.0427]
25-01-23 08:29:27 | I |       - range scale = [    1.0000]
25-01-23 08:29:27 | I |         sum  error  = [    0.6616]
25-01-23 08:29:27 | I |         best error  = [    0.6616]
25-01-23 08:29:27 | I |     + error = [0.6616]
25-01-23 08:29:27 | I |   - Calibrating model.layers.0.self_attn.k_proj.weight
25-01-23 08:29:28 | I |       - range scale = [    1.0000]
25-01-23 08:29:28 | I |         sum  error  = [    0.0526]
25-01-23 08:29:28 | I |         best error  = [    0.0526]
25-01-23 08:29:28 | I |     + error = [0.0526]
25-01-23 08:29:28 | I |       - range scale = [    1.0000]
25-01-23 08:29:28 | I |         sum  error  = [    0.9020]
25-01-23 08:29:28 | I |         best error  = [    0.9020]
25-01-23 08:29:28 | I |     + error = [0.9020]
25-01-23 08:29:29 | I |   - Calibrating model.layers.0.self_attn.v_proj.weight
25-01-23 08:29:29 | I |       - range scale = [    1.0000]
25-01-23 08:29:29 | I |         sum  error  = [    0.7077]
25-01-23 08:29:29 | I |         best error  = [    0.7077]
25-01-23 08:29:29 | I |     + error = [0.7077]
25-01-23 08:29:30 | I |       - range scale = [    1.0000]
25-01-23 08:29:30 | I |         sum  error  = [    6.5902]
25-01-23 08:29:30 | I |         best error  = [    6.5902]
25-01-23 08:29:30 | I |     + error = [6.5902]
25-01-23 08:29:30 | I |   - Calibrating model.layers.0.self_attn.o_proj.weight
25-01-23 08:29:31 | I |       - range scale = [    1.0000]
25-01-23 08:29:31 | I |         sum  error  = [    0.2067]
25-01-23 08:29:31 | I |         best error  = [    0.2067]
25-01-23 08:29:31 | I |     + error = [0.2067]
25-01-23 08:29:32 | I |       - range scale = [    1.0000]
25-01-23 08:29:32 | I |         sum  error  = [    1.6460]
25-01-23 08:29:32 | I |         best error  = [    1.6460]
25-01-23 08:29:32 | I |     + error = [1.6460]
25-01-23 08:29:32 | I |   - Calibrating model.layers.0.mlp.up_proj.weight
25-01-23 08:29:33 | I |       - range scale = [    1.0000]
25-01-23 08:29:33 | I |         sum  error  = [    1.0905]
25-01-23 08:29:33 | I |         best error  = [    1.0905]
25-01-23 08:29:33 | I |     + error = [1.0905]
25-01-23 08:29:34 | I |       - range scale = [    1.0000]
25-01-23 08:29:34 | I |         sum  error  = [   11.5954]
25-01-23 08:29:34 | I |         best error  = [   11.5954]
25-01-23 08:29:34 | I |     + error = [11.5954]
25-01-23 08:29:34 | I |   - Calibrating model.layers.0.mlp.gate_proj.weight
25-01-23 08:29:35 | I |       - range scale = [    1.0000]
25-01-23 08:29:35 | I |         sum  error  = [    1.1531]
25-01-23 08:29:35 | I |         best error  = [    1.1531]
25-01-23 08:29:35 | I |     + error = [1.1531]
25-01-23 08:29:36 | I |       - range scale = [    1.0000]
25-01-23 08:29:36 | I |         sum  error  = [   11.9890]
25-01-23 08:29:36 | I |         best error  = [   11.9890]
25-01-23 08:29:36 | I |     + error = [11.9890]
25-01-23 08:29:36 | I |   - Calibrating model.layers.0.mlp.down_proj.weight
25-01-23 08:29:37 | I |       - range scale = [    1.0000]
25-01-23 08:29:37 | I |         sum  error  = [    0.1681]
25-01-23 08:29:37 | I |         best error  = [    0.1681]
25-01-23 08:29:37 | I |     + error = [0.1681]
25-01-23 08:29:38 | I |       - range scale = [    1.0000]
25-01-23 08:29:38 | I |         sum  error  = [    1.6383]
25-01-23 08:29:38 | I |         best error  = [    1.6383]
25-01-23 08:29:38 | I |     + error = [1.6383]
25-01-23 08:29:38 | I |   - Quantizing model.layers.0.self_attn.q_proj.weight
25-01-23 08:29:41 | I |   - Quantizing model.layers.0.self_attn.k_proj.weight
25-01-23 08:29:43 | I |   - Quantizing model.layers.0.self_attn.v_proj.weight
25-01-23 08:29:45 | I |   - Quantizing model.layers.0.self_attn.o_proj.weight
25-01-23 08:29:48 | I |   - Quantizing model.layers.0.mlp.up_proj.weight
25-01-23 08:29:50 | I |   - Quantizing model.layers.0.mlp.gate_proj.weight
25-01-23 08:29:52 | I |   - Quantizing model.layers.0.mlp.down_proj.weight
25-01-23 08:29:58 | I | quantizing activations for layer model.layers.0
25-01-23 08:29:58 | I | collecting calibration activations in model.layers.0
25-01-23 08:29:59 | I | collecting calibration activations in model.layers.0
25-01-23 08:30:00 | I | forward this layer
25-01-23 08:30:00 | I | input_file: /data/gyy/lmquant-main/lmquant/data/data_without_preprocess_llama_7b_bs16/shard/val_input_args_layer_0/46.pt
25-01-23 08:30:00 | I | output_file: /data/gyy/lmquant-main/lmquant/data/data_without_preprocess_llama_7b_bs16/shard/val_teacher_output_layer_0/46.pt
25-01-23 08:30:01 | I | In layer model.layers.0, gradient of model.layers.0.self_attn.q_proj has nan: False
25-01-23 08:30:01 | I | In layer model.layers.0, gradient of model.layers.0.self_attn.q_proj has inf: False
25-01-23 08:30:01 | I | In layer model.layers.0, gradient of model.layers.0.self_attn.k_proj has nan: False
25-01-23 08:30:01 | I | In layer model.layers.0, gradient of model.layers.0.self_attn.k_proj has inf: False
25-01-23 08:30:01 | I | In layer model.layers.0, gradient of model.layers.0.self_attn.v_proj has nan: False
25-01-23 08:30:01 | I | In layer model.layers.0, gradient of model.layers.0.self_attn.v_proj has inf: False
25-01-23 08:30:01 | I | In layer model.layers.0, gradient of model.layers.0.self_attn.o_proj has nan: False
25-01-23 08:30:01 | I | In layer model.layers.0, gradient of model.layers.0.self_attn.o_proj has inf: False
25-01-23 08:30:01 | I | In layer model.layers.0, gradient of model.layers.0.mlp.up_proj has nan: False
25-01-23 08:30:01 | I | In layer model.layers.0, gradient of model.layers.0.mlp.up_proj has inf: False
25-01-23 08:30:01 | I | In layer model.layers.0, gradient of model.layers.0.mlp.gate_proj has nan: False
25-01-23 08:30:01 | I | In layer model.layers.0, gradient of model.layers.0.mlp.gate_proj has inf: False
25-01-23 08:30:01 | I | In layer model.layers.0, gradient of model.layers.0.mlp.down_proj has nan: False
25-01-23 08:30:01 | I | In layer model.layers.0, gradient of model.layers.0.mlp.down_proj has inf: False
25-01-23 08:30:01 | I | [42] done with optimizer step
25-01-23 08:30:01 | I | epoch 001:     47 / 819200000 loss=1.27223e-05, loss_per_token=0.0521105, loss_sum=426.889, wps=226, ups=0.03, wpb=8192, bsz=16, num_updates=43, lr=0.0043, gnorm=0.129, clip=0, loss_scale=8, train_wall=36, cuda_gb_allocated=17.1, cuda_gb_reserved=18.3, cuda_gb_free=6.6, wall=3948
25-01-23 08:30:01 | I | TRAIN CURRENT LAYER_IDX = 0
25-01-23 08:30:01 | I | in layer model.layers.0
25-01-23 08:30:01 | I | quantizing weights for layer model.layers.0
25-01-23 08:30:01 | I | collecting calibration activations in model.layers.0
25-01-23 08:30:01 | I | collecting calibration activations in model.layers.0
25-01-23 08:30:02 | I | - Quantizing decoder layer model.layers.0
25-01-23 08:30:02 | I |   - Calibrating model.layers.0.self_attn.q_proj.weight
25-01-23 08:30:02 | I |       - range scale = [    1.0000]
25-01-23 08:30:02 | I |         sum  error  = [    0.0447]
25-01-23 08:30:02 | I |         best error  = [    0.0447]
25-01-23 08:30:02 | I |     + error = [0.0447]
25-01-23 08:30:03 | I |       - range scale = [    1.0000]
25-01-23 08:30:03 | I |         sum  error  = [    0.6893]
25-01-23 08:30:03 | I |         best error  = [    0.6893]
25-01-23 08:30:03 | I |     + error = [0.6893]
25-01-23 08:30:03 | I |   - Calibrating model.layers.0.self_attn.k_proj.weight
25-01-23 08:30:04 | I |       - range scale = [    1.0000]
25-01-23 08:30:04 | I |         sum  error  = [    0.0539]
25-01-23 08:30:04 | I |         best error  = [    0.0539]
25-01-23 08:30:04 | I |     + error = [0.0539]
25-01-23 08:30:05 | I |       - range scale = [    1.0000]
25-01-23 08:30:05 | I |         sum  error  = [    0.8931]
25-01-23 08:30:05 | I |         best error  = [    0.8931]
25-01-23 08:30:05 | I |     + error = [0.8931]
25-01-23 08:30:05 | I |   - Calibrating model.layers.0.self_attn.v_proj.weight
25-01-23 08:30:06 | I |       - range scale = [    1.0000]
25-01-23 08:30:06 | I |         sum  error  = [    0.6907]
25-01-23 08:30:06 | I |         best error  = [    0.6907]
25-01-23 08:30:06 | I |     + error = [0.6907]
25-01-23 08:30:06 | I |       - range scale = [    1.0000]
25-01-23 08:30:06 | I |         sum  error  = [    6.4276]
25-01-23 08:30:06 | I |         best error  = [    6.4276]
25-01-23 08:30:06 | I |     + error = [6.4276]
25-01-23 08:30:07 | I |   - Calibrating model.layers.0.self_attn.o_proj.weight
25-01-23 08:30:07 | I |       - range scale = [    1.0000]
25-01-23 08:30:07 | I |         sum  error  = [    0.2023]
25-01-23 08:30:07 | I |         best error  = [    0.2023]
25-01-23 08:30:07 | I |     + error = [0.2023]
25-01-23 08:30:08 | I |       - range scale = [    1.0000]
25-01-23 08:30:08 | I |         sum  error  = [    1.6101]
25-01-23 08:30:08 | I |         best error  = [    1.6101]
25-01-23 08:30:08 | I |     + error = [1.6101]
25-01-23 08:30:08 | I |   - Calibrating model.layers.0.mlp.up_proj.weight
25-01-23 08:30:09 | I |       - range scale = [    1.0000]
25-01-23 08:30:09 | I |         sum  error  = [    1.0938]
25-01-23 08:30:09 | I |         best error  = [    1.0938]
25-01-23 08:30:09 | I |     + error = [1.0938]
25-01-23 08:30:10 | I |       - range scale = [    1.0000]
25-01-23 08:30:10 | I |         sum  error  = [   11.6326]
25-01-23 08:30:10 | I |         best error  = [   11.6326]
25-01-23 08:30:10 | I |     + error = [11.6326]
25-01-23 08:30:11 | I |   - Calibrating model.layers.0.mlp.gate_proj.weight
25-01-23 08:30:11 | I |       - range scale = [    1.0000]
25-01-23 08:30:11 | I |         sum  error  = [    1.1579]
25-01-23 08:30:11 | I |         best error  = [    1.1579]
25-01-23 08:30:11 | I |     + error = [1.1579]
25-01-23 08:30:12 | I |       - range scale = [    1.0000]
25-01-23 08:30:12 | I |         sum  error  = [   12.0210]
25-01-23 08:30:12 | I |         best error  = [   12.0210]
25-01-23 08:30:12 | I |     + error = [12.0210]
25-01-23 08:30:13 | I |   - Calibrating model.layers.0.mlp.down_proj.weight
25-01-23 08:30:13 | I |       - range scale = [    1.0000]
25-01-23 08:30:13 | I |         sum  error  = [    0.1652]
25-01-23 08:30:13 | I |         best error  = [    0.1652]
25-01-23 08:30:13 | I |     + error = [0.1652]
25-01-23 08:30:14 | I |       - range scale = [    1.0000]
25-01-23 08:30:14 | I |         sum  error  = [    1.6075]
25-01-23 08:30:14 | I |         best error  = [    1.6075]
25-01-23 08:30:14 | I |     + error = [1.6075]
25-01-23 08:30:15 | I |   - Quantizing model.layers.0.self_attn.q_proj.weight
25-01-23 08:30:17 | I |   - Quantizing model.layers.0.self_attn.k_proj.weight
25-01-23 08:30:19 | I |   - Quantizing model.layers.0.self_attn.v_proj.weight
25-01-23 08:30:22 | I |   - Quantizing model.layers.0.self_attn.o_proj.weight
25-01-23 08:30:24 | I |   - Quantizing model.layers.0.mlp.up_proj.weight
25-01-23 08:30:26 | I |   - Quantizing model.layers.0.mlp.gate_proj.weight
25-01-23 08:30:29 | I |   - Quantizing model.layers.0.mlp.down_proj.weight
25-01-23 08:30:35 | I | quantizing activations for layer model.layers.0
25-01-23 08:30:35 | I | collecting calibration activations in model.layers.0
25-01-23 08:30:35 | I | collecting calibration activations in model.layers.0
25-01-23 08:30:37 | I | forward this layer
25-01-23 08:30:37 | I | input_file: /data/gyy/lmquant-main/lmquant/data/data_without_preprocess_llama_7b_bs16/shard/val_input_args_layer_0/47.pt
25-01-23 08:30:37 | I | output_file: /data/gyy/lmquant-main/lmquant/data/data_without_preprocess_llama_7b_bs16/shard/val_teacher_output_layer_0/47.pt
25-01-23 08:30:37 | I | In layer model.layers.0, gradient of model.layers.0.self_attn.q_proj has nan: False
25-01-23 08:30:37 | I | In layer model.layers.0, gradient of model.layers.0.self_attn.q_proj has inf: False
25-01-23 08:30:37 | I | In layer model.layers.0, gradient of model.layers.0.self_attn.k_proj has nan: False
25-01-23 08:30:37 | I | In layer model.layers.0, gradient of model.layers.0.self_attn.k_proj has inf: False
25-01-23 08:30:37 | I | In layer model.layers.0, gradient of model.layers.0.self_attn.v_proj has nan: False
25-01-23 08:30:37 | I | In layer model.layers.0, gradient of model.layers.0.self_attn.v_proj has inf: False
25-01-23 08:30:37 | I | In layer model.layers.0, gradient of model.layers.0.self_attn.o_proj has nan: False
25-01-23 08:30:37 | I | In layer model.layers.0, gradient of model.layers.0.self_attn.o_proj has inf: False
25-01-23 08:30:37 | I | In layer model.layers.0, gradient of model.layers.0.mlp.up_proj has nan: False
25-01-23 08:30:37 | I | In layer model.layers.0, gradient of model.layers.0.mlp.up_proj has inf: False
25-01-23 08:30:37 | I | In layer model.layers.0, gradient of model.layers.0.mlp.gate_proj has nan: False
25-01-23 08:30:37 | I | In layer model.layers.0, gradient of model.layers.0.mlp.gate_proj has inf: False
25-01-23 08:30:37 | I | In layer model.layers.0, gradient of model.layers.0.mlp.down_proj has nan: False
25-01-23 08:30:37 | I | In layer model.layers.0, gradient of model.layers.0.mlp.down_proj has inf: False
25-01-23 08:30:37 | I | [43] done with optimizer step
25-01-23 08:30:37 | I | epoch 001:     48 / 819200000 loss=1.25564e-05, loss_per_token=0.0514309, loss_sum=421.322, wps=224.6, ups=0.03, wpb=8192, bsz=16, num_updates=44, lr=0.0044, gnorm=0.188, clip=0, loss_scale=8, train_wall=36, cuda_gb_allocated=17.1, cuda_gb_reserved=18.3, cuda_gb_free=6.6, wall=3985
25-01-23 08:30:37 | I | TRAIN CURRENT LAYER_IDX = 0
25-01-23 08:30:37 | I | in layer model.layers.0
25-01-23 08:30:37 | I | quantizing weights for layer model.layers.0
25-01-23 08:30:38 | I | collecting calibration activations in model.layers.0
25-01-23 08:30:38 | I | collecting calibration activations in model.layers.0
25-01-23 08:30:38 | I | - Quantizing decoder layer model.layers.0
25-01-23 08:30:38 | I |   - Calibrating model.layers.0.self_attn.q_proj.weight
25-01-23 08:30:39 | I |       - range scale = [    1.0000]
25-01-23 08:30:39 | I |         sum  error  = [    0.0475]
25-01-23 08:30:39 | I |         best error  = [    0.0475]
25-01-23 08:30:39 | I |     + error = [0.0475]
25-01-23 08:30:40 | I |       - range scale = [    1.0000]
25-01-23 08:30:40 | I |         sum  error  = [    0.7248]
25-01-23 08:30:40 | I |         best error  = [    0.7248]
25-01-23 08:30:40 | I |     + error = [0.7248]
25-01-23 08:30:40 | I |   - Calibrating model.layers.0.self_attn.k_proj.weight
25-01-23 08:30:40 | I |       - range scale = [    1.0000]
25-01-23 08:30:40 | I |         sum  error  = [    0.0639]
25-01-23 08:30:40 | I |         best error  = [    0.0639]
25-01-23 08:30:40 | I |     + error = [0.0639]
25-01-23 08:30:42 | I |       - range scale = [    1.0000]
25-01-23 08:30:42 | I |         sum  error  = [    0.9687]
25-01-23 08:30:42 | I |         best error  = [    0.9687]
25-01-23 08:30:42 | I |     + error = [0.9687]
25-01-23 08:30:42 | I |   - Calibrating model.layers.0.self_attn.v_proj.weight
25-01-23 08:30:43 | I |       - range scale = [    1.0000]
25-01-23 08:30:43 | I |         sum  error  = [    0.7123]
25-01-23 08:30:43 | I |         best error  = [    0.7123]
25-01-23 08:30:43 | I |     + error = [0.7123]
25-01-23 08:30:44 | I |       - range scale = [    1.0000]
25-01-23 08:30:44 | I |         sum  error  = [    6.6492]
25-01-23 08:30:44 | I |         best error  = [    6.6492]
25-01-23 08:30:44 | I |     + error = [6.6492]
25-01-23 08:30:44 | I |   - Calibrating model.layers.0.self_attn.o_proj.weight
25-01-23 08:30:44 | I |       - range scale = [    1.0000]
25-01-23 08:30:44 | I |         sum  error  = [    0.2039]
25-01-23 08:30:44 | I |         best error  = [    0.2039]
25-01-23 08:30:44 | I |     + error = [0.2039]
25-01-23 08:30:45 | I |       - range scale = [    1.0000]
25-01-23 08:30:45 | I |         sum  error  = [    1.6278]
25-01-23 08:30:45 | I |         best error  = [    1.6278]
25-01-23 08:30:45 | I |     + error = [1.6278]
25-01-23 08:30:45 | I |   - Calibrating model.layers.0.mlp.up_proj.weight
25-01-23 08:30:46 | I |       - range scale = [    1.0000]
25-01-23 08:30:46 | I |         sum  error  = [    1.0854]
25-01-23 08:30:46 | I |         best error  = [    1.0854]
25-01-23 08:30:46 | I |     + error = [1.0854]
25-01-23 08:30:47 | I |       - range scale = [    1.0000]
25-01-23 08:30:47 | I |         sum  error  = [   11.5494]
25-01-23 08:30:47 | I |         best error  = [   11.5494]
25-01-23 08:30:47 | I |     + error = [11.5494]
25-01-23 08:30:48 | I |   - Calibrating model.layers.0.mlp.gate_proj.weight
25-01-23 08:30:48 | I |       - range scale = [    1.0000]
25-01-23 08:30:48 | I |         sum  error  = [    1.1468]
25-01-23 08:30:48 | I |         best error  = [    1.1468]
25-01-23 08:30:48 | I |     + error = [1.1468]
25-01-23 08:30:49 | I |       - range scale = [    1.0000]
25-01-23 08:30:49 | I |         sum  error  = [   11.9340]
25-01-23 08:30:49 | I |         best error  = [   11.9340]
25-01-23 08:30:49 | I |     + error = [11.9340]
25-01-23 08:30:50 | I |   - Calibrating model.layers.0.mlp.down_proj.weight
25-01-23 08:30:50 | I |       - range scale = [    1.0000]
25-01-23 08:30:50 | I |         sum  error  = [    0.1658]
25-01-23 08:30:50 | I |         best error  = [    0.1658]
25-01-23 08:30:50 | I |     + error = [0.1658]
25-01-23 08:30:51 | I |       - range scale = [    1.0000]
25-01-23 08:30:51 | I |         sum  error  = [    1.6135]
25-01-23 08:30:51 | I |         best error  = [    1.6135]
25-01-23 08:30:51 | I |     + error = [1.6135]
25-01-23 08:30:52 | I |   - Quantizing model.layers.0.self_attn.q_proj.weight
25-01-23 08:30:54 | I |   - Quantizing model.layers.0.self_attn.k_proj.weight
25-01-23 08:30:56 | I |   - Quantizing model.layers.0.self_attn.v_proj.weight
25-01-23 08:30:58 | I |   - Quantizing model.layers.0.self_attn.o_proj.weight
25-01-23 08:31:01 | I |   - Quantizing model.layers.0.mlp.up_proj.weight
25-01-23 08:31:03 | I |   - Quantizing model.layers.0.mlp.gate_proj.weight
25-01-23 08:31:05 | I |   - Quantizing model.layers.0.mlp.down_proj.weight
25-01-23 08:31:11 | I | quantizing activations for layer model.layers.0
25-01-23 08:31:12 | I | collecting calibration activations in model.layers.0
25-01-23 08:31:12 | I | collecting calibration activations in model.layers.0
25-01-23 08:31:14 | I | forward this layer
25-01-23 08:31:14 | I | input_file: /data/gyy/lmquant-main/lmquant/data/data_without_preprocess_llama_7b_bs16/shard/val_input_args_layer_0/48.pt
25-01-23 08:31:14 | I | output_file: /data/gyy/lmquant-main/lmquant/data/data_without_preprocess_llama_7b_bs16/shard/val_teacher_output_layer_0/48.pt
25-01-23 08:31:14 | I | In layer model.layers.0, gradient of model.layers.0.self_attn.q_proj has nan: False
25-01-23 08:31:14 | I | In layer model.layers.0, gradient of model.layers.0.self_attn.q_proj has inf: False
25-01-23 08:31:14 | I | In layer model.layers.0, gradient of model.layers.0.self_attn.k_proj has nan: False
25-01-23 08:31:14 | I | In layer model.layers.0, gradient of model.layers.0.self_attn.k_proj has inf: False
25-01-23 08:31:14 | I | In layer model.layers.0, gradient of model.layers.0.self_attn.v_proj has nan: False
25-01-23 08:31:14 | I | In layer model.layers.0, gradient of model.layers.0.self_attn.v_proj has inf: False
25-01-23 08:31:14 | I | In layer model.layers.0, gradient of model.layers.0.self_attn.o_proj has nan: False
25-01-23 08:31:14 | I | In layer model.layers.0, gradient of model.layers.0.self_attn.o_proj has inf: False
25-01-23 08:31:14 | I | In layer model.layers.0, gradient of model.layers.0.mlp.up_proj has nan: False
25-01-23 08:31:14 | I | In layer model.layers.0, gradient of model.layers.0.mlp.up_proj has inf: False
25-01-23 08:31:14 | I | In layer model.layers.0, gradient of model.layers.0.mlp.gate_proj has nan: False
25-01-23 08:31:14 | I | In layer model.layers.0, gradient of model.layers.0.mlp.gate_proj has inf: False
25-01-23 08:31:14 | I | In layer model.layers.0, gradient of model.layers.0.mlp.down_proj has nan: False
25-01-23 08:31:14 | I | In layer model.layers.0, gradient of model.layers.0.mlp.down_proj has inf: False
25-01-23 08:31:14 | I | [44] done with optimizer step
25-01-23 08:31:14 | I | epoch 001:     49 / 819200000 loss=1.23457e-05, loss_per_token=0.0505681, loss_sum=414.254, wps=221.4, ups=0.03, wpb=8192, bsz=16, num_updates=45, lr=0.0045, gnorm=0.244, clip=0, loss_scale=8, train_wall=37, cuda_gb_allocated=17.1, cuda_gb_reserved=18.4, cuda_gb_free=6.6, wall=4022
25-01-23 08:31:14 | I | TRAIN CURRENT LAYER_IDX = 0
25-01-23 08:31:14 | I | in layer model.layers.0
25-01-23 08:31:14 | I | quantizing weights for layer model.layers.0
25-01-23 08:31:15 | I | collecting calibration activations in model.layers.0
25-01-23 08:31:15 | I | collecting calibration activations in model.layers.0
25-01-23 08:31:15 | I | - Quantizing decoder layer model.layers.0
25-01-23 08:31:15 | I |   - Calibrating model.layers.0.self_attn.q_proj.weight
25-01-23 08:31:16 | I |       - range scale = [    1.0000]
25-01-23 08:31:16 | I |         sum  error  = [    0.0460]
25-01-23 08:31:16 | I |         best error  = [    0.0460]
25-01-23 08:31:16 | I |     + error = [0.0460]
25-01-23 08:31:17 | I |       - range scale = [    1.0000]
25-01-23 08:31:17 | I |         sum  error  = [    0.7111]
25-01-23 08:31:17 | I |         best error  = [    0.7111]
25-01-23 08:31:17 | I |     + error = [0.7111]
25-01-23 08:31:17 | I |   - Calibrating model.layers.0.self_attn.k_proj.weight
25-01-23 08:31:18 | I |       - range scale = [    1.0000]
25-01-23 08:31:18 | I |         sum  error  = [    0.0598]
25-01-23 08:31:18 | I |         best error  = [    0.0598]
25-01-23 08:31:18 | I |     + error = [0.0598]
25-01-23 08:31:18 | I |       - range scale = [    1.0000]
25-01-23 08:31:18 | I |         sum  error  = [    0.9011]
25-01-23 08:31:18 | I |         best error  = [    0.9011]
25-01-23 08:31:18 | I |     + error = [0.9011]
25-01-23 08:31:18 | I |   - Calibrating model.layers.0.self_attn.v_proj.weight
25-01-23 08:31:19 | I |       - range scale = [    1.0000]
25-01-23 08:31:19 | I |         sum  error  = [    0.7029]
25-01-23 08:31:19 | I |         best error  = [    0.7029]
25-01-23 08:31:19 | I |     + error = [0.7029]
25-01-23 08:31:20 | I |       - range scale = [    1.0000]
25-01-23 08:31:20 | I |         sum  error  = [    6.5100]
25-01-23 08:31:20 | I |         best error  = [    6.5100]
25-01-23 08:31:20 | I |     + error = [6.5100]
25-01-23 08:31:20 | I |   - Calibrating model.layers.0.self_attn.o_proj.weight
25-01-23 08:31:21 | I |       - range scale = [    1.0000]
25-01-23 08:31:21 | I |         sum  error  = [    0.1973]
25-01-23 08:31:21 | I |         best error  = [    0.1973]
25-01-23 08:31:21 | I |     + error = [0.1973]
25-01-23 08:31:22 | I |       - range scale = [    1.0000]
25-01-23 08:31:22 | I |         sum  error  = [    1.5671]
25-01-23 08:31:22 | I |         best error  = [    1.5671]
25-01-23 08:31:22 | I |     + error = [1.5671]
25-01-23 08:31:22 | I |   - Calibrating model.layers.0.mlp.up_proj.weight
25-01-23 08:31:23 | I |       - range scale = [    1.0000]
25-01-23 08:31:23 | I |         sum  error  = [    1.0640]
25-01-23 08:31:23 | I |         best error  = [    1.0640]
25-01-23 08:31:23 | I |     + error = [1.0640]
25-01-23 08:31:24 | I |       - range scale = [    1.0000]
25-01-23 08:31:24 | I |         sum  error  = [   11.3248]
25-01-23 08:31:24 | I |         best error  = [   11.3248]
25-01-23 08:31:24 | I |     + error = [11.3248]
25-01-23 08:31:24 | I |   - Calibrating model.layers.0.mlp.gate_proj.weight
25-01-23 08:31:25 | I |       - range scale = [    1.0000]
25-01-23 08:31:25 | I |         sum  error  = [    1.1318]
25-01-23 08:31:25 | I |         best error  = [    1.1318]
25-01-23 08:31:25 | I |     + error = [1.1318]
25-01-23 08:31:26 | I |       - range scale = [    1.0000]
25-01-23 08:31:26 | I |         sum  error  = [   11.7080]
25-01-23 08:31:26 | I |         best error  = [   11.7080]
25-01-23 08:31:26 | I |     + error = [11.7080]
25-01-23 08:31:26 | I |   - Calibrating model.layers.0.mlp.down_proj.weight
25-01-23 08:31:27 | I |       - range scale = [    1.0000]
25-01-23 08:31:27 | I |         sum  error  = [    0.1573]
25-01-23 08:31:27 | I |         best error  = [    0.1573]
25-01-23 08:31:27 | I |     + error = [0.1573]
25-01-23 08:31:28 | I |       - range scale = [    1.0000]
25-01-23 08:31:28 | I |         sum  error  = [    1.5299]
25-01-23 08:31:28 | I |         best error  = [    1.5299]
25-01-23 08:31:28 | I |     + error = [1.5299]
25-01-23 08:31:28 | I |   - Quantizing model.layers.0.self_attn.q_proj.weight
25-01-23 08:31:31 | I |   - Quantizing model.layers.0.self_attn.k_proj.weight
25-01-23 08:31:33 | I |   - Quantizing model.layers.0.self_attn.v_proj.weight
25-01-23 08:31:35 | I |   - Quantizing model.layers.0.self_attn.o_proj.weight
25-01-23 08:31:37 | I |   - Quantizing model.layers.0.mlp.up_proj.weight
25-01-23 08:31:40 | I |   - Quantizing model.layers.0.mlp.gate_proj.weight
25-01-23 08:31:42 | I |   - Quantizing model.layers.0.mlp.down_proj.weight
25-01-23 08:31:48 | I | quantizing activations for layer model.layers.0
25-01-23 08:31:48 | I | collecting calibration activations in model.layers.0
25-01-23 08:31:49 | I | collecting calibration activations in model.layers.0
25-01-23 08:31:50 | I | forward this layer
25-01-23 08:31:50 | I | input_file: /data/gyy/lmquant-main/lmquant/data/data_without_preprocess_llama_7b_bs16/shard/val_input_args_layer_0/49.pt
25-01-23 08:31:50 | I | output_file: /data/gyy/lmquant-main/lmquant/data/data_without_preprocess_llama_7b_bs16/shard/val_teacher_output_layer_0/49.pt
25-01-23 08:31:51 | I | In layer model.layers.0, gradient of model.layers.0.self_attn.q_proj has nan: False
25-01-23 08:31:51 | I | In layer model.layers.0, gradient of model.layers.0.self_attn.q_proj has inf: False
25-01-23 08:31:51 | I | In layer model.layers.0, gradient of model.layers.0.self_attn.k_proj has nan: False
25-01-23 08:31:51 | I | In layer model.layers.0, gradient of model.layers.0.self_attn.k_proj has inf: False
25-01-23 08:31:51 | I | In layer model.layers.0, gradient of model.layers.0.self_attn.v_proj has nan: False
25-01-23 08:31:51 | I | In layer model.layers.0, gradient of model.layers.0.self_attn.v_proj has inf: False
25-01-23 08:31:51 | I | In layer model.layers.0, gradient of model.layers.0.self_attn.o_proj has nan: False
25-01-23 08:31:51 | I | In layer model.layers.0, gradient of model.layers.0.self_attn.o_proj has inf: False
25-01-23 08:31:51 | I | In layer model.layers.0, gradient of model.layers.0.mlp.up_proj has nan: False
25-01-23 08:31:51 | I | In layer model.layers.0, gradient of model.layers.0.mlp.up_proj has inf: False
25-01-23 08:31:51 | I | In layer model.layers.0, gradient of model.layers.0.mlp.gate_proj has nan: False
25-01-23 08:31:51 | I | In layer model.layers.0, gradient of model.layers.0.mlp.gate_proj has inf: False
25-01-23 08:31:51 | I | In layer model.layers.0, gradient of model.layers.0.mlp.down_proj has nan: False
25-01-23 08:31:51 | I | In layer model.layers.0, gradient of model.layers.0.mlp.down_proj has inf: False
25-01-23 08:31:51 | I | [45] done with optimizer step
25-01-23 08:31:51 | I | epoch 001:     50 / 819200000 loss=1.30404e-05, loss_per_token=0.0534134, loss_sum=437.563, wps=224.6, ups=0.03, wpb=8192, bsz=16, num_updates=46, lr=0.0046, gnorm=0.206, clip=0, loss_scale=8, train_wall=36, cuda_gb_allocated=17.1, cuda_gb_reserved=18.3, cuda_gb_free=6.6, wall=4058
25-01-23 08:31:51 | I | TRAIN CURRENT LAYER_IDX = 0
25-01-23 08:31:51 | I | in layer model.layers.0
25-01-23 08:31:51 | I | quantizing weights for layer model.layers.0
25-01-23 08:31:51 | I | collecting calibration activations in model.layers.0
25-01-23 08:31:51 | I | collecting calibration activations in model.layers.0
25-01-23 08:31:52 | I | - Quantizing decoder layer model.layers.0
25-01-23 08:31:52 | I |   - Calibrating model.layers.0.self_attn.q_proj.weight
25-01-23 08:31:52 | I |       - range scale = [    1.0000]
25-01-23 08:31:52 | I |         sum  error  = [    0.0465]
25-01-23 08:31:52 | I |         best error  = [    0.0465]
25-01-23 08:31:52 | I |     + error = [0.0465]
25-01-23 08:31:53 | I |       - range scale = [    1.0000]
25-01-23 08:31:53 | I |         sum  error  = [    0.7123]
25-01-23 08:31:53 | I |         best error  = [    0.7123]
25-01-23 08:31:53 | I |     + error = [0.7123]
25-01-23 08:31:53 | I |   - Calibrating model.layers.0.self_attn.k_proj.weight
25-01-23 08:31:54 | I |       - range scale = [    1.0000]
25-01-23 08:31:54 | I |         sum  error  = [    0.0585]
25-01-23 08:31:54 | I |         best error  = [    0.0585]
25-01-23 08:31:54 | I |     + error = [0.0585]
25-01-23 08:31:55 | I |       - range scale = [    1.0000]
25-01-23 08:31:55 | I |         sum  error  = [    0.9315]
25-01-23 08:31:55 | I |         best error  = [    0.9315]
25-01-23 08:31:55 | I |     + error = [0.9315]
25-01-23 08:31:55 | I |   - Calibrating model.layers.0.self_attn.v_proj.weight
25-01-23 08:31:56 | I |       - range scale = [    1.0000]
25-01-23 08:31:56 | I |         sum  error  = [    0.7058]
25-01-23 08:31:56 | I |         best error  = [    0.7058]
25-01-23 08:31:56 | I |     + error = [0.7058]
25-01-23 08:31:56 | I |       - range scale = [    1.0000]
25-01-23 08:31:56 | I |         sum  error  = [    6.5261]
25-01-23 08:31:56 | I |         best error  = [    6.5261]
25-01-23 08:31:56 | I |     + error = [6.5261]
25-01-23 08:31:57 | I |   - Calibrating model.layers.0.self_attn.o_proj.weight
25-01-23 08:31:57 | I |       - range scale = [    1.0000]
25-01-23 08:31:57 | I |         sum  error  = [    0.2022]
25-01-23 08:31:57 | I |         best error  = [    0.2022]
25-01-23 08:31:57 | I |     + error = [0.2022]
25-01-23 08:31:58 | I |       - range scale = [    1.0000]
25-01-23 08:31:58 | I |         sum  error  = [    1.6054]
25-01-23 08:31:58 | I |         best error  = [    1.6054]
25-01-23 08:31:58 | I |     + error = [1.6054]
25-01-23 08:31:58 | I |   - Calibrating model.layers.0.mlp.up_proj.weight
25-01-23 08:31:59 | I |       - range scale = [    1.0000]
25-01-23 08:31:59 | I |         sum  error  = [    1.0833]
25-01-23 08:31:59 | I |         best error  = [    1.0833]
25-01-23 08:31:59 | I |     + error = [1.0833]
25-01-23 08:32:00 | I |       - range scale = [    1.0000]
25-01-23 08:32:00 | I |         sum  error  = [   11.5196]
25-01-23 08:32:00 | I |         best error  = [   11.5196]
25-01-23 08:32:00 | I |     + error = [11.5196]
25-01-23 08:32:00 | I |   - Calibrating model.layers.0.mlp.gate_proj.weight
25-01-23 08:32:01 | I |       - range scale = [    1.0000]
25-01-23 08:32:01 | I |         sum  error  = [    1.1482]
25-01-23 08:32:01 | I |         best error  = [    1.1482]
25-01-23 08:32:01 | I |     + error = [1.1482]
25-01-23 08:32:02 | I |       - range scale = [    1.0000]
25-01-23 08:32:02 | I |         sum  error  = [   11.9142]
25-01-23 08:32:02 | I |         best error  = [   11.9142]
25-01-23 08:32:02 | I |     + error = [11.9142]
25-01-23 08:32:03 | I |   - Calibrating model.layers.0.mlp.down_proj.weight
25-01-23 08:32:03 | I |       - range scale = [    1.0000]
25-01-23 08:32:03 | I |         sum  error  = [    0.1629]
25-01-23 08:32:03 | I |         best error  = [    0.1629]
25-01-23 08:32:03 | I |     + error = [0.1629]
25-01-23 08:32:04 | I |       - range scale = [    1.0000]
25-01-23 08:32:04 | I |         sum  error  = [    1.5814]
25-01-23 08:32:04 | I |         best error  = [    1.5814]
25-01-23 08:32:04 | I |     + error = [1.5814]
25-01-23 08:32:05 | I |   - Quantizing model.layers.0.self_attn.q_proj.weight
25-01-23 08:32:07 | I |   - Quantizing model.layers.0.self_attn.k_proj.weight
25-01-23 08:32:09 | I |   - Quantizing model.layers.0.self_attn.v_proj.weight
25-01-23 08:32:11 | I |   - Quantizing model.layers.0.self_attn.o_proj.weight
25-01-23 08:32:14 | I |   - Quantizing model.layers.0.mlp.up_proj.weight
25-01-23 08:32:16 | I |   - Quantizing model.layers.0.mlp.gate_proj.weight
25-01-23 08:32:18 | I |   - Quantizing model.layers.0.mlp.down_proj.weight
25-01-23 08:32:24 | I | quantizing activations for layer model.layers.0
25-01-23 08:32:25 | I | collecting calibration activations in model.layers.0
25-01-23 08:32:25 | I | collecting calibration activations in model.layers.0
25-01-23 08:32:27 | I | forward this layer
25-01-23 08:32:27 | I | input_file: /data/gyy/lmquant-main/lmquant/data/data_without_preprocess_llama_7b_bs16/shard/val_input_args_layer_0/50.pt
25-01-23 08:32:27 | I | output_file: /data/gyy/lmquant-main/lmquant/data/data_without_preprocess_llama_7b_bs16/shard/val_teacher_output_layer_0/50.pt
25-01-23 08:32:27 | I | In layer model.layers.0, gradient of model.layers.0.self_attn.q_proj has nan: False
25-01-23 08:32:27 | I | In layer model.layers.0, gradient of model.layers.0.self_attn.q_proj has inf: False
25-01-23 08:32:27 | I | In layer model.layers.0, gradient of model.layers.0.self_attn.k_proj has nan: False
25-01-23 08:32:27 | I | In layer model.layers.0, gradient of model.layers.0.self_attn.k_proj has inf: False
25-01-23 08:32:27 | I | In layer model.layers.0, gradient of model.layers.0.self_attn.v_proj has nan: False
25-01-23 08:32:27 | I | In layer model.layers.0, gradient of model.layers.0.self_attn.v_proj has inf: False
25-01-23 08:32:27 | I | In layer model.layers.0, gradient of model.layers.0.self_attn.o_proj has nan: False
25-01-23 08:32:27 | I | In layer model.layers.0, gradient of model.layers.0.self_attn.o_proj has inf: False
25-01-23 08:32:27 | I | In layer model.layers.0, gradient of model.layers.0.mlp.up_proj has nan: False
25-01-23 08:32:27 | I | In layer model.layers.0, gradient of model.layers.0.mlp.up_proj has inf: False
25-01-23 08:32:27 | I | In layer model.layers.0, gradient of model.layers.0.mlp.gate_proj has nan: False
25-01-23 08:32:27 | I | In layer model.layers.0, gradient of model.layers.0.mlp.gate_proj has inf: False
25-01-23 08:32:27 | I | In layer model.layers.0, gradient of model.layers.0.mlp.down_proj has nan: False
25-01-23 08:32:27 | I | In layer model.layers.0, gradient of model.layers.0.mlp.down_proj has inf: False
25-01-23 08:32:27 | I | [46] done with optimizer step
25-01-23 08:32:27 | I | epoch 001:     51 / 819200000 loss=1.2919e-05, loss_per_token=0.0529164, loss_sum=433.491, wps=226.7, ups=0.03, wpb=8192, bsz=16, num_updates=47, lr=0.0047, gnorm=0.132, clip=0, loss_scale=8, train_wall=36, cuda_gb_allocated=17.1, cuda_gb_reserved=18.3, cuda_gb_free=6.6, wall=4095
25-01-23 08:32:27 | I | TRAIN CURRENT LAYER_IDX = 0
25-01-23 08:32:27 | I | in layer model.layers.0
25-01-23 08:32:27 | I | quantizing weights for layer model.layers.0
25-01-23 08:32:28 | I | collecting calibration activations in model.layers.0
25-01-23 08:32:28 | I | collecting calibration activations in model.layers.0
25-01-23 08:32:28 | I | - Quantizing decoder layer model.layers.0
25-01-23 08:32:28 | I |   - Calibrating model.layers.0.self_attn.q_proj.weight
25-01-23 08:32:29 | I |       - range scale = [    1.0000]
25-01-23 08:32:29 | I |         sum  error  = [    0.0431]
25-01-23 08:32:29 | I |         best error  = [    0.0431]
25-01-23 08:32:29 | I |     + error = [0.0431]
25-01-23 08:32:29 | I |       - range scale = [    1.0000]
25-01-23 08:32:29 | I |         sum  error  = [    0.6696]
25-01-23 08:32:29 | I |         best error  = [    0.6696]
25-01-23 08:32:29 | I |     + error = [0.6696]
25-01-23 08:32:29 | I |   - Calibrating model.layers.0.self_attn.k_proj.weight
25-01-23 08:32:30 | I |       - range scale = [    1.0000]
25-01-23 08:32:30 | I |         sum  error  = [    0.0513]
25-01-23 08:32:30 | I |         best error  = [    0.0513]
25-01-23 08:32:30 | I |     + error = [0.0513]
25-01-23 08:32:31 | I |       - range scale = [    1.0000]
25-01-23 08:32:31 | I |         sum  error  = [    0.8706]
25-01-23 08:32:31 | I |         best error  = [    0.8706]
25-01-23 08:32:31 | I |     + error = [0.8706]
25-01-23 08:32:31 | I |   - Calibrating model.layers.0.self_attn.v_proj.weight
25-01-23 08:32:32 | I |       - range scale = [    1.0000]
25-01-23 08:32:32 | I |         sum  error  = [    0.6865]
25-01-23 08:32:32 | I |         best error  = [    0.6865]
25-01-23 08:32:32 | I |     + error = [0.6865]
25-01-23 08:32:33 | I |       - range scale = [    1.0000]
25-01-23 08:32:33 | I |         sum  error  = [    6.4542]
25-01-23 08:32:33 | I |         best error  = [    6.4542]
25-01-23 08:32:33 | I |     + error = [6.4542]
25-01-23 08:32:33 | I |   - Calibrating model.layers.0.self_attn.o_proj.weight
25-01-23 08:32:33 | I |       - range scale = [    1.0000]
25-01-23 08:32:33 | I |         sum  error  = [    0.1979]
25-01-23 08:32:33 | I |         best error  = [    0.1979]
25-01-23 08:32:33 | I |     + error = [0.1979]
25-01-23 08:32:34 | I |       - range scale = [    1.0000]
25-01-23 08:32:34 | I |         sum  error  = [    1.5612]
25-01-23 08:32:34 | I |         best error  = [    1.5612]
25-01-23 08:32:34 | I |     + error = [1.5612]
25-01-23 08:32:35 | I |   - Calibrating model.layers.0.mlp.up_proj.weight
25-01-23 08:32:35 | I |       - range scale = [    1.0000]
25-01-23 08:32:35 | I |         sum  error  = [    1.1340]
25-01-23 08:32:35 | I |         best error  = [    1.1340]
25-01-23 08:32:35 | I |     + error = [1.1340]
25-01-23 08:32:36 | I |       - range scale = [    1.0000]
25-01-23 08:32:36 | I |         sum  error  = [   12.0687]
25-01-23 08:32:36 | I |         best error  = [   12.0687]
25-01-23 08:32:36 | I |     + error = [12.0687]
25-01-23 08:32:37 | I |   - Calibrating model.layers.0.mlp.gate_proj.weight
25-01-23 08:32:37 | I |       - range scale = [    1.0000]
25-01-23 08:32:37 | I |         sum  error  = [    1.2126]
25-01-23 08:32:37 | I |         best error  = [    1.2126]
25-01-23 08:32:37 | I |     + error = [1.2126]
25-01-23 08:32:38 | I |       - range scale = [    1.0000]
25-01-23 08:32:38 | I |         sum  error  = [   12.4686]
25-01-23 08:32:38 | I |         best error  = [   12.4686]
25-01-23 08:32:38 | I |     + error = [12.4686]
25-01-23 08:32:39 | I |   - Calibrating model.layers.0.mlp.down_proj.weight
25-01-23 08:32:39 | I |       - range scale = [    1.0000]
25-01-23 08:32:39 | I |         sum  error  = [    0.1738]
25-01-23 08:32:39 | I |         best error  = [    0.1738]
25-01-23 08:32:39 | I |     + error = [0.1738]
25-01-23 08:32:41 | I |       - range scale = [    1.0000]
25-01-23 08:32:41 | I |         sum  error  = [    1.6847]
25-01-23 08:32:41 | I |         best error  = [    1.6847]
25-01-23 08:32:41 | I |     + error = [1.6847]
25-01-23 08:32:41 | I |   - Quantizing model.layers.0.self_attn.q_proj.weight
25-01-23 08:32:43 | I |   - Quantizing model.layers.0.self_attn.k_proj.weight
25-01-23 08:32:45 | I |   - Quantizing model.layers.0.self_attn.v_proj.weight
25-01-23 08:32:48 | I |   - Quantizing model.layers.0.self_attn.o_proj.weight
25-01-23 08:32:50 | I |   - Quantizing model.layers.0.mlp.up_proj.weight
25-01-23 08:32:52 | I |   - Quantizing model.layers.0.mlp.gate_proj.weight
25-01-23 08:32:55 | I |   - Quantizing model.layers.0.mlp.down_proj.weight
25-01-23 08:33:01 | I | quantizing activations for layer model.layers.0
25-01-23 08:33:01 | I | collecting calibration activations in model.layers.0
25-01-23 08:33:01 | I | collecting calibration activations in model.layers.0
25-01-23 08:33:03 | I | forward this layer
25-01-23 08:33:03 | I | input_file: /data/gyy/lmquant-main/lmquant/data/data_without_preprocess_llama_7b_bs16/shard/val_input_args_layer_0/51.pt
25-01-23 08:33:03 | I | output_file: /data/gyy/lmquant-main/lmquant/data/data_without_preprocess_llama_7b_bs16/shard/val_teacher_output_layer_0/51.pt
25-01-23 08:33:03 | I | In layer model.layers.0, gradient of model.layers.0.self_attn.q_proj has nan: False
25-01-23 08:33:03 | I | In layer model.layers.0, gradient of model.layers.0.self_attn.q_proj has inf: False
25-01-23 08:33:03 | I | In layer model.layers.0, gradient of model.layers.0.self_attn.k_proj has nan: False
25-01-23 08:33:03 | I | In layer model.layers.0, gradient of model.layers.0.self_attn.k_proj has inf: False
25-01-23 08:33:03 | I | In layer model.layers.0, gradient of model.layers.0.self_attn.v_proj has nan: False
25-01-23 08:33:03 | I | In layer model.layers.0, gradient of model.layers.0.self_attn.v_proj has inf: False
25-01-23 08:33:03 | I | In layer model.layers.0, gradient of model.layers.0.self_attn.o_proj has nan: False
25-01-23 08:33:03 | I | In layer model.layers.0, gradient of model.layers.0.self_attn.o_proj has inf: False
25-01-23 08:33:03 | I | In layer model.layers.0, gradient of model.layers.0.mlp.up_proj has nan: False
25-01-23 08:33:03 | I | In layer model.layers.0, gradient of model.layers.0.mlp.up_proj has inf: False
25-01-23 08:33:04 | I | In layer model.layers.0, gradient of model.layers.0.mlp.gate_proj has nan: False
25-01-23 08:33:04 | I | In layer model.layers.0, gradient of model.layers.0.mlp.gate_proj has inf: False
25-01-23 08:33:04 | I | In layer model.layers.0, gradient of model.layers.0.mlp.down_proj has nan: False
25-01-23 08:33:04 | I | In layer model.layers.0, gradient of model.layers.0.mlp.down_proj has inf: False
25-01-23 08:33:04 | I | [47] done with optimizer step
25-01-23 08:33:04 | I | epoch 001:     52 / 819200000 loss=1.60557e-05, loss_per_token=0.0657642, loss_sum=538.741, wps=224.1, ups=0.03, wpb=8192, bsz=16, num_updates=48, lr=0.0048, gnorm=0.287, clip=0, loss_scale=8, train_wall=36, cuda_gb_allocated=17.1, cuda_gb_reserved=18.3, cuda_gb_free=6.6, wall=4131
25-01-23 08:33:04 | I | TRAIN CURRENT LAYER_IDX = 0
25-01-23 08:33:04 | I | in layer model.layers.0
25-01-23 08:33:04 | I | quantizing weights for layer model.layers.0
25-01-23 08:33:04 | I | collecting calibration activations in model.layers.0
25-01-23 08:33:04 | I | collecting calibration activations in model.layers.0
25-01-23 08:33:04 | I | - Quantizing decoder layer model.layers.0
25-01-23 08:33:04 | I |   - Calibrating model.layers.0.self_attn.q_proj.weight
25-01-23 08:33:05 | I |       - range scale = [    1.0000]
25-01-23 08:33:05 | I |         sum  error  = [    0.0445]
25-01-23 08:33:05 | I |         best error  = [    0.0445]
25-01-23 08:33:05 | I |     + error = [0.0445]
25-01-23 08:33:06 | I |       - range scale = [    1.0000]
25-01-23 08:33:06 | I |         sum  error  = [    0.6756]
25-01-23 08:33:06 | I |         best error  = [    0.6756]
25-01-23 08:33:06 | I |     + error = [0.6756]
25-01-23 08:33:06 | I |   - Calibrating model.layers.0.self_attn.k_proj.weight
25-01-23 08:33:07 | I |       - range scale = [    1.0000]
25-01-23 08:33:07 | I |         sum  error  = [    0.0551]
25-01-23 08:33:07 | I |         best error  = [    0.0551]
25-01-23 08:33:07 | I |     + error = [0.0551]
25-01-23 08:33:07 | I |       - range scale = [    1.0000]
25-01-23 08:33:07 | I |         sum  error  = [    0.8981]
25-01-23 08:33:07 | I |         best error  = [    0.8981]
25-01-23 08:33:07 | I |     + error = [0.8981]
25-01-23 08:33:08 | I |   - Calibrating model.layers.0.self_attn.v_proj.weight
25-01-23 08:33:08 | I |       - range scale = [    1.0000]
25-01-23 08:33:08 | I |         sum  error  = [    0.6891]
25-01-23 08:33:08 | I |         best error  = [    0.6891]
25-01-23 08:33:08 | I |     + error = [0.6891]
25-01-23 08:33:09 | I |       - range scale = [    1.0000]
25-01-23 08:33:09 | I |         sum  error  = [    6.4227]
25-01-23 08:33:09 | I |         best error  = [    6.4227]
25-01-23 08:33:09 | I |     + error = [6.4227]
25-01-23 08:33:09 | I |   - Calibrating model.layers.0.self_attn.o_proj.weight
25-01-23 08:33:10 | I |       - range scale = [    1.0000]
25-01-23 08:33:10 | I |         sum  error  = [    0.1972]
25-01-23 08:33:10 | I |         best error  = [    0.1972]
25-01-23 08:33:10 | I |     + error = [0.1972]
25-01-23 08:33:11 | I |       - range scale = [    1.0000]
25-01-23 08:33:11 | I |         sum  error  = [    1.5701]
25-01-23 08:33:11 | I |         best error  = [    1.5701]
25-01-23 08:33:11 | I |     + error = [1.5701]
25-01-23 08:33:11 | I |   - Calibrating model.layers.0.mlp.up_proj.weight
25-01-23 08:33:12 | I |       - range scale = [    1.0000]
25-01-23 08:33:12 | I |         sum  error  = [    1.1093]
25-01-23 08:33:12 | I |         best error  = [    1.1093]
25-01-23 08:33:12 | I |     + error = [1.1093]
25-01-23 08:33:13 | I |       - range scale = [    1.0000]
25-01-23 08:33:13 | I |         sum  error  = [   11.7956]
25-01-23 08:33:13 | I |         best error  = [   11.7956]
25-01-23 08:33:13 | I |     + error = [11.7956]
25-01-23 08:33:13 | I |   - Calibrating model.layers.0.mlp.gate_proj.weight
25-01-23 08:33:14 | I |       - range scale = [    1.0000]
25-01-23 08:33:14 | I |         sum  error  = [    1.1755]
25-01-23 08:33:14 | I |         best error  = [    1.1755]
25-01-23 08:33:14 | I |     + error = [1.1755]
25-01-23 08:33:15 | I |       - range scale = [    1.0000]
25-01-23 08:33:15 | I |         sum  error  = [   12.1913]
25-01-23 08:33:15 | I |         best error  = [   12.1913]
25-01-23 08:33:15 | I |     + error = [12.1913]
25-01-23 08:33:15 | I |   - Calibrating model.layers.0.mlp.down_proj.weight
25-01-23 08:33:16 | I |       - range scale = [    1.0000]
25-01-23 08:33:16 | I |         sum  error  = [    0.1690]
25-01-23 08:33:16 | I |         best error  = [    0.1690]
25-01-23 08:33:16 | I |     + error = [0.1690]
25-01-23 08:33:17 | I |       - range scale = [    1.0000]
25-01-23 08:33:17 | I |         sum  error  = [    1.6445]
25-01-23 08:33:17 | I |         best error  = [    1.6445]
25-01-23 08:33:17 | I |     + error = [1.6445]
25-01-23 08:33:17 | I |   - Quantizing model.layers.0.self_attn.q_proj.weight
25-01-23 08:33:20 | I |   - Quantizing model.layers.0.self_attn.k_proj.weight
25-01-23 08:33:22 | I |   - Quantizing model.layers.0.self_attn.v_proj.weight
25-01-23 08:33:24 | I |   - Quantizing model.layers.0.self_attn.o_proj.weight
25-01-23 08:33:26 | I |   - Quantizing model.layers.0.mlp.up_proj.weight
25-01-23 08:33:29 | I |   - Quantizing model.layers.0.mlp.gate_proj.weight
25-01-23 08:33:31 | I |   - Quantizing model.layers.0.mlp.down_proj.weight
25-01-23 08:33:37 | I | quantizing activations for layer model.layers.0
25-01-23 08:33:37 | I | collecting calibration activations in model.layers.0
25-01-23 08:33:38 | I | collecting calibration activations in model.layers.0
25-01-23 08:33:39 | I | forward this layer
25-01-23 08:33:39 | I | input_file: /data/gyy/lmquant-main/lmquant/data/data_without_preprocess_llama_7b_bs16/shard/val_input_args_layer_0/52.pt
25-01-23 08:33:39 | I | output_file: /data/gyy/lmquant-main/lmquant/data/data_without_preprocess_llama_7b_bs16/shard/val_teacher_output_layer_0/52.pt
25-01-23 08:33:40 | I | In layer model.layers.0, gradient of model.layers.0.self_attn.q_proj has nan: False
25-01-23 08:33:40 | I | In layer model.layers.0, gradient of model.layers.0.self_attn.q_proj has inf: False
25-01-23 08:33:40 | I | In layer model.layers.0, gradient of model.layers.0.self_attn.k_proj has nan: False
25-01-23 08:33:40 | I | In layer model.layers.0, gradient of model.layers.0.self_attn.k_proj has inf: False
25-01-23 08:33:40 | I | In layer model.layers.0, gradient of model.layers.0.self_attn.v_proj has nan: False
25-01-23 08:33:40 | I | In layer model.layers.0, gradient of model.layers.0.self_attn.v_proj has inf: False
25-01-23 08:33:40 | I | In layer model.layers.0, gradient of model.layers.0.self_attn.o_proj has nan: False
25-01-23 08:33:40 | I | In layer model.layers.0, gradient of model.layers.0.self_attn.o_proj has inf: False
25-01-23 08:33:40 | I | In layer model.layers.0, gradient of model.layers.0.mlp.up_proj has nan: False
25-01-23 08:33:40 | I | In layer model.layers.0, gradient of model.layers.0.mlp.up_proj has inf: False
25-01-23 08:33:40 | I | In layer model.layers.0, gradient of model.layers.0.mlp.gate_proj has nan: False
25-01-23 08:33:40 | I | In layer model.layers.0, gradient of model.layers.0.mlp.gate_proj has inf: False
25-01-23 08:33:40 | I | In layer model.layers.0, gradient of model.layers.0.mlp.down_proj has nan: False
25-01-23 08:33:40 | I | In layer model.layers.0, gradient of model.layers.0.mlp.down_proj has inf: False
25-01-23 08:33:40 | I | [48] done with optimizer step
25-01-23 08:33:40 | I | epoch 001:     53 / 819200000 loss=1.33906e-05, loss_per_token=0.0548481, loss_sum=449.315, wps=226.2, ups=0.03, wpb=8192, bsz=16, num_updates=49, lr=0.0049, gnorm=0.22, clip=0, loss_scale=8, train_wall=36, cuda_gb_allocated=17.1, cuda_gb_reserved=18.3, cuda_gb_free=6.6, wall=4167
25-01-23 08:33:40 | I | TRAIN CURRENT LAYER_IDX = 0
25-01-23 08:33:40 | I | in layer model.layers.0
25-01-23 08:33:40 | I | quantizing weights for layer model.layers.0
25-01-23 08:33:40 | I | collecting calibration activations in model.layers.0
25-01-23 08:33:40 | I | collecting calibration activations in model.layers.0
25-01-23 08:33:41 | I | - Quantizing decoder layer model.layers.0
25-01-23 08:33:41 | I |   - Calibrating model.layers.0.self_attn.q_proj.weight
25-01-23 08:33:41 | I |       - range scale = [    1.0000]
25-01-23 08:33:41 | I |         sum  error  = [    0.0423]
25-01-23 08:33:41 | I |         best error  = [    0.0423]
25-01-23 08:33:41 | I |     + error = [0.0423]
25-01-23 08:33:42 | I |       - range scale = [    1.0000]
25-01-23 08:33:42 | I |         sum  error  = [    0.6634]
25-01-23 08:33:42 | I |         best error  = [    0.6634]
25-01-23 08:33:42 | I |     + error = [0.6634]
25-01-23 08:33:42 | I |   - Calibrating model.layers.0.self_attn.k_proj.weight
25-01-23 08:33:43 | I |       - range scale = [    1.0000]
25-01-23 08:33:43 | I |         sum  error  = [    0.0527]
25-01-23 08:33:43 | I |         best error  = [    0.0527]
25-01-23 08:33:43 | I |     + error = [0.0527]
25-01-23 08:33:44 | I |       - range scale = [    1.0000]
25-01-23 08:33:44 | I |         sum  error  = [    0.8952]
25-01-23 08:33:44 | I |         best error  = [    0.8952]
25-01-23 08:33:44 | I |     + error = [0.8952]
25-01-23 08:33:44 | I |   - Calibrating model.layers.0.self_attn.v_proj.weight
25-01-23 08:33:45 | I |       - range scale = [    1.0000]
25-01-23 08:33:45 | I |         sum  error  = [    0.6743]
25-01-23 08:33:45 | I |         best error  = [    0.6743]
25-01-23 08:33:45 | I |     + error = [0.6743]
25-01-23 08:33:45 | I |       - range scale = [    1.0000]
25-01-23 08:33:45 | I |         sum  error  = [    6.3028]
25-01-23 08:33:45 | I |         best error  = [    6.3028]
25-01-23 08:33:45 | I |     + error = [6.3028]
25-01-23 08:33:46 | I |   - Calibrating model.layers.0.self_attn.o_proj.weight
25-01-23 08:33:46 | I |       - range scale = [    1.0000]
25-01-23 08:33:46 | I |         sum  error  = [    0.1934]
25-01-23 08:33:46 | I |         best error  = [    0.1934]
25-01-23 08:33:46 | I |     + error = [0.1934]
25-01-23 08:33:47 | I |       - range scale = [    1.0000]
25-01-23 08:33:47 | I |         sum  error  = [    1.5367]
25-01-23 08:33:47 | I |         best error  = [    1.5367]
25-01-23 08:33:47 | I |     + error = [1.5367]
25-01-23 08:33:47 | I |   - Calibrating model.layers.0.mlp.up_proj.weight
25-01-23 08:33:48 | I |       - range scale = [    1.0000]
25-01-23 08:33:48 | I |         sum  error  = [    1.1041]
25-01-23 08:33:48 | I |         best error  = [    1.1041]
25-01-23 08:33:48 | I |     + error = [1.1041]
25-01-23 08:33:49 | I |       - range scale = [    1.0000]
25-01-23 08:33:49 | I |         sum  error  = [   11.7465]
25-01-23 08:33:49 | I |         best error  = [   11.7465]
25-01-23 08:33:49 | I |     + error = [11.7465]
25-01-23 08:33:50 | I |   - Calibrating model.layers.0.mlp.gate_proj.weight
25-01-23 08:33:50 | I |       - range scale = [    1.0000]
25-01-23 08:33:50 | I |         sum  error  = [    1.1673]
25-01-23 08:33:50 | I |         best error  = [    1.1673]
25-01-23 08:33:50 | I |     + error = [1.1673]
25-01-23 08:33:51 | I |       - range scale = [    1.0000]
25-01-23 08:33:51 | I |         sum  error  = [   12.1410]
25-01-23 08:33:51 | I |         best error  = [   12.1410]
25-01-23 08:33:51 | I |     + error = [12.1410]
25-01-23 08:33:52 | I |   - Calibrating model.layers.0.mlp.down_proj.weight
25-01-23 08:33:52 | I |       - range scale = [    1.0000]
25-01-23 08:33:52 | I |         sum  error  = [    0.1667]
25-01-23 08:33:52 | I |         best error  = [    0.1667]
25-01-23 08:33:52 | I |     + error = [0.1667]
25-01-23 08:33:54 | I |       - range scale = [    1.0000]
25-01-23 08:33:54 | I |         sum  error  = [    1.6254]
25-01-23 08:33:54 | I |         best error  = [    1.6254]
25-01-23 08:33:54 | I |     + error = [1.6254]
25-01-23 08:33:54 | I |   - Quantizing model.layers.0.self_attn.q_proj.weight
25-01-23 08:33:56 | I |   - Quantizing model.layers.0.self_attn.k_proj.weight
25-01-23 08:33:58 | I |   - Quantizing model.layers.0.self_attn.v_proj.weight
25-01-23 08:34:01 | I |   - Quantizing model.layers.0.self_attn.o_proj.weight
25-01-23 08:34:03 | I |   - Quantizing model.layers.0.mlp.up_proj.weight
25-01-23 08:34:05 | I |   - Quantizing model.layers.0.mlp.gate_proj.weight
25-01-23 08:34:08 | I |   - Quantizing model.layers.0.mlp.down_proj.weight
25-01-23 08:34:14 | I | quantizing activations for layer model.layers.0
25-01-23 08:34:14 | I | collecting calibration activations in model.layers.0
25-01-23 08:34:14 | I | collecting calibration activations in model.layers.0
25-01-23 08:34:16 | I | forward this layer
25-01-23 08:34:16 | I | input_file: /data/gyy/lmquant-main/lmquant/data/data_without_preprocess_llama_7b_bs16/shard/val_input_args_layer_0/53.pt
25-01-23 08:34:16 | I | output_file: /data/gyy/lmquant-main/lmquant/data/data_without_preprocess_llama_7b_bs16/shard/val_teacher_output_layer_0/53.pt
25-01-23 08:34:16 | I | In layer model.layers.0, gradient of model.layers.0.self_attn.q_proj has nan: False
25-01-23 08:34:16 | I | In layer model.layers.0, gradient of model.layers.0.self_attn.q_proj has inf: False
25-01-23 08:34:16 | I | In layer model.layers.0, gradient of model.layers.0.self_attn.k_proj has nan: False
25-01-23 08:34:16 | I | In layer model.layers.0, gradient of model.layers.0.self_attn.k_proj has inf: False
25-01-23 08:34:16 | I | In layer model.layers.0, gradient of model.layers.0.self_attn.v_proj has nan: False
25-01-23 08:34:16 | I | In layer model.layers.0, gradient of model.layers.0.self_attn.v_proj has inf: False
25-01-23 08:34:16 | I | In layer model.layers.0, gradient of model.layers.0.self_attn.o_proj has nan: False
25-01-23 08:34:16 | I | In layer model.layers.0, gradient of model.layers.0.self_attn.o_proj has inf: False
25-01-23 08:34:16 | I | In layer model.layers.0, gradient of model.layers.0.mlp.up_proj has nan: False
25-01-23 08:34:16 | I | In layer model.layers.0, gradient of model.layers.0.mlp.up_proj has inf: False
25-01-23 08:34:16 | I | In layer model.layers.0, gradient of model.layers.0.mlp.gate_proj has nan: False
25-01-23 08:34:16 | I | In layer model.layers.0, gradient of model.layers.0.mlp.gate_proj has inf: False
25-01-23 08:34:16 | I | In layer model.layers.0, gradient of model.layers.0.mlp.down_proj has nan: False
25-01-23 08:34:16 | I | In layer model.layers.0, gradient of model.layers.0.mlp.down_proj has inf: False
25-01-23 08:34:16 | I | [49] done with optimizer step
25-01-23 08:34:16 | I | epoch 001:     54 / 819200000 loss=1.18629e-05, loss_per_token=0.0485903, loss_sum=398.052, wps=223.1, ups=0.03, wpb=8192, bsz=16, num_updates=50, lr=0.005, gnorm=0.12, clip=0, loss_scale=8, train_wall=37, cuda_gb_allocated=17.1, cuda_gb_reserved=18.3, cuda_gb_free=6.6, wall=4204
25-01-23 08:34:16 | I | begin validation on "valid" subset on rank 0
25-01-23 08:34:16 | I | got valid iterator on "valid" subset on rank 0
25-01-23 08:34:16 | I | Valid: Start iterating over samples
25-01-23 08:34:17 | W |   `pretrained` model kwarg is not of type `str`. Many other model arguments may be ignored. Please do not launch via accelerate or use `parallelize=True` if passing an existing model this way.
25-01-23 08:34:17 | W |   Passed an already-initialized model through `pretrained`, assuming single-process call to evaluate() or custom distributed integration
25-01-23 08:34:17 | I | - Evaluator: gptq
25-01-23 08:34:17 | I | - Tasks: ['wikitext', 'val_valid']
25-01-23 08:34:17 | I | - Batch_size: 8
25-01-23 08:34:17 | I |   + Max_seq_length: 2048
25-01-23 08:35:51 | I |     - Results:
25-01-23 08:35:51 | I |       |  Task   |Version|    Metric     |Value |   |Stderr|
25-01-23 08:35:51 | I |       |---------|------:|---------------|-----:|---|-----:|
25-01-23 08:35:51 | I |       |wikitext |      1|word_perplexity|5.4817|±  |5.4817|
25-01-23 08:35:51 | I |       |val_valid|      1|word_perplexity|5.0246|±  |5.0246|
25-01-23 08:35:51 | I |       
25-01-23 08:35:51 | I |   + Max_seq_length: 4096
25-01-23 08:37:25 | I |     - Results:
25-01-23 08:37:25 | I |       |  Task   |Version|    Metric     |Value |   |Stderr|
25-01-23 08:37:25 | I |       |---------|------:|---------------|-----:|---|-----:|
25-01-23 08:37:25 | I |       |wikitext |      1|word_perplexity|5.1232|±  |5.1232|
25-01-23 08:37:25 | I |       |val_valid|      1|word_perplexity|4.8203|±  |4.8203|
25-01-23 08:37:25 | I |       
25-01-23 08:37:25 | I | in valid, quantize current layer weights
25-01-23 08:37:25 | W | Repo card metadata block was not found. Setting CardData to empty.
25-01-23 08:38:09 | I | collecting calibration activations in model.layers.0
25-01-23 08:38:09 | I | collecting calibration activations in model.layers.0
25-01-23 08:38:10 | I | collecting calibration activations in model.layers.0
25-01-23 08:38:10 | I | collecting calibration activations in model.layers.0
25-01-23 08:38:10 | I | collecting calibration activations in model.layers.0
25-01-23 08:38:10 | I | collecting calibration activations in model.layers.0
25-01-23 08:38:10 | I | collecting calibration activations in model.layers.0
25-01-23 08:38:10 | I | collecting calibration activations in model.layers.0
25-01-23 08:38:10 | I | collecting calibration activations in model.layers.0
25-01-23 08:38:10 | I | collecting calibration activations in model.layers.0
25-01-23 08:38:10 | I | collecting calibration activations in model.layers.0
25-01-23 08:38:10 | I | collecting calibration activations in model.layers.0
25-01-23 08:38:10 | I | collecting calibration activations in model.layers.0
25-01-23 08:38:10 | I | collecting calibration activations in model.layers.0
25-01-23 08:38:10 | I | collecting calibration activations in model.layers.0
25-01-23 08:38:10 | I | collecting calibration activations in model.layers.0
25-01-23 08:38:10 | I | collecting calibration activations in model.layers.0
25-01-23 08:38:10 | I | collecting calibration activations in model.layers.0
25-01-23 08:38:10 | I | collecting calibration activations in model.layers.0
25-01-23 08:38:10 | I | collecting calibration activations in model.layers.0
25-01-23 08:38:10 | I | collecting calibration activations in model.layers.0
25-01-23 08:38:11 | I | collecting calibration activations in model.layers.0
25-01-23 08:38:11 | I | collecting calibration activations in model.layers.0
25-01-23 08:38:11 | I | collecting calibration activations in model.layers.0
25-01-23 08:38:11 | I | collecting calibration activations in model.layers.0
25-01-23 08:38:11 | I | collecting calibration activations in model.layers.0
25-01-23 08:38:11 | I | collecting calibration activations in model.layers.0
25-01-23 08:38:11 | I | collecting calibration activations in model.layers.0
25-01-23 08:38:11 | I | collecting calibration activations in model.layers.0
25-01-23 08:38:11 | I | collecting calibration activations in model.layers.0
25-01-23 08:38:11 | I | collecting calibration activations in model.layers.0
25-01-23 08:38:11 | I | collecting calibration activations in model.layers.0
25-01-23 08:38:11 | I | collecting calibration activations in model.layers.0
25-01-23 08:38:11 | I | collecting calibration activations in model.layers.0
25-01-23 08:38:11 | I | collecting calibration activations in model.layers.0
25-01-23 08:38:11 | I | collecting calibration activations in model.layers.0
25-01-23 08:38:11 | I | collecting calibration activations in model.layers.0
25-01-23 08:38:11 | I | collecting calibration activations in model.layers.0
25-01-23 08:38:11 | I | collecting calibration activations in model.layers.0
25-01-23 08:38:11 | I | collecting calibration activations in model.layers.0
25-01-23 08:38:12 | I | collecting calibration activations in model.layers.0
25-01-23 08:38:12 | I | collecting calibration activations in model.layers.0
25-01-23 08:38:12 | I | collecting calibration activations in model.layers.0
25-01-23 08:38:12 | I | collecting calibration activations in model.layers.0
25-01-23 08:38:12 | I | collecting calibration activations in model.layers.0
25-01-23 08:38:12 | I | collecting calibration activations in model.layers.0
25-01-23 08:38:12 | I | collecting calibration activations in model.layers.0
25-01-23 08:38:12 | I | collecting calibration activations in model.layers.0
25-01-23 08:38:12 | I | collecting calibration activations in model.layers.0
25-01-23 08:38:12 | I | collecting calibration activations in model.layers.0
25-01-23 08:38:12 | I | collecting calibration activations in model.layers.0
25-01-23 08:38:12 | I | collecting calibration activations in model.layers.0
25-01-23 08:38:12 | I | collecting calibration activations in model.layers.0
25-01-23 08:38:12 | I | collecting calibration activations in model.layers.0
25-01-23 08:38:12 | I | collecting calibration activations in model.layers.0
25-01-23 08:38:12 | I | collecting calibration activations in model.layers.0
25-01-23 08:38:12 | I | collecting calibration activations in model.layers.0
25-01-23 08:38:12 | I | collecting calibration activations in model.layers.0
25-01-23 08:38:12 | I | collecting calibration activations in model.layers.0
25-01-23 08:38:13 | I | collecting calibration activations in model.layers.0
25-01-23 08:38:13 | I | collecting calibration activations in model.layers.0
25-01-23 08:38:13 | I | collecting calibration activations in model.layers.0
25-01-23 08:38:13 | I | collecting calibration activations in model.layers.0
25-01-23 08:38:13 | I | collecting calibration activations in model.layers.0
25-01-23 08:38:13 | I | collecting calibration activations in model.layers.0
25-01-23 08:38:13 | I | collecting calibration activations in model.layers.0
25-01-23 08:38:13 | I | collecting calibration activations in model.layers.0
25-01-23 08:38:13 | I | collecting calibration activations in model.layers.0
25-01-23 08:38:13 | I | collecting calibration activations in model.layers.0
25-01-23 08:38:13 | I | collecting calibration activations in model.layers.0
25-01-23 08:38:13 | I | collecting calibration activations in model.layers.0
25-01-23 08:38:13 | I | collecting calibration activations in model.layers.0
25-01-23 08:38:13 | I | collecting calibration activations in model.layers.0
25-01-23 08:38:13 | I | collecting calibration activations in model.layers.0
25-01-23 08:38:13 | I | collecting calibration activations in model.layers.0
25-01-23 08:38:13 | I | collecting calibration activations in model.layers.0
25-01-23 08:38:13 | I | collecting calibration activations in model.layers.0
25-01-23 08:38:13 | I | collecting calibration activations in model.layers.0
25-01-23 08:38:14 | I | collecting calibration activations in model.layers.0
25-01-23 08:38:14 | I | collecting calibration activations in model.layers.0
25-01-23 08:38:14 | I | collecting calibration activations in model.layers.0
25-01-23 08:38:14 | I | collecting calibration activations in model.layers.0
25-01-23 08:38:14 | I | collecting calibration activations in model.layers.0
25-01-23 08:38:14 | I | collecting calibration activations in model.layers.0
25-01-23 08:38:14 | I | collecting calibration activations in model.layers.0
25-01-23 08:38:14 | I | collecting calibration activations in model.layers.0
25-01-23 08:38:14 | I | collecting calibration activations in model.layers.0
25-01-23 08:38:14 | I | collecting calibration activations in model.layers.0
25-01-23 08:38:14 | I | collecting calibration activations in model.layers.0
25-01-23 08:38:14 | I | collecting calibration activations in model.layers.0
25-01-23 08:38:14 | I | collecting calibration activations in model.layers.0
25-01-23 08:38:14 | I | collecting calibration activations in model.layers.0
25-01-23 08:38:14 | I | collecting calibration activations in model.layers.0
25-01-23 08:38:14 | I | collecting calibration activations in model.layers.0
25-01-23 08:38:14 | I | collecting calibration activations in model.layers.0
25-01-23 08:38:14 | I | collecting calibration activations in model.layers.0
25-01-23 08:38:14 | I | collecting calibration activations in model.layers.0
25-01-23 08:38:15 | I | collecting calibration activations in model.layers.0
25-01-23 08:38:15 | I | collecting calibration activations in model.layers.0
25-01-23 08:38:15 | I | collecting calibration activations in model.layers.0
25-01-23 08:38:15 | I | collecting calibration activations in model.layers.0
25-01-23 08:38:15 | I | collecting calibration activations in model.layers.0
25-01-23 08:38:15 | I | collecting calibration activations in model.layers.0
25-01-23 08:38:15 | I | collecting calibration activations in model.layers.0
25-01-23 08:38:15 | I | collecting calibration activations in model.layers.0
25-01-23 08:38:15 | I | collecting calibration activations in model.layers.0
25-01-23 08:38:15 | I | collecting calibration activations in model.layers.0
25-01-23 08:38:15 | I | collecting calibration activations in model.layers.0
25-01-23 08:38:15 | I | collecting calibration activations in model.layers.0
25-01-23 08:38:15 | I | collecting calibration activations in model.layers.0
25-01-23 08:38:15 | I | collecting calibration activations in model.layers.0
25-01-23 08:38:15 | I | collecting calibration activations in model.layers.0
25-01-23 08:38:15 | I | collecting calibration activations in model.layers.0
25-01-23 08:38:15 | I | collecting calibration activations in model.layers.0
25-01-23 08:38:15 | I | collecting calibration activations in model.layers.0
25-01-23 08:38:15 | I | collecting calibration activations in model.layers.0
25-01-23 08:38:15 | I | collecting calibration activations in model.layers.0
25-01-23 08:38:16 | I | collecting calibration activations in model.layers.0
25-01-23 08:38:16 | I | collecting calibration activations in model.layers.0
25-01-23 08:38:16 | I | collecting calibration activations in model.layers.0
25-01-23 08:38:16 | I | collecting calibration activations in model.layers.0
25-01-23 08:38:16 | I | collecting calibration activations in model.layers.0
25-01-23 08:38:16 | I | collecting calibration activations in model.layers.0
25-01-23 08:38:16 | I | collecting calibration activations in model.layers.0
25-01-23 08:38:16 | I | collecting calibration activations in model.layers.0
25-01-23 08:38:16 | I | collecting calibration activations in model.layers.0
25-01-23 08:38:16 | I | collecting calibration activations in model.layers.0
25-01-23 08:38:16 | I | collecting calibration activations in model.layers.0
25-01-23 08:38:18 | I |       - range scale = [    1.0000]
25-01-23 08:38:18 | I |         sum  error  = [    0.1577]
25-01-23 08:38:18 | I |         best error  = [    0.1577]
25-01-23 08:38:18 | I |     + error = [0.1577]
25-01-23 08:38:19 | I |       - range scale = [    1.0000]
25-01-23 08:38:19 | I |         sum  error  = [    2.7104]
25-01-23 08:38:19 | I |         best error  = [    2.7104]
25-01-23 08:38:19 | I |     + error = [2.7104]
25-01-23 08:38:21 | I |       - range scale = [    1.0000]
25-01-23 08:38:21 | I |         sum  error  = [    0.1891]
25-01-23 08:38:21 | I |         best error  = [    0.1891]
25-01-23 08:38:21 | I |     + error = [0.1891]
25-01-23 08:38:22 | I |       - range scale = [    1.0000]
25-01-23 08:38:22 | I |         sum  error  = [    3.7861]
25-01-23 08:38:22 | I |         best error  = [    3.7861]
25-01-23 08:38:22 | I |     + error = [3.7861]
25-01-23 08:38:23 | I |       - range scale = [    1.0000]
25-01-23 08:38:23 | I |         sum  error  = [    0.6476]
25-01-23 08:38:23 | I |         best error  = [    0.6476]
25-01-23 08:38:23 | I |     + error = [0.6476]
25-01-23 08:38:24 | I |       - range scale = [    1.0000]
25-01-23 08:38:24 | I |         sum  error  = [    5.9702]
25-01-23 08:38:24 | I |         best error  = [    5.9702]
25-01-23 08:38:24 | I |     + error = [5.9702]
25-01-23 08:38:25 | I |       - range scale = [    1.0000]
25-01-23 08:38:25 | I |         sum  error  = [    0.1480]
25-01-23 08:38:25 | I |         best error  = [    0.1480]
25-01-23 08:38:25 | I |     + error = [0.1480]
25-01-23 08:38:26 | I |       - range scale = [    1.0000]
25-01-23 08:38:26 | I |         sum  error  = [    1.2183]
25-01-23 08:38:26 | I |         best error  = [    1.2183]
25-01-23 08:38:26 | I |     + error = [1.2183]
25-01-23 08:38:27 | I |       - range scale = [    1.0000]
25-01-23 08:38:27 | I |         sum  error  = [    1.1026]
25-01-23 08:38:27 | I |         best error  = [    1.1026]
25-01-23 08:38:27 | I |     + error = [1.1026]
25-01-23 08:38:28 | I |       - range scale = [    1.0000]
25-01-23 08:38:28 | I |         sum  error  = [   11.7433]
25-01-23 08:38:28 | I |         best error  = [   11.7433]
25-01-23 08:38:28 | I |     + error = [11.7433]
25-01-23 08:38:29 | I |       - range scale = [    1.0000]
25-01-23 08:38:29 | I |         sum  error  = [    1.1683]
25-01-23 08:38:29 | I |         best error  = [    1.1683]
25-01-23 08:38:29 | I |     + error = [1.1683]
25-01-23 08:38:30 | I |       - range scale = [    1.0000]
25-01-23 08:38:30 | I |         sum  error  = [   12.1244]
25-01-23 08:38:30 | I |         best error  = [   12.1244]
25-01-23 08:38:30 | I |     + error = [12.1244]
25-01-23 08:38:31 | I |       - range scale = [    1.0000]
25-01-23 08:38:31 | I |         sum  error  = [    0.1883]
25-01-23 08:38:31 | I |         best error  = [    0.1883]
25-01-23 08:38:31 | I |     + error = [0.1883]
25-01-23 08:38:32 | I |       - range scale = [    1.0000]
25-01-23 08:38:32 | I |         sum  error  = [    1.8599]
25-01-23 08:38:32 | I |         best error  = [    1.8599]
25-01-23 08:38:32 | I |     + error = [1.8599]
25-01-23 08:38:58 | I | in valid, quantize current layer acts
25-01-23 08:38:59 | I | collecting calibration activations in model.layers.0
25-01-23 08:38:59 | I | collecting calibration activations in model.layers.0
25-01-23 08:38:59 | I | collecting calibration activations in model.layers.0
25-01-23 08:38:59 | I | collecting calibration activations in model.layers.0
25-01-23 08:39:00 | I | collecting calibration activations in model.layers.0
25-01-23 08:39:00 | I | collecting calibration activations in model.layers.0
25-01-23 08:39:00 | I | collecting calibration activations in model.layers.0
25-01-23 08:39:00 | I | collecting calibration activations in model.layers.0
25-01-23 08:39:00 | I | collecting calibration activations in model.layers.0
25-01-23 08:39:00 | I | collecting calibration activations in model.layers.0
25-01-23 08:39:00 | I | collecting calibration activations in model.layers.0
25-01-23 08:39:00 | I | collecting calibration activations in model.layers.0
25-01-23 08:39:00 | I | collecting calibration activations in model.layers.0
25-01-23 08:39:00 | I | collecting calibration activations in model.layers.0
25-01-23 08:39:00 | I | collecting calibration activations in model.layers.0
25-01-23 08:39:00 | I | collecting calibration activations in model.layers.0
25-01-23 08:39:00 | I | collecting calibration activations in model.layers.0
25-01-23 08:39:00 | I | collecting calibration activations in model.layers.0
25-01-23 08:39:00 | I | collecting calibration activations in model.layers.0
25-01-23 08:39:00 | I | collecting calibration activations in model.layers.0
25-01-23 08:39:01 | I | collecting calibration activations in model.layers.0
25-01-23 08:39:01 | I | collecting calibration activations in model.layers.0
25-01-23 08:39:01 | I | collecting calibration activations in model.layers.0
25-01-23 08:39:01 | I | collecting calibration activations in model.layers.0
25-01-23 08:39:01 | I | collecting calibration activations in model.layers.0
25-01-23 08:39:01 | I | collecting calibration activations in model.layers.0
25-01-23 08:39:01 | I | collecting calibration activations in model.layers.0
25-01-23 08:39:01 | I | collecting calibration activations in model.layers.0
25-01-23 08:39:01 | I | collecting calibration activations in model.layers.0
25-01-23 08:39:01 | I | collecting calibration activations in model.layers.0
25-01-23 08:39:01 | I | collecting calibration activations in model.layers.0
25-01-23 08:39:01 | I | collecting calibration activations in model.layers.0
25-01-23 08:39:01 | I | collecting calibration activations in model.layers.0
25-01-23 08:39:01 | I | collecting calibration activations in model.layers.0
25-01-23 08:39:01 | I | collecting calibration activations in model.layers.0
25-01-23 08:39:01 | I | collecting calibration activations in model.layers.0
25-01-23 08:39:02 | I | collecting calibration activations in model.layers.0
25-01-23 08:39:02 | I | collecting calibration activations in model.layers.0
25-01-23 08:39:02 | I | collecting calibration activations in model.layers.0
25-01-23 08:39:02 | I | collecting calibration activations in model.layers.0
25-01-23 08:39:02 | I | collecting calibration activations in model.layers.0
25-01-23 08:39:02 | I | collecting calibration activations in model.layers.0
25-01-23 08:39:02 | I | collecting calibration activations in model.layers.0
25-01-23 08:39:02 | I | collecting calibration activations in model.layers.0
25-01-23 08:39:02 | I | collecting calibration activations in model.layers.0
25-01-23 08:39:02 | I | collecting calibration activations in model.layers.0
25-01-23 08:39:02 | I | collecting calibration activations in model.layers.0
25-01-23 08:39:02 | I | collecting calibration activations in model.layers.0
25-01-23 08:39:02 | I | collecting calibration activations in model.layers.0
25-01-23 08:39:02 | I | collecting calibration activations in model.layers.0
25-01-23 08:39:02 | I | collecting calibration activations in model.layers.0
25-01-23 08:39:03 | I | collecting calibration activations in model.layers.0
25-01-23 08:39:03 | I | collecting calibration activations in model.layers.0
25-01-23 08:39:03 | I | collecting calibration activations in model.layers.0
25-01-23 08:39:03 | I | collecting calibration activations in model.layers.0
25-01-23 08:39:03 | I | collecting calibration activations in model.layers.0
25-01-23 08:39:03 | I | collecting calibration activations in model.layers.0
25-01-23 08:39:03 | I | collecting calibration activations in model.layers.0
25-01-23 08:39:03 | I | collecting calibration activations in model.layers.0
25-01-23 08:39:03 | I | collecting calibration activations in model.layers.0
25-01-23 08:39:03 | I | collecting calibration activations in model.layers.0
25-01-23 08:39:03 | I | collecting calibration activations in model.layers.0
25-01-23 08:39:03 | I | collecting calibration activations in model.layers.0
25-01-23 08:39:03 | I | collecting calibration activations in model.layers.0
25-01-23 08:39:03 | I | collecting calibration activations in model.layers.0
25-01-23 08:39:03 | I | collecting calibration activations in model.layers.0
25-01-23 08:39:03 | I | collecting calibration activations in model.layers.0
25-01-23 08:39:04 | I | collecting calibration activations in model.layers.0
25-01-23 08:39:04 | I | collecting calibration activations in model.layers.0
25-01-23 08:39:04 | I | collecting calibration activations in model.layers.0
25-01-23 08:39:04 | I | collecting calibration activations in model.layers.0
25-01-23 08:39:04 | I | collecting calibration activations in model.layers.0
25-01-23 08:39:04 | I | collecting calibration activations in model.layers.0
25-01-23 08:39:04 | I | collecting calibration activations in model.layers.0
25-01-23 08:39:04 | I | collecting calibration activations in model.layers.0
25-01-23 08:39:04 | I | collecting calibration activations in model.layers.0
25-01-23 08:39:04 | I | collecting calibration activations in model.layers.0
25-01-23 08:39:04 | I | collecting calibration activations in model.layers.0
25-01-23 08:39:04 | I | collecting calibration activations in model.layers.0
25-01-23 08:39:04 | I | collecting calibration activations in model.layers.0
25-01-23 08:39:04 | I | collecting calibration activations in model.layers.0
25-01-23 08:39:04 | I | collecting calibration activations in model.layers.0
25-01-23 08:39:04 | I | collecting calibration activations in model.layers.0
25-01-23 08:39:05 | I | collecting calibration activations in model.layers.0
25-01-23 08:39:05 | I | collecting calibration activations in model.layers.0
25-01-23 08:39:05 | I | collecting calibration activations in model.layers.0
25-01-23 08:39:05 | I | collecting calibration activations in model.layers.0
25-01-23 08:39:05 | I | collecting calibration activations in model.layers.0
25-01-23 08:39:05 | I | collecting calibration activations in model.layers.0
25-01-23 08:39:05 | I | collecting calibration activations in model.layers.0
25-01-23 08:39:05 | I | collecting calibration activations in model.layers.0
25-01-23 08:39:05 | I | collecting calibration activations in model.layers.0
25-01-23 08:39:05 | I | collecting calibration activations in model.layers.0
25-01-23 08:39:05 | I | collecting calibration activations in model.layers.0
25-01-23 08:39:05 | I | collecting calibration activations in model.layers.0
25-01-23 08:39:05 | I | collecting calibration activations in model.layers.0
25-01-23 08:39:05 | I | collecting calibration activations in model.layers.0
25-01-23 08:39:05 | I | collecting calibration activations in model.layers.0
25-01-23 08:39:06 | I | collecting calibration activations in model.layers.0
25-01-23 08:39:06 | I | collecting calibration activations in model.layers.0
25-01-23 08:39:06 | I | collecting calibration activations in model.layers.0
25-01-23 08:39:06 | I | collecting calibration activations in model.layers.0
25-01-23 08:39:06 | I | collecting calibration activations in model.layers.0
25-01-23 08:39:06 | I | collecting calibration activations in model.layers.0
25-01-23 08:39:06 | I | collecting calibration activations in model.layers.0
25-01-23 08:39:06 | I | collecting calibration activations in model.layers.0
25-01-23 08:39:06 | I | collecting calibration activations in model.layers.0
25-01-23 08:39:06 | I | collecting calibration activations in model.layers.0
25-01-23 08:39:06 | I | collecting calibration activations in model.layers.0
25-01-23 08:39:06 | I | collecting calibration activations in model.layers.0
25-01-23 08:39:06 | I | collecting calibration activations in model.layers.0
25-01-23 08:39:06 | I | collecting calibration activations in model.layers.0
25-01-23 08:39:06 | I | collecting calibration activations in model.layers.0
25-01-23 08:39:06 | I | collecting calibration activations in model.layers.0
25-01-23 08:39:07 | I | collecting calibration activations in model.layers.0
25-01-23 08:39:07 | I | collecting calibration activations in model.layers.0
25-01-23 08:39:07 | I | collecting calibration activations in model.layers.0
25-01-23 08:39:07 | I | collecting calibration activations in model.layers.0
25-01-23 08:39:07 | I | collecting calibration activations in model.layers.0
25-01-23 08:39:07 | I | collecting calibration activations in model.layers.0
25-01-23 08:39:07 | I | collecting calibration activations in model.layers.0
25-01-23 08:39:07 | I | collecting calibration activations in model.layers.0
25-01-23 08:39:07 | I | collecting calibration activations in model.layers.0
25-01-23 08:39:07 | I | collecting calibration activations in model.layers.0
25-01-23 08:39:07 | I | collecting calibration activations in model.layers.0
25-01-23 08:39:07 | I | collecting calibration activations in model.layers.0
25-01-23 08:39:07 | I | collecting calibration activations in model.layers.0
25-01-23 08:39:07 | I | collecting calibration activations in model.layers.0
25-01-23 08:39:10 | I | use gptq_eval(lmquant) to calculate ppl, partly quanted
25-01-23 08:39:10 | W |   `pretrained` model kwarg is not of type `str`. Many other model arguments may be ignored. Please do not launch via accelerate or use `parallelize=True` if passing an existing model this way.
25-01-23 08:39:10 | W |   Passed an already-initialized model through `pretrained`, assuming single-process call to evaluate() or custom distributed integration
25-01-23 08:39:10 | I | - Evaluator: gptq
25-01-23 08:39:10 | I | - Tasks: ['wikitext', 'val_valid']
25-01-23 08:39:10 | I | - Batch_size: 8
25-01-23 08:39:10 | I |   + Max_seq_length: 2048
25-01-23 08:40:47 | I |     - Results:
25-01-23 08:40:47 | I |       |  Task   |Version|    Metric     |Value |   |Stderr|
25-01-23 08:40:47 | I |       |---------|------:|---------------|-----:|---|-----:|
25-01-23 08:40:47 | I |       |wikitext |      1|word_perplexity|5.4862|±  |5.4862|
25-01-23 08:40:47 | I |       |val_valid|      1|word_perplexity|5.0577|±  |5.0577|
25-01-23 08:40:47 | I |       
25-01-23 08:40:47 | I |   + Max_seq_length: 4096
25-01-23 08:42:22 | I |     - Results:
25-01-23 08:42:22 | I |       |  Task   |Version|    Metric     |Value |   |Stderr|
25-01-23 08:42:22 | I |       |---------|------:|---------------|-----:|---|-----:|
25-01-23 08:42:22 | I |       |wikitext |      1|word_perplexity|5.1264|±  |5.1264|
25-01-23 08:42:22 | I |       |val_valid|      1|word_perplexity|4.8515|±  |4.8515|
25-01-23 08:42:22 | I |       
25-01-23 08:43:22 | I | epoch 001 | valid on 'valid' subset:    102 / 819200000 loss_valid=2.149, lmquant_ppl_result_val=-1, lmquant_ppl_result_wikitext=-1, ppl=4.44, wps=7241.4, wpb=4058.2, bsz=2, num_updates=50, lmquant_ppl_wikitext_all_quanted=5.48165, lmquant_ppl_val_all_quanted=5.02464, lmquant_ppl_wikitext_partly_quanted=5.48616, lmquant_ppl_val_partly_quanted=5.05767
25-01-23 08:43:22 | I | epoch 001 | valid on 'valid' subset | loss_valid 2.149 | lmquant_ppl_result_val -1 | lmquant_ppl_result_wikitext -1 | ppl 4.44 | wps 7241.4 | wpb 4058.2 | bsz 2 | num_updates 50 | lmquant_ppl_wikitext_all_quanted 5.48165 | lmquant_ppl_val_all_quanted 5.02464 | lmquant_ppl_wikitext_partly_quanted 5.48616 | lmquant_ppl_val_partly_quanted 5.05767
25-01-23 08:43:22 | I | TRAIN CURRENT LAYER_IDX = 0
25-01-23 08:43:22 | I | in layer model.layers.0
25-01-23 08:43:22 | I | quantizing weights for layer model.layers.0
25-01-23 08:43:22 | I | collecting calibration activations in model.layers.0
25-01-23 08:43:22 | I | collecting calibration activations in model.layers.0
25-01-23 08:43:23 | I | - Quantizing decoder layer model.layers.0
25-01-23 08:43:23 | I |   - Calibrating model.layers.0.self_attn.q_proj.weight
25-01-23 08:43:23 | I |       - range scale = [    1.0000]
25-01-23 08:43:23 | I |         sum  error  = [    0.0428]
25-01-23 08:43:23 | I |         best error  = [    0.0428]
25-01-23 08:43:23 | I |     + error = [0.0428]
25-01-23 08:43:24 | I |       - range scale = [    1.0000]
25-01-23 08:43:24 | I |         sum  error  = [    0.6721]
25-01-23 08:43:24 | I |         best error  = [    0.6721]
25-01-23 08:43:24 | I |     + error = [0.6721]
25-01-23 08:43:24 | I |   - Calibrating model.layers.0.self_attn.k_proj.weight
25-01-23 08:43:25 | I |       - range scale = [    1.0000]
25-01-23 08:43:25 | I |         sum  error  = [    0.0532]
25-01-23 08:43:25 | I |         best error  = [    0.0532]
25-01-23 08:43:25 | I |     + error = [0.0532]
25-01-23 08:43:26 | I |       - range scale = [    1.0000]
25-01-23 08:43:26 | I |         sum  error  = [    0.9106]
25-01-23 08:43:26 | I |         best error  = [    0.9106]
25-01-23 08:43:26 | I |     + error = [0.9106]
25-01-23 08:43:26 | I |   - Calibrating model.layers.0.self_attn.v_proj.weight
25-01-23 08:43:26 | I |       - range scale = [    1.0000]
25-01-23 08:43:26 | I |         sum  error  = [    0.6659]
25-01-23 08:43:26 | I |         best error  = [    0.6659]
25-01-23 08:43:26 | I |     + error = [0.6659]
25-01-23 08:43:27 | I |       - range scale = [    1.0000]
25-01-23 08:43:27 | I |         sum  error  = [    6.1970]
25-01-23 08:43:27 | I |         best error  = [    6.1970]
25-01-23 08:43:27 | I |     + error = [6.1970]
25-01-23 08:43:28 | I |   - Calibrating model.layers.0.self_attn.o_proj.weight
25-01-23 08:43:28 | I |       - range scale = [    1.0000]
25-01-23 08:43:28 | I |         sum  error  = [    0.1902]
25-01-23 08:43:28 | I |         best error  = [    0.1902]
25-01-23 08:43:28 | I |     + error = [0.1902]
25-01-23 08:43:29 | I |       - range scale = [    1.0000]
25-01-23 08:43:29 | I |         sum  error  = [    1.5162]
25-01-23 08:43:29 | I |         best error  = [    1.5162]
25-01-23 08:43:29 | I |     + error = [1.5162]
25-01-23 08:43:29 | I |   - Calibrating model.layers.0.mlp.up_proj.weight
25-01-23 08:43:30 | I |       - range scale = [    1.0000]
25-01-23 08:43:30 | I |         sum  error  = [    1.0954]
25-01-23 08:43:30 | I |         best error  = [    1.0954]
25-01-23 08:43:30 | I |     + error = [1.0954]
25-01-23 08:43:31 | I |       - range scale = [    1.0000]
25-01-23 08:43:31 | I |         sum  error  = [   11.6489]
25-01-23 08:43:31 | I |         best error  = [   11.6489]
25-01-23 08:43:31 | I |     + error = [11.6489]
25-01-23 08:43:31 | I |   - Calibrating model.layers.0.mlp.gate_proj.weight
25-01-23 08:43:32 | I |       - range scale = [    1.0000]
25-01-23 08:43:32 | I |         sum  error  = [    1.1564]
25-01-23 08:43:32 | I |         best error  = [    1.1564]
25-01-23 08:43:32 | I |     + error = [1.1564]
25-01-23 08:43:33 | I |       - range scale = [    1.0000]
25-01-23 08:43:33 | I |         sum  error  = [   12.0432]
25-01-23 08:43:33 | I |         best error  = [   12.0432]
25-01-23 08:43:33 | I |     + error = [12.0432]
25-01-23 08:43:33 | I |   - Calibrating model.layers.0.mlp.down_proj.weight
25-01-23 08:43:34 | I |       - range scale = [    1.0000]
25-01-23 08:43:34 | I |         sum  error  = [    0.1631]
25-01-23 08:43:34 | I |         best error  = [    0.1631]
25-01-23 08:43:34 | I |     + error = [0.1631]
25-01-23 08:43:35 | I |       - range scale = [    1.0000]
25-01-23 08:43:35 | I |         sum  error  = [    1.5890]
25-01-23 08:43:35 | I |         best error  = [    1.5890]
25-01-23 08:43:35 | I |     + error = [1.5890]
25-01-23 08:43:35 | I |   - Quantizing model.layers.0.self_attn.q_proj.weight
25-01-23 08:43:38 | I |   - Quantizing model.layers.0.self_attn.k_proj.weight
25-01-23 08:43:40 | I |   - Quantizing model.layers.0.self_attn.v_proj.weight
25-01-23 08:43:42 | I |   - Quantizing model.layers.0.self_attn.o_proj.weight
25-01-23 08:43:45 | I |   - Quantizing model.layers.0.mlp.up_proj.weight
25-01-23 08:43:47 | I |   - Quantizing model.layers.0.mlp.gate_proj.weight
25-01-23 08:43:49 | I |   - Quantizing model.layers.0.mlp.down_proj.weight
25-01-23 08:43:55 | I | quantizing activations for layer model.layers.0
25-01-23 08:43:56 | I | collecting calibration activations in model.layers.0
25-01-23 08:43:56 | I | collecting calibration activations in model.layers.0
25-01-23 08:43:57 | I | forward this layer
25-01-23 08:43:57 | I | input_file: /data/gyy/lmquant-main/lmquant/data/data_without_preprocess_llama_7b_bs16/shard/val_input_args_layer_0/54.pt
25-01-23 08:43:57 | I | output_file: /data/gyy/lmquant-main/lmquant/data/data_without_preprocess_llama_7b_bs16/shard/val_teacher_output_layer_0/54.pt
25-01-23 08:43:58 | I | In layer model.layers.0, gradient of model.layers.0.self_attn.q_proj has nan: False
25-01-23 08:43:58 | I | In layer model.layers.0, gradient of model.layers.0.self_attn.q_proj has inf: False
25-01-23 08:43:58 | I | In layer model.layers.0, gradient of model.layers.0.self_attn.k_proj has nan: False
25-01-23 08:43:58 | I | In layer model.layers.0, gradient of model.layers.0.self_attn.k_proj has inf: False
25-01-23 08:43:58 | I | In layer model.layers.0, gradient of model.layers.0.self_attn.v_proj has nan: False
25-01-23 08:43:58 | I | In layer model.layers.0, gradient of model.layers.0.self_attn.v_proj has inf: False
25-01-23 08:43:58 | I | In layer model.layers.0, gradient of model.layers.0.self_attn.o_proj has nan: False
25-01-23 08:43:58 | I | In layer model.layers.0, gradient of model.layers.0.self_attn.o_proj has inf: False
25-01-23 08:43:58 | I | In layer model.layers.0, gradient of model.layers.0.mlp.up_proj has nan: False
25-01-23 08:43:58 | I | In layer model.layers.0, gradient of model.layers.0.mlp.up_proj has inf: False
25-01-23 08:43:58 | I | In layer model.layers.0, gradient of model.layers.0.mlp.gate_proj has nan: False
25-01-23 08:43:58 | I | In layer model.layers.0, gradient of model.layers.0.mlp.gate_proj has inf: False
25-01-23 08:43:58 | I | In layer model.layers.0, gradient of model.layers.0.mlp.down_proj has nan: False
25-01-23 08:43:58 | I | In layer model.layers.0, gradient of model.layers.0.mlp.down_proj has inf: False
25-01-23 08:43:58 | I | [50] done with optimizer step
25-01-23 08:43:58 | I | epoch 001:     55 / 819200000 loss=1.27912e-05, loss_per_token=0.0523928, loss_sum=429.202, wps=14.1, ups=0, wpb=8192, bsz=16, num_updates=51, lr=0.00499998, gnorm=0.134, clip=0, loss_scale=8, train_wall=36, cuda_gb_allocated=17.2, cuda_gb_reserved=20.9, cuda_gb_free=6.5, wall=4785
25-01-23 08:43:58 | I | TRAIN CURRENT LAYER_IDX = 0
25-01-23 08:43:58 | I | in layer model.layers.0
25-01-23 08:43:58 | I | quantizing weights for layer model.layers.0
25-01-23 08:43:58 | I | collecting calibration activations in model.layers.0
25-01-23 08:43:58 | I | collecting calibration activations in model.layers.0
25-01-23 08:43:59 | I | - Quantizing decoder layer model.layers.0
25-01-23 08:43:59 | I |   - Calibrating model.layers.0.self_attn.q_proj.weight
25-01-23 08:43:59 | I |       - range scale = [    1.0000]
25-01-23 08:43:59 | I |         sum  error  = [    0.0440]
25-01-23 08:43:59 | I |         best error  = [    0.0440]
25-01-23 08:43:59 | I |     + error = [0.0440]
25-01-23 08:44:00 | I |       - range scale = [    1.0000]
25-01-23 08:44:00 | I |         sum  error  = [    0.6899]
25-01-23 08:44:00 | I |         best error  = [    0.6899]
25-01-23 08:44:00 | I |     + error = [0.6899]
25-01-23 08:44:00 | I |   - Calibrating model.layers.0.self_attn.k_proj.weight
25-01-23 08:44:01 | I |       - range scale = [    1.0000]
25-01-23 08:44:01 | I |         sum  error  = [    0.0542]
25-01-23 08:44:01 | I |         best error  = [    0.0542]
25-01-23 08:44:01 | I |     + error = [0.0542]
25-01-23 08:44:02 | I |       - range scale = [    1.0000]
25-01-23 08:44:02 | I |         sum  error  = [    0.9429]
25-01-23 08:44:02 | I |         best error  = [    0.9429]
25-01-23 08:44:02 | I |     + error = [0.9429]
25-01-23 08:44:02 | I |   - Calibrating model.layers.0.self_attn.v_proj.weight
25-01-23 08:44:03 | I |       - range scale = [    1.0000]
25-01-23 08:44:03 | I |         sum  error  = [    0.6800]
25-01-23 08:44:03 | I |         best error  = [    0.6800]
25-01-23 08:44:03 | I |     + error = [0.6800]
25-01-23 08:44:03 | I |       - range scale = [    1.0000]
25-01-23 08:44:03 | I |         sum  error  = [    6.3027]
25-01-23 08:44:03 | I |         best error  = [    6.3027]
25-01-23 08:44:03 | I |     + error = [6.3027]
25-01-23 08:44:04 | I |   - Calibrating model.layers.0.self_attn.o_proj.weight
25-01-23 08:44:04 | I |       - range scale = [    1.0000]
25-01-23 08:44:04 | I |         sum  error  = [    0.1918]
25-01-23 08:44:04 | I |         best error  = [    0.1918]
25-01-23 08:44:04 | I |     + error = [0.1918]
25-01-23 08:44:05 | I |       - range scale = [    1.0000]
25-01-23 08:44:05 | I |         sum  error  = [    1.5401]
25-01-23 08:44:05 | I |         best error  = [    1.5401]
25-01-23 08:44:05 | I |     + error = [1.5401]
25-01-23 08:44:05 | I |   - Calibrating model.layers.0.mlp.up_proj.weight
25-01-23 08:44:06 | I |       - range scale = [    1.0000]
25-01-23 08:44:06 | I |         sum  error  = [    1.0542]
25-01-23 08:44:06 | I |         best error  = [    1.0542]
25-01-23 08:44:06 | I |     + error = [1.0542]
25-01-23 08:44:07 | I |       - range scale = [    1.0000]
25-01-23 08:44:07 | I |         sum  error  = [   11.2117]
25-01-23 08:44:07 | I |         best error  = [   11.2117]
25-01-23 08:44:07 | I |     + error = [11.2117]
25-01-23 08:44:07 | I |   - Calibrating model.layers.0.mlp.gate_proj.weight
25-01-23 08:44:08 | I |       - range scale = [    1.0000]
25-01-23 08:44:08 | I |         sum  error  = [    1.1172]
25-01-23 08:44:08 | I |         best error  = [    1.1172]
25-01-23 08:44:08 | I |     + error = [1.1172]
25-01-23 08:44:09 | I |       - range scale = [    1.0000]
25-01-23 08:44:09 | I |         sum  error  = [   11.5903]
25-01-23 08:44:09 | I |         best error  = [   11.5903]
25-01-23 08:44:09 | I |     + error = [11.5903]
25-01-23 08:44:10 | I |   - Calibrating model.layers.0.mlp.down_proj.weight
25-01-23 08:44:10 | I |       - range scale = [    1.0000]
25-01-23 08:44:10 | I |         sum  error  = [    0.1605]
25-01-23 08:44:10 | I |         best error  = [    0.1605]
25-01-23 08:44:10 | I |     + error = [0.1605]
25-01-23 08:44:11 | I |       - range scale = [    1.0000]
25-01-23 08:44:11 | I |         sum  error  = [    1.5697]
25-01-23 08:44:11 | I |         best error  = [    1.5697]
25-01-23 08:44:11 | I |     + error = [1.5697]
25-01-23 08:44:12 | I |   - Quantizing model.layers.0.self_attn.q_proj.weight
25-01-23 08:44:14 | I |   - Quantizing model.layers.0.self_attn.k_proj.weight
25-01-23 08:44:16 | I |   - Quantizing model.layers.0.self_attn.v_proj.weight
25-01-23 08:44:19 | I |   - Quantizing model.layers.0.self_attn.o_proj.weight
25-01-23 08:44:21 | I |   - Quantizing model.layers.0.mlp.up_proj.weight
25-01-23 08:44:23 | I |   - Quantizing model.layers.0.mlp.gate_proj.weight
25-01-23 08:44:25 | I |   - Quantizing model.layers.0.mlp.down_proj.weight
25-01-23 08:44:31 | I | quantizing activations for layer model.layers.0
25-01-23 08:44:32 | I | collecting calibration activations in model.layers.0
25-01-23 08:44:32 | I | collecting calibration activations in model.layers.0
25-01-23 08:44:34 | I | forward this layer
25-01-23 08:44:34 | I | input_file: /data/gyy/lmquant-main/lmquant/data/data_without_preprocess_llama_7b_bs16/shard/val_input_args_layer_0/55.pt
25-01-23 08:44:34 | I | output_file: /data/gyy/lmquant-main/lmquant/data/data_without_preprocess_llama_7b_bs16/shard/val_teacher_output_layer_0/55.pt
25-01-23 08:44:34 | I | In layer model.layers.0, gradient of model.layers.0.self_attn.q_proj has nan: False
25-01-23 08:44:34 | I | In layer model.layers.0, gradient of model.layers.0.self_attn.q_proj has inf: False
25-01-23 08:44:34 | I | In layer model.layers.0, gradient of model.layers.0.self_attn.k_proj has nan: False
25-01-23 08:44:34 | I | In layer model.layers.0, gradient of model.layers.0.self_attn.k_proj has inf: False
25-01-23 08:44:34 | I | In layer model.layers.0, gradient of model.layers.0.self_attn.v_proj has nan: False
25-01-23 08:44:34 | I | In layer model.layers.0, gradient of model.layers.0.self_attn.v_proj has inf: False
25-01-23 08:44:34 | I | In layer model.layers.0, gradient of model.layers.0.self_attn.o_proj has nan: False
25-01-23 08:44:34 | I | In layer model.layers.0, gradient of model.layers.0.self_attn.o_proj has inf: False
25-01-23 08:44:34 | I | In layer model.layers.0, gradient of model.layers.0.mlp.up_proj has nan: False
25-01-23 08:44:34 | I | In layer model.layers.0, gradient of model.layers.0.mlp.up_proj has inf: False
25-01-23 08:44:34 | I | In layer model.layers.0, gradient of model.layers.0.mlp.gate_proj has nan: False
25-01-23 08:44:34 | I | In layer model.layers.0, gradient of model.layers.0.mlp.gate_proj has inf: False
25-01-23 08:44:34 | I | In layer model.layers.0, gradient of model.layers.0.mlp.down_proj has nan: False
25-01-23 08:44:34 | I | In layer model.layers.0, gradient of model.layers.0.mlp.down_proj has inf: False
25-01-23 08:44:34 | I | [51] done with optimizer step
25-01-23 08:44:34 | I | epoch 001:     56 / 819200000 loss=1.10743e-05, loss_per_token=0.0453602, loss_sum=371.591, wps=225.2, ups=0.03, wpb=8192, bsz=16, num_updates=52, lr=0.00499997, gnorm=0.225, clip=0, loss_scale=8, train_wall=36, cuda_gb_allocated=17.1, cuda_gb_reserved=18.7, cuda_gb_free=6.6, wall=4822
25-01-23 08:44:34 | I | TRAIN CURRENT LAYER_IDX = 0
25-01-23 08:44:34 | I | in layer model.layers.0
25-01-23 08:44:34 | I | quantizing weights for layer model.layers.0
25-01-23 08:44:35 | I | collecting calibration activations in model.layers.0
25-01-23 08:44:35 | I | collecting calibration activations in model.layers.0
25-01-23 08:44:35 | I | - Quantizing decoder layer model.layers.0
25-01-23 08:44:35 | I |   - Calibrating model.layers.0.self_attn.q_proj.weight
25-01-23 08:44:36 | I |       - range scale = [    1.0000]
25-01-23 08:44:36 | I |         sum  error  = [    0.0444]
25-01-23 08:44:36 | I |         best error  = [    0.0444]
25-01-23 08:44:36 | I |     + error = [0.0444]
25-01-23 08:44:37 | I |       - range scale = [    1.0000]
25-01-23 08:44:37 | I |         sum  error  = [    0.7043]
25-01-23 08:44:37 | I |         best error  = [    0.7043]
25-01-23 08:44:37 | I |     + error = [0.7043]
25-01-23 08:44:37 | I |   - Calibrating model.layers.0.self_attn.k_proj.weight
25-01-23 08:44:37 | I |       - range scale = [    1.0000]
25-01-23 08:44:37 | I |         sum  error  = [    0.0562]
25-01-23 08:44:37 | I |         best error  = [    0.0562]
25-01-23 08:44:37 | I |     + error = [0.0562]
25-01-23 08:44:38 | I |       - range scale = [    1.0000]
25-01-23 08:44:38 | I |         sum  error  = [    0.9339]
25-01-23 08:44:38 | I |         best error  = [    0.9339]
25-01-23 08:44:38 | I |     + error = [0.9339]
25-01-23 08:44:38 | I |   - Calibrating model.layers.0.self_attn.v_proj.weight
25-01-23 08:44:39 | I |       - range scale = [    1.0000]
25-01-23 08:44:39 | I |         sum  error  = [    0.6680]
25-01-23 08:44:39 | I |         best error  = [    0.6680]
25-01-23 08:44:39 | I |     + error = [0.6680]
25-01-23 08:44:40 | I |       - range scale = [    1.0000]
25-01-23 08:44:40 | I |         sum  error  = [    6.2207]
25-01-23 08:44:40 | I |         best error  = [    6.2207]
25-01-23 08:44:40 | I |     + error = [6.2207]
25-01-23 08:44:40 | I |   - Calibrating model.layers.0.self_attn.o_proj.weight
25-01-23 08:44:41 | I |       - range scale = [    1.0000]
25-01-23 08:44:41 | I |         sum  error  = [    0.1886]
25-01-23 08:44:41 | I |         best error  = [    0.1886]
25-01-23 08:44:41 | I |     + error = [0.1886]
25-01-23 08:44:42 | I |       - range scale = [    1.0000]
25-01-23 08:44:42 | I |         sum  error  = [    1.5092]
25-01-23 08:44:42 | I |         best error  = [    1.5092]
25-01-23 08:44:42 | I |     + error = [1.5092]
25-01-23 08:44:42 | I |   - Calibrating model.layers.0.mlp.up_proj.weight
25-01-23 08:44:42 | I |       - range scale = [    1.0000]
25-01-23 08:44:42 | I |         sum  error  = [    1.0394]
25-01-23 08:44:42 | I |         best error  = [    1.0394]
25-01-23 08:44:42 | I |     + error = [1.0394]
25-01-23 08:44:44 | I |       - range scale = [    1.0000]
25-01-23 08:44:44 | I |         sum  error  = [   11.0461]
25-01-23 08:44:44 | I |         best error  = [   11.0461]
25-01-23 08:44:44 | I |     + error = [11.0461]
25-01-23 08:44:44 | I |   - Calibrating model.layers.0.mlp.gate_proj.weight
25-01-23 08:44:45 | I |       - range scale = [    1.0000]
25-01-23 08:44:45 | I |         sum  error  = [    1.0963]
25-01-23 08:44:45 | I |         best error  = [    1.0963]
25-01-23 08:44:45 | I |     + error = [1.0963]
25-01-23 08:44:46 | I |       - range scale = [    1.0000]
25-01-23 08:44:46 | I |         sum  error  = [   11.4125]
25-01-23 08:44:46 | I |         best error  = [   11.4125]
25-01-23 08:44:46 | I |     + error = [11.4125]
25-01-23 08:44:46 | I |   - Calibrating model.layers.0.mlp.down_proj.weight
25-01-23 08:44:47 | I |       - range scale = [    1.0000]
25-01-23 08:44:47 | I |         sum  error  = [    0.1597]
25-01-23 08:44:47 | I |         best error  = [    0.1597]
25-01-23 08:44:47 | I |     + error = [0.1597]
25-01-23 08:44:48 | I |       - range scale = [    1.0000]
25-01-23 08:44:48 | I |         sum  error  = [    1.5631]
25-01-23 08:44:48 | I |         best error  = [    1.5631]
25-01-23 08:44:48 | I |     + error = [1.5631]
25-01-23 08:44:48 | I |   - Quantizing model.layers.0.self_attn.q_proj.weight
25-01-23 08:44:50 | I |   - Quantizing model.layers.0.self_attn.k_proj.weight
25-01-23 08:44:53 | I |   - Quantizing model.layers.0.self_attn.v_proj.weight
25-01-23 08:44:55 | I |   - Quantizing model.layers.0.self_attn.o_proj.weight
25-01-23 08:44:57 | I |   - Quantizing model.layers.0.mlp.up_proj.weight
25-01-23 08:44:59 | I |   - Quantizing model.layers.0.mlp.gate_proj.weight
25-01-23 08:45:02 | I |   - Quantizing model.layers.0.mlp.down_proj.weight
25-01-23 08:45:08 | I | quantizing activations for layer model.layers.0
25-01-23 08:45:08 | I | collecting calibration activations in model.layers.0
25-01-23 08:45:08 | I | collecting calibration activations in model.layers.0
25-01-23 08:45:10 | I | forward this layer
25-01-23 08:45:10 | I | input_file: /data/gyy/lmquant-main/lmquant/data/data_without_preprocess_llama_7b_bs16/shard/val_input_args_layer_0/56.pt
25-01-23 08:45:10 | I | output_file: /data/gyy/lmquant-main/lmquant/data/data_without_preprocess_llama_7b_bs16/shard/val_teacher_output_layer_0/56.pt
25-01-23 08:45:10 | I | In layer model.layers.0, gradient of model.layers.0.self_attn.q_proj has nan: False
25-01-23 08:45:10 | I | In layer model.layers.0, gradient of model.layers.0.self_attn.q_proj has inf: False
25-01-23 08:45:10 | I | In layer model.layers.0, gradient of model.layers.0.self_attn.k_proj has nan: False
25-01-23 08:45:10 | I | In layer model.layers.0, gradient of model.layers.0.self_attn.k_proj has inf: False
25-01-23 08:45:10 | I | In layer model.layers.0, gradient of model.layers.0.self_attn.v_proj has nan: False
25-01-23 08:45:10 | I | In layer model.layers.0, gradient of model.layers.0.self_attn.v_proj has inf: False
25-01-23 08:45:10 | I | In layer model.layers.0, gradient of model.layers.0.self_attn.o_proj has nan: False
25-01-23 08:45:10 | I | In layer model.layers.0, gradient of model.layers.0.self_attn.o_proj has inf: False
25-01-23 08:45:10 | I | In layer model.layers.0, gradient of model.layers.0.mlp.up_proj has nan: False
25-01-23 08:45:10 | I | In layer model.layers.0, gradient of model.layers.0.mlp.up_proj has inf: False
25-01-23 08:45:10 | I | In layer model.layers.0, gradient of model.layers.0.mlp.gate_proj has nan: False
25-01-23 08:45:10 | I | In layer model.layers.0, gradient of model.layers.0.mlp.gate_proj has inf: False
25-01-23 08:45:10 | I | In layer model.layers.0, gradient of model.layers.0.mlp.down_proj has nan: False
25-01-23 08:45:10 | I | In layer model.layers.0, gradient of model.layers.0.mlp.down_proj has inf: False
25-01-23 08:45:10 | I | [52] done with optimizer step
25-01-23 08:45:10 | I | epoch 001:     57 / 819200000 loss=1.11273e-05, loss_per_token=0.0455774, loss_sum=373.37, wps=227.2, ups=0.03, wpb=8192, bsz=16, num_updates=53, lr=0.00499995, gnorm=0.101, clip=0, loss_scale=8, train_wall=36, cuda_gb_allocated=17.1, cuda_gb_reserved=18.7, cuda_gb_free=6.6, wall=4858
25-01-23 08:45:10 | I | TRAIN CURRENT LAYER_IDX = 0
25-01-23 08:45:10 | I | in layer model.layers.0
25-01-23 08:45:10 | I | quantizing weights for layer model.layers.0
25-01-23 08:45:11 | I | collecting calibration activations in model.layers.0
25-01-23 08:45:11 | I | collecting calibration activations in model.layers.0
25-01-23 08:45:11 | I | - Quantizing decoder layer model.layers.0
25-01-23 08:45:11 | I |   - Calibrating model.layers.0.self_attn.q_proj.weight
25-01-23 08:45:12 | I |       - range scale = [    1.0000]
25-01-23 08:45:12 | I |         sum  error  = [    0.0460]
25-01-23 08:45:12 | I |         best error  = [    0.0460]
25-01-23 08:45:12 | I |     + error = [0.0460]
25-01-23 08:45:13 | I |       - range scale = [    1.0000]
25-01-23 08:45:13 | I |         sum  error  = [    0.7227]
25-01-23 08:45:13 | I |         best error  = [    0.7227]
25-01-23 08:45:13 | I |     + error = [0.7227]
25-01-23 08:45:13 | I |   - Calibrating model.layers.0.self_attn.k_proj.weight
25-01-23 08:45:13 | I |       - range scale = [    1.0000]
25-01-23 08:45:13 | I |         sum  error  = [    0.0583]
25-01-23 08:45:13 | I |         best error  = [    0.0583]
25-01-23 08:45:13 | I |     + error = [0.0583]
25-01-23 08:45:14 | I |       - range scale = [    1.0000]
25-01-23 08:45:14 | I |         sum  error  = [    0.9123]
25-01-23 08:45:14 | I |         best error  = [    0.9123]
25-01-23 08:45:14 | I |     + error = [0.9123]
25-01-23 08:45:14 | I |   - Calibrating model.layers.0.self_attn.v_proj.weight
25-01-23 08:45:15 | I |       - range scale = [    1.0000]
25-01-23 08:45:15 | I |         sum  error  = [    0.7087]
25-01-23 08:45:15 | I |         best error  = [    0.7087]
25-01-23 08:45:15 | I |     + error = [0.7087]
25-01-23 08:45:16 | I |       - range scale = [    1.0000]
25-01-23 08:45:16 | I |         sum  error  = [    6.5833]
25-01-23 08:45:16 | I |         best error  = [    6.5833]
25-01-23 08:45:16 | I |     + error = [6.5833]
25-01-23 08:45:16 | I |   - Calibrating model.layers.0.self_attn.o_proj.weight
25-01-23 08:45:17 | I |       - range scale = [    1.0000]
25-01-23 08:45:17 | I |         sum  error  = [    0.1993]
25-01-23 08:45:17 | I |         best error  = [    0.1993]
25-01-23 08:45:17 | I |     + error = [0.1993]
25-01-23 08:45:18 | I |       - range scale = [    1.0000]
25-01-23 08:45:18 | I |         sum  error  = [    1.5843]
25-01-23 08:45:18 | I |         best error  = [    1.5843]
25-01-23 08:45:18 | I |     + error = [1.5843]
25-01-23 08:45:18 | I |   - Calibrating model.layers.0.mlp.up_proj.weight
25-01-23 08:45:19 | I |       - range scale = [    1.0000]
25-01-23 08:45:19 | I |         sum  error  = [    0.9997]
25-01-23 08:45:19 | I |         best error  = [    0.9997]
25-01-23 08:45:19 | I |     + error = [0.9997]
25-01-23 08:45:20 | I |       - range scale = [    1.0000]
25-01-23 08:45:20 | I |         sum  error  = [   10.6297]
25-01-23 08:45:20 | I |         best error  = [   10.6297]
25-01-23 08:45:20 | I |     + error = [10.6297]
25-01-23 08:45:20 | I |   - Calibrating model.layers.0.mlp.gate_proj.weight
25-01-23 08:45:21 | I |       - range scale = [    1.0000]
25-01-23 08:45:21 | I |         sum  error  = [    1.0566]
25-01-23 08:45:21 | I |         best error  = [    1.0566]
25-01-23 08:45:21 | I |     + error = [1.0566]
25-01-23 08:45:22 | I |       - range scale = [    1.0000]
25-01-23 08:45:22 | I |         sum  error  = [   10.9936]
25-01-23 08:45:22 | I |         best error  = [   10.9936]
25-01-23 08:45:22 | I |     + error = [10.9936]
25-01-23 08:45:22 | I |   - Calibrating model.layers.0.mlp.down_proj.weight
25-01-23 08:45:23 | I |       - range scale = [    1.0000]
25-01-23 08:45:23 | I |         sum  error  = [    0.1649]
25-01-23 08:45:23 | I |         best error  = [    0.1649]
25-01-23 08:45:23 | I |     + error = [0.1649]
25-01-23 08:45:24 | I |       - range scale = [    1.0000]
25-01-23 08:45:24 | I |         sum  error  = [    1.6133]
25-01-23 08:45:24 | I |         best error  = [    1.6133]
25-01-23 08:45:24 | I |     + error = [1.6133]
25-01-23 08:45:24 | I |   - Quantizing model.layers.0.self_attn.q_proj.weight
25-01-23 08:45:26 | I |   - Quantizing model.layers.0.self_attn.k_proj.weight
25-01-23 08:45:29 | I |   - Quantizing model.layers.0.self_attn.v_proj.weight
25-01-23 08:45:31 | I |   - Quantizing model.layers.0.self_attn.o_proj.weight
25-01-23 08:45:33 | I |   - Quantizing model.layers.0.mlp.up_proj.weight
25-01-23 08:45:35 | I |   - Quantizing model.layers.0.mlp.gate_proj.weight
25-01-23 08:45:38 | I |   - Quantizing model.layers.0.mlp.down_proj.weight
25-01-23 08:45:44 | I | quantizing activations for layer model.layers.0
25-01-23 08:45:44 | I | collecting calibration activations in model.layers.0
25-01-23 08:45:44 | I | collecting calibration activations in model.layers.0
25-01-23 08:45:46 | I | forward this layer
25-01-23 08:45:46 | I | input_file: /data/gyy/lmquant-main/lmquant/data/data_without_preprocess_llama_7b_bs16/shard/val_input_args_layer_0/57.pt
25-01-23 08:45:46 | I | output_file: /data/gyy/lmquant-main/lmquant/data/data_without_preprocess_llama_7b_bs16/shard/val_teacher_output_layer_0/57.pt
25-01-23 08:45:46 | I | In layer model.layers.0, gradient of model.layers.0.self_attn.q_proj has nan: False
25-01-23 08:45:46 | I | In layer model.layers.0, gradient of model.layers.0.self_attn.q_proj has inf: False
25-01-23 08:45:46 | I | In layer model.layers.0, gradient of model.layers.0.self_attn.k_proj has nan: False
25-01-23 08:45:46 | I | In layer model.layers.0, gradient of model.layers.0.self_attn.k_proj has inf: False
25-01-23 08:45:46 | I | In layer model.layers.0, gradient of model.layers.0.self_attn.v_proj has nan: False
25-01-23 08:45:46 | I | In layer model.layers.0, gradient of model.layers.0.self_attn.v_proj has inf: False
25-01-23 08:45:46 | I | In layer model.layers.0, gradient of model.layers.0.self_attn.o_proj has nan: False
25-01-23 08:45:46 | I | In layer model.layers.0, gradient of model.layers.0.self_attn.o_proj has inf: False
25-01-23 08:45:46 | I | In layer model.layers.0, gradient of model.layers.0.mlp.up_proj has nan: False
25-01-23 08:45:46 | I | In layer model.layers.0, gradient of model.layers.0.mlp.up_proj has inf: False
25-01-23 08:45:46 | I | In layer model.layers.0, gradient of model.layers.0.mlp.gate_proj has nan: False
25-01-23 08:45:46 | I | In layer model.layers.0, gradient of model.layers.0.mlp.gate_proj has inf: False
25-01-23 08:45:46 | I | In layer model.layers.0, gradient of model.layers.0.mlp.down_proj has nan: False
25-01-23 08:45:46 | I | In layer model.layers.0, gradient of model.layers.0.mlp.down_proj has inf: False
25-01-23 08:45:46 | I | [53] done with optimizer step
25-01-23 08:45:46 | I | epoch 001:     58 / 819200000 loss=1.13275e-05, loss_per_token=0.0463972, loss_sum=380.086, wps=227.4, ups=0.03, wpb=8192, bsz=16, num_updates=54, lr=0.00499993, gnorm=0.157, clip=0, loss_scale=8, train_wall=36, cuda_gb_allocated=17.1, cuda_gb_reserved=18.3, cuda_gb_free=6.6, wall=4894
25-01-23 08:45:46 | I | TRAIN CURRENT LAYER_IDX = 0
25-01-23 08:45:46 | I | in layer model.layers.0
25-01-23 08:45:46 | I | quantizing weights for layer model.layers.0
25-01-23 08:45:47 | I | collecting calibration activations in model.layers.0
25-01-23 08:45:47 | I | collecting calibration activations in model.layers.0
25-01-23 08:45:47 | I | - Quantizing decoder layer model.layers.0
25-01-23 08:45:47 | I |   - Calibrating model.layers.0.self_attn.q_proj.weight
25-01-23 08:45:48 | I |       - range scale = [    1.0000]
25-01-23 08:45:48 | I |         sum  error  = [    0.0437]
25-01-23 08:45:48 | I |         best error  = [    0.0437]
25-01-23 08:45:48 | I |     + error = [0.0437]
25-01-23 08:45:49 | I |       - range scale = [    1.0000]
25-01-23 08:45:49 | I |         sum  error  = [    0.6769]
25-01-23 08:45:49 | I |         best error  = [    0.6769]
25-01-23 08:45:49 | I |     + error = [0.6769]
25-01-23 08:45:49 | I |   - Calibrating model.layers.0.self_attn.k_proj.weight
25-01-23 08:45:50 | I |       - range scale = [    1.0000]
25-01-23 08:45:50 | I |         sum  error  = [    0.0511]
25-01-23 08:45:50 | I |         best error  = [    0.0511]
25-01-23 08:45:50 | I |     + error = [0.0511]
25-01-23 08:45:50 | I |       - range scale = [    1.0000]
25-01-23 08:45:50 | I |         sum  error  = [    0.8309]
25-01-23 08:45:50 | I |         best error  = [    0.8309]
25-01-23 08:45:50 | I |     + error = [0.8309]
25-01-23 08:45:50 | I |   - Calibrating model.layers.0.self_attn.v_proj.weight
25-01-23 08:45:51 | I |       - range scale = [    1.0000]
25-01-23 08:45:51 | I |         sum  error  = [    0.6771]
25-01-23 08:45:51 | I |         best error  = [    0.6771]
25-01-23 08:45:51 | I |     + error = [0.6771]
25-01-23 08:45:52 | I |       - range scale = [    1.0000]
25-01-23 08:45:52 | I |         sum  error  = [    6.3488]
25-01-23 08:45:52 | I |         best error  = [    6.3488]
25-01-23 08:45:52 | I |     + error = [6.3488]
25-01-23 08:45:52 | I |   - Calibrating model.layers.0.self_attn.o_proj.weight
25-01-23 08:45:53 | I |       - range scale = [    1.0000]
25-01-23 08:45:53 | I |         sum  error  = [    0.1965]
25-01-23 08:45:53 | I |         best error  = [    0.1965]
25-01-23 08:45:53 | I |     + error = [0.1965]
25-01-23 08:45:54 | I |       - range scale = [    1.0000]
25-01-23 08:45:54 | I |         sum  error  = [    1.5868]
25-01-23 08:45:54 | I |         best error  = [    1.5868]
25-01-23 08:45:54 | I |     + error = [1.5868]
25-01-23 08:45:54 | I |   - Calibrating model.layers.0.mlp.up_proj.weight
25-01-23 08:45:55 | I |       - range scale = [    1.0000]
25-01-23 08:45:55 | I |         sum  error  = [    1.1393]
25-01-23 08:45:55 | I |         best error  = [    1.1393]
25-01-23 08:45:55 | I |     + error = [1.1393]
25-01-23 08:45:56 | I |       - range scale = [    1.0000]
25-01-23 08:45:56 | I |         sum  error  = [   12.1418]
25-01-23 08:45:56 | I |         best error  = [   12.1418]
25-01-23 08:45:56 | I |     + error = [12.1418]
25-01-23 08:45:56 | I |   - Calibrating model.layers.0.mlp.gate_proj.weight
25-01-23 08:45:57 | I |       - range scale = [    1.0000]
25-01-23 08:45:57 | I |         sum  error  = [    1.2139]
25-01-23 08:45:57 | I |         best error  = [    1.2139]
25-01-23 08:45:57 | I |     + error = [1.2139]
25-01-23 08:45:58 | I |       - range scale = [    1.0000]
25-01-23 08:45:58 | I |         sum  error  = [   12.5393]
25-01-23 08:45:58 | I |         best error  = [   12.5393]
25-01-23 08:45:58 | I |     + error = [12.5393]
25-01-23 08:45:58 | I |   - Calibrating model.layers.0.mlp.down_proj.weight
25-01-23 08:45:59 | I |       - range scale = [    1.0000]
25-01-23 08:45:59 | I |         sum  error  = [    0.1798]
25-01-23 08:45:59 | I |         best error  = [    0.1798]
25-01-23 08:45:59 | I |     + error = [0.1798]
25-01-23 08:46:00 | I |       - range scale = [    1.0000]
25-01-23 08:46:00 | I |         sum  error  = [    1.7446]
25-01-23 08:46:00 | I |         best error  = [    1.7446]
25-01-23 08:46:00 | I |     + error = [1.7446]
25-01-23 08:46:00 | I |   - Quantizing model.layers.0.self_attn.q_proj.weight
25-01-23 08:46:03 | I |   - Quantizing model.layers.0.self_attn.k_proj.weight
25-01-23 08:46:05 | I |   - Quantizing model.layers.0.self_attn.v_proj.weight
25-01-23 08:46:07 | I |   - Quantizing model.layers.0.self_attn.o_proj.weight
25-01-23 08:46:09 | I |   - Quantizing model.layers.0.mlp.up_proj.weight
25-01-23 08:46:12 | I |   - Quantizing model.layers.0.mlp.gate_proj.weight
25-01-23 08:46:14 | I |   - Quantizing model.layers.0.mlp.down_proj.weight
25-01-23 08:46:20 | I | quantizing activations for layer model.layers.0
25-01-23 08:46:21 | I | collecting calibration activations in model.layers.0
25-01-23 08:46:21 | I | collecting calibration activations in model.layers.0
25-01-23 08:46:22 | I | forward this layer
25-01-23 08:46:22 | I | input_file: /data/gyy/lmquant-main/lmquant/data/data_without_preprocess_llama_7b_bs16/shard/val_input_args_layer_0/58.pt
25-01-23 08:46:22 | I | output_file: /data/gyy/lmquant-main/lmquant/data/data_without_preprocess_llama_7b_bs16/shard/val_teacher_output_layer_0/58.pt
25-01-23 08:46:23 | I | In layer model.layers.0, gradient of model.layers.0.self_attn.q_proj has nan: False
25-01-23 08:46:23 | I | In layer model.layers.0, gradient of model.layers.0.self_attn.q_proj has inf: False
25-01-23 08:46:23 | I | In layer model.layers.0, gradient of model.layers.0.self_attn.k_proj has nan: False
25-01-23 08:46:23 | I | In layer model.layers.0, gradient of model.layers.0.self_attn.k_proj has inf: False
25-01-23 08:46:23 | I | In layer model.layers.0, gradient of model.layers.0.self_attn.v_proj has nan: False
25-01-23 08:46:23 | I | In layer model.layers.0, gradient of model.layers.0.self_attn.v_proj has inf: False
25-01-23 08:46:23 | I | In layer model.layers.0, gradient of model.layers.0.self_attn.o_proj has nan: False
25-01-23 08:46:23 | I | In layer model.layers.0, gradient of model.layers.0.self_attn.o_proj has inf: False
25-01-23 08:46:23 | I | In layer model.layers.0, gradient of model.layers.0.mlp.up_proj has nan: False
25-01-23 08:46:23 | I | In layer model.layers.0, gradient of model.layers.0.mlp.up_proj has inf: False
25-01-23 08:46:23 | I | In layer model.layers.0, gradient of model.layers.0.mlp.gate_proj has nan: False
25-01-23 08:46:23 | I | In layer model.layers.0, gradient of model.layers.0.mlp.gate_proj has inf: False
25-01-23 08:46:23 | I | In layer model.layers.0, gradient of model.layers.0.mlp.down_proj has nan: False
25-01-23 08:46:23 | I | In layer model.layers.0, gradient of model.layers.0.mlp.down_proj has inf: False
25-01-23 08:46:23 | I | [54] done with optimizer step
25-01-23 08:46:23 | I | epoch 001:     59 / 819200000 loss=1.70259e-05, loss_per_token=0.0697382, loss_sum=571.295, wps=224.2, ups=0.03, wpb=8192, bsz=16, num_updates=55, lr=0.00499992, gnorm=0.45, clip=0, loss_scale=8, train_wall=36, cuda_gb_allocated=17.1, cuda_gb_reserved=18.3, cuda_gb_free=6.6, wall=4931
25-01-23 08:46:23 | I | TRAIN CURRENT LAYER_IDX = 0
25-01-23 08:46:23 | I | in layer model.layers.0
25-01-23 08:46:23 | I | quantizing weights for layer model.layers.0
25-01-23 08:46:23 | I | collecting calibration activations in model.layers.0
25-01-23 08:46:24 | I | collecting calibration activations in model.layers.0
25-01-23 08:46:24 | I | - Quantizing decoder layer model.layers.0
25-01-23 08:46:24 | I |   - Calibrating model.layers.0.self_attn.q_proj.weight
25-01-23 08:46:24 | I |       - range scale = [    1.0000]
25-01-23 08:46:24 | I |         sum  error  = [    0.0418]
25-01-23 08:46:24 | I |         best error  = [    0.0418]
25-01-23 08:46:24 | I |     + error = [0.0418]
25-01-23 08:46:25 | I |       - range scale = [    1.0000]
25-01-23 08:46:25 | I |         sum  error  = [    0.6589]
25-01-23 08:46:25 | I |         best error  = [    0.6589]
25-01-23 08:46:25 | I |     + error = [0.6589]
25-01-23 08:46:25 | I |   - Calibrating model.layers.0.self_attn.k_proj.weight
25-01-23 08:46:26 | I |       - range scale = [    1.0000]
25-01-23 08:46:26 | I |         sum  error  = [    0.0478]
25-01-23 08:46:26 | I |         best error  = [    0.0478]
25-01-23 08:46:26 | I |     + error = [0.0478]
25-01-23 08:46:27 | I |       - range scale = [    1.0000]
25-01-23 08:46:27 | I |         sum  error  = [    0.8023]
25-01-23 08:46:27 | I |         best error  = [    0.8023]
25-01-23 08:46:27 | I |     + error = [0.8023]
25-01-23 08:46:27 | I |   - Calibrating model.layers.0.self_attn.v_proj.weight
25-01-23 08:46:28 | I |       - range scale = [    1.0000]
25-01-23 08:46:28 | I |         sum  error  = [    0.6773]
25-01-23 08:46:28 | I |         best error  = [    0.6773]
25-01-23 08:46:28 | I |     + error = [0.6773]
25-01-23 08:46:29 | I |       - range scale = [    1.0000]
25-01-23 08:46:29 | I |         sum  error  = [    6.4231]
25-01-23 08:46:29 | I |         best error  = [    6.4231]
25-01-23 08:46:29 | I |     + error = [6.4231]
25-01-23 08:46:29 | I |   - Calibrating model.layers.0.self_attn.o_proj.weight
25-01-23 08:46:29 | I |       - range scale = [    1.0000]
25-01-23 08:46:29 | I |         sum  error  = [    0.1958]
25-01-23 08:46:29 | I |         best error  = [    0.1958]
25-01-23 08:46:29 | I |     + error = [0.1958]
25-01-23 08:46:30 | I |       - range scale = [    1.0000]
25-01-23 08:46:30 | I |         sum  error  = [    1.5693]
25-01-23 08:46:30 | I |         best error  = [    1.5693]
25-01-23 08:46:30 | I |     + error = [1.5693]
25-01-23 08:46:30 | I |   - Calibrating model.layers.0.mlp.up_proj.weight
25-01-23 08:46:31 | I |       - range scale = [    1.0000]
25-01-23 08:46:31 | I |         sum  error  = [    1.1588]
25-01-23 08:46:31 | I |         best error  = [    1.1588]
25-01-23 08:46:31 | I |     + error = [1.1588]
25-01-23 08:46:32 | I |       - range scale = [    1.0000]
25-01-23 08:46:32 | I |         sum  error  = [   12.3494]
25-01-23 08:46:32 | I |         best error  = [   12.3494]
25-01-23 08:46:32 | I |     + error = [12.3494]
25-01-23 08:46:32 | I |   - Calibrating model.layers.0.mlp.gate_proj.weight
25-01-23 08:46:33 | I |       - range scale = [    1.0000]
25-01-23 08:46:33 | I |         sum  error  = [    1.2345]
25-01-23 08:46:33 | I |         best error  = [    1.2345]
25-01-23 08:46:33 | I |     + error = [1.2345]
25-01-23 08:46:34 | I |       - range scale = [    1.0000]
25-01-23 08:46:34 | I |         sum  error  = [   12.7500]
25-01-23 08:46:34 | I |         best error  = [   12.7500]
25-01-23 08:46:34 | I |     + error = [12.7500]
25-01-23 08:46:35 | I |   - Calibrating model.layers.0.mlp.down_proj.weight
25-01-23 08:46:36 | I |       - range scale = [    1.0000]
25-01-23 08:46:36 | I |         sum  error  = [    0.1844]
25-01-23 08:46:36 | I |         best error  = [    0.1844]
25-01-23 08:46:36 | I |     + error = [0.1844]
25-01-23 08:46:37 | I |       - range scale = [    1.0000]
25-01-23 08:46:37 | I |         sum  error  = [    1.7820]
25-01-23 08:46:37 | I |         best error  = [    1.7820]
25-01-23 08:46:37 | I |     + error = [1.7820]
25-01-23 08:46:37 | I |   - Quantizing model.layers.0.self_attn.q_proj.weight
25-01-23 08:46:40 | I |   - Quantizing model.layers.0.self_attn.k_proj.weight
25-01-23 08:46:43 | I |   - Quantizing model.layers.0.self_attn.v_proj.weight
25-01-23 08:46:46 | I |   - Quantizing model.layers.0.self_attn.o_proj.weight
25-01-23 08:46:48 | I |   - Quantizing model.layers.0.mlp.up_proj.weight
25-01-23 08:46:51 | I |   - Quantizing model.layers.0.mlp.gate_proj.weight
25-01-23 08:46:54 | I |   - Quantizing model.layers.0.mlp.down_proj.weight
25-01-23 08:47:02 | I | quantizing activations for layer model.layers.0
25-01-23 08:47:02 | I | collecting calibration activations in model.layers.0
25-01-23 08:47:02 | I | collecting calibration activations in model.layers.0
25-01-23 08:47:04 | I | forward this layer
25-01-23 08:47:04 | I | input_file: /data/gyy/lmquant-main/lmquant/data/data_without_preprocess_llama_7b_bs16/shard/val_input_args_layer_0/59.pt
25-01-23 08:47:04 | I | output_file: /data/gyy/lmquant-main/lmquant/data/data_without_preprocess_llama_7b_bs16/shard/val_teacher_output_layer_0/59.pt
25-01-23 08:47:04 | I | In layer model.layers.0, gradient of model.layers.0.self_attn.q_proj has nan: False
25-01-23 08:47:04 | I | In layer model.layers.0, gradient of model.layers.0.self_attn.q_proj has inf: False
25-01-23 08:47:04 | I | In layer model.layers.0, gradient of model.layers.0.self_attn.k_proj has nan: False
25-01-23 08:47:04 | I | In layer model.layers.0, gradient of model.layers.0.self_attn.k_proj has inf: False
25-01-23 08:47:04 | I | In layer model.layers.0, gradient of model.layers.0.self_attn.v_proj has nan: False
25-01-23 08:47:04 | I | In layer model.layers.0, gradient of model.layers.0.self_attn.v_proj has inf: False
25-01-23 08:47:04 | I | In layer model.layers.0, gradient of model.layers.0.self_attn.o_proj has nan: False
25-01-23 08:47:04 | I | In layer model.layers.0, gradient of model.layers.0.self_attn.o_proj has inf: False
25-01-23 08:47:04 | I | In layer model.layers.0, gradient of model.layers.0.mlp.up_proj has nan: False
25-01-23 08:47:04 | I | In layer model.layers.0, gradient of model.layers.0.mlp.up_proj has inf: False
25-01-23 08:47:04 | I | In layer model.layers.0, gradient of model.layers.0.mlp.gate_proj has nan: False
25-01-23 08:47:04 | I | In layer model.layers.0, gradient of model.layers.0.mlp.gate_proj has inf: False
25-01-23 08:47:04 | I | In layer model.layers.0, gradient of model.layers.0.mlp.down_proj has nan: False
25-01-23 08:47:04 | I | In layer model.layers.0, gradient of model.layers.0.mlp.down_proj has inf: False
25-01-23 08:47:04 | I | [55] done with optimizer step
25-01-23 08:47:04 | I | epoch 001:     60 / 819200000 loss=1.50799e-05, loss_per_token=0.0617674, loss_sum=505.998, wps=197.2, ups=0.02, wpb=8192, bsz=16, num_updates=56, lr=0.0049999, gnorm=0.401, clip=0, loss_scale=8, train_wall=41, cuda_gb_allocated=17.1, cuda_gb_reserved=18.3, cuda_gb_free=6.6, wall=4972
25-01-23 08:47:05 | I | TRAIN CURRENT LAYER_IDX = 0
25-01-23 08:47:05 | I | in layer model.layers.0
25-01-23 08:47:05 | I | quantizing weights for layer model.layers.0
25-01-23 08:47:05 | I | collecting calibration activations in model.layers.0
25-01-23 08:47:05 | I | collecting calibration activations in model.layers.0
25-01-23 08:47:05 | I | - Quantizing decoder layer model.layers.0
25-01-23 08:47:05 | I |   - Calibrating model.layers.0.self_attn.q_proj.weight
25-01-23 08:47:06 | I |       - range scale = [    1.0000]
25-01-23 08:47:06 | I |         sum  error  = [    0.0441]
25-01-23 08:47:06 | I |         best error  = [    0.0441]
25-01-23 08:47:06 | I |     + error = [0.0441]
25-01-23 08:47:07 | I |       - range scale = [    1.0000]
25-01-23 08:47:07 | I |         sum  error  = [    0.6740]
25-01-23 08:47:07 | I |         best error  = [    0.6740]
25-01-23 08:47:07 | I |     + error = [0.6740]
25-01-23 08:47:07 | I |   - Calibrating model.layers.0.self_attn.k_proj.weight
25-01-23 08:47:08 | I |       - range scale = [    1.0000]
25-01-23 08:47:08 | I |         sum  error  = [    0.0532]
25-01-23 08:47:08 | I |         best error  = [    0.0532]
25-01-23 08:47:08 | I |     + error = [0.0532]
25-01-23 08:47:08 | I |       - range scale = [    1.0000]
25-01-23 08:47:08 | I |         sum  error  = [    0.8822]
25-01-23 08:47:08 | I |         best error  = [    0.8822]
25-01-23 08:47:08 | I |     + error = [0.8822]
25-01-23 08:47:09 | I |   - Calibrating model.layers.0.self_attn.v_proj.weight
25-01-23 08:47:09 | I |       - range scale = [    1.0000]
25-01-23 08:47:09 | I |         sum  error  = [    0.6651]
25-01-23 08:47:09 | I |         best error  = [    0.6651]
25-01-23 08:47:09 | I |     + error = [0.6651]
25-01-23 08:47:10 | I |       - range scale = [    1.0000]
25-01-23 08:47:10 | I |         sum  error  = [    6.2831]
25-01-23 08:47:10 | I |         best error  = [    6.2831]
25-01-23 08:47:10 | I |     + error = [6.2831]
25-01-23 08:47:10 | I |   - Calibrating model.layers.0.self_attn.o_proj.weight
25-01-23 08:47:11 | I |       - range scale = [    1.0000]
25-01-23 08:47:11 | I |         sum  error  = [    0.1921]
25-01-23 08:47:11 | I |         best error  = [    0.1921]
25-01-23 08:47:11 | I |     + error = [0.1921]
25-01-23 08:47:12 | I |       - range scale = [    1.0000]
25-01-23 08:47:12 | I |         sum  error  = [    1.5378]
25-01-23 08:47:12 | I |         best error  = [    1.5378]
25-01-23 08:47:12 | I |     + error = [1.5378]
25-01-23 08:47:12 | I |   - Calibrating model.layers.0.mlp.up_proj.weight
25-01-23 08:47:13 | I |       - range scale = [    1.0000]
25-01-23 08:47:13 | I |         sum  error  = [    1.1500]
25-01-23 08:47:13 | I |         best error  = [    1.1500]
25-01-23 08:47:13 | I |     + error = [1.1500]
25-01-23 08:47:14 | I |       - range scale = [    1.0000]
25-01-23 08:47:14 | I |         sum  error  = [   12.2488]
25-01-23 08:47:14 | I |         best error  = [   12.2488]
25-01-23 08:47:14 | I |     + error = [12.2488]
25-01-23 08:47:14 | I |   - Calibrating model.layers.0.mlp.gate_proj.weight
25-01-23 08:47:15 | I |       - range scale = [    1.0000]
25-01-23 08:47:15 | I |         sum  error  = [    1.2205]
25-01-23 08:47:15 | I |         best error  = [    1.2205]
25-01-23 08:47:15 | I |     + error = [1.2205]
25-01-23 08:47:16 | I |       - range scale = [    1.0000]
25-01-23 08:47:16 | I |         sum  error  = [   12.6342]
25-01-23 08:47:16 | I |         best error  = [   12.6342]
25-01-23 08:47:16 | I |     + error = [12.6342]
25-01-23 08:47:16 | I |   - Calibrating model.layers.0.mlp.down_proj.weight
25-01-23 08:47:17 | I |       - range scale = [    1.0000]
25-01-23 08:47:17 | I |         sum  error  = [    0.1859]
25-01-23 08:47:17 | I |         best error  = [    0.1859]
25-01-23 08:47:17 | I |     + error = [0.1859]
25-01-23 08:47:18 | I |       - range scale = [    1.0000]
25-01-23 08:47:18 | I |         sum  error  = [    1.8058]
25-01-23 08:47:18 | I |         best error  = [    1.8058]
25-01-23 08:47:18 | I |     + error = [1.8058]
25-01-23 08:47:18 | I |   - Quantizing model.layers.0.self_attn.q_proj.weight
25-01-23 08:47:21 | I |   - Quantizing model.layers.0.self_attn.k_proj.weight
25-01-23 08:47:23 | I |   - Quantizing model.layers.0.self_attn.v_proj.weight
25-01-23 08:47:25 | I |   - Quantizing model.layers.0.self_attn.o_proj.weight
25-01-23 08:47:27 | I |   - Quantizing model.layers.0.mlp.up_proj.weight
25-01-23 08:47:30 | I |   - Quantizing model.layers.0.mlp.gate_proj.weight
25-01-23 08:47:32 | I |   - Quantizing model.layers.0.mlp.down_proj.weight
25-01-23 08:47:38 | I | quantizing activations for layer model.layers.0
25-01-23 08:47:38 | I | collecting calibration activations in model.layers.0
25-01-23 08:47:38 | I | collecting calibration activations in model.layers.0
25-01-23 08:47:40 | I | forward this layer
25-01-23 08:47:40 | I | input_file: /data/gyy/lmquant-main/lmquant/data/data_without_preprocess_llama_7b_bs16/shard/val_input_args_layer_0/60.pt
25-01-23 08:47:40 | I | output_file: /data/gyy/lmquant-main/lmquant/data/data_without_preprocess_llama_7b_bs16/shard/val_teacher_output_layer_0/60.pt
25-01-23 08:47:41 | I | In layer model.layers.0, gradient of model.layers.0.self_attn.q_proj has nan: False
25-01-23 08:47:41 | I | In layer model.layers.0, gradient of model.layers.0.self_attn.q_proj has inf: False
25-01-23 08:47:41 | I | In layer model.layers.0, gradient of model.layers.0.self_attn.k_proj has nan: False
25-01-23 08:47:41 | I | In layer model.layers.0, gradient of model.layers.0.self_attn.k_proj has inf: False
25-01-23 08:47:41 | I | In layer model.layers.0, gradient of model.layers.0.self_attn.v_proj has nan: False
25-01-23 08:47:41 | I | In layer model.layers.0, gradient of model.layers.0.self_attn.v_proj has inf: False
25-01-23 08:47:41 | I | In layer model.layers.0, gradient of model.layers.0.self_attn.o_proj has nan: False
25-01-23 08:47:41 | I | In layer model.layers.0, gradient of model.layers.0.self_attn.o_proj has inf: False
25-01-23 08:47:41 | I | In layer model.layers.0, gradient of model.layers.0.mlp.up_proj has nan: False
25-01-23 08:47:41 | I | In layer model.layers.0, gradient of model.layers.0.mlp.up_proj has inf: False
25-01-23 08:47:41 | I | In layer model.layers.0, gradient of model.layers.0.mlp.gate_proj has nan: False
25-01-23 08:47:41 | I | In layer model.layers.0, gradient of model.layers.0.mlp.gate_proj has inf: False
25-01-23 08:47:41 | I | In layer model.layers.0, gradient of model.layers.0.mlp.down_proj has nan: False
25-01-23 08:47:41 | I | In layer model.layers.0, gradient of model.layers.0.mlp.down_proj has inf: False
25-01-23 08:47:41 | I | [56] done with optimizer step
25-01-23 08:47:41 | I | epoch 001:     61 / 819200000 loss=1.36806e-05, loss_per_token=0.0560355, loss_sum=459.043, wps=225.6, ups=0.03, wpb=8192, bsz=16, num_updates=57, lr=0.00499988, gnorm=0.184, clip=0, loss_scale=8, train_wall=36, cuda_gb_allocated=17.1, cuda_gb_reserved=18.2, cuda_gb_free=6.6, wall=5008
25-01-23 08:47:41 | I | TRAIN CURRENT LAYER_IDX = 0
25-01-23 08:47:41 | I | in layer model.layers.0
25-01-23 08:47:41 | I | quantizing weights for layer model.layers.0
25-01-23 08:47:41 | I | collecting calibration activations in model.layers.0
25-01-23 08:47:41 | I | collecting calibration activations in model.layers.0
25-01-23 08:47:42 | I | - Quantizing decoder layer model.layers.0
25-01-23 08:47:42 | I |   - Calibrating model.layers.0.self_attn.q_proj.weight
25-01-23 08:47:42 | I |       - range scale = [    1.0000]
25-01-23 08:47:42 | I |         sum  error  = [    0.0477]
25-01-23 08:47:42 | I |         best error  = [    0.0477]
25-01-23 08:47:42 | I |     + error = [0.0477]
25-01-23 08:47:43 | I |       - range scale = [    1.0000]
25-01-23 08:47:43 | I |         sum  error  = [    0.7229]
25-01-23 08:47:43 | I |         best error  = [    0.7229]
25-01-23 08:47:43 | I |     + error = [0.7229]
25-01-23 08:47:43 | I |   - Calibrating model.layers.0.self_attn.k_proj.weight
25-01-23 08:47:44 | I |       - range scale = [    1.0000]
25-01-23 08:47:44 | I |         sum  error  = [    0.0568]
25-01-23 08:47:44 | I |         best error  = [    0.0568]
25-01-23 08:47:44 | I |     + error = [0.0568]
25-01-23 08:47:45 | I |       - range scale = [    1.0000]
25-01-23 08:47:45 | I |         sum  error  = [    0.8687]
25-01-23 08:47:45 | I |         best error  = [    0.8687]
25-01-23 08:47:45 | I |     + error = [0.8687]
25-01-23 08:47:45 | I |   - Calibrating model.layers.0.self_attn.v_proj.weight
25-01-23 08:47:46 | I |       - range scale = [    1.0000]
25-01-23 08:47:46 | I |         sum  error  = [    0.6951]
25-01-23 08:47:46 | I |         best error  = [    0.6951]
25-01-23 08:47:46 | I |     + error = [0.6951]
25-01-23 08:47:46 | I |       - range scale = [    1.0000]
25-01-23 08:47:46 | I |         sum  error  = [    6.5002]
25-01-23 08:47:46 | I |         best error  = [    6.5002]
25-01-23 08:47:46 | I |     + error = [6.5002]
25-01-23 08:47:47 | I |   - Calibrating model.layers.0.self_attn.o_proj.weight
25-01-23 08:47:47 | I |       - range scale = [    1.0000]
25-01-23 08:47:47 | I |         sum  error  = [    0.1989]
25-01-23 08:47:47 | I |         best error  = [    0.1989]
25-01-23 08:47:47 | I |     + error = [0.1989]
25-01-23 08:47:48 | I |       - range scale = [    1.0000]
25-01-23 08:47:48 | I |         sum  error  = [    1.5725]
25-01-23 08:47:48 | I |         best error  = [    1.5725]
25-01-23 08:47:48 | I |     + error = [1.5725]
25-01-23 08:47:48 | I |   - Calibrating model.layers.0.mlp.up_proj.weight
25-01-23 08:47:49 | I |       - range scale = [    1.0000]
25-01-23 08:47:49 | I |         sum  error  = [    1.0796]
25-01-23 08:47:49 | I |         best error  = [    1.0796]
25-01-23 08:47:49 | I |     + error = [1.0796]
25-01-23 08:47:50 | I |       - range scale = [    1.0000]
25-01-23 08:47:50 | I |         sum  error  = [   11.4859]
25-01-23 08:47:50 | I |         best error  = [   11.4859]
25-01-23 08:47:50 | I |     + error = [11.4859]
25-01-23 08:47:51 | I |   - Calibrating model.layers.0.mlp.gate_proj.weight
25-01-23 08:47:51 | I |       - range scale = [    1.0000]
25-01-23 08:47:51 | I |         sum  error  = [    1.1389]
25-01-23 08:47:51 | I |         best error  = [    1.1389]
25-01-23 08:47:51 | I |     + error = [1.1389]
25-01-23 08:47:52 | I |       - range scale = [    1.0000]
25-01-23 08:47:52 | I |         sum  error  = [   11.8510]
25-01-23 08:47:52 | I |         best error  = [   11.8510]
25-01-23 08:47:52 | I |     + error = [11.8510]
25-01-23 08:47:53 | I |   - Calibrating model.layers.0.mlp.down_proj.weight
25-01-23 08:47:53 | I |       - range scale = [    1.0000]
25-01-23 08:47:53 | I |         sum  error  = [    0.1741]
25-01-23 08:47:53 | I |         best error  = [    0.1741]
25-01-23 08:47:53 | I |     + error = [0.1741]
25-01-23 08:47:55 | I |       - range scale = [    1.0000]
25-01-23 08:47:55 | I |         sum  error  = [    1.6935]
25-01-23 08:47:55 | I |         best error  = [    1.6935]
25-01-23 08:47:55 | I |     + error = [1.6935]
25-01-23 08:47:55 | I |   - Quantizing model.layers.0.self_attn.q_proj.weight
25-01-23 08:47:57 | I |   - Quantizing model.layers.0.self_attn.k_proj.weight
25-01-23 08:47:59 | I |   - Quantizing model.layers.0.self_attn.v_proj.weight
25-01-23 08:48:02 | I |   - Quantizing model.layers.0.self_attn.o_proj.weight
25-01-23 08:48:04 | I |   - Quantizing model.layers.0.mlp.up_proj.weight
25-01-23 08:48:06 | I |   - Quantizing model.layers.0.mlp.gate_proj.weight
25-01-23 08:48:09 | I |   - Quantizing model.layers.0.mlp.down_proj.weight
25-01-23 08:48:14 | I | quantizing activations for layer model.layers.0
25-01-23 08:48:15 | I | collecting calibration activations in model.layers.0
25-01-23 08:48:15 | I | collecting calibration activations in model.layers.0
25-01-23 08:48:17 | I | forward this layer
25-01-23 08:48:17 | I | input_file: /data/gyy/lmquant-main/lmquant/data/data_without_preprocess_llama_7b_bs16/shard/val_input_args_layer_0/61.pt
25-01-23 08:48:17 | I | output_file: /data/gyy/lmquant-main/lmquant/data/data_without_preprocess_llama_7b_bs16/shard/val_teacher_output_layer_0/61.pt
25-01-23 08:48:17 | I | In layer model.layers.0, gradient of model.layers.0.self_attn.q_proj has nan: False
25-01-23 08:48:17 | I | In layer model.layers.0, gradient of model.layers.0.self_attn.q_proj has inf: False
25-01-23 08:48:17 | I | In layer model.layers.0, gradient of model.layers.0.self_attn.k_proj has nan: False
25-01-23 08:48:17 | I | In layer model.layers.0, gradient of model.layers.0.self_attn.k_proj has inf: False
25-01-23 08:48:17 | I | In layer model.layers.0, gradient of model.layers.0.self_attn.v_proj has nan: False
25-01-23 08:48:17 | I | In layer model.layers.0, gradient of model.layers.0.self_attn.v_proj has inf: False
25-01-23 08:48:17 | I | In layer model.layers.0, gradient of model.layers.0.self_attn.o_proj has nan: False
25-01-23 08:48:17 | I | In layer model.layers.0, gradient of model.layers.0.self_attn.o_proj has inf: False
25-01-23 08:48:17 | I | In layer model.layers.0, gradient of model.layers.0.mlp.up_proj has nan: False
25-01-23 08:48:17 | I | In layer model.layers.0, gradient of model.layers.0.mlp.up_proj has inf: False
25-01-23 08:48:17 | I | In layer model.layers.0, gradient of model.layers.0.mlp.gate_proj has nan: False
25-01-23 08:48:17 | I | In layer model.layers.0, gradient of model.layers.0.mlp.gate_proj has inf: False
25-01-23 08:48:17 | I | In layer model.layers.0, gradient of model.layers.0.mlp.down_proj has nan: False
25-01-23 08:48:17 | I | In layer model.layers.0, gradient of model.layers.0.mlp.down_proj has inf: False
25-01-23 08:48:17 | I | [57] done with optimizer step
25-01-23 08:48:17 | I | epoch 001:     62 / 819200000 loss=1.49142e-05, loss_per_token=0.0610885, loss_sum=500.437, wps=224.4, ups=0.03, wpb=8192, bsz=16, num_updates=58, lr=0.00499987, gnorm=0.447, clip=0, loss_scale=8, train_wall=36, cuda_gb_allocated=17.1, cuda_gb_reserved=18.7, cuda_gb_free=6.6, wall=5045
25-01-23 08:48:17 | I | TRAIN CURRENT LAYER_IDX = 0
25-01-23 08:48:17 | I | in layer model.layers.0
25-01-23 08:48:17 | I | quantizing weights for layer model.layers.0
25-01-23 08:48:18 | I | collecting calibration activations in model.layers.0
25-01-23 08:48:18 | I | collecting calibration activations in model.layers.0
25-01-23 08:48:18 | I | - Quantizing decoder layer model.layers.0
25-01-23 08:48:18 | I |   - Calibrating model.layers.0.self_attn.q_proj.weight
25-01-23 08:48:19 | I |       - range scale = [    1.0000]
25-01-23 08:48:19 | I |         sum  error  = [    0.0419]
25-01-23 08:48:19 | I |         best error  = [    0.0419]
25-01-23 08:48:19 | I |     + error = [0.0419]
25-01-23 08:48:20 | I |       - range scale = [    1.0000]
25-01-23 08:48:20 | I |         sum  error  = [    0.6438]
25-01-23 08:48:20 | I |         best error  = [    0.6438]
25-01-23 08:48:20 | I |     + error = [0.6438]
25-01-23 08:48:20 | I |   - Calibrating model.layers.0.self_attn.k_proj.weight
25-01-23 08:48:21 | I |       - range scale = [    1.0000]
25-01-23 08:48:21 | I |         sum  error  = [    0.0482]
25-01-23 08:48:21 | I |         best error  = [    0.0482]
25-01-23 08:48:21 | I |     + error = [0.0482]
25-01-23 08:48:21 | I |       - range scale = [    1.0000]
25-01-23 08:48:21 | I |         sum  error  = [    0.8246]
25-01-23 08:48:21 | I |         best error  = [    0.8246]
25-01-23 08:48:21 | I |     + error = [0.8246]
25-01-23 08:48:21 | I |   - Calibrating model.layers.0.self_attn.v_proj.weight
25-01-23 08:48:22 | I |       - range scale = [    1.0000]
25-01-23 08:48:22 | I |         sum  error  = [    0.6596]
25-01-23 08:48:22 | I |         best error  = [    0.6596]
25-01-23 08:48:22 | I |     + error = [0.6596]
25-01-23 08:48:23 | I |       - range scale = [    1.0000]
25-01-23 08:48:23 | I |         sum  error  = [    6.1704]
25-01-23 08:48:23 | I |         best error  = [    6.1704]
25-01-23 08:48:23 | I |     + error = [6.1704]
25-01-23 08:48:23 | I |   - Calibrating model.layers.0.self_attn.o_proj.weight
25-01-23 08:48:24 | I |       - range scale = [    1.0000]
25-01-23 08:48:24 | I |         sum  error  = [    0.1894]
25-01-23 08:48:24 | I |         best error  = [    0.1894]
25-01-23 08:48:24 | I |     + error = [0.1894]
25-01-23 08:48:25 | I |       - range scale = [    1.0000]
25-01-23 08:48:25 | I |         sum  error  = [    1.5116]
25-01-23 08:48:25 | I |         best error  = [    1.5116]
25-01-23 08:48:25 | I |     + error = [1.5116]
25-01-23 08:48:25 | I |   - Calibrating model.layers.0.mlp.up_proj.weight
25-01-23 08:48:26 | I |       - range scale = [    1.0000]
25-01-23 08:48:26 | I |         sum  error  = [    1.0785]
25-01-23 08:48:26 | I |         best error  = [    1.0785]
25-01-23 08:48:26 | I |     + error = [1.0785]
25-01-23 08:48:27 | I |       - range scale = [    1.0000]
25-01-23 08:48:27 | I |         sum  error  = [   11.4904]
25-01-23 08:48:27 | I |         best error  = [   11.4904]
25-01-23 08:48:27 | I |     + error = [11.4904]
25-01-23 08:48:27 | I |   - Calibrating model.layers.0.mlp.gate_proj.weight
25-01-23 08:48:28 | I |       - range scale = [    1.0000]
25-01-23 08:48:28 | I |         sum  error  = [    1.1435]
25-01-23 08:48:28 | I |         best error  = [    1.1435]
25-01-23 08:48:28 | I |     + error = [1.1435]
25-01-23 08:48:29 | I |       - range scale = [    1.0000]
25-01-23 08:48:29 | I |         sum  error  = [   11.8496]
25-01-23 08:48:29 | I |         best error  = [   11.8496]
25-01-23 08:48:29 | I |     + error = [11.8496]
25-01-23 08:48:29 | I |   - Calibrating model.layers.0.mlp.down_proj.weight
25-01-23 08:48:30 | I |       - range scale = [    1.0000]
25-01-23 08:48:30 | I |         sum  error  = [    0.1677]
25-01-23 08:48:30 | I |         best error  = [    0.1677]
25-01-23 08:48:30 | I |     + error = [0.1677]
25-01-23 08:48:31 | I |       - range scale = [    1.0000]
25-01-23 08:48:31 | I |         sum  error  = [    1.6304]
25-01-23 08:48:31 | I |         best error  = [    1.6304]
25-01-23 08:48:31 | I |     + error = [1.6304]
25-01-23 08:48:31 | I |   - Quantizing model.layers.0.self_attn.q_proj.weight
25-01-23 08:48:34 | I |   - Quantizing model.layers.0.self_attn.k_proj.weight
25-01-23 08:48:36 | I |   - Quantizing model.layers.0.self_attn.v_proj.weight
25-01-23 08:48:38 | I |   - Quantizing model.layers.0.self_attn.o_proj.weight
25-01-23 08:48:40 | I |   - Quantizing model.layers.0.mlp.up_proj.weight
25-01-23 08:48:43 | I |   - Quantizing model.layers.0.mlp.gate_proj.weight
25-01-23 08:48:45 | I |   - Quantizing model.layers.0.mlp.down_proj.weight
25-01-23 08:48:51 | I | quantizing activations for layer model.layers.0
25-01-23 08:48:51 | I | collecting calibration activations in model.layers.0
25-01-23 08:48:51 | I | collecting calibration activations in model.layers.0
25-01-23 08:48:53 | I | forward this layer
25-01-23 08:48:53 | I | input_file: /data/gyy/lmquant-main/lmquant/data/data_without_preprocess_llama_7b_bs16/shard/val_input_args_layer_0/62.pt
25-01-23 08:48:53 | I | output_file: /data/gyy/lmquant-main/lmquant/data/data_without_preprocess_llama_7b_bs16/shard/val_teacher_output_layer_0/62.pt
25-01-23 08:48:54 | I | In layer model.layers.0, gradient of model.layers.0.self_attn.q_proj has nan: False
25-01-23 08:48:54 | I | In layer model.layers.0, gradient of model.layers.0.self_attn.q_proj has inf: False
25-01-23 08:48:54 | I | In layer model.layers.0, gradient of model.layers.0.self_attn.k_proj has nan: False
25-01-23 08:48:54 | I | In layer model.layers.0, gradient of model.layers.0.self_attn.k_proj has inf: False
25-01-23 08:48:54 | I | In layer model.layers.0, gradient of model.layers.0.self_attn.v_proj has nan: False
25-01-23 08:48:54 | I | In layer model.layers.0, gradient of model.layers.0.self_attn.v_proj has inf: False
25-01-23 08:48:54 | I | In layer model.layers.0, gradient of model.layers.0.self_attn.o_proj has nan: False
25-01-23 08:48:54 | I | In layer model.layers.0, gradient of model.layers.0.self_attn.o_proj has inf: False
25-01-23 08:48:54 | I | In layer model.layers.0, gradient of model.layers.0.mlp.up_proj has nan: False
25-01-23 08:48:54 | I | In layer model.layers.0, gradient of model.layers.0.mlp.up_proj has inf: False
25-01-23 08:48:54 | I | In layer model.layers.0, gradient of model.layers.0.mlp.gate_proj has nan: False
25-01-23 08:48:54 | I | In layer model.layers.0, gradient of model.layers.0.mlp.gate_proj has inf: False
25-01-23 08:48:54 | I | In layer model.layers.0, gradient of model.layers.0.mlp.down_proj has nan: False
25-01-23 08:48:54 | I | In layer model.layers.0, gradient of model.layers.0.mlp.down_proj has inf: False
25-01-23 08:48:54 | I | [58] done with optimizer step
25-01-23 08:48:54 | I | epoch 001:     63 / 819200000 loss=1.3242e-05, loss_per_token=0.0542391, loss_sum=444.326, wps=225.9, ups=0.03, wpb=8192, bsz=16, num_updates=59, lr=0.00499985, gnorm=0.506, clip=0, loss_scale=8, train_wall=36, cuda_gb_allocated=17.1, cuda_gb_reserved=18.3, cuda_gb_free=6.6, wall=5081
25-01-23 08:48:54 | I | TRAIN CURRENT LAYER_IDX = 0
25-01-23 08:48:54 | I | in layer model.layers.0
25-01-23 08:48:54 | I | quantizing weights for layer model.layers.0
25-01-23 08:48:54 | I | collecting calibration activations in model.layers.0
25-01-23 08:48:54 | I | collecting calibration activations in model.layers.0
25-01-23 08:48:54 | I | - Quantizing decoder layer model.layers.0
25-01-23 08:48:54 | I |   - Calibrating model.layers.0.self_attn.q_proj.weight
25-01-23 08:48:55 | I |       - range scale = [    1.0000]
25-01-23 08:48:55 | I |         sum  error  = [    0.0397]
25-01-23 08:48:55 | I |         best error  = [    0.0397]
25-01-23 08:48:55 | I |     + error = [0.0397]
25-01-23 08:48:56 | I |       - range scale = [    1.0000]
25-01-23 08:48:56 | I |         sum  error  = [    0.6122]
25-01-23 08:48:56 | I |         best error  = [    0.6122]
25-01-23 08:48:56 | I |     + error = [0.6122]
25-01-23 08:48:56 | I |   - Calibrating model.layers.0.self_attn.k_proj.weight
25-01-23 08:48:57 | I |       - range scale = [    1.0000]
25-01-23 08:48:57 | I |         sum  error  = [    0.0463]
25-01-23 08:48:57 | I |         best error  = [    0.0463]
25-01-23 08:48:57 | I |     + error = [0.0463]
25-01-23 08:48:57 | I |       - range scale = [    1.0000]
25-01-23 08:48:57 | I |         sum  error  = [    0.8666]
25-01-23 08:48:57 | I |         best error  = [    0.8666]
25-01-23 08:48:57 | I |     + error = [0.8666]
25-01-23 08:48:58 | I |   - Calibrating model.layers.0.self_attn.v_proj.weight
25-01-23 08:48:58 | I |       - range scale = [    1.0000]
25-01-23 08:48:58 | I |         sum  error  = [    0.6508]
25-01-23 08:48:58 | I |         best error  = [    0.6508]
25-01-23 08:48:58 | I |     + error = [0.6508]
25-01-23 08:48:59 | I |       - range scale = [    1.0000]
25-01-23 08:48:59 | I |         sum  error  = [    6.0978]
25-01-23 08:48:59 | I |         best error  = [    6.0978]
25-01-23 08:48:59 | I |     + error = [6.0978]
25-01-23 08:48:59 | I |   - Calibrating model.layers.0.self_attn.o_proj.weight
25-01-23 08:49:00 | I |       - range scale = [    1.0000]
25-01-23 08:49:00 | I |         sum  error  = [    0.1869]
25-01-23 08:49:00 | I |         best error  = [    0.1869]
25-01-23 08:49:00 | I |     + error = [0.1869]
25-01-23 08:49:01 | I |       - range scale = [    1.0000]
25-01-23 08:49:01 | I |         sum  error  = [    1.4898]
25-01-23 08:49:01 | I |         best error  = [    1.4898]
25-01-23 08:49:01 | I |     + error = [1.4898]
25-01-23 08:49:01 | I |   - Calibrating model.layers.0.mlp.up_proj.weight
25-01-23 08:49:02 | I |       - range scale = [    1.0000]
25-01-23 08:49:02 | I |         sum  error  = [    1.0988]
25-01-23 08:49:02 | I |         best error  = [    1.0988]
25-01-23 08:49:02 | I |     + error = [1.0988]
25-01-23 08:49:03 | I |       - range scale = [    1.0000]
25-01-23 08:49:03 | I |         sum  error  = [   11.6964]
25-01-23 08:49:03 | I |         best error  = [   11.6964]
25-01-23 08:49:03 | I |     + error = [11.6964]
25-01-23 08:49:03 | I |   - Calibrating model.layers.0.mlp.gate_proj.weight
25-01-23 08:49:04 | I |       - range scale = [    1.0000]
25-01-23 08:49:04 | I |         sum  error  = [    1.1668]
25-01-23 08:49:04 | I |         best error  = [    1.1668]
25-01-23 08:49:04 | I |     + error = [1.1668]
25-01-23 08:49:05 | I |       - range scale = [    1.0000]
25-01-23 08:49:05 | I |         sum  error  = [   12.0653]
25-01-23 08:49:05 | I |         best error  = [   12.0653]
25-01-23 08:49:05 | I |     + error = [12.0653]
25-01-23 08:49:06 | I |   - Calibrating model.layers.0.mlp.down_proj.weight
25-01-23 08:49:06 | I |       - range scale = [    1.0000]
25-01-23 08:49:06 | I |         sum  error  = [    0.1703]
25-01-23 08:49:06 | I |         best error  = [    0.1703]
25-01-23 08:49:06 | I |     + error = [0.1703]
25-01-23 08:49:08 | I |       - range scale = [    1.0000]
25-01-23 08:49:08 | I |         sum  error  = [    1.6582]
25-01-23 08:49:08 | I |         best error  = [    1.6582]
25-01-23 08:49:08 | I |     + error = [1.6582]
25-01-23 08:49:08 | I |   - Quantizing model.layers.0.self_attn.q_proj.weight
25-01-23 08:49:10 | I |   - Quantizing model.layers.0.self_attn.k_proj.weight
25-01-23 08:49:12 | I |   - Quantizing model.layers.0.self_attn.v_proj.weight
25-01-23 08:49:15 | I |   - Quantizing model.layers.0.self_attn.o_proj.weight
25-01-23 08:49:17 | I |   - Quantizing model.layers.0.mlp.up_proj.weight
25-01-23 08:49:19 | I |   - Quantizing model.layers.0.mlp.gate_proj.weight
25-01-23 08:49:21 | I |   - Quantizing model.layers.0.mlp.down_proj.weight
25-01-23 08:49:27 | I | quantizing activations for layer model.layers.0
25-01-23 08:49:28 | I | collecting calibration activations in model.layers.0
25-01-23 08:49:28 | I | collecting calibration activations in model.layers.0
25-01-23 08:49:30 | I | forward this layer
25-01-23 08:49:30 | I | input_file: /data/gyy/lmquant-main/lmquant/data/data_without_preprocess_llama_7b_bs16/shard/val_input_args_layer_0/63.pt
25-01-23 08:49:30 | I | output_file: /data/gyy/lmquant-main/lmquant/data/data_without_preprocess_llama_7b_bs16/shard/val_teacher_output_layer_0/63.pt
25-01-23 08:49:30 | I | In layer model.layers.0, gradient of model.layers.0.self_attn.q_proj has nan: False
25-01-23 08:49:30 | I | In layer model.layers.0, gradient of model.layers.0.self_attn.q_proj has inf: False
25-01-23 08:49:30 | I | In layer model.layers.0, gradient of model.layers.0.self_attn.k_proj has nan: False
25-01-23 08:49:30 | I | In layer model.layers.0, gradient of model.layers.0.self_attn.k_proj has inf: False
25-01-23 08:49:30 | I | In layer model.layers.0, gradient of model.layers.0.self_attn.v_proj has nan: False
25-01-23 08:49:30 | I | In layer model.layers.0, gradient of model.layers.0.self_attn.v_proj has inf: False
25-01-23 08:49:30 | I | In layer model.layers.0, gradient of model.layers.0.self_attn.o_proj has nan: False
25-01-23 08:49:30 | I | In layer model.layers.0, gradient of model.layers.0.self_attn.o_proj has inf: False
25-01-23 08:49:30 | I | In layer model.layers.0, gradient of model.layers.0.mlp.up_proj has nan: False
25-01-23 08:49:30 | I | In layer model.layers.0, gradient of model.layers.0.mlp.up_proj has inf: False
25-01-23 08:49:30 | I | In layer model.layers.0, gradient of model.layers.0.mlp.gate_proj has nan: False
25-01-23 08:49:30 | I | In layer model.layers.0, gradient of model.layers.0.mlp.gate_proj has inf: False
25-01-23 08:49:30 | I | In layer model.layers.0, gradient of model.layers.0.mlp.down_proj has nan: False
25-01-23 08:49:30 | I | In layer model.layers.0, gradient of model.layers.0.mlp.down_proj has inf: False
25-01-23 08:49:30 | I | [59] done with optimizer step
25-01-23 08:49:30 | I | epoch 001:     64 / 819200000 loss=1.41172e-05, loss_per_token=0.0578238, loss_sum=473.693, wps=223.7, ups=0.03, wpb=8192, bsz=16, num_updates=60, lr=0.00499983, gnorm=0.505, clip=0, loss_scale=8, train_wall=37, cuda_gb_allocated=17.1, cuda_gb_reserved=18.3, cuda_gb_free=6.6, wall=5118
25-01-23 08:49:30 | I | begin validation on "valid" subset on rank 0
25-01-23 08:49:30 | I | got valid iterator on "valid" subset on rank 0
25-01-23 08:49:30 | I | Valid: Start iterating over samples
25-01-23 08:49:30 | W |   `pretrained` model kwarg is not of type `str`. Many other model arguments may be ignored. Please do not launch via accelerate or use `parallelize=True` if passing an existing model this way.
25-01-23 08:49:30 | W |   Passed an already-initialized model through `pretrained`, assuming single-process call to evaluate() or custom distributed integration
25-01-23 08:49:30 | I | - Evaluator: gptq
25-01-23 08:49:30 | I | - Tasks: ['wikitext', 'val_valid']
25-01-23 08:49:30 | I | - Batch_size: 8
25-01-23 08:49:30 | I |   + Max_seq_length: 2048
25-01-23 08:51:05 | I |     - Results:
25-01-23 08:51:05 | I |       |  Task   |Version|    Metric     |Value |   |Stderr|
25-01-23 08:51:05 | I |       |---------|------:|---------------|-----:|---|-----:|
25-01-23 08:51:05 | I |       |wikitext |      1|word_perplexity|5.4844|±  |5.4844|
25-01-23 08:51:05 | I |       |val_valid|      1|word_perplexity|5.0309|±  |5.0309|
25-01-23 08:51:05 | I |       
25-01-23 08:51:05 | I |   + Max_seq_length: 4096
25-01-23 08:52:38 | I |     - Results:
25-01-23 08:52:38 | I |       |  Task   |Version|    Metric     |Value |   |Stderr|
25-01-23 08:52:38 | I |       |---------|------:|---------------|-----:|---|-----:|
25-01-23 08:52:38 | I |       |wikitext |      1|word_perplexity|5.1259|±  |5.1259|
25-01-23 08:52:38 | I |       |val_valid|      1|word_perplexity|4.8263|±  |4.8263|
25-01-23 08:52:38 | I |       
25-01-23 08:52:38 | I | in valid, quantize current layer weights
25-01-23 08:52:39 | W | Repo card metadata block was not found. Setting CardData to empty.
25-01-23 08:53:24 | I | collecting calibration activations in model.layers.0
25-01-23 08:53:24 | I | collecting calibration activations in model.layers.0
25-01-23 08:53:24 | I | collecting calibration activations in model.layers.0
25-01-23 08:53:24 | I | collecting calibration activations in model.layers.0
25-01-23 08:53:24 | I | collecting calibration activations in model.layers.0
25-01-23 08:53:24 | I | collecting calibration activations in model.layers.0
25-01-23 08:53:24 | I | collecting calibration activations in model.layers.0
25-01-23 08:53:24 | I | collecting calibration activations in model.layers.0
25-01-23 08:53:24 | I | collecting calibration activations in model.layers.0
25-01-23 08:53:24 | I | collecting calibration activations in model.layers.0
25-01-23 08:53:24 | I | collecting calibration activations in model.layers.0
25-01-23 08:53:24 | I | collecting calibration activations in model.layers.0
25-01-23 08:53:24 | I | collecting calibration activations in model.layers.0
25-01-23 08:53:24 | I | collecting calibration activations in model.layers.0
25-01-23 08:53:24 | I | collecting calibration activations in model.layers.0
25-01-23 08:53:24 | I | collecting calibration activations in model.layers.0
25-01-23 08:53:24 | I | collecting calibration activations in model.layers.0
25-01-23 08:53:25 | I | collecting calibration activations in model.layers.0
25-01-23 08:53:25 | I | collecting calibration activations in model.layers.0
25-01-23 08:53:25 | I | collecting calibration activations in model.layers.0
25-01-23 08:53:25 | I | collecting calibration activations in model.layers.0
25-01-23 08:53:25 | I | collecting calibration activations in model.layers.0
25-01-23 08:53:25 | I | collecting calibration activations in model.layers.0
25-01-23 08:53:25 | I | collecting calibration activations in model.layers.0
25-01-23 08:53:25 | I | collecting calibration activations in model.layers.0
25-01-23 08:53:25 | I | collecting calibration activations in model.layers.0
25-01-23 08:53:25 | I | collecting calibration activations in model.layers.0
25-01-23 08:53:25 | I | collecting calibration activations in model.layers.0
25-01-23 08:53:25 | I | collecting calibration activations in model.layers.0
25-01-23 08:53:25 | I | collecting calibration activations in model.layers.0
25-01-23 08:53:25 | I | collecting calibration activations in model.layers.0
25-01-23 08:53:25 | I | collecting calibration activations in model.layers.0
25-01-23 08:53:25 | I | collecting calibration activations in model.layers.0
25-01-23 08:53:25 | I | collecting calibration activations in model.layers.0
25-01-23 08:53:25 | I | collecting calibration activations in model.layers.0
25-01-23 08:53:25 | I | collecting calibration activations in model.layers.0
25-01-23 08:53:26 | I | collecting calibration activations in model.layers.0
25-01-23 08:53:26 | I | collecting calibration activations in model.layers.0
25-01-23 08:53:26 | I | collecting calibration activations in model.layers.0
25-01-23 08:53:26 | I | collecting calibration activations in model.layers.0
25-01-23 08:53:26 | I | collecting calibration activations in model.layers.0
25-01-23 08:53:26 | I | collecting calibration activations in model.layers.0
25-01-23 08:53:26 | I | collecting calibration activations in model.layers.0
25-01-23 08:53:26 | I | collecting calibration activations in model.layers.0
25-01-23 08:53:26 | I | collecting calibration activations in model.layers.0
25-01-23 08:53:26 | I | collecting calibration activations in model.layers.0
25-01-23 08:53:26 | I | collecting calibration activations in model.layers.0
25-01-23 08:53:26 | I | collecting calibration activations in model.layers.0
25-01-23 08:53:26 | I | collecting calibration activations in model.layers.0
25-01-23 08:53:26 | I | collecting calibration activations in model.layers.0
25-01-23 08:53:26 | I | collecting calibration activations in model.layers.0
25-01-23 08:53:26 | I | collecting calibration activations in model.layers.0
25-01-23 08:53:26 | I | collecting calibration activations in model.layers.0
25-01-23 08:53:26 | I | collecting calibration activations in model.layers.0
25-01-23 08:53:26 | I | collecting calibration activations in model.layers.0
25-01-23 08:53:26 | I | collecting calibration activations in model.layers.0
25-01-23 08:53:27 | I | collecting calibration activations in model.layers.0
25-01-23 08:53:27 | I | collecting calibration activations in model.layers.0
25-01-23 08:53:27 | I | collecting calibration activations in model.layers.0
25-01-23 08:53:27 | I | collecting calibration activations in model.layers.0
25-01-23 08:53:27 | I | collecting calibration activations in model.layers.0
25-01-23 08:53:27 | I | collecting calibration activations in model.layers.0
25-01-23 08:53:27 | I | collecting calibration activations in model.layers.0
25-01-23 08:53:27 | I | collecting calibration activations in model.layers.0
25-01-23 08:53:27 | I | collecting calibration activations in model.layers.0
25-01-23 08:53:27 | I | collecting calibration activations in model.layers.0
25-01-23 08:53:27 | I | collecting calibration activations in model.layers.0
25-01-23 08:53:27 | I | collecting calibration activations in model.layers.0
25-01-23 08:53:27 | I | collecting calibration activations in model.layers.0
25-01-23 08:53:27 | I | collecting calibration activations in model.layers.0
25-01-23 08:53:27 | I | collecting calibration activations in model.layers.0
25-01-23 08:53:27 | I | collecting calibration activations in model.layers.0
25-01-23 08:53:27 | I | collecting calibration activations in model.layers.0
25-01-23 08:53:27 | I | collecting calibration activations in model.layers.0
25-01-23 08:53:27 | I | collecting calibration activations in model.layers.0
25-01-23 08:53:28 | I | collecting calibration activations in model.layers.0
25-01-23 08:53:28 | I | collecting calibration activations in model.layers.0
25-01-23 08:53:28 | I | collecting calibration activations in model.layers.0
25-01-23 08:53:28 | I | collecting calibration activations in model.layers.0
25-01-23 08:53:28 | I | collecting calibration activations in model.layers.0
25-01-23 08:53:28 | I | collecting calibration activations in model.layers.0
25-01-23 08:53:28 | I | collecting calibration activations in model.layers.0
25-01-23 08:53:28 | I | collecting calibration activations in model.layers.0
25-01-23 08:53:28 | I | collecting calibration activations in model.layers.0
25-01-23 08:53:28 | I | collecting calibration activations in model.layers.0
25-01-23 08:53:28 | I | collecting calibration activations in model.layers.0
25-01-23 08:53:28 | I | collecting calibration activations in model.layers.0
25-01-23 08:53:28 | I | collecting calibration activations in model.layers.0
25-01-23 08:53:28 | I | collecting calibration activations in model.layers.0
25-01-23 08:53:28 | I | collecting calibration activations in model.layers.0
25-01-23 08:53:28 | I | collecting calibration activations in model.layers.0
25-01-23 08:53:28 | I | collecting calibration activations in model.layers.0
25-01-23 08:53:28 | I | collecting calibration activations in model.layers.0
25-01-23 08:53:28 | I | collecting calibration activations in model.layers.0
25-01-23 08:53:29 | I | collecting calibration activations in model.layers.0
25-01-23 08:53:29 | I | collecting calibration activations in model.layers.0
25-01-23 08:53:29 | I | collecting calibration activations in model.layers.0
25-01-23 08:53:29 | I | collecting calibration activations in model.layers.0
25-01-23 08:53:29 | I | collecting calibration activations in model.layers.0
25-01-23 08:53:29 | I | collecting calibration activations in model.layers.0
25-01-23 08:53:29 | I | collecting calibration activations in model.layers.0
25-01-23 08:53:29 | I | collecting calibration activations in model.layers.0
25-01-23 08:53:29 | I | collecting calibration activations in model.layers.0
25-01-23 08:53:29 | I | collecting calibration activations in model.layers.0
25-01-23 08:53:29 | I | collecting calibration activations in model.layers.0
25-01-23 08:53:29 | I | collecting calibration activations in model.layers.0
25-01-23 08:53:29 | I | collecting calibration activations in model.layers.0
25-01-23 08:53:29 | I | collecting calibration activations in model.layers.0
25-01-23 08:53:29 | I | collecting calibration activations in model.layers.0
25-01-23 08:53:29 | I | collecting calibration activations in model.layers.0
25-01-23 08:53:29 | I | collecting calibration activations in model.layers.0
25-01-23 08:53:29 | I | collecting calibration activations in model.layers.0
25-01-23 08:53:29 | I | collecting calibration activations in model.layers.0
25-01-23 08:53:30 | I | collecting calibration activations in model.layers.0
25-01-23 08:53:30 | I | collecting calibration activations in model.layers.0
25-01-23 08:53:30 | I | collecting calibration activations in model.layers.0
25-01-23 08:53:30 | I | collecting calibration activations in model.layers.0
25-01-23 08:53:30 | I | collecting calibration activations in model.layers.0
25-01-23 08:53:30 | I | collecting calibration activations in model.layers.0
25-01-23 08:53:30 | I | collecting calibration activations in model.layers.0
25-01-23 08:53:30 | I | collecting calibration activations in model.layers.0
25-01-23 08:53:30 | I | collecting calibration activations in model.layers.0
25-01-23 08:53:30 | I | collecting calibration activations in model.layers.0
25-01-23 08:53:30 | I | collecting calibration activations in model.layers.0
25-01-23 08:53:30 | I | collecting calibration activations in model.layers.0
25-01-23 08:53:30 | I | collecting calibration activations in model.layers.0
25-01-23 08:53:30 | I | collecting calibration activations in model.layers.0
25-01-23 08:53:30 | I | collecting calibration activations in model.layers.0
25-01-23 08:53:32 | I |       - range scale = [    1.0000]
25-01-23 08:53:32 | I |         sum  error  = [    0.1566]
25-01-23 08:53:32 | I |         best error  = [    0.1566]
25-01-23 08:53:32 | I |     + error = [0.1566]
25-01-23 08:53:33 | I |       - range scale = [    1.0000]
25-01-23 08:53:33 | I |         sum  error  = [    2.7089]
25-01-23 08:53:33 | I |         best error  = [    2.7089]
25-01-23 08:53:33 | I |     + error = [2.7089]
25-01-23 08:53:35 | I |       - range scale = [    1.0000]
25-01-23 08:53:35 | I |         sum  error  = [    0.1905]
25-01-23 08:53:35 | I |         best error  = [    0.1905]
25-01-23 08:53:35 | I |     + error = [0.1905]
25-01-23 08:53:37 | I |       - range scale = [    1.0000]
25-01-23 08:53:37 | I |         sum  error  = [    3.7453]
25-01-23 08:53:37 | I |         best error  = [    3.7453]
25-01-23 08:53:37 | I |     + error = [3.7453]
25-01-23 08:53:38 | I |       - range scale = [    1.0000]
25-01-23 08:53:38 | I |         sum  error  = [    0.6435]
25-01-23 08:53:38 | I |         best error  = [    0.6435]
25-01-23 08:53:38 | I |     + error = [0.6435]
25-01-23 08:53:38 | I |       - range scale = [    1.0000]
25-01-23 08:53:38 | I |         sum  error  = [    5.9353]
25-01-23 08:53:38 | I |         best error  = [    5.9353]
25-01-23 08:53:38 | I |     + error = [5.9353]
25-01-23 08:53:39 | I |       - range scale = [    1.0000]
25-01-23 08:53:39 | I |         sum  error  = [    0.1473]
25-01-23 08:53:39 | I |         best error  = [    0.1473]
25-01-23 08:53:39 | I |     + error = [0.1473]
25-01-23 08:53:40 | I |       - range scale = [    1.0000]
25-01-23 08:53:40 | I |         sum  error  = [    1.2098]
25-01-23 08:53:40 | I |         best error  = [    1.2098]
25-01-23 08:53:40 | I |     + error = [1.2098]
25-01-23 08:53:41 | I |       - range scale = [    1.0000]
25-01-23 08:53:41 | I |         sum  error  = [    1.0701]
25-01-23 08:53:41 | I |         best error  = [    1.0701]
25-01-23 08:53:41 | I |     + error = [1.0701]
25-01-23 08:53:42 | I |       - range scale = [    1.0000]
25-01-23 08:53:42 | I |         sum  error  = [   11.4016]
25-01-23 08:53:42 | I |         best error  = [   11.4016]
25-01-23 08:53:42 | I |     + error = [11.4016]
25-01-23 08:53:43 | I |       - range scale = [    1.0000]
25-01-23 08:53:43 | I |         sum  error  = [    1.1351]
25-01-23 08:53:43 | I |         best error  = [    1.1351]
25-01-23 08:53:43 | I |     + error = [1.1351]
25-01-23 08:53:44 | I |       - range scale = [    1.0000]
25-01-23 08:53:44 | I |         sum  error  = [   11.7642]
25-01-23 08:53:44 | I |         best error  = [   11.7642]
25-01-23 08:53:44 | I |     + error = [11.7642]
25-01-23 08:53:45 | I |       - range scale = [    1.0000]
25-01-23 08:53:45 | I |         sum  error  = [    0.1813]
25-01-23 08:53:45 | I |         best error  = [    0.1813]
25-01-23 08:53:45 | I |     + error = [0.1813]
25-01-23 08:53:46 | I |       - range scale = [    1.0000]
25-01-23 08:53:46 | I |         sum  error  = [    1.7903]
25-01-23 08:53:46 | I |         best error  = [    1.7903]
25-01-23 08:53:46 | I |     + error = [1.7903]
25-01-23 08:54:11 | I | in valid, quantize current layer acts
25-01-23 08:54:12 | I | collecting calibration activations in model.layers.0
25-01-23 08:54:12 | I | collecting calibration activations in model.layers.0
25-01-23 08:54:12 | I | collecting calibration activations in model.layers.0
25-01-23 08:54:12 | I | collecting calibration activations in model.layers.0
25-01-23 08:54:12 | I | collecting calibration activations in model.layers.0
25-01-23 08:54:13 | I | collecting calibration activations in model.layers.0
25-01-23 08:54:13 | I | collecting calibration activations in model.layers.0
25-01-23 08:54:13 | I | collecting calibration activations in model.layers.0
25-01-23 08:54:13 | I | collecting calibration activations in model.layers.0
25-01-23 08:54:13 | I | collecting calibration activations in model.layers.0
25-01-23 08:54:13 | I | collecting calibration activations in model.layers.0
25-01-23 08:54:13 | I | collecting calibration activations in model.layers.0
25-01-23 08:54:13 | I | collecting calibration activations in model.layers.0
25-01-23 08:54:13 | I | collecting calibration activations in model.layers.0
25-01-23 08:54:13 | I | collecting calibration activations in model.layers.0
25-01-23 08:54:13 | I | collecting calibration activations in model.layers.0
25-01-23 08:54:13 | I | collecting calibration activations in model.layers.0
25-01-23 08:54:13 | I | collecting calibration activations in model.layers.0
25-01-23 08:54:13 | I | collecting calibration activations in model.layers.0
25-01-23 08:54:13 | I | collecting calibration activations in model.layers.0
25-01-23 08:54:14 | I | collecting calibration activations in model.layers.0
25-01-23 08:54:14 | I | collecting calibration activations in model.layers.0
25-01-23 08:54:14 | I | collecting calibration activations in model.layers.0
25-01-23 08:54:14 | I | collecting calibration activations in model.layers.0
25-01-23 08:54:14 | I | collecting calibration activations in model.layers.0
25-01-23 08:54:14 | I | collecting calibration activations in model.layers.0
25-01-23 08:54:14 | I | collecting calibration activations in model.layers.0
25-01-23 08:54:14 | I | collecting calibration activations in model.layers.0
25-01-23 08:54:14 | I | collecting calibration activations in model.layers.0
25-01-23 08:54:14 | I | collecting calibration activations in model.layers.0
25-01-23 08:54:14 | I | collecting calibration activations in model.layers.0
25-01-23 08:54:14 | I | collecting calibration activations in model.layers.0
25-01-23 08:54:14 | I | collecting calibration activations in model.layers.0
25-01-23 08:54:14 | I | collecting calibration activations in model.layers.0
25-01-23 08:54:14 | I | collecting calibration activations in model.layers.0
25-01-23 08:54:14 | I | collecting calibration activations in model.layers.0
25-01-23 08:54:15 | I | collecting calibration activations in model.layers.0
25-01-23 08:54:15 | I | collecting calibration activations in model.layers.0
25-01-23 08:54:15 | I | collecting calibration activations in model.layers.0
25-01-23 08:54:15 | I | collecting calibration activations in model.layers.0
25-01-23 08:54:15 | I | collecting calibration activations in model.layers.0
25-01-23 08:54:15 | I | collecting calibration activations in model.layers.0
25-01-23 08:54:15 | I | collecting calibration activations in model.layers.0
25-01-23 08:54:15 | I | collecting calibration activations in model.layers.0
25-01-23 08:54:15 | I | collecting calibration activations in model.layers.0
25-01-23 08:54:15 | I | collecting calibration activations in model.layers.0
25-01-23 08:54:15 | I | collecting calibration activations in model.layers.0
25-01-23 08:54:15 | I | collecting calibration activations in model.layers.0
25-01-23 08:54:15 | I | collecting calibration activations in model.layers.0
25-01-23 08:54:15 | I | collecting calibration activations in model.layers.0
25-01-23 08:54:15 | I | collecting calibration activations in model.layers.0
25-01-23 08:54:15 | I | collecting calibration activations in model.layers.0
25-01-23 08:54:16 | I | collecting calibration activations in model.layers.0
25-01-23 08:54:16 | I | collecting calibration activations in model.layers.0
25-01-23 08:54:16 | I | collecting calibration activations in model.layers.0
25-01-23 08:54:16 | I | collecting calibration activations in model.layers.0
25-01-23 08:54:16 | I | collecting calibration activations in model.layers.0
25-01-23 08:54:16 | I | collecting calibration activations in model.layers.0
25-01-23 08:54:16 | I | collecting calibration activations in model.layers.0
25-01-23 08:54:16 | I | collecting calibration activations in model.layers.0
25-01-23 08:54:16 | I | collecting calibration activations in model.layers.0
25-01-23 08:54:16 | I | collecting calibration activations in model.layers.0
25-01-23 08:54:16 | I | collecting calibration activations in model.layers.0
25-01-23 08:54:16 | I | collecting calibration activations in model.layers.0
25-01-23 08:54:16 | I | collecting calibration activations in model.layers.0
25-01-23 08:54:16 | I | collecting calibration activations in model.layers.0
25-01-23 08:54:16 | I | collecting calibration activations in model.layers.0
25-01-23 08:54:17 | I | collecting calibration activations in model.layers.0
25-01-23 08:54:17 | I | collecting calibration activations in model.layers.0
25-01-23 08:54:17 | I | collecting calibration activations in model.layers.0
25-01-23 08:54:17 | I | collecting calibration activations in model.layers.0
25-01-23 08:54:17 | I | collecting calibration activations in model.layers.0
25-01-23 08:54:17 | I | collecting calibration activations in model.layers.0
25-01-23 08:54:17 | I | collecting calibration activations in model.layers.0
25-01-23 08:54:17 | I | collecting calibration activations in model.layers.0
25-01-23 08:54:17 | I | collecting calibration activations in model.layers.0
25-01-23 08:54:17 | I | collecting calibration activations in model.layers.0
25-01-23 08:54:17 | I | collecting calibration activations in model.layers.0
25-01-23 08:54:17 | I | collecting calibration activations in model.layers.0
25-01-23 08:54:17 | I | collecting calibration activations in model.layers.0
25-01-23 08:54:17 | I | collecting calibration activations in model.layers.0
25-01-23 08:54:17 | I | collecting calibration activations in model.layers.0
25-01-23 08:54:17 | I | collecting calibration activations in model.layers.0
25-01-23 08:54:18 | I | collecting calibration activations in model.layers.0
25-01-23 08:54:18 | I | collecting calibration activations in model.layers.0
25-01-23 08:54:18 | I | collecting calibration activations in model.layers.0
25-01-23 08:54:18 | I | collecting calibration activations in model.layers.0
25-01-23 08:54:18 | I | collecting calibration activations in model.layers.0
25-01-23 08:54:18 | I | collecting calibration activations in model.layers.0
25-01-23 08:54:18 | I | collecting calibration activations in model.layers.0
25-01-23 08:54:18 | I | collecting calibration activations in model.layers.0
25-01-23 08:54:18 | I | collecting calibration activations in model.layers.0
25-01-23 08:54:18 | I | collecting calibration activations in model.layers.0
25-01-23 08:54:18 | I | collecting calibration activations in model.layers.0
25-01-23 08:54:18 | I | collecting calibration activations in model.layers.0
25-01-23 08:54:18 | I | collecting calibration activations in model.layers.0
25-01-23 08:54:18 | I | collecting calibration activations in model.layers.0
25-01-23 08:54:18 | I | collecting calibration activations in model.layers.0
25-01-23 08:54:19 | I | collecting calibration activations in model.layers.0
25-01-23 08:54:19 | I | collecting calibration activations in model.layers.0
25-01-23 08:54:19 | I | collecting calibration activations in model.layers.0
25-01-23 08:54:19 | I | collecting calibration activations in model.layers.0
25-01-23 08:54:19 | I | collecting calibration activations in model.layers.0
25-01-23 08:54:19 | I | collecting calibration activations in model.layers.0
25-01-23 08:54:19 | I | collecting calibration activations in model.layers.0
25-01-23 08:54:19 | I | collecting calibration activations in model.layers.0
25-01-23 08:54:19 | I | collecting calibration activations in model.layers.0
25-01-23 08:54:19 | I | collecting calibration activations in model.layers.0
25-01-23 08:54:19 | I | collecting calibration activations in model.layers.0
25-01-23 08:54:19 | I | collecting calibration activations in model.layers.0
25-01-23 08:54:19 | I | collecting calibration activations in model.layers.0
25-01-23 08:54:19 | I | collecting calibration activations in model.layers.0
25-01-23 08:54:19 | I | collecting calibration activations in model.layers.0
25-01-23 08:54:20 | I | collecting calibration activations in model.layers.0
25-01-23 08:54:20 | I | collecting calibration activations in model.layers.0
25-01-23 08:54:20 | I | collecting calibration activations in model.layers.0
25-01-23 08:54:20 | I | collecting calibration activations in model.layers.0
25-01-23 08:54:20 | I | collecting calibration activations in model.layers.0
25-01-23 08:54:20 | I | collecting calibration activations in model.layers.0
25-01-23 08:54:20 | I | collecting calibration activations in model.layers.0
25-01-23 08:54:20 | I | collecting calibration activations in model.layers.0
25-01-23 08:54:20 | I | collecting calibration activations in model.layers.0
25-01-23 08:54:20 | I | collecting calibration activations in model.layers.0
25-01-23 08:54:20 | I | collecting calibration activations in model.layers.0
25-01-23 08:54:20 | I | collecting calibration activations in model.layers.0
25-01-23 08:54:20 | I | collecting calibration activations in model.layers.0
25-01-23 08:54:20 | I | collecting calibration activations in model.layers.0
25-01-23 08:54:20 | I | collecting calibration activations in model.layers.0
25-01-23 08:54:23 | I | use gptq_eval(lmquant) to calculate ppl, partly quanted
25-01-23 08:54:23 | W |   `pretrained` model kwarg is not of type `str`. Many other model arguments may be ignored. Please do not launch via accelerate or use `parallelize=True` if passing an existing model this way.
25-01-23 08:54:23 | W |   Passed an already-initialized model through `pretrained`, assuming single-process call to evaluate() or custom distributed integration
25-01-23 08:54:23 | I | - Evaluator: gptq
25-01-23 08:54:23 | I | - Tasks: ['wikitext', 'val_valid']
25-01-23 08:54:23 | I | - Batch_size: 8
25-01-23 08:54:23 | I |   + Max_seq_length: 2048
25-01-23 08:56:00 | I |     - Results:
25-01-23 08:56:00 | I |       |  Task   |Version|    Metric     |Value |   |Stderr|
25-01-23 08:56:00 | I |       |---------|------:|---------------|-----:|---|-----:|
25-01-23 08:56:00 | I |       |wikitext |      1|word_perplexity|5.4918|±  |5.4918|
25-01-23 08:56:00 | I |       |val_valid|      1|word_perplexity|5.0602|±  |5.0602|
25-01-23 08:56:00 | I |       
25-01-23 08:56:00 | I |   + Max_seq_length: 4096
25-01-23 08:57:35 | I |     - Results:
25-01-23 08:57:36 | I |       |  Task   |Version|    Metric     |Value |   |Stderr|
25-01-23 08:57:36 | I |       |---------|------:|---------------|-----:|---|-----:|
25-01-23 08:57:36 | I |       |wikitext |      1|word_perplexity|5.1312|±  |5.1312|
25-01-23 08:57:36 | I |       |val_valid|      1|word_perplexity|4.8536|±  |4.8536|
25-01-23 08:57:36 | I |       
25-01-23 08:58:35 | I | epoch 001 | valid on 'valid' subset:    102 / 819200000 loss_valid=2.15, lmquant_ppl_result_val=-1, lmquant_ppl_result_wikitext=-1, ppl=4.44, wps=7233.1, wpb=4058.2, bsz=2, num_updates=60, lmquant_ppl_wikitext_all_quanted=5.48439, lmquant_ppl_val_all_quanted=5.03088, lmquant_ppl_wikitext_partly_quanted=5.49176, lmquant_ppl_val_partly_quanted=5.06017
25-01-23 08:58:35 | I | epoch 001 | valid on 'valid' subset | loss_valid 2.15 | lmquant_ppl_result_val -1 | lmquant_ppl_result_wikitext -1 | ppl 4.44 | wps 7233.1 | wpb 4058.2 | bsz 2 | num_updates 60 | lmquant_ppl_wikitext_all_quanted 5.48439 | lmquant_ppl_val_all_quanted 5.03088 | lmquant_ppl_wikitext_partly_quanted 5.49176 | lmquant_ppl_val_partly_quanted 5.06017
25-01-23 08:58:35 | I | TRAIN CURRENT LAYER_IDX = 0
25-01-23 08:58:35 | I | in layer model.layers.0
25-01-23 08:58:35 | I | quantizing weights for layer model.layers.0
25-01-23 08:58:35 | I | collecting calibration activations in model.layers.0
25-01-23 08:58:35 | I | collecting calibration activations in model.layers.0
25-01-23 08:58:36 | I | - Quantizing decoder layer model.layers.0
25-01-23 08:58:36 | I |   - Calibrating model.layers.0.self_attn.q_proj.weight
25-01-23 08:58:36 | I |       - range scale = [    1.0000]
25-01-23 08:58:36 | I |         sum  error  = [    0.0429]
25-01-23 08:58:36 | I |         best error  = [    0.0429]
25-01-23 08:58:36 | I |     + error = [0.0429]
25-01-23 08:58:37 | I |       - range scale = [    1.0000]
25-01-23 08:58:37 | I |         sum  error  = [    0.6399]
25-01-23 08:58:37 | I |         best error  = [    0.6399]
25-01-23 08:58:37 | I |     + error = [0.6399]
25-01-23 08:58:37 | I |   - Calibrating model.layers.0.self_attn.k_proj.weight
25-01-23 08:58:38 | I |       - range scale = [    1.0000]
25-01-23 08:58:38 | I |         sum  error  = [    0.0516]
25-01-23 08:58:38 | I |         best error  = [    0.0516]
25-01-23 08:58:38 | I |     + error = [0.0516]
25-01-23 08:58:39 | I |       - range scale = [    1.0000]
25-01-23 08:58:39 | I |         sum  error  = [    0.8802]
25-01-23 08:58:39 | I |         best error  = [    0.8802]
25-01-23 08:58:39 | I |     + error = [0.8802]
25-01-23 08:58:39 | I |   - Calibrating model.layers.0.self_attn.v_proj.weight
25-01-23 08:58:40 | I |       - range scale = [    1.0000]
25-01-23 08:58:40 | I |         sum  error  = [    0.6857]
25-01-23 08:58:40 | I |         best error  = [    0.6857]
25-01-23 08:58:40 | I |     + error = [0.6857]
25-01-23 08:58:40 | I |       - range scale = [    1.0000]
25-01-23 08:58:40 | I |         sum  error  = [    6.4344]
25-01-23 08:58:40 | I |         best error  = [    6.4344]
25-01-23 08:58:40 | I |     + error = [6.4344]
25-01-23 08:58:41 | I |   - Calibrating model.layers.0.self_attn.o_proj.weight
25-01-23 08:58:41 | I |       - range scale = [    1.0000]
25-01-23 08:58:41 | I |         sum  error  = [    0.1946]
25-01-23 08:58:41 | I |         best error  = [    0.1946]
25-01-23 08:58:41 | I |     + error = [0.1946]
25-01-23 08:58:42 | I |       - range scale = [    1.0000]
25-01-23 08:58:42 | I |         sum  error  = [    1.5476]
25-01-23 08:58:42 | I |         best error  = [    1.5476]
25-01-23 08:58:42 | I |     + error = [1.5476]
25-01-23 08:58:42 | I |   - Calibrating model.layers.0.mlp.up_proj.weight
25-01-23 08:58:43 | I |       - range scale = [    1.0000]
25-01-23 08:58:43 | I |         sum  error  = [    1.0648]
25-01-23 08:58:43 | I |         best error  = [    1.0648]
25-01-23 08:58:43 | I |     + error = [1.0648]
25-01-23 08:58:44 | I |       - range scale = [    1.0000]
25-01-23 08:58:44 | I |         sum  error  = [   11.3124]
25-01-23 08:58:44 | I |         best error  = [   11.3124]
25-01-23 08:58:44 | I |     + error = [11.3124]
25-01-23 08:58:44 | I |   - Calibrating model.layers.0.mlp.gate_proj.weight
25-01-23 08:58:45 | I |       - range scale = [    1.0000]
25-01-23 08:58:45 | I |         sum  error  = [    1.1245]
25-01-23 08:58:45 | I |         best error  = [    1.1245]
25-01-23 08:58:45 | I |     + error = [1.1245]
25-01-23 08:58:46 | I |       - range scale = [    1.0000]
25-01-23 08:58:46 | I |         sum  error  = [   11.6726]
25-01-23 08:58:46 | I |         best error  = [   11.6726]
25-01-23 08:58:46 | I |     + error = [11.6726]
25-01-23 08:58:46 | I |   - Calibrating model.layers.0.mlp.down_proj.weight
25-01-23 08:58:47 | I |       - range scale = [    1.0000]
25-01-23 08:58:47 | I |         sum  error  = [    0.1582]
25-01-23 08:58:47 | I |         best error  = [    0.1582]
25-01-23 08:58:47 | I |     + error = [0.1582]
25-01-23 08:58:48 | I |       - range scale = [    1.0000]
25-01-23 08:58:48 | I |         sum  error  = [    1.5389]
25-01-23 08:58:48 | I |         best error  = [    1.5389]
25-01-23 08:58:48 | I |     + error = [1.5389]
25-01-23 08:58:48 | I |   - Quantizing model.layers.0.self_attn.q_proj.weight
25-01-23 08:58:51 | I |   - Quantizing model.layers.0.self_attn.k_proj.weight
25-01-23 08:58:53 | I |   - Quantizing model.layers.0.self_attn.v_proj.weight
25-01-23 08:58:55 | I |   - Quantizing model.layers.0.self_attn.o_proj.weight
25-01-23 08:58:57 | I |   - Quantizing model.layers.0.mlp.up_proj.weight
25-01-23 08:59:00 | I |   - Quantizing model.layers.0.mlp.gate_proj.weight
25-01-23 08:59:02 | I |   - Quantizing model.layers.0.mlp.down_proj.weight
25-01-23 08:59:08 | I | quantizing activations for layer model.layers.0
25-01-23 08:59:08 | I | collecting calibration activations in model.layers.0
25-01-23 08:59:08 | I | collecting calibration activations in model.layers.0
25-01-23 08:59:10 | I | forward this layer
25-01-23 08:59:10 | I | input_file: /data/gyy/lmquant-main/lmquant/data/data_without_preprocess_llama_7b_bs16/shard/val_input_args_layer_0/64.pt
25-01-23 08:59:10 | I | output_file: /data/gyy/lmquant-main/lmquant/data/data_without_preprocess_llama_7b_bs16/shard/val_teacher_output_layer_0/64.pt
25-01-23 08:59:11 | I | In layer model.layers.0, gradient of model.layers.0.self_attn.q_proj has nan: False
25-01-23 08:59:11 | I | In layer model.layers.0, gradient of model.layers.0.self_attn.q_proj has inf: False
25-01-23 08:59:11 | I | In layer model.layers.0, gradient of model.layers.0.self_attn.k_proj has nan: False
25-01-23 08:59:11 | I | In layer model.layers.0, gradient of model.layers.0.self_attn.k_proj has inf: False
25-01-23 08:59:11 | I | In layer model.layers.0, gradient of model.layers.0.self_attn.v_proj has nan: False
25-01-23 08:59:11 | I | In layer model.layers.0, gradient of model.layers.0.self_attn.v_proj has inf: False
25-01-23 08:59:11 | I | In layer model.layers.0, gradient of model.layers.0.self_attn.o_proj has nan: False
25-01-23 08:59:11 | I | In layer model.layers.0, gradient of model.layers.0.self_attn.o_proj has inf: False
25-01-23 08:59:11 | I | In layer model.layers.0, gradient of model.layers.0.mlp.up_proj has nan: False
25-01-23 08:59:11 | I | In layer model.layers.0, gradient of model.layers.0.mlp.up_proj has inf: False
25-01-23 08:59:11 | I | In layer model.layers.0, gradient of model.layers.0.mlp.gate_proj has nan: False
25-01-23 08:59:11 | I | In layer model.layers.0, gradient of model.layers.0.mlp.gate_proj has inf: False
25-01-23 08:59:11 | I | In layer model.layers.0, gradient of model.layers.0.mlp.down_proj has nan: False
25-01-23 08:59:11 | I | In layer model.layers.0, gradient of model.layers.0.mlp.down_proj has inf: False
25-01-23 08:59:11 | I | [60] done with optimizer step
25-01-23 08:59:11 | I | epoch 001:     65 / 819200000 loss=1.29737e-05, loss_per_token=0.0531405, loss_sum=435.327, wps=14.1, ups=0, wpb=8192, bsz=16, num_updates=61, lr=0.00499982, gnorm=0.155, clip=0, loss_scale=8, train_wall=36, cuda_gb_allocated=17.2, cuda_gb_reserved=20.5, cuda_gb_free=6.5, wall=5698
25-01-23 08:59:11 | I | TRAIN CURRENT LAYER_IDX = 0
25-01-23 08:59:11 | I | in layer model.layers.0
25-01-23 08:59:11 | I | quantizing weights for layer model.layers.0
25-01-23 08:59:11 | I | collecting calibration activations in model.layers.0
25-01-23 08:59:11 | I | collecting calibration activations in model.layers.0
25-01-23 08:59:11 | I | - Quantizing decoder layer model.layers.0
25-01-23 08:59:11 | I |   - Calibrating model.layers.0.self_attn.q_proj.weight
25-01-23 08:59:12 | I |       - range scale = [    1.0000]
25-01-23 08:59:12 | I |         sum  error  = [    0.0467]
25-01-23 08:59:12 | I |         best error  = [    0.0467]
25-01-23 08:59:12 | I |     + error = [0.0467]
25-01-23 08:59:13 | I |       - range scale = [    1.0000]
25-01-23 08:59:13 | I |         sum  error  = [    0.7170]
25-01-23 08:59:13 | I |         best error  = [    0.7170]
25-01-23 08:59:13 | I |     + error = [0.7170]
25-01-23 08:59:13 | I |   - Calibrating model.layers.0.self_attn.k_proj.weight
25-01-23 08:59:14 | I |       - range scale = [    1.0000]
25-01-23 08:59:14 | I |         sum  error  = [    0.0601]
25-01-23 08:59:14 | I |         best error  = [    0.0601]
25-01-23 08:59:14 | I |     + error = [0.0601]
25-01-23 08:59:14 | I |       - range scale = [    1.0000]
25-01-23 08:59:14 | I |         sum  error  = [    0.9706]
25-01-23 08:59:14 | I |         best error  = [    0.9706]
25-01-23 08:59:14 | I |     + error = [0.9706]
25-01-23 08:59:15 | I |   - Calibrating model.layers.0.self_attn.v_proj.weight
25-01-23 08:59:15 | I |       - range scale = [    1.0000]
25-01-23 08:59:15 | I |         sum  error  = [    0.6803]
25-01-23 08:59:15 | I |         best error  = [    0.6803]
25-01-23 08:59:15 | I |     + error = [0.6803]
25-01-23 08:59:16 | I |       - range scale = [    1.0000]
25-01-23 08:59:16 | I |         sum  error  = [    6.3176]
25-01-23 08:59:16 | I |         best error  = [    6.3176]
25-01-23 08:59:16 | I |     + error = [6.3176]
25-01-23 08:59:16 | I |   - Calibrating model.layers.0.self_attn.o_proj.weight
25-01-23 08:59:17 | I |       - range scale = [    1.0000]
25-01-23 08:59:17 | I |         sum  error  = [    0.1923]
25-01-23 08:59:17 | I |         best error  = [    0.1923]
25-01-23 08:59:17 | I |     + error = [0.1923]
25-01-23 08:59:18 | I |       - range scale = [    1.0000]
25-01-23 08:59:18 | I |         sum  error  = [    1.5345]
25-01-23 08:59:18 | I |         best error  = [    1.5345]
25-01-23 08:59:18 | I |     + error = [1.5345]
25-01-23 08:59:18 | I |   - Calibrating model.layers.0.mlp.up_proj.weight
25-01-23 08:59:19 | I |       - range scale = [    1.0000]
25-01-23 08:59:19 | I |         sum  error  = [    1.0455]
25-01-23 08:59:19 | I |         best error  = [    1.0455]
25-01-23 08:59:19 | I |     + error = [1.0455]
25-01-23 08:59:20 | I |       - range scale = [    1.0000]
25-01-23 08:59:20 | I |         sum  error  = [   11.1148]
25-01-23 08:59:20 | I |         best error  = [   11.1148]
25-01-23 08:59:20 | I |     + error = [11.1148]
25-01-23 08:59:20 | I |   - Calibrating model.layers.0.mlp.gate_proj.weight
25-01-23 08:59:21 | I |       - range scale = [    1.0000]
25-01-23 08:59:21 | I |         sum  error  = [    1.1050]
25-01-23 08:59:21 | I |         best error  = [    1.1050]
25-01-23 08:59:21 | I |     + error = [1.1050]
25-01-23 08:59:22 | I |       - range scale = [    1.0000]
25-01-23 08:59:22 | I |         sum  error  = [   11.4928]
25-01-23 08:59:22 | I |         best error  = [   11.4928]
25-01-23 08:59:22 | I |     + error = [11.4928]
25-01-23 08:59:22 | I |   - Calibrating model.layers.0.mlp.down_proj.weight
25-01-23 08:59:23 | I |       - range scale = [    1.0000]
25-01-23 08:59:23 | I |         sum  error  = [    0.1521]
25-01-23 08:59:23 | I |         best error  = [    0.1521]
25-01-23 08:59:23 | I |     + error = [0.1521]
25-01-23 08:59:24 | I |       - range scale = [    1.0000]
25-01-23 08:59:24 | I |         sum  error  = [    1.4846]
25-01-23 08:59:24 | I |         best error  = [    1.4846]
25-01-23 08:59:24 | I |     + error = [1.4846]
25-01-23 08:59:24 | I |   - Quantizing model.layers.0.self_attn.q_proj.weight
25-01-23 08:59:27 | I |   - Quantizing model.layers.0.self_attn.k_proj.weight
25-01-23 08:59:29 | I |   - Quantizing model.layers.0.self_attn.v_proj.weight
25-01-23 08:59:31 | I |   - Quantizing model.layers.0.self_attn.o_proj.weight
25-01-23 08:59:33 | I |   - Quantizing model.layers.0.mlp.up_proj.weight
25-01-23 08:59:36 | I |   - Quantizing model.layers.0.mlp.gate_proj.weight
25-01-23 08:59:38 | I |   - Quantizing model.layers.0.mlp.down_proj.weight
25-01-23 08:59:44 | I | quantizing activations for layer model.layers.0
25-01-23 08:59:44 | I | collecting calibration activations in model.layers.0
25-01-23 08:59:44 | I | collecting calibration activations in model.layers.0
25-01-23 08:59:46 | I | forward this layer
25-01-23 08:59:46 | I | input_file: /data/gyy/lmquant-main/lmquant/data/data_without_preprocess_llama_7b_bs16/shard/val_input_args_layer_0/65.pt
25-01-23 08:59:46 | I | output_file: /data/gyy/lmquant-main/lmquant/data/data_without_preprocess_llama_7b_bs16/shard/val_teacher_output_layer_0/65.pt
25-01-23 08:59:47 | I | In layer model.layers.0, gradient of model.layers.0.self_attn.q_proj has nan: False
25-01-23 08:59:47 | I | In layer model.layers.0, gradient of model.layers.0.self_attn.q_proj has inf: False
25-01-23 08:59:47 | I | In layer model.layers.0, gradient of model.layers.0.self_attn.k_proj has nan: False
25-01-23 08:59:47 | I | In layer model.layers.0, gradient of model.layers.0.self_attn.k_proj has inf: False
25-01-23 08:59:47 | I | In layer model.layers.0, gradient of model.layers.0.self_attn.v_proj has nan: False
25-01-23 08:59:47 | I | In layer model.layers.0, gradient of model.layers.0.self_attn.v_proj has inf: False
25-01-23 08:59:47 | I | In layer model.layers.0, gradient of model.layers.0.self_attn.o_proj has nan: False
25-01-23 08:59:47 | I | In layer model.layers.0, gradient of model.layers.0.self_attn.o_proj has inf: False
25-01-23 08:59:47 | I | In layer model.layers.0, gradient of model.layers.0.mlp.up_proj has nan: False
25-01-23 08:59:47 | I | In layer model.layers.0, gradient of model.layers.0.mlp.up_proj has inf: False
25-01-23 08:59:47 | I | In layer model.layers.0, gradient of model.layers.0.mlp.gate_proj has nan: False
25-01-23 08:59:47 | I | In layer model.layers.0, gradient of model.layers.0.mlp.gate_proj has inf: False
25-01-23 08:59:47 | I | In layer model.layers.0, gradient of model.layers.0.mlp.down_proj has nan: False
25-01-23 08:59:47 | I | In layer model.layers.0, gradient of model.layers.0.mlp.down_proj has inf: False
25-01-23 08:59:47 | I | [61] done with optimizer step
25-01-23 08:59:47 | I | epoch 001:     66 / 819200000 loss=1.15901e-05, loss_per_token=0.0474732, loss_sum=388.901, wps=228, ups=0.03, wpb=8192, bsz=16, num_updates=62, lr=0.0049998, gnorm=0.097, clip=0, loss_scale=8, train_wall=36, cuda_gb_allocated=17.1, cuda_gb_reserved=18.4, cuda_gb_free=6.6, wall=5734
25-01-23 08:59:47 | I | TRAIN CURRENT LAYER_IDX = 0
25-01-23 08:59:47 | I | in layer model.layers.0
25-01-23 08:59:47 | I | quantizing weights for layer model.layers.0
25-01-23 08:59:47 | I | collecting calibration activations in model.layers.0
25-01-23 08:59:47 | I | collecting calibration activations in model.layers.0
25-01-23 08:59:47 | I | - Quantizing decoder layer model.layers.0
25-01-23 08:59:47 | I |   - Calibrating model.layers.0.self_attn.q_proj.weight
25-01-23 08:59:48 | I |       - range scale = [    1.0000]
25-01-23 08:59:48 | I |         sum  error  = [    0.0452]
25-01-23 08:59:48 | I |         best error  = [    0.0452]
25-01-23 08:59:48 | I |     + error = [0.0452]
25-01-23 08:59:49 | I |       - range scale = [    1.0000]
25-01-23 08:59:49 | I |         sum  error  = [    0.7040]
25-01-23 08:59:49 | I |         best error  = [    0.7040]
25-01-23 08:59:49 | I |     + error = [0.7040]
25-01-23 08:59:49 | I |   - Calibrating model.layers.0.self_attn.k_proj.weight
25-01-23 08:59:50 | I |       - range scale = [    1.0000]
25-01-23 08:59:50 | I |         sum  error  = [    0.0584]
25-01-23 08:59:50 | I |         best error  = [    0.0584]
25-01-23 08:59:50 | I |     + error = [0.0584]
25-01-23 08:59:50 | I |       - range scale = [    1.0000]
25-01-23 08:59:50 | I |         sum  error  = [    0.9302]
25-01-23 08:59:50 | I |         best error  = [    0.9302]
25-01-23 08:59:50 | I |     + error = [0.9302]
25-01-23 08:59:51 | I |   - Calibrating model.layers.0.self_attn.v_proj.weight
25-01-23 08:59:51 | I |       - range scale = [    1.0000]
25-01-23 08:59:51 | I |         sum  error  = [    0.6797]
25-01-23 08:59:51 | I |         best error  = [    0.6797]
25-01-23 08:59:51 | I |     + error = [0.6797]
25-01-23 08:59:52 | I |       - range scale = [    1.0000]
25-01-23 08:59:52 | I |         sum  error  = [    6.3211]
25-01-23 08:59:52 | I |         best error  = [    6.3211]
25-01-23 08:59:52 | I |     + error = [6.3211]
25-01-23 08:59:52 | I |   - Calibrating model.layers.0.self_attn.o_proj.weight
25-01-23 08:59:53 | I |       - range scale = [    1.0000]
25-01-23 08:59:53 | I |         sum  error  = [    0.1934]
25-01-23 08:59:53 | I |         best error  = [    0.1934]
25-01-23 08:59:53 | I |     + error = [0.1934]
25-01-23 08:59:54 | I |       - range scale = [    1.0000]
25-01-23 08:59:54 | I |         sum  error  = [    1.5483]
25-01-23 08:59:54 | I |         best error  = [    1.5483]
25-01-23 08:59:54 | I |     + error = [1.5483]
25-01-23 08:59:54 | I |   - Calibrating model.layers.0.mlp.up_proj.weight
25-01-23 08:59:55 | I |       - range scale = [    1.0000]
25-01-23 08:59:55 | I |         sum  error  = [    1.0412]
25-01-23 08:59:55 | I |         best error  = [    1.0412]
25-01-23 08:59:55 | I |     + error = [1.0412]
25-01-23 08:59:56 | I |       - range scale = [    1.0000]
25-01-23 08:59:56 | I |         sum  error  = [   11.0809]
25-01-23 08:59:56 | I |         best error  = [   11.0809]
25-01-23 08:59:56 | I |     + error = [11.0809]
25-01-23 08:59:56 | I |   - Calibrating model.layers.0.mlp.gate_proj.weight
25-01-23 08:59:57 | I |       - range scale = [    1.0000]
25-01-23 08:59:57 | I |         sum  error  = [    1.1034]
25-01-23 08:59:57 | I |         best error  = [    1.1034]
25-01-23 08:59:57 | I |     + error = [1.1034]
25-01-23 08:59:58 | I |       - range scale = [    1.0000]
25-01-23 08:59:58 | I |         sum  error  = [   11.4631]
25-01-23 08:59:58 | I |         best error  = [   11.4631]
25-01-23 08:59:58 | I |     + error = [11.4631]
25-01-23 08:59:58 | I |   - Calibrating model.layers.0.mlp.down_proj.weight
25-01-23 08:59:59 | I |       - range scale = [    1.0000]
25-01-23 08:59:59 | I |         sum  error  = [    0.1552]
25-01-23 08:59:59 | I |         best error  = [    0.1552]
25-01-23 08:59:59 | I |     + error = [0.1552]
25-01-23 09:00:00 | I |       - range scale = [    1.0000]
25-01-23 09:00:00 | I |         sum  error  = [    1.5138]
25-01-23 09:00:00 | I |         best error  = [    1.5138]
25-01-23 09:00:00 | I |     + error = [1.5138]
25-01-23 09:00:00 | I |   - Quantizing model.layers.0.self_attn.q_proj.weight
25-01-23 09:00:03 | I |   - Quantizing model.layers.0.self_attn.k_proj.weight
25-01-23 09:00:05 | I |   - Quantizing model.layers.0.self_attn.v_proj.weight
25-01-23 09:00:07 | I |   - Quantizing model.layers.0.self_attn.o_proj.weight
25-01-23 09:00:09 | I |   - Quantizing model.layers.0.mlp.up_proj.weight
25-01-23 09:00:12 | I |   - Quantizing model.layers.0.mlp.gate_proj.weight
25-01-23 09:00:14 | I |   - Quantizing model.layers.0.mlp.down_proj.weight
25-01-23 09:00:20 | I | quantizing activations for layer model.layers.0
25-01-23 09:00:20 | I | collecting calibration activations in model.layers.0
25-01-23 09:00:20 | I | collecting calibration activations in model.layers.0
25-01-23 09:00:22 | I | forward this layer
25-01-23 09:00:22 | I | input_file: /data/gyy/lmquant-main/lmquant/data/data_without_preprocess_llama_7b_bs16/shard/val_input_args_layer_0/66.pt
25-01-23 09:00:22 | I | output_file: /data/gyy/lmquant-main/lmquant/data/data_without_preprocess_llama_7b_bs16/shard/val_teacher_output_layer_0/66.pt
25-01-23 09:00:23 | I | In layer model.layers.0, gradient of model.layers.0.self_attn.q_proj has nan: False
25-01-23 09:00:23 | I | In layer model.layers.0, gradient of model.layers.0.self_attn.q_proj has inf: False
25-01-23 09:00:23 | I | In layer model.layers.0, gradient of model.layers.0.self_attn.k_proj has nan: False
25-01-23 09:00:23 | I | In layer model.layers.0, gradient of model.layers.0.self_attn.k_proj has inf: False
25-01-23 09:00:23 | I | In layer model.layers.0, gradient of model.layers.0.self_attn.v_proj has nan: False
25-01-23 09:00:23 | I | In layer model.layers.0, gradient of model.layers.0.self_attn.v_proj has inf: False
25-01-23 09:00:23 | I | In layer model.layers.0, gradient of model.layers.0.self_attn.o_proj has nan: False
25-01-23 09:00:23 | I | In layer model.layers.0, gradient of model.layers.0.self_attn.o_proj has inf: False
25-01-23 09:00:23 | I | In layer model.layers.0, gradient of model.layers.0.mlp.up_proj has nan: False
25-01-23 09:00:23 | I | In layer model.layers.0, gradient of model.layers.0.mlp.up_proj has inf: False
25-01-23 09:00:23 | I | In layer model.layers.0, gradient of model.layers.0.mlp.gate_proj has nan: False
25-01-23 09:00:23 | I | In layer model.layers.0, gradient of model.layers.0.mlp.gate_proj has inf: False
25-01-23 09:00:23 | I | In layer model.layers.0, gradient of model.layers.0.mlp.down_proj has nan: False
25-01-23 09:00:23 | I | In layer model.layers.0, gradient of model.layers.0.mlp.down_proj has inf: False
25-01-23 09:00:23 | I | [62] done with optimizer step
25-01-23 09:00:23 | I | epoch 001:     67 / 819200000 loss=1.16499e-05, loss_per_token=0.047718, loss_sum=390.906, wps=227.6, ups=0.03, wpb=8192, bsz=16, num_updates=63, lr=0.00499978, gnorm=0.299, clip=0, loss_scale=8, train_wall=36, cuda_gb_allocated=17.1, cuda_gb_reserved=18.4, cuda_gb_free=6.6, wall=5770
25-01-23 09:00:23 | I | TRAIN CURRENT LAYER_IDX = 0
25-01-23 09:00:23 | I | in layer model.layers.0
25-01-23 09:00:23 | I | quantizing weights for layer model.layers.0
25-01-23 09:00:23 | I | collecting calibration activations in model.layers.0
25-01-23 09:00:23 | I | collecting calibration activations in model.layers.0
25-01-23 09:00:23 | I | - Quantizing decoder layer model.layers.0
25-01-23 09:00:23 | I |   - Calibrating model.layers.0.self_attn.q_proj.weight
25-01-23 09:00:24 | I |       - range scale = [    1.0000]
25-01-23 09:00:24 | I |         sum  error  = [    0.0470]
25-01-23 09:00:24 | I |         best error  = [    0.0470]
25-01-23 09:00:24 | I |     + error = [0.0470]
25-01-23 09:00:25 | I |       - range scale = [    1.0000]
25-01-23 09:00:25 | I |         sum  error  = [    0.7231]
25-01-23 09:00:25 | I |         best error  = [    0.7231]
25-01-23 09:00:25 | I |     + error = [0.7231]
25-01-23 09:00:25 | I |   - Calibrating model.layers.0.self_attn.k_proj.weight
25-01-23 09:00:26 | I |       - range scale = [    1.0000]
25-01-23 09:00:26 | I |         sum  error  = [    0.0583]
25-01-23 09:00:26 | I |         best error  = [    0.0583]
25-01-23 09:00:26 | I |     + error = [0.0583]
25-01-23 09:00:26 | I |       - range scale = [    1.0000]
25-01-23 09:00:26 | I |         sum  error  = [    0.9486]
25-01-23 09:00:26 | I |         best error  = [    0.9486]
25-01-23 09:00:26 | I |     + error = [0.9486]
25-01-23 09:00:27 | I |   - Calibrating model.layers.0.self_attn.v_proj.weight
25-01-23 09:00:27 | I |       - range scale = [    1.0000]
25-01-23 09:00:27 | I |         sum  error  = [    0.6912]
25-01-23 09:00:27 | I |         best error  = [    0.6912]
25-01-23 09:00:27 | I |     + error = [0.6912]
25-01-23 09:00:28 | I |       - range scale = [    1.0000]
25-01-23 09:00:28 | I |         sum  error  = [    6.4591]
25-01-23 09:00:28 | I |         best error  = [    6.4591]
25-01-23 09:00:28 | I |     + error = [6.4591]
25-01-23 09:00:28 | I |   - Calibrating model.layers.0.self_attn.o_proj.weight
25-01-23 09:00:29 | I |       - range scale = [    1.0000]
25-01-23 09:00:29 | I |         sum  error  = [    0.1931]
25-01-23 09:00:29 | I |         best error  = [    0.1931]
25-01-23 09:00:29 | I |     + error = [0.1931]
25-01-23 09:00:30 | I |       - range scale = [    1.0000]
25-01-23 09:00:30 | I |         sum  error  = [    1.5471]
25-01-23 09:00:30 | I |         best error  = [    1.5471]
25-01-23 09:00:30 | I |     + error = [1.5471]
25-01-23 09:00:30 | I |   - Calibrating model.layers.0.mlp.up_proj.weight
25-01-23 09:00:31 | I |       - range scale = [    1.0000]
25-01-23 09:00:31 | I |         sum  error  = [    1.0408]
25-01-23 09:00:31 | I |         best error  = [    1.0408]
25-01-23 09:00:31 | I |     + error = [1.0408]
25-01-23 09:00:32 | I |       - range scale = [    1.0000]
25-01-23 09:00:32 | I |         sum  error  = [   11.0740]
25-01-23 09:00:32 | I |         best error  = [   11.0740]
25-01-23 09:00:32 | I |     + error = [11.0740]
25-01-23 09:00:32 | I |   - Calibrating model.layers.0.mlp.gate_proj.weight
25-01-23 09:00:33 | I |       - range scale = [    1.0000]
25-01-23 09:00:33 | I |         sum  error  = [    1.1035]
25-01-23 09:00:33 | I |         best error  = [    1.1035]
25-01-23 09:00:33 | I |     + error = [1.1035]
25-01-23 09:00:34 | I |       - range scale = [    1.0000]
25-01-23 09:00:34 | I |         sum  error  = [   11.4537]
25-01-23 09:00:34 | I |         best error  = [   11.4537]
25-01-23 09:00:34 | I |     + error = [11.4537]
25-01-23 09:00:34 | I |   - Calibrating model.layers.0.mlp.down_proj.weight
25-01-23 09:00:35 | I |       - range scale = [    1.0000]
25-01-23 09:00:35 | I |         sum  error  = [    0.1553]
25-01-23 09:00:35 | I |         best error  = [    0.1553]
25-01-23 09:00:35 | I |     + error = [0.1553]
25-01-23 09:00:36 | I |       - range scale = [    1.0000]
25-01-23 09:00:36 | I |         sum  error  = [    1.5162]
25-01-23 09:00:36 | I |         best error  = [    1.5162]
25-01-23 09:00:36 | I |     + error = [1.5162]
25-01-23 09:00:36 | I |   - Quantizing model.layers.0.self_attn.q_proj.weight
25-01-23 09:00:38 | I |   - Quantizing model.layers.0.self_attn.k_proj.weight
25-01-23 09:00:41 | I |   - Quantizing model.layers.0.self_attn.v_proj.weight
25-01-23 09:00:43 | I |   - Quantizing model.layers.0.self_attn.o_proj.weight
25-01-23 09:00:45 | I |   - Quantizing model.layers.0.mlp.up_proj.weight
25-01-23 09:00:48 | I |   - Quantizing model.layers.0.mlp.gate_proj.weight
25-01-23 09:00:50 | I |   - Quantizing model.layers.0.mlp.down_proj.weight
25-01-23 09:00:56 | I | quantizing activations for layer model.layers.0
25-01-23 09:00:56 | I | collecting calibration activations in model.layers.0
25-01-23 09:00:56 | I | collecting calibration activations in model.layers.0
25-01-23 09:00:58 | I | forward this layer
25-01-23 09:00:58 | I | input_file: /data/gyy/lmquant-main/lmquant/data/data_without_preprocess_llama_7b_bs16/shard/val_input_args_layer_0/67.pt
25-01-23 09:00:58 | I | output_file: /data/gyy/lmquant-main/lmquant/data/data_without_preprocess_llama_7b_bs16/shard/val_teacher_output_layer_0/67.pt
25-01-23 09:00:58 | I | In layer model.layers.0, gradient of model.layers.0.self_attn.q_proj has nan: False
25-01-23 09:00:58 | I | In layer model.layers.0, gradient of model.layers.0.self_attn.q_proj has inf: False
25-01-23 09:00:58 | I | In layer model.layers.0, gradient of model.layers.0.self_attn.k_proj has nan: False
25-01-23 09:00:58 | I | In layer model.layers.0, gradient of model.layers.0.self_attn.k_proj has inf: False
25-01-23 09:00:58 | I | In layer model.layers.0, gradient of model.layers.0.self_attn.v_proj has nan: False
25-01-23 09:00:58 | I | In layer model.layers.0, gradient of model.layers.0.self_attn.v_proj has inf: False
25-01-23 09:00:58 | I | In layer model.layers.0, gradient of model.layers.0.self_attn.o_proj has nan: False
25-01-23 09:00:58 | I | In layer model.layers.0, gradient of model.layers.0.self_attn.o_proj has inf: False
25-01-23 09:00:58 | I | In layer model.layers.0, gradient of model.layers.0.mlp.up_proj has nan: False
25-01-23 09:00:58 | I | In layer model.layers.0, gradient of model.layers.0.mlp.up_proj has inf: False
25-01-23 09:00:58 | I | In layer model.layers.0, gradient of model.layers.0.mlp.gate_proj has nan: False
25-01-23 09:00:58 | I | In layer model.layers.0, gradient of model.layers.0.mlp.gate_proj has inf: False
25-01-23 09:00:58 | I | In layer model.layers.0, gradient of model.layers.0.mlp.down_proj has nan: False
25-01-23 09:00:58 | I | In layer model.layers.0, gradient of model.layers.0.mlp.down_proj has inf: False
25-01-23 09:00:58 | I | [63] done with optimizer step
25-01-23 09:00:58 | I | epoch 001:     68 / 819200000 loss=1.14428e-05, loss_per_token=0.0468698, loss_sum=383.957, wps=228, ups=0.03, wpb=8192, bsz=16, num_updates=64, lr=0.00499977, gnorm=0.356, clip=0, loss_scale=8, train_wall=36, cuda_gb_allocated=17.1, cuda_gb_reserved=18.7, cuda_gb_free=6.6, wall=5806
25-01-23 09:00:59 | I | TRAIN CURRENT LAYER_IDX = 0
25-01-23 09:00:59 | I | in layer model.layers.0
25-01-23 09:00:59 | I | quantizing weights for layer model.layers.0
25-01-23 09:00:59 | I | collecting calibration activations in model.layers.0
25-01-23 09:00:59 | I | collecting calibration activations in model.layers.0
25-01-23 09:00:59 | I | - Quantizing decoder layer model.layers.0
25-01-23 09:00:59 | I |   - Calibrating model.layers.0.self_attn.q_proj.weight
25-01-23 09:01:00 | I |       - range scale = [    1.0000]
25-01-23 09:01:00 | I |         sum  error  = [    0.0473]
25-01-23 09:01:00 | I |         best error  = [    0.0473]
25-01-23 09:01:00 | I |     + error = [0.0473]
25-01-23 09:01:01 | I |       - range scale = [    1.0000]
25-01-23 09:01:01 | I |         sum  error  = [    0.7331]
25-01-23 09:01:01 | I |         best error  = [    0.7331]
25-01-23 09:01:01 | I |     + error = [0.7331]
25-01-23 09:01:01 | I |   - Calibrating model.layers.0.self_attn.k_proj.weight
25-01-23 09:01:02 | I |       - range scale = [    1.0000]
25-01-23 09:01:02 | I |         sum  error  = [    0.0613]
25-01-23 09:01:02 | I |         best error  = [    0.0613]
25-01-23 09:01:02 | I |     + error = [0.0613]
25-01-23 09:01:02 | I |       - range scale = [    1.0000]
25-01-23 09:01:02 | I |         sum  error  = [    0.9535]
25-01-23 09:01:02 | I |         best error  = [    0.9535]
25-01-23 09:01:02 | I |     + error = [0.9535]
25-01-23 09:01:03 | I |   - Calibrating model.layers.0.self_attn.v_proj.weight
25-01-23 09:01:03 | I |       - range scale = [    1.0000]
25-01-23 09:01:03 | I |         sum  error  = [    0.7006]
25-01-23 09:01:03 | I |         best error  = [    0.7006]
25-01-23 09:01:03 | I |     + error = [0.7006]
25-01-23 09:01:04 | I |       - range scale = [    1.0000]
25-01-23 09:01:04 | I |         sum  error  = [    6.5093]
25-01-23 09:01:04 | I |         best error  = [    6.5093]
25-01-23 09:01:04 | I |     + error = [6.5093]
25-01-23 09:01:04 | I |   - Calibrating model.layers.0.self_attn.o_proj.weight
25-01-23 09:01:05 | I |       - range scale = [    1.0000]
25-01-23 09:01:05 | I |         sum  error  = [    0.1927]
25-01-23 09:01:05 | I |         best error  = [    0.1927]
25-01-23 09:01:05 | I |     + error = [0.1927]
25-01-23 09:01:06 | I |       - range scale = [    1.0000]
25-01-23 09:01:06 | I |         sum  error  = [    1.5397]
25-01-23 09:01:06 | I |         best error  = [    1.5397]
25-01-23 09:01:06 | I |     + error = [1.5397]
25-01-23 09:01:06 | I |   - Calibrating model.layers.0.mlp.up_proj.weight
25-01-23 09:01:07 | I |       - range scale = [    1.0000]
25-01-23 09:01:07 | I |         sum  error  = [    1.0442]
25-01-23 09:01:07 | I |         best error  = [    1.0442]
25-01-23 09:01:07 | I |     + error = [1.0442]
25-01-23 09:01:08 | I |       - range scale = [    1.0000]
25-01-23 09:01:08 | I |         sum  error  = [   11.1127]
25-01-23 09:01:08 | I |         best error  = [   11.1127]
25-01-23 09:01:08 | I |     + error = [11.1127]
25-01-23 09:01:08 | I |   - Calibrating model.layers.0.mlp.gate_proj.weight
25-01-23 09:01:09 | I |       - range scale = [    1.0000]
25-01-23 09:01:09 | I |         sum  error  = [    1.1071]
25-01-23 09:01:09 | I |         best error  = [    1.1071]
25-01-23 09:01:09 | I |     + error = [1.1071]
25-01-23 09:01:10 | I |       - range scale = [    1.0000]
25-01-23 09:01:10 | I |         sum  error  = [   11.4997]
25-01-23 09:01:10 | I |         best error  = [   11.4997]
25-01-23 09:01:10 | I |     + error = [11.4997]
25-01-23 09:01:10 | I |   - Calibrating model.layers.0.mlp.down_proj.weight
25-01-23 09:01:11 | I |       - range scale = [    1.0000]
25-01-23 09:01:11 | I |         sum  error  = [    0.1610]
25-01-23 09:01:11 | I |         best error  = [    0.1610]
25-01-23 09:01:11 | I |     + error = [0.1610]
25-01-23 09:01:12 | I |       - range scale = [    1.0000]
25-01-23 09:01:12 | I |         sum  error  = [    1.5754]
25-01-23 09:01:12 | I |         best error  = [    1.5754]
25-01-23 09:01:12 | I |     + error = [1.5754]
25-01-23 09:01:12 | I |   - Quantizing model.layers.0.self_attn.q_proj.weight
25-01-23 09:01:15 | I |   - Quantizing model.layers.0.self_attn.k_proj.weight
25-01-23 09:01:17 | I |   - Quantizing model.layers.0.self_attn.v_proj.weight
25-01-23 09:01:19 | I |   - Quantizing model.layers.0.self_attn.o_proj.weight
25-01-23 09:01:22 | I |   - Quantizing model.layers.0.mlp.up_proj.weight
25-01-23 09:01:24 | I |   - Quantizing model.layers.0.mlp.gate_proj.weight
25-01-23 09:01:26 | I |   - Quantizing model.layers.0.mlp.down_proj.weight
25-01-23 09:01:32 | I | quantizing activations for layer model.layers.0
25-01-23 09:01:33 | I | collecting calibration activations in model.layers.0
25-01-23 09:01:33 | I | collecting calibration activations in model.layers.0
25-01-23 09:01:35 | I | forward this layer
25-01-23 09:01:35 | I | input_file: /data/gyy/lmquant-main/lmquant/data/data_without_preprocess_llama_7b_bs16/shard/val_input_args_layer_0/68.pt
25-01-23 09:01:35 | I | output_file: /data/gyy/lmquant-main/lmquant/data/data_without_preprocess_llama_7b_bs16/shard/val_teacher_output_layer_0/68.pt
25-01-23 09:01:35 | I | In layer model.layers.0, gradient of model.layers.0.self_attn.q_proj has nan: False
25-01-23 09:01:35 | I | In layer model.layers.0, gradient of model.layers.0.self_attn.q_proj has inf: False
25-01-23 09:01:35 | I | In layer model.layers.0, gradient of model.layers.0.self_attn.k_proj has nan: False
25-01-23 09:01:35 | I | In layer model.layers.0, gradient of model.layers.0.self_attn.k_proj has inf: False
25-01-23 09:01:35 | I | In layer model.layers.0, gradient of model.layers.0.self_attn.v_proj has nan: False
25-01-23 09:01:35 | I | In layer model.layers.0, gradient of model.layers.0.self_attn.v_proj has inf: False
25-01-23 09:01:35 | I | In layer model.layers.0, gradient of model.layers.0.self_attn.o_proj has nan: False
25-01-23 09:01:35 | I | In layer model.layers.0, gradient of model.layers.0.self_attn.o_proj has inf: False
25-01-23 09:01:35 | I | In layer model.layers.0, gradient of model.layers.0.mlp.up_proj has nan: False
25-01-23 09:01:35 | I | In layer model.layers.0, gradient of model.layers.0.mlp.up_proj has inf: False
25-01-23 09:01:35 | I | In layer model.layers.0, gradient of model.layers.0.mlp.gate_proj has nan: False
25-01-23 09:01:35 | I | In layer model.layers.0, gradient of model.layers.0.mlp.gate_proj has inf: False
25-01-23 09:01:35 | I | In layer model.layers.0, gradient of model.layers.0.mlp.down_proj has nan: False
25-01-23 09:01:35 | I | In layer model.layers.0, gradient of model.layers.0.mlp.down_proj has inf: False
25-01-23 09:01:35 | I | [64] done with optimizer step
25-01-23 09:01:35 | I | epoch 001:     69 / 819200000 loss=1.14426e-05, loss_per_token=0.0468689, loss_sum=383.95, wps=224.7, ups=0.03, wpb=8192, bsz=16, num_updates=65, lr=0.00499975, gnorm=0.274, clip=0, loss_scale=8, train_wall=36, cuda_gb_allocated=17.1, cuda_gb_reserved=18.5, cuda_gb_free=6.6, wall=5843
25-01-23 09:01:35 | I | TRAIN CURRENT LAYER_IDX = 0
25-01-23 09:01:35 | I | in layer model.layers.0
25-01-23 09:01:35 | I | quantizing weights for layer model.layers.0
25-01-23 09:01:36 | I | collecting calibration activations in model.layers.0
25-01-23 09:01:36 | I | collecting calibration activations in model.layers.0
25-01-23 09:01:36 | I | - Quantizing decoder layer model.layers.0
25-01-23 09:01:36 | I |   - Calibrating model.layers.0.self_attn.q_proj.weight
25-01-23 09:01:37 | I |       - range scale = [    1.0000]
25-01-23 09:01:37 | I |         sum  error  = [    0.0467]
25-01-23 09:01:37 | I |         best error  = [    0.0467]
25-01-23 09:01:37 | I |     + error = [0.0467]
25-01-23 09:01:37 | I |       - range scale = [    1.0000]
25-01-23 09:01:37 | I |         sum  error  = [    0.7223]
25-01-23 09:01:37 | I |         best error  = [    0.7223]
25-01-23 09:01:37 | I |     + error = [0.7223]
25-01-23 09:01:37 | I |   - Calibrating model.layers.0.self_attn.k_proj.weight
25-01-23 09:01:38 | I |       - range scale = [    1.0000]
25-01-23 09:01:38 | I |         sum  error  = [    0.0610]
25-01-23 09:01:38 | I |         best error  = [    0.0610]
25-01-23 09:01:38 | I |     + error = [0.0610]
25-01-23 09:01:39 | I |       - range scale = [    1.0000]
25-01-23 09:01:39 | I |         sum  error  = [    0.9446]
25-01-23 09:01:39 | I |         best error  = [    0.9446]
25-01-23 09:01:39 | I |     + error = [0.9446]
25-01-23 09:01:39 | I |   - Calibrating model.layers.0.self_attn.v_proj.weight
25-01-23 09:01:40 | I |       - range scale = [    1.0000]
25-01-23 09:01:40 | I |         sum  error  = [    0.6993]
25-01-23 09:01:40 | I |         best error  = [    0.6993]
25-01-23 09:01:40 | I |     + error = [0.6993]
25-01-23 09:01:41 | I |       - range scale = [    1.0000]
25-01-23 09:01:41 | I |         sum  error  = [    6.4951]
25-01-23 09:01:41 | I |         best error  = [    6.4951]
25-01-23 09:01:41 | I |     + error = [6.4951]
25-01-23 09:01:41 | I |   - Calibrating model.layers.0.self_attn.o_proj.weight
25-01-23 09:01:41 | I |       - range scale = [    1.0000]
25-01-23 09:01:41 | I |         sum  error  = [    0.1936]
25-01-23 09:01:41 | I |         best error  = [    0.1936]
25-01-23 09:01:41 | I |     + error = [0.1936]
25-01-23 09:01:42 | I |       - range scale = [    1.0000]
25-01-23 09:01:42 | I |         sum  error  = [    1.5503]
25-01-23 09:01:42 | I |         best error  = [    1.5503]
25-01-23 09:01:42 | I |     + error = [1.5503]
25-01-23 09:01:42 | I |   - Calibrating model.layers.0.mlp.up_proj.weight
25-01-23 09:01:43 | I |       - range scale = [    1.0000]
25-01-23 09:01:43 | I |         sum  error  = [    1.0449]
25-01-23 09:01:43 | I |         best error  = [    1.0449]
25-01-23 09:01:43 | I |     + error = [1.0449]
25-01-23 09:01:44 | I |       - range scale = [    1.0000]
25-01-23 09:01:44 | I |         sum  error  = [   11.1172]
25-01-23 09:01:44 | I |         best error  = [   11.1172]
25-01-23 09:01:44 | I |     + error = [11.1172]
25-01-23 09:01:45 | I |   - Calibrating model.layers.0.mlp.gate_proj.weight
25-01-23 09:01:45 | I |       - range scale = [    1.0000]
25-01-23 09:01:45 | I |         sum  error  = [    1.1084]
25-01-23 09:01:45 | I |         best error  = [    1.1084]
25-01-23 09:01:45 | I |     + error = [1.1084]
25-01-23 09:01:46 | I |       - range scale = [    1.0000]
25-01-23 09:01:46 | I |         sum  error  = [   11.4951]
25-01-23 09:01:46 | I |         best error  = [   11.4951]
25-01-23 09:01:46 | I |     + error = [11.4951]
25-01-23 09:01:47 | I |   - Calibrating model.layers.0.mlp.down_proj.weight
25-01-23 09:01:47 | I |       - range scale = [    1.0000]
25-01-23 09:01:47 | I |         sum  error  = [    0.1636]
25-01-23 09:01:47 | I |         best error  = [    0.1636]
25-01-23 09:01:47 | I |     + error = [0.1636]
25-01-23 09:01:49 | I |       - range scale = [    1.0000]
25-01-23 09:01:49 | I |         sum  error  = [    1.5969]
25-01-23 09:01:49 | I |         best error  = [    1.5969]
25-01-23 09:01:49 | I |     + error = [1.5969]
25-01-23 09:01:49 | I |   - Quantizing model.layers.0.self_attn.q_proj.weight
25-01-23 09:01:51 | I |   - Quantizing model.layers.0.self_attn.k_proj.weight
25-01-23 09:01:53 | I |   - Quantizing model.layers.0.self_attn.v_proj.weight
25-01-23 09:01:56 | I |   - Quantizing model.layers.0.self_attn.o_proj.weight
25-01-23 09:01:58 | I |   - Quantizing model.layers.0.mlp.up_proj.weight
25-01-23 09:02:00 | I |   - Quantizing model.layers.0.mlp.gate_proj.weight
25-01-23 09:02:02 | I |   - Quantizing model.layers.0.mlp.down_proj.weight
25-01-23 09:02:08 | I | quantizing activations for layer model.layers.0
25-01-23 09:02:09 | I | collecting calibration activations in model.layers.0
25-01-23 09:02:09 | I | collecting calibration activations in model.layers.0
25-01-23 09:02:11 | I | forward this layer
25-01-23 09:02:11 | I | input_file: /data/gyy/lmquant-main/lmquant/data/data_without_preprocess_llama_7b_bs16/shard/val_input_args_layer_0/69.pt
25-01-23 09:02:11 | I | output_file: /data/gyy/lmquant-main/lmquant/data/data_without_preprocess_llama_7b_bs16/shard/val_teacher_output_layer_0/69.pt
25-01-23 09:02:11 | I | In layer model.layers.0, gradient of model.layers.0.self_attn.q_proj has nan: False
25-01-23 09:02:11 | I | In layer model.layers.0, gradient of model.layers.0.self_attn.q_proj has inf: False
25-01-23 09:02:11 | I | In layer model.layers.0, gradient of model.layers.0.self_attn.k_proj has nan: False
25-01-23 09:02:11 | I | In layer model.layers.0, gradient of model.layers.0.self_attn.k_proj has inf: False
25-01-23 09:02:11 | I | In layer model.layers.0, gradient of model.layers.0.self_attn.v_proj has nan: False
25-01-23 09:02:11 | I | In layer model.layers.0, gradient of model.layers.0.self_attn.v_proj has inf: False
25-01-23 09:02:11 | I | In layer model.layers.0, gradient of model.layers.0.self_attn.o_proj has nan: False
25-01-23 09:02:11 | I | In layer model.layers.0, gradient of model.layers.0.self_attn.o_proj has inf: False
25-01-23 09:02:11 | I | In layer model.layers.0, gradient of model.layers.0.mlp.up_proj has nan: False
25-01-23 09:02:11 | I | In layer model.layers.0, gradient of model.layers.0.mlp.up_proj has inf: False
25-01-23 09:02:11 | I | In layer model.layers.0, gradient of model.layers.0.mlp.gate_proj has nan: False
25-01-23 09:02:11 | I | In layer model.layers.0, gradient of model.layers.0.mlp.gate_proj has inf: False
25-01-23 09:02:11 | I | In layer model.layers.0, gradient of model.layers.0.mlp.down_proj has nan: False
25-01-23 09:02:11 | I | In layer model.layers.0, gradient of model.layers.0.mlp.down_proj has inf: False
25-01-23 09:02:11 | I | [65] done with optimizer step
25-01-23 09:02:11 | I | epoch 001:     70 / 819200000 loss=1.11651e-05, loss_per_token=0.0457322, loss_sum=374.638, wps=226.9, ups=0.03, wpb=8192, bsz=16, num_updates=66, lr=0.00499973, gnorm=0.134, clip=0, loss_scale=8, train_wall=36, cuda_gb_allocated=17.1, cuda_gb_reserved=18.7, cuda_gb_free=6.6, wall=5879
25-01-23 09:02:11 | I | TRAIN CURRENT LAYER_IDX = 0
25-01-23 09:02:11 | I | in layer model.layers.0
25-01-23 09:02:11 | I | quantizing weights for layer model.layers.0
25-01-23 09:02:12 | I | collecting calibration activations in model.layers.0
25-01-23 09:02:12 | I | collecting calibration activations in model.layers.0
25-01-23 09:02:12 | I | - Quantizing decoder layer model.layers.0
25-01-23 09:02:12 | I |   - Calibrating model.layers.0.self_attn.q_proj.weight
25-01-23 09:02:13 | I |       - range scale = [    1.0000]
25-01-23 09:02:13 | I |         sum  error  = [    0.0470]
25-01-23 09:02:13 | I |         best error  = [    0.0470]
25-01-23 09:02:13 | I |     + error = [0.0470]
25-01-23 09:02:13 | I |       - range scale = [    1.0000]
25-01-23 09:02:13 | I |         sum  error  = [    0.7285]
25-01-23 09:02:13 | I |         best error  = [    0.7285]
25-01-23 09:02:13 | I |     + error = [0.7285]
25-01-23 09:02:14 | I |   - Calibrating model.layers.0.self_attn.k_proj.weight
25-01-23 09:02:14 | I |       - range scale = [    1.0000]
25-01-23 09:02:14 | I |         sum  error  = [    0.0594]
25-01-23 09:02:14 | I |         best error  = [    0.0594]
25-01-23 09:02:14 | I |     + error = [0.0594]
25-01-23 09:02:15 | I |       - range scale = [    1.0000]
25-01-23 09:02:15 | I |         sum  error  = [    0.9496]
25-01-23 09:02:15 | I |         best error  = [    0.9496]
25-01-23 09:02:15 | I |     + error = [0.9496]
25-01-23 09:02:15 | I |   - Calibrating model.layers.0.self_attn.v_proj.weight
25-01-23 09:02:16 | I |       - range scale = [    1.0000]
25-01-23 09:02:16 | I |         sum  error  = [    0.6984]
25-01-23 09:02:16 | I |         best error  = [    0.6984]
25-01-23 09:02:16 | I |     + error = [0.6984]
25-01-23 09:02:17 | I |       - range scale = [    1.0000]
25-01-23 09:02:17 | I |         sum  error  = [    6.5084]
25-01-23 09:02:17 | I |         best error  = [    6.5084]
25-01-23 09:02:17 | I |     + error = [6.5084]
25-01-23 09:02:17 | I |   - Calibrating model.layers.0.self_attn.o_proj.weight
25-01-23 09:02:18 | I |       - range scale = [    1.0000]
25-01-23 09:02:18 | I |         sum  error  = [    0.1961]
25-01-23 09:02:18 | I |         best error  = [    0.1961]
25-01-23 09:02:18 | I |     + error = [0.1961]
25-01-23 09:02:18 | I |       - range scale = [    1.0000]
25-01-23 09:02:18 | I |         sum  error  = [    1.5770]
25-01-23 09:02:18 | I |         best error  = [    1.5770]
25-01-23 09:02:18 | I |     + error = [1.5770]
25-01-23 09:02:19 | I |   - Calibrating model.layers.0.mlp.up_proj.weight
25-01-23 09:02:19 | I |       - range scale = [    1.0000]
25-01-23 09:02:19 | I |         sum  error  = [    1.0607]
25-01-23 09:02:19 | I |         best error  = [    1.0607]
25-01-23 09:02:19 | I |     + error = [1.0607]
25-01-23 09:02:20 | I |       - range scale = [    1.0000]
25-01-23 09:02:20 | I |         sum  error  = [   11.2832]
25-01-23 09:02:20 | I |         best error  = [   11.2832]
25-01-23 09:02:20 | I |     + error = [11.2832]
25-01-23 09:02:21 | I |   - Calibrating model.layers.0.mlp.gate_proj.weight
25-01-23 09:02:21 | I |       - range scale = [    1.0000]
25-01-23 09:02:21 | I |         sum  error  = [    1.1239]
25-01-23 09:02:21 | I |         best error  = [    1.1239]
25-01-23 09:02:21 | I |     + error = [1.1239]
25-01-23 09:02:23 | I |       - range scale = [    1.0000]
25-01-23 09:02:23 | I |         sum  error  = [   11.6648]
25-01-23 09:02:23 | I |         best error  = [   11.6648]
25-01-23 09:02:23 | I |     + error = [11.6648]
25-01-23 09:02:23 | I |   - Calibrating model.layers.0.mlp.down_proj.weight
25-01-23 09:02:23 | I |       - range scale = [    1.0000]
25-01-23 09:02:23 | I |         sum  error  = [    0.1685]
25-01-23 09:02:23 | I |         best error  = [    0.1685]
25-01-23 09:02:23 | I |     + error = [0.1685]
25-01-23 09:02:25 | I |       - range scale = [    1.0000]
25-01-23 09:02:25 | I |         sum  error  = [    1.6430]
25-01-23 09:02:25 | I |         best error  = [    1.6430]
25-01-23 09:02:25 | I |     + error = [1.6430]
25-01-23 09:02:25 | I |   - Quantizing model.layers.0.self_attn.q_proj.weight
25-01-23 09:02:27 | I |   - Quantizing model.layers.0.self_attn.k_proj.weight
25-01-23 09:02:30 | I |   - Quantizing model.layers.0.self_attn.v_proj.weight
25-01-23 09:02:32 | I |   - Quantizing model.layers.0.self_attn.o_proj.weight
25-01-23 09:02:34 | I |   - Quantizing model.layers.0.mlp.up_proj.weight
25-01-23 09:02:36 | I |   - Quantizing model.layers.0.mlp.gate_proj.weight
25-01-23 09:02:39 | I |   - Quantizing model.layers.0.mlp.down_proj.weight
25-01-23 09:02:45 | I | quantizing activations for layer model.layers.0
25-01-23 09:02:45 | I | collecting calibration activations in model.layers.0
25-01-23 09:02:45 | I | collecting calibration activations in model.layers.0
25-01-23 09:02:47 | I | forward this layer
25-01-23 09:02:47 | I | input_file: /data/gyy/lmquant-main/lmquant/data/data_without_preprocess_llama_7b_bs16/shard/val_input_args_layer_0/70.pt
25-01-23 09:02:47 | I | output_file: /data/gyy/lmquant-main/lmquant/data/data_without_preprocess_llama_7b_bs16/shard/val_teacher_output_layer_0/70.pt
25-01-23 09:02:47 | I | In layer model.layers.0, gradient of model.layers.0.self_attn.q_proj has nan: False
25-01-23 09:02:47 | I | In layer model.layers.0, gradient of model.layers.0.self_attn.q_proj has inf: False
25-01-23 09:02:47 | I | In layer model.layers.0, gradient of model.layers.0.self_attn.k_proj has nan: False
25-01-23 09:02:47 | I | In layer model.layers.0, gradient of model.layers.0.self_attn.k_proj has inf: False
25-01-23 09:02:47 | I | In layer model.layers.0, gradient of model.layers.0.self_attn.v_proj has nan: False
25-01-23 09:02:47 | I | In layer model.layers.0, gradient of model.layers.0.self_attn.v_proj has inf: False
25-01-23 09:02:47 | I | In layer model.layers.0, gradient of model.layers.0.self_attn.o_proj has nan: False
25-01-23 09:02:47 | I | In layer model.layers.0, gradient of model.layers.0.self_attn.o_proj has inf: False
25-01-23 09:02:47 | I | In layer model.layers.0, gradient of model.layers.0.mlp.up_proj has nan: False
25-01-23 09:02:47 | I | In layer model.layers.0, gradient of model.layers.0.mlp.up_proj has inf: False
25-01-23 09:02:47 | I | In layer model.layers.0, gradient of model.layers.0.mlp.gate_proj has nan: False
25-01-23 09:02:47 | I | In layer model.layers.0, gradient of model.layers.0.mlp.gate_proj has inf: False
25-01-23 09:02:47 | I | In layer model.layers.0, gradient of model.layers.0.mlp.down_proj has nan: False
25-01-23 09:02:47 | I | In layer model.layers.0, gradient of model.layers.0.mlp.down_proj has inf: False
25-01-23 09:02:47 | I | [66] done with optimizer step
25-01-23 09:02:47 | I | epoch 001:     71 / 819200000 loss=1.11119e-05, loss_per_token=0.0455145, loss_sum=372.854, wps=225, ups=0.03, wpb=8192, bsz=16, num_updates=67, lr=0.00499972, gnorm=0.197, clip=0, loss_scale=8, train_wall=36, cuda_gb_allocated=17.1, cuda_gb_reserved=18.5, cuda_gb_free=6.6, wall=5915
25-01-23 09:02:48 | I | TRAIN CURRENT LAYER_IDX = 0
25-01-23 09:02:48 | I | in layer model.layers.0
25-01-23 09:02:48 | I | quantizing weights for layer model.layers.0
25-01-23 09:02:48 | I | collecting calibration activations in model.layers.0
25-01-23 09:02:48 | I | collecting calibration activations in model.layers.0
25-01-23 09:02:48 | I | - Quantizing decoder layer model.layers.0
25-01-23 09:02:48 | I |   - Calibrating model.layers.0.self_attn.q_proj.weight
25-01-23 09:02:49 | I |       - range scale = [    1.0000]
25-01-23 09:02:49 | I |         sum  error  = [    0.0472]
25-01-23 09:02:49 | I |         best error  = [    0.0472]
25-01-23 09:02:49 | I |     + error = [0.0472]
25-01-23 09:02:50 | I |       - range scale = [    1.0000]
25-01-23 09:02:50 | I |         sum  error  = [    0.7317]
25-01-23 09:02:50 | I |         best error  = [    0.7317]
25-01-23 09:02:50 | I |     + error = [0.7317]
25-01-23 09:02:50 | I |   - Calibrating model.layers.0.self_attn.k_proj.weight
25-01-23 09:02:51 | I |       - range scale = [    1.0000]
25-01-23 09:02:51 | I |         sum  error  = [    0.0600]
25-01-23 09:02:51 | I |         best error  = [    0.0600]
25-01-23 09:02:51 | I |     + error = [0.0600]
25-01-23 09:02:51 | I |       - range scale = [    1.0000]
25-01-23 09:02:51 | I |         sum  error  = [    0.9433]
25-01-23 09:02:51 | I |         best error  = [    0.9433]
25-01-23 09:02:51 | I |     + error = [0.9433]
25-01-23 09:02:52 | I |   - Calibrating model.layers.0.self_attn.v_proj.weight
25-01-23 09:02:52 | I |       - range scale = [    1.0000]
25-01-23 09:02:52 | I |         sum  error  = [    0.6841]
25-01-23 09:02:52 | I |         best error  = [    0.6841]
25-01-23 09:02:52 | I |     + error = [0.6841]
25-01-23 09:02:53 | I |       - range scale = [    1.0000]
25-01-23 09:02:53 | I |         sum  error  = [    6.3727]
25-01-23 09:02:53 | I |         best error  = [    6.3727]
25-01-23 09:02:53 | I |     + error = [6.3727]
25-01-23 09:02:53 | I |   - Calibrating model.layers.0.self_attn.o_proj.weight
25-01-23 09:02:54 | I |       - range scale = [    1.0000]
25-01-23 09:02:54 | I |         sum  error  = [    0.1878]
25-01-23 09:02:54 | I |         best error  = [    0.1878]
25-01-23 09:02:54 | I |     + error = [0.1878]
25-01-23 09:02:55 | I |       - range scale = [    1.0000]
25-01-23 09:02:55 | I |         sum  error  = [    1.5047]
25-01-23 09:02:55 | I |         best error  = [    1.5047]
25-01-23 09:02:55 | I |     + error = [1.5047]
25-01-23 09:02:55 | I |   - Calibrating model.layers.0.mlp.up_proj.weight
25-01-23 09:02:56 | I |       - range scale = [    1.0000]
25-01-23 09:02:56 | I |         sum  error  = [    1.0500]
25-01-23 09:02:56 | I |         best error  = [    1.0500]
25-01-23 09:02:56 | I |     + error = [1.0500]
25-01-23 09:02:57 | I |       - range scale = [    1.0000]
25-01-23 09:02:57 | I |         sum  error  = [   11.1648]
25-01-23 09:02:57 | I |         best error  = [   11.1648]
25-01-23 09:02:57 | I |     + error = [11.1648]
25-01-23 09:02:57 | I |   - Calibrating model.layers.0.mlp.gate_proj.weight
25-01-23 09:02:58 | I |       - range scale = [    1.0000]
25-01-23 09:02:58 | I |         sum  error  = [    1.1119]
25-01-23 09:02:58 | I |         best error  = [    1.1119]
25-01-23 09:02:58 | I |     + error = [1.1119]
25-01-23 09:02:59 | I |       - range scale = [    1.0000]
25-01-23 09:02:59 | I |         sum  error  = [   11.5412]
25-01-23 09:02:59 | I |         best error  = [   11.5412]
25-01-23 09:02:59 | I |     + error = [11.5412]
25-01-23 09:02:59 | I |   - Calibrating model.layers.0.mlp.down_proj.weight
25-01-23 09:03:00 | I |       - range scale = [    1.0000]
25-01-23 09:03:00 | I |         sum  error  = [    0.1666]
25-01-23 09:03:00 | I |         best error  = [    0.1666]
25-01-23 09:03:00 | I |     + error = [0.1666]
25-01-23 09:03:01 | I |       - range scale = [    1.0000]
25-01-23 09:03:01 | I |         sum  error  = [    1.6293]
25-01-23 09:03:01 | I |         best error  = [    1.6293]
25-01-23 09:03:01 | I |     + error = [1.6293]
25-01-23 09:03:01 | I |   - Quantizing model.layers.0.self_attn.q_proj.weight
25-01-23 09:03:03 | I |   - Quantizing model.layers.0.self_attn.k_proj.weight
25-01-23 09:03:06 | I |   - Quantizing model.layers.0.self_attn.v_proj.weight
25-01-23 09:03:08 | I |   - Quantizing model.layers.0.self_attn.o_proj.weight
25-01-23 09:03:10 | I |   - Quantizing model.layers.0.mlp.up_proj.weight
25-01-23 09:03:13 | I |   - Quantizing model.layers.0.mlp.gate_proj.weight
25-01-23 09:03:15 | I |   - Quantizing model.layers.0.mlp.down_proj.weight
25-01-23 09:03:21 | I | quantizing activations for layer model.layers.0
25-01-23 09:03:21 | I | collecting calibration activations in model.layers.0
25-01-23 09:03:21 | I | collecting calibration activations in model.layers.0
25-01-23 09:03:23 | I | forward this layer
25-01-23 09:03:23 | I | input_file: /data/gyy/lmquant-main/lmquant/data/data_without_preprocess_llama_7b_bs16/shard/val_input_args_layer_0/71.pt
25-01-23 09:03:23 | I | output_file: /data/gyy/lmquant-main/lmquant/data/data_without_preprocess_llama_7b_bs16/shard/val_teacher_output_layer_0/71.pt
25-01-23 09:03:23 | I | In layer model.layers.0, gradient of model.layers.0.self_attn.q_proj has nan: False
25-01-23 09:03:23 | I | In layer model.layers.0, gradient of model.layers.0.self_attn.q_proj has inf: False
25-01-23 09:03:23 | I | In layer model.layers.0, gradient of model.layers.0.self_attn.k_proj has nan: False
25-01-23 09:03:23 | I | In layer model.layers.0, gradient of model.layers.0.self_attn.k_proj has inf: False
25-01-23 09:03:23 | I | In layer model.layers.0, gradient of model.layers.0.self_attn.v_proj has nan: False
25-01-23 09:03:23 | I | In layer model.layers.0, gradient of model.layers.0.self_attn.v_proj has inf: False
25-01-23 09:03:23 | I | In layer model.layers.0, gradient of model.layers.0.self_attn.o_proj has nan: False
25-01-23 09:03:23 | I | In layer model.layers.0, gradient of model.layers.0.self_attn.o_proj has inf: False
25-01-23 09:03:23 | I | In layer model.layers.0, gradient of model.layers.0.mlp.up_proj has nan: False
25-01-23 09:03:23 | I | In layer model.layers.0, gradient of model.layers.0.mlp.up_proj has inf: False
25-01-23 09:03:23 | I | In layer model.layers.0, gradient of model.layers.0.mlp.gate_proj has nan: False
25-01-23 09:03:23 | I | In layer model.layers.0, gradient of model.layers.0.mlp.gate_proj has inf: False
25-01-23 09:03:23 | I | In layer model.layers.0, gradient of model.layers.0.mlp.down_proj has nan: False
25-01-23 09:03:23 | I | In layer model.layers.0, gradient of model.layers.0.mlp.down_proj has inf: False
25-01-23 09:03:23 | I | [67] done with optimizer step
25-01-23 09:03:23 | I | epoch 001:     72 / 819200000 loss=1.09667e-05, loss_per_token=0.0449194, loss_sum=367.98, wps=228.1, ups=0.03, wpb=8192, bsz=16, num_updates=68, lr=0.0049997, gnorm=0.212, clip=0, loss_scale=8, train_wall=36, cuda_gb_allocated=17.1, cuda_gb_reserved=18.5, cuda_gb_free=6.6, wall=5951
25-01-23 09:03:24 | I | TRAIN CURRENT LAYER_IDX = 0
25-01-23 09:03:24 | I | in layer model.layers.0
25-01-23 09:03:24 | I | quantizing weights for layer model.layers.0
25-01-23 09:03:24 | I | collecting calibration activations in model.layers.0
25-01-23 09:03:24 | I | collecting calibration activations in model.layers.0
25-01-23 09:03:24 | I | - Quantizing decoder layer model.layers.0
25-01-23 09:03:24 | I |   - Calibrating model.layers.0.self_attn.q_proj.weight
25-01-23 09:03:25 | I |       - range scale = [    1.0000]
25-01-23 09:03:25 | I |         sum  error  = [    0.0459]
25-01-23 09:03:25 | I |         best error  = [    0.0459]
25-01-23 09:03:25 | I |     + error = [0.0459]
25-01-23 09:03:26 | I |       - range scale = [    1.0000]
25-01-23 09:03:26 | I |         sum  error  = [    0.7117]
25-01-23 09:03:26 | I |         best error  = [    0.7117]
25-01-23 09:03:26 | I |     + error = [0.7117]
25-01-23 09:03:26 | I |   - Calibrating model.layers.0.self_attn.k_proj.weight
25-01-23 09:03:27 | I |       - range scale = [    1.0000]
25-01-23 09:03:27 | I |         sum  error  = [    0.0602]
25-01-23 09:03:27 | I |         best error  = [    0.0602]
25-01-23 09:03:27 | I |     + error = [0.0602]
25-01-23 09:03:27 | I |       - range scale = [    1.0000]
25-01-23 09:03:27 | I |         sum  error  = [    0.9263]
25-01-23 09:03:27 | I |         best error  = [    0.9263]
25-01-23 09:03:27 | I |     + error = [0.9263]
25-01-23 09:03:27 | I |   - Calibrating model.layers.0.self_attn.v_proj.weight
25-01-23 09:03:28 | I |       - range scale = [    1.0000]
25-01-23 09:03:28 | I |         sum  error  = [    0.7150]
25-01-23 09:03:28 | I |         best error  = [    0.7150]
25-01-23 09:03:28 | I |     + error = [0.7150]
25-01-23 09:03:29 | I |       - range scale = [    1.0000]
25-01-23 09:03:29 | I |         sum  error  = [    6.6666]
25-01-23 09:03:29 | I |         best error  = [    6.6666]
25-01-23 09:03:29 | I |     + error = [6.6666]
25-01-23 09:03:29 | I |   - Calibrating model.layers.0.self_attn.o_proj.weight
25-01-23 09:03:30 | I |       - range scale = [    1.0000]
25-01-23 09:03:30 | I |         sum  error  = [    0.1953]
25-01-23 09:03:30 | I |         best error  = [    0.1953]
25-01-23 09:03:30 | I |     + error = [0.1953]
25-01-23 09:03:31 | I |       - range scale = [    1.0000]
25-01-23 09:03:31 | I |         sum  error  = [    1.5688]
25-01-23 09:03:31 | I |         best error  = [    1.5688]
25-01-23 09:03:31 | I |     + error = [1.5688]
25-01-23 09:03:31 | I |   - Calibrating model.layers.0.mlp.up_proj.weight
25-01-23 09:03:32 | I |       - range scale = [    1.0000]
25-01-23 09:03:32 | I |         sum  error  = [    1.0313]
25-01-23 09:03:32 | I |         best error  = [    1.0313]
25-01-23 09:03:32 | I |     + error = [1.0313]
25-01-23 09:03:33 | I |       - range scale = [    1.0000]
25-01-23 09:03:33 | I |         sum  error  = [   10.9651]
25-01-23 09:03:33 | I |         best error  = [   10.9651]
25-01-23 09:03:33 | I |     + error = [10.9651]
25-01-23 09:03:33 | I |   - Calibrating model.layers.0.mlp.gate_proj.weight
25-01-23 09:03:34 | I |       - range scale = [    1.0000]
25-01-23 09:03:34 | I |         sum  error  = [    1.0916]
25-01-23 09:03:34 | I |         best error  = [    1.0916]
25-01-23 09:03:34 | I |     + error = [1.0916]
25-01-23 09:03:35 | I |       - range scale = [    1.0000]
25-01-23 09:03:35 | I |         sum  error  = [   11.3323]
25-01-23 09:03:35 | I |         best error  = [   11.3323]
25-01-23 09:03:35 | I |     + error = [11.3323]
25-01-23 09:03:35 | I |   - Calibrating model.layers.0.mlp.down_proj.weight
25-01-23 09:03:36 | I |       - range scale = [    1.0000]
25-01-23 09:03:36 | I |         sum  error  = [    0.1637]
25-01-23 09:03:36 | I |         best error  = [    0.1637]
25-01-23 09:03:36 | I |     + error = [0.1637]
25-01-23 09:03:37 | I |       - range scale = [    1.0000]
25-01-23 09:03:37 | I |         sum  error  = [    1.6001]
25-01-23 09:03:37 | I |         best error  = [    1.6001]
25-01-23 09:03:37 | I |     + error = [1.6001]
25-01-23 09:03:37 | I |   - Quantizing model.layers.0.self_attn.q_proj.weight
25-01-23 09:03:39 | I |   - Quantizing model.layers.0.self_attn.k_proj.weight
25-01-23 09:03:42 | I |   - Quantizing model.layers.0.self_attn.v_proj.weight
25-01-23 09:03:44 | I |   - Quantizing model.layers.0.self_attn.o_proj.weight
25-01-23 09:03:46 | I |   - Quantizing model.layers.0.mlp.up_proj.weight
25-01-23 09:03:49 | I |   - Quantizing model.layers.0.mlp.gate_proj.weight
25-01-23 09:03:51 | I |   - Quantizing model.layers.0.mlp.down_proj.weight
25-01-23 09:03:57 | I | quantizing activations for layer model.layers.0
25-01-23 09:03:57 | I | collecting calibration activations in model.layers.0
25-01-23 09:03:57 | I | collecting calibration activations in model.layers.0
25-01-23 09:03:59 | I | forward this layer
25-01-23 09:03:59 | I | input_file: /data/gyy/lmquant-main/lmquant/data/data_without_preprocess_llama_7b_bs16/shard/val_input_args_layer_0/72.pt
25-01-23 09:03:59 | I | output_file: /data/gyy/lmquant-main/lmquant/data/data_without_preprocess_llama_7b_bs16/shard/val_teacher_output_layer_0/72.pt
25-01-23 09:03:59 | I | In layer model.layers.0, gradient of model.layers.0.self_attn.q_proj has nan: False
25-01-23 09:03:59 | I | In layer model.layers.0, gradient of model.layers.0.self_attn.q_proj has inf: False
25-01-23 09:03:59 | I | In layer model.layers.0, gradient of model.layers.0.self_attn.k_proj has nan: False
25-01-23 09:03:59 | I | In layer model.layers.0, gradient of model.layers.0.self_attn.k_proj has inf: False
25-01-23 09:03:59 | I | In layer model.layers.0, gradient of model.layers.0.self_attn.v_proj has nan: False
25-01-23 09:03:59 | I | In layer model.layers.0, gradient of model.layers.0.self_attn.v_proj has inf: False
25-01-23 09:03:59 | I | In layer model.layers.0, gradient of model.layers.0.self_attn.o_proj has nan: False
25-01-23 09:03:59 | I | In layer model.layers.0, gradient of model.layers.0.self_attn.o_proj has inf: False
25-01-23 09:03:59 | I | In layer model.layers.0, gradient of model.layers.0.mlp.up_proj has nan: False
25-01-23 09:03:59 | I | In layer model.layers.0, gradient of model.layers.0.mlp.up_proj has inf: False
25-01-23 09:03:59 | I | In layer model.layers.0, gradient of model.layers.0.mlp.gate_proj has nan: False
25-01-23 09:03:59 | I | In layer model.layers.0, gradient of model.layers.0.mlp.gate_proj has inf: False
25-01-23 09:03:59 | I | In layer model.layers.0, gradient of model.layers.0.mlp.down_proj has nan: False
25-01-23 09:03:59 | I | In layer model.layers.0, gradient of model.layers.0.mlp.down_proj has inf: False
25-01-23 09:03:59 | I | [68] done with optimizer step
25-01-23 09:03:59 | I | epoch 001:     73 / 819200000 loss=1.08423e-05, loss_per_token=0.0444099, loss_sum=363.806, wps=227.3, ups=0.03, wpb=8192, bsz=16, num_updates=69, lr=0.00499968, gnorm=0.124, clip=0, loss_scale=8, train_wall=36, cuda_gb_allocated=17.1, cuda_gb_reserved=18.7, cuda_gb_free=6.6, wall=5987
25-01-23 09:04:00 | I | TRAIN CURRENT LAYER_IDX = 0
25-01-23 09:04:00 | I | in layer model.layers.0
25-01-23 09:04:00 | I | quantizing weights for layer model.layers.0
25-01-23 09:04:00 | I | collecting calibration activations in model.layers.0
25-01-23 09:04:00 | I | collecting calibration activations in model.layers.0
25-01-23 09:04:00 | I | - Quantizing decoder layer model.layers.0
25-01-23 09:04:00 | I |   - Calibrating model.layers.0.self_attn.q_proj.weight
25-01-23 09:04:01 | I |       - range scale = [    1.0000]
25-01-23 09:04:01 | I |         sum  error  = [    0.0450]
25-01-23 09:04:01 | I |         best error  = [    0.0450]
25-01-23 09:04:01 | I |     + error = [0.0450]
25-01-23 09:04:02 | I |       - range scale = [    1.0000]
25-01-23 09:04:02 | I |         sum  error  = [    0.6938]
25-01-23 09:04:02 | I |         best error  = [    0.6938]
25-01-23 09:04:02 | I |     + error = [0.6938]
25-01-23 09:04:02 | I |   - Calibrating model.layers.0.self_attn.k_proj.weight
25-01-23 09:04:03 | I |       - range scale = [    1.0000]
25-01-23 09:04:03 | I |         sum  error  = [    0.0578]
25-01-23 09:04:03 | I |         best error  = [    0.0578]
25-01-23 09:04:03 | I |     + error = [0.0578]
25-01-23 09:04:03 | I |       - range scale = [    1.0000]
25-01-23 09:04:03 | I |         sum  error  = [    0.9127]
25-01-23 09:04:03 | I |         best error  = [    0.9127]
25-01-23 09:04:03 | I |     + error = [0.9127]
25-01-23 09:04:04 | I |   - Calibrating model.layers.0.self_attn.v_proj.weight
25-01-23 09:04:04 | I |       - range scale = [    1.0000]
25-01-23 09:04:04 | I |         sum  error  = [    0.6895]
25-01-23 09:04:04 | I |         best error  = [    0.6895]
25-01-23 09:04:04 | I |     + error = [0.6895]
25-01-23 09:04:05 | I |       - range scale = [    1.0000]
25-01-23 09:04:05 | I |         sum  error  = [    6.3930]
25-01-23 09:04:05 | I |         best error  = [    6.3930]
25-01-23 09:04:05 | I |     + error = [6.3930]
25-01-23 09:04:05 | I |   - Calibrating model.layers.0.self_attn.o_proj.weight
25-01-23 09:04:06 | I |       - range scale = [    1.0000]
25-01-23 09:04:06 | I |         sum  error  = [    0.1879]
25-01-23 09:04:06 | I |         best error  = [    0.1879]
25-01-23 09:04:06 | I |     + error = [0.1879]
25-01-23 09:04:07 | I |       - range scale = [    1.0000]
25-01-23 09:04:07 | I |         sum  error  = [    1.5019]
25-01-23 09:04:07 | I |         best error  = [    1.5019]
25-01-23 09:04:07 | I |     + error = [1.5019]
25-01-23 09:04:07 | I |   - Calibrating model.layers.0.mlp.up_proj.weight
25-01-23 09:04:08 | I |       - range scale = [    1.0000]
25-01-23 09:04:08 | I |         sum  error  = [    1.0310]
25-01-23 09:04:08 | I |         best error  = [    1.0310]
25-01-23 09:04:08 | I |     + error = [1.0310]
25-01-23 09:04:09 | I |       - range scale = [    1.0000]
25-01-23 09:04:09 | I |         sum  error  = [   10.9704]
25-01-23 09:04:09 | I |         best error  = [   10.9704]
25-01-23 09:04:09 | I |     + error = [10.9704]
25-01-23 09:04:09 | I |   - Calibrating model.layers.0.mlp.gate_proj.weight
25-01-23 09:04:10 | I |       - range scale = [    1.0000]
25-01-23 09:04:10 | I |         sum  error  = [    1.0941]
25-01-23 09:04:10 | I |         best error  = [    1.0941]
25-01-23 09:04:10 | I |     + error = [1.0941]
25-01-23 09:04:11 | I |       - range scale = [    1.0000]
25-01-23 09:04:11 | I |         sum  error  = [   11.3421]
25-01-23 09:04:11 | I |         best error  = [   11.3421]
25-01-23 09:04:11 | I |     + error = [11.3421]
25-01-23 09:04:11 | I |   - Calibrating model.layers.0.mlp.down_proj.weight
25-01-23 09:04:12 | I |       - range scale = [    1.0000]
25-01-23 09:04:12 | I |         sum  error  = [    0.1561]
25-01-23 09:04:12 | I |         best error  = [    0.1561]
25-01-23 09:04:12 | I |     + error = [0.1561]
25-01-23 09:04:13 | I |       - range scale = [    1.0000]
25-01-23 09:04:13 | I |         sum  error  = [    1.5200]
25-01-23 09:04:13 | I |         best error  = [    1.5200]
25-01-23 09:04:13 | I |     + error = [1.5200]
25-01-23 09:04:13 | I |   - Quantizing model.layers.0.self_attn.q_proj.weight
25-01-23 09:04:16 | I |   - Quantizing model.layers.0.self_attn.k_proj.weight
25-01-23 09:04:18 | I |   - Quantizing model.layers.0.self_attn.v_proj.weight
25-01-23 09:04:20 | I |   - Quantizing model.layers.0.self_attn.o_proj.weight
25-01-23 09:04:22 | I |   - Quantizing model.layers.0.mlp.up_proj.weight
25-01-23 09:04:25 | I |   - Quantizing model.layers.0.mlp.gate_proj.weight
25-01-23 09:04:27 | I |   - Quantizing model.layers.0.mlp.down_proj.weight
25-01-23 09:04:33 | I | quantizing activations for layer model.layers.0
25-01-23 09:04:33 | I | collecting calibration activations in model.layers.0
25-01-23 09:04:33 | I | collecting calibration activations in model.layers.0
25-01-23 09:04:35 | I | forward this layer
25-01-23 09:04:35 | I | input_file: /data/gyy/lmquant-main/lmquant/data/data_without_preprocess_llama_7b_bs16/shard/val_input_args_layer_0/73.pt
25-01-23 09:04:35 | I | output_file: /data/gyy/lmquant-main/lmquant/data/data_without_preprocess_llama_7b_bs16/shard/val_teacher_output_layer_0/73.pt
25-01-23 09:04:35 | I | In layer model.layers.0, gradient of model.layers.0.self_attn.q_proj has nan: False
25-01-23 09:04:35 | I | In layer model.layers.0, gradient of model.layers.0.self_attn.q_proj has inf: False
25-01-23 09:04:35 | I | In layer model.layers.0, gradient of model.layers.0.self_attn.k_proj has nan: False
25-01-23 09:04:35 | I | In layer model.layers.0, gradient of model.layers.0.self_attn.k_proj has inf: False
25-01-23 09:04:35 | I | In layer model.layers.0, gradient of model.layers.0.self_attn.v_proj has nan: False
25-01-23 09:04:35 | I | In layer model.layers.0, gradient of model.layers.0.self_attn.v_proj has inf: False
25-01-23 09:04:35 | I | In layer model.layers.0, gradient of model.layers.0.self_attn.o_proj has nan: False
25-01-23 09:04:35 | I | In layer model.layers.0, gradient of model.layers.0.self_attn.o_proj has inf: False
25-01-23 09:04:35 | I | In layer model.layers.0, gradient of model.layers.0.mlp.up_proj has nan: False
25-01-23 09:04:35 | I | In layer model.layers.0, gradient of model.layers.0.mlp.up_proj has inf: False
25-01-23 09:04:35 | I | In layer model.layers.0, gradient of model.layers.0.mlp.gate_proj has nan: False
25-01-23 09:04:35 | I | In layer model.layers.0, gradient of model.layers.0.mlp.gate_proj has inf: False
25-01-23 09:04:36 | I | In layer model.layers.0, gradient of model.layers.0.mlp.down_proj has nan: False
25-01-23 09:04:36 | I | In layer model.layers.0, gradient of model.layers.0.mlp.down_proj has inf: False
25-01-23 09:04:36 | I | [69] done with optimizer step
25-01-23 09:04:36 | I | epoch 001:     74 / 819200000 loss=1.09391e-05, loss_per_token=0.0448066, loss_sum=367.056, wps=227.3, ups=0.03, wpb=8192, bsz=16, num_updates=70, lr=0.00499967, gnorm=0.153, clip=0, loss_scale=8, train_wall=36, cuda_gb_allocated=17.1, cuda_gb_reserved=18.8, cuda_gb_free=6.6, wall=6023
25-01-23 09:04:36 | I | begin validation on "valid" subset on rank 0
25-01-23 09:04:36 | I | got valid iterator on "valid" subset on rank 0
25-01-23 09:04:36 | I | Valid: Start iterating over samples
25-01-23 09:04:36 | W |   `pretrained` model kwarg is not of type `str`. Many other model arguments may be ignored. Please do not launch via accelerate or use `parallelize=True` if passing an existing model this way.
25-01-23 09:04:36 | W |   Passed an already-initialized model through `pretrained`, assuming single-process call to evaluate() or custom distributed integration
25-01-23 09:04:36 | I | - Evaluator: gptq
25-01-23 09:04:36 | I | - Tasks: ['wikitext', 'val_valid']
25-01-23 09:04:36 | I | - Batch_size: 8
25-01-23 09:04:36 | I |   + Max_seq_length: 2048
25-01-23 09:06:11 | I |     - Results:
25-01-23 09:06:11 | I |       |  Task   |Version|    Metric     |Value |   |Stderr|
25-01-23 09:06:11 | I |       |---------|------:|---------------|-----:|---|-----:|
25-01-23 09:06:11 | I |       |wikitext |      1|word_perplexity|5.4846|±  |5.4846|
25-01-23 09:06:11 | I |       |val_valid|      1|word_perplexity|5.0289|±  |5.0289|
25-01-23 09:06:11 | I |       
25-01-23 09:06:11 | I |   + Max_seq_length: 4096
25-01-23 09:07:44 | I |     - Results:
25-01-23 09:07:44 | I |       |  Task   |Version|    Metric     |Value |   |Stderr|
25-01-23 09:07:44 | I |       |---------|------:|---------------|-----:|---|-----:|
25-01-23 09:07:44 | I |       |wikitext |      1|word_perplexity|5.1255|±  |5.1255|
25-01-23 09:07:44 | I |       |val_valid|      1|word_perplexity|4.8253|±  |4.8253|
25-01-23 09:07:44 | I |       
25-01-23 09:07:44 | I | in valid, quantize current layer weights
25-01-23 09:07:45 | W | Repo card metadata block was not found. Setting CardData to empty.
25-01-23 09:08:29 | I | collecting calibration activations in model.layers.0
25-01-23 09:08:29 | I | collecting calibration activations in model.layers.0
25-01-23 09:08:29 | I | collecting calibration activations in model.layers.0
25-01-23 09:08:29 | I | collecting calibration activations in model.layers.0
25-01-23 09:08:29 | I | collecting calibration activations in model.layers.0
25-01-23 09:08:29 | I | collecting calibration activations in model.layers.0
25-01-23 09:08:29 | I | collecting calibration activations in model.layers.0
25-01-23 09:08:30 | I | collecting calibration activations in model.layers.0
25-01-23 09:08:30 | I | collecting calibration activations in model.layers.0
25-01-23 09:08:30 | I | collecting calibration activations in model.layers.0
25-01-23 09:08:30 | I | collecting calibration activations in model.layers.0
25-01-23 09:08:30 | I | collecting calibration activations in model.layers.0
25-01-23 09:08:30 | I | collecting calibration activations in model.layers.0
25-01-23 09:08:30 | I | collecting calibration activations in model.layers.0
25-01-23 09:08:30 | I | collecting calibration activations in model.layers.0
25-01-23 09:08:30 | I | collecting calibration activations in model.layers.0
25-01-23 09:08:30 | I | collecting calibration activations in model.layers.0
25-01-23 09:08:30 | I | collecting calibration activations in model.layers.0
25-01-23 09:08:30 | I | collecting calibration activations in model.layers.0
25-01-23 09:08:30 | I | collecting calibration activations in model.layers.0
25-01-23 09:08:30 | I | collecting calibration activations in model.layers.0
25-01-23 09:08:30 | I | collecting calibration activations in model.layers.0
25-01-23 09:08:30 | I | collecting calibration activations in model.layers.0
25-01-23 09:08:30 | I | collecting calibration activations in model.layers.0
25-01-23 09:08:30 | I | collecting calibration activations in model.layers.0
25-01-23 09:08:30 | I | collecting calibration activations in model.layers.0
25-01-23 09:08:30 | I | collecting calibration activations in model.layers.0
25-01-23 09:08:31 | I | collecting calibration activations in model.layers.0
25-01-23 09:08:31 | I | collecting calibration activations in model.layers.0
25-01-23 09:08:31 | I | collecting calibration activations in model.layers.0
25-01-23 09:08:31 | I | collecting calibration activations in model.layers.0
25-01-23 09:08:31 | I | collecting calibration activations in model.layers.0
25-01-23 09:08:31 | I | collecting calibration activations in model.layers.0
25-01-23 09:08:31 | I | collecting calibration activations in model.layers.0
25-01-23 09:08:31 | I | collecting calibration activations in model.layers.0
25-01-23 09:08:31 | I | collecting calibration activations in model.layers.0
25-01-23 09:08:31 | I | collecting calibration activations in model.layers.0
25-01-23 09:08:31 | I | collecting calibration activations in model.layers.0
25-01-23 09:08:31 | I | collecting calibration activations in model.layers.0
25-01-23 09:08:31 | I | collecting calibration activations in model.layers.0
25-01-23 09:08:31 | I | collecting calibration activations in model.layers.0
25-01-23 09:08:31 | I | collecting calibration activations in model.layers.0
25-01-23 09:08:31 | I | collecting calibration activations in model.layers.0
25-01-23 09:08:31 | I | collecting calibration activations in model.layers.0
25-01-23 09:08:31 | I | collecting calibration activations in model.layers.0
25-01-23 09:08:31 | I | collecting calibration activations in model.layers.0
25-01-23 09:08:32 | I | collecting calibration activations in model.layers.0
25-01-23 09:08:32 | I | collecting calibration activations in model.layers.0
25-01-23 09:08:32 | I | collecting calibration activations in model.layers.0
25-01-23 09:08:32 | I | collecting calibration activations in model.layers.0
25-01-23 09:08:32 | I | collecting calibration activations in model.layers.0
25-01-23 09:08:32 | I | collecting calibration activations in model.layers.0
25-01-23 09:08:32 | I | collecting calibration activations in model.layers.0
25-01-23 09:08:32 | I | collecting calibration activations in model.layers.0
25-01-23 09:08:32 | I | collecting calibration activations in model.layers.0
25-01-23 09:08:32 | I | collecting calibration activations in model.layers.0
25-01-23 09:08:32 | I | collecting calibration activations in model.layers.0
25-01-23 09:08:32 | I | collecting calibration activations in model.layers.0
25-01-23 09:08:32 | I | collecting calibration activations in model.layers.0
25-01-23 09:08:32 | I | collecting calibration activations in model.layers.0
25-01-23 09:08:32 | I | collecting calibration activations in model.layers.0
25-01-23 09:08:32 | I | collecting calibration activations in model.layers.0
25-01-23 09:08:32 | I | collecting calibration activations in model.layers.0
25-01-23 09:08:32 | I | collecting calibration activations in model.layers.0
25-01-23 09:08:32 | I | collecting calibration activations in model.layers.0
25-01-23 09:08:33 | I | collecting calibration activations in model.layers.0
25-01-23 09:08:33 | I | collecting calibration activations in model.layers.0
25-01-23 09:08:33 | I | collecting calibration activations in model.layers.0
25-01-23 09:08:33 | I | collecting calibration activations in model.layers.0
25-01-23 09:08:33 | I | collecting calibration activations in model.layers.0
25-01-23 09:08:33 | I | collecting calibration activations in model.layers.0
25-01-23 09:08:33 | I | collecting calibration activations in model.layers.0
25-01-23 09:08:33 | I | collecting calibration activations in model.layers.0
25-01-23 09:08:33 | I | collecting calibration activations in model.layers.0
25-01-23 09:08:33 | I | collecting calibration activations in model.layers.0
25-01-23 09:08:33 | I | collecting calibration activations in model.layers.0
25-01-23 09:08:33 | I | collecting calibration activations in model.layers.0
25-01-23 09:08:33 | I | collecting calibration activations in model.layers.0
25-01-23 09:08:33 | I | collecting calibration activations in model.layers.0
25-01-23 09:08:33 | I | collecting calibration activations in model.layers.0
25-01-23 09:08:33 | I | collecting calibration activations in model.layers.0
25-01-23 09:08:33 | I | collecting calibration activations in model.layers.0
25-01-23 09:08:33 | I | collecting calibration activations in model.layers.0
25-01-23 09:08:33 | I | collecting calibration activations in model.layers.0
25-01-23 09:08:33 | I | collecting calibration activations in model.layers.0
25-01-23 09:08:34 | I | collecting calibration activations in model.layers.0
25-01-23 09:08:34 | I | collecting calibration activations in model.layers.0
25-01-23 09:08:34 | I | collecting calibration activations in model.layers.0
25-01-23 09:08:34 | I | collecting calibration activations in model.layers.0
25-01-23 09:08:34 | I | collecting calibration activations in model.layers.0
25-01-23 09:08:34 | I | collecting calibration activations in model.layers.0
25-01-23 09:08:34 | I | collecting calibration activations in model.layers.0
25-01-23 09:08:34 | I | collecting calibration activations in model.layers.0
25-01-23 09:08:34 | I | collecting calibration activations in model.layers.0
25-01-23 09:08:34 | I | collecting calibration activations in model.layers.0
25-01-23 09:08:34 | I | collecting calibration activations in model.layers.0
25-01-23 09:08:34 | I | collecting calibration activations in model.layers.0
25-01-23 09:08:34 | I | collecting calibration activations in model.layers.0
25-01-23 09:08:34 | I | collecting calibration activations in model.layers.0
25-01-23 09:08:34 | I | collecting calibration activations in model.layers.0
25-01-23 09:08:34 | I | collecting calibration activations in model.layers.0
25-01-23 09:08:34 | I | collecting calibration activations in model.layers.0
25-01-23 09:08:34 | I | collecting calibration activations in model.layers.0
25-01-23 09:08:34 | I | collecting calibration activations in model.layers.0
25-01-23 09:08:35 | I | collecting calibration activations in model.layers.0
25-01-23 09:08:35 | I | collecting calibration activations in model.layers.0
25-01-23 09:08:35 | I | collecting calibration activations in model.layers.0
25-01-23 09:08:35 | I | collecting calibration activations in model.layers.0
25-01-23 09:08:35 | I | collecting calibration activations in model.layers.0
25-01-23 09:08:35 | I | collecting calibration activations in model.layers.0
25-01-23 09:08:35 | I | collecting calibration activations in model.layers.0
25-01-23 09:08:35 | I | collecting calibration activations in model.layers.0
25-01-23 09:08:35 | I | collecting calibration activations in model.layers.0
25-01-23 09:08:35 | I | collecting calibration activations in model.layers.0
25-01-23 09:08:35 | I | collecting calibration activations in model.layers.0
25-01-23 09:08:35 | I | collecting calibration activations in model.layers.0
25-01-23 09:08:35 | I | collecting calibration activations in model.layers.0
25-01-23 09:08:35 | I | collecting calibration activations in model.layers.0
25-01-23 09:08:35 | I | collecting calibration activations in model.layers.0
25-01-23 09:08:35 | I | collecting calibration activations in model.layers.0
25-01-23 09:08:35 | I | collecting calibration activations in model.layers.0
25-01-23 09:08:35 | I | collecting calibration activations in model.layers.0
25-01-23 09:08:35 | I | collecting calibration activations in model.layers.0
25-01-23 09:08:36 | I | collecting calibration activations in model.layers.0
25-01-23 09:08:36 | I | collecting calibration activations in model.layers.0
25-01-23 09:08:36 | I | collecting calibration activations in model.layers.0
25-01-23 09:08:36 | I | collecting calibration activations in model.layers.0
25-01-23 09:08:36 | I | collecting calibration activations in model.layers.0
25-01-23 09:08:37 | I |       - range scale = [    1.0000]
25-01-23 09:08:37 | I |         sum  error  = [    0.1538]
25-01-23 09:08:37 | I |         best error  = [    0.1538]
25-01-23 09:08:37 | I |     + error = [0.1538]
25-01-23 09:08:39 | I |       - range scale = [    1.0000]
25-01-23 09:08:39 | I |         sum  error  = [    2.6338]
25-01-23 09:08:39 | I |         best error  = [    2.6338]
25-01-23 09:08:39 | I |     + error = [2.6338]
25-01-23 09:08:41 | I |       - range scale = [    1.0000]
25-01-23 09:08:41 | I |         sum  error  = [    0.1861]
25-01-23 09:08:41 | I |         best error  = [    0.1861]
25-01-23 09:08:41 | I |     + error = [0.1861]
25-01-23 09:08:42 | I |       - range scale = [    1.0000]
25-01-23 09:08:42 | I |         sum  error  = [    3.6184]
25-01-23 09:08:42 | I |         best error  = [    3.6184]
25-01-23 09:08:42 | I |     + error = [3.6184]
25-01-23 09:08:43 | I |       - range scale = [    1.0000]
25-01-23 09:08:43 | I |         sum  error  = [    0.6459]
25-01-23 09:08:43 | I |         best error  = [    0.6459]
25-01-23 09:08:43 | I |     + error = [0.6459]
25-01-23 09:08:44 | I |       - range scale = [    1.0000]
25-01-23 09:08:44 | I |         sum  error  = [    5.9453]
25-01-23 09:08:44 | I |         best error  = [    5.9453]
25-01-23 09:08:44 | I |     + error = [5.9453]
25-01-23 09:08:45 | I |       - range scale = [    1.0000]
25-01-23 09:08:45 | I |         sum  error  = [    0.1449]
25-01-23 09:08:45 | I |         best error  = [    0.1449]
25-01-23 09:08:45 | I |     + error = [0.1449]
25-01-23 09:08:46 | I |       - range scale = [    1.0000]
25-01-23 09:08:46 | I |         sum  error  = [    1.1902]
25-01-23 09:08:46 | I |         best error  = [    1.1902]
25-01-23 09:08:46 | I |     + error = [1.1902]
25-01-23 09:08:46 | I |       - range scale = [    1.0000]
25-01-23 09:08:46 | I |         sum  error  = [    1.0759]
25-01-23 09:08:46 | I |         best error  = [    1.0759]
25-01-23 09:08:47 | I |     + error = [1.0759]
25-01-23 09:08:48 | I |       - range scale = [    1.0000]
25-01-23 09:08:48 | I |         sum  error  = [   11.4752]
25-01-23 09:08:48 | I |         best error  = [   11.4752]
25-01-23 09:08:48 | I |     + error = [11.4752]
25-01-23 09:08:49 | I |       - range scale = [    1.0000]
25-01-23 09:08:49 | I |         sum  error  = [    1.1428]
25-01-23 09:08:49 | I |         best error  = [    1.1428]
25-01-23 09:08:49 | I |     + error = [1.1428]
25-01-23 09:08:50 | I |       - range scale = [    1.0000]
25-01-23 09:08:50 | I |         sum  error  = [   11.8445]
25-01-23 09:08:50 | I |         best error  = [   11.8445]
25-01-23 09:08:50 | I |     + error = [11.8445]
25-01-23 09:08:51 | I |       - range scale = [    1.0000]
25-01-23 09:08:51 | I |         sum  error  = [    0.1753]
25-01-23 09:08:51 | I |         best error  = [    0.1753]
25-01-23 09:08:51 | I |     + error = [0.1753]
25-01-23 09:08:52 | I |       - range scale = [    1.0000]
25-01-23 09:08:52 | I |         sum  error  = [    1.7283]
25-01-23 09:08:52 | I |         best error  = [    1.7283]
25-01-23 09:08:52 | I |     + error = [1.7283]
25-01-23 09:09:16 | I | in valid, quantize current layer acts
25-01-23 09:09:18 | I | collecting calibration activations in model.layers.0
25-01-23 09:09:18 | I | collecting calibration activations in model.layers.0
25-01-23 09:09:18 | I | collecting calibration activations in model.layers.0
25-01-23 09:09:18 | I | collecting calibration activations in model.layers.0
25-01-23 09:09:18 | I | collecting calibration activations in model.layers.0
25-01-23 09:09:18 | I | collecting calibration activations in model.layers.0
25-01-23 09:09:18 | I | collecting calibration activations in model.layers.0
25-01-23 09:09:19 | I | collecting calibration activations in model.layers.0
25-01-23 09:09:19 | I | collecting calibration activations in model.layers.0
25-01-23 09:09:19 | I | collecting calibration activations in model.layers.0
25-01-23 09:09:19 | I | collecting calibration activations in model.layers.0
25-01-23 09:09:19 | I | collecting calibration activations in model.layers.0
25-01-23 09:09:19 | I | collecting calibration activations in model.layers.0
25-01-23 09:09:19 | I | collecting calibration activations in model.layers.0
25-01-23 09:09:19 | I | collecting calibration activations in model.layers.0
25-01-23 09:09:19 | I | collecting calibration activations in model.layers.0
25-01-23 09:09:19 | I | collecting calibration activations in model.layers.0
25-01-23 09:09:19 | I | collecting calibration activations in model.layers.0
25-01-23 09:09:19 | I | collecting calibration activations in model.layers.0
25-01-23 09:09:19 | I | collecting calibration activations in model.layers.0
25-01-23 09:09:19 | I | collecting calibration activations in model.layers.0
25-01-23 09:09:19 | I | collecting calibration activations in model.layers.0
25-01-23 09:09:19 | I | collecting calibration activations in model.layers.0
25-01-23 09:09:20 | I | collecting calibration activations in model.layers.0
25-01-23 09:09:20 | I | collecting calibration activations in model.layers.0
25-01-23 09:09:20 | I | collecting calibration activations in model.layers.0
25-01-23 09:09:20 | I | collecting calibration activations in model.layers.0
25-01-23 09:09:20 | I | collecting calibration activations in model.layers.0
25-01-23 09:09:20 | I | collecting calibration activations in model.layers.0
25-01-23 09:09:20 | I | collecting calibration activations in model.layers.0
25-01-23 09:09:20 | I | collecting calibration activations in model.layers.0
25-01-23 09:09:20 | I | collecting calibration activations in model.layers.0
25-01-23 09:09:20 | I | collecting calibration activations in model.layers.0
25-01-23 09:09:20 | I | collecting calibration activations in model.layers.0
25-01-23 09:09:20 | I | collecting calibration activations in model.layers.0
25-01-23 09:09:20 | I | collecting calibration activations in model.layers.0
25-01-23 09:09:20 | I | collecting calibration activations in model.layers.0
25-01-23 09:09:20 | I | collecting calibration activations in model.layers.0
25-01-23 09:09:21 | I | collecting calibration activations in model.layers.0
25-01-23 09:09:21 | I | collecting calibration activations in model.layers.0
25-01-23 09:09:21 | I | collecting calibration activations in model.layers.0
25-01-23 09:09:21 | I | collecting calibration activations in model.layers.0
25-01-23 09:09:21 | I | collecting calibration activations in model.layers.0
25-01-23 09:09:21 | I | collecting calibration activations in model.layers.0
25-01-23 09:09:21 | I | collecting calibration activations in model.layers.0
25-01-23 09:09:21 | I | collecting calibration activations in model.layers.0
25-01-23 09:09:21 | I | collecting calibration activations in model.layers.0
25-01-23 09:09:21 | I | collecting calibration activations in model.layers.0
25-01-23 09:09:21 | I | collecting calibration activations in model.layers.0
25-01-23 09:09:21 | I | collecting calibration activations in model.layers.0
25-01-23 09:09:21 | I | collecting calibration activations in model.layers.0
25-01-23 09:09:21 | I | collecting calibration activations in model.layers.0
25-01-23 09:09:21 | I | collecting calibration activations in model.layers.0
25-01-23 09:09:21 | I | collecting calibration activations in model.layers.0
25-01-23 09:09:22 | I | collecting calibration activations in model.layers.0
25-01-23 09:09:22 | I | collecting calibration activations in model.layers.0
25-01-23 09:09:22 | I | collecting calibration activations in model.layers.0
25-01-23 09:09:22 | I | collecting calibration activations in model.layers.0
25-01-23 09:09:22 | I | collecting calibration activations in model.layers.0
25-01-23 09:09:22 | I | collecting calibration activations in model.layers.0
25-01-23 09:09:22 | I | collecting calibration activations in model.layers.0
25-01-23 09:09:22 | I | collecting calibration activations in model.layers.0
25-01-23 09:09:22 | I | collecting calibration activations in model.layers.0
25-01-23 09:09:22 | I | collecting calibration activations in model.layers.0
25-01-23 09:09:22 | I | collecting calibration activations in model.layers.0
25-01-23 09:09:22 | I | collecting calibration activations in model.layers.0
25-01-23 09:09:22 | I | collecting calibration activations in model.layers.0
25-01-23 09:09:22 | I | collecting calibration activations in model.layers.0
25-01-23 09:09:22 | I | collecting calibration activations in model.layers.0
25-01-23 09:09:22 | I | collecting calibration activations in model.layers.0
25-01-23 09:09:23 | I | collecting calibration activations in model.layers.0
25-01-23 09:09:23 | I | collecting calibration activations in model.layers.0
25-01-23 09:09:23 | I | collecting calibration activations in model.layers.0
25-01-23 09:09:23 | I | collecting calibration activations in model.layers.0
25-01-23 09:09:23 | I | collecting calibration activations in model.layers.0
25-01-23 09:09:23 | I | collecting calibration activations in model.layers.0
25-01-23 09:09:23 | I | collecting calibration activations in model.layers.0
25-01-23 09:09:23 | I | collecting calibration activations in model.layers.0
25-01-23 09:09:23 | I | collecting calibration activations in model.layers.0
25-01-23 09:09:23 | I | collecting calibration activations in model.layers.0
25-01-23 09:09:23 | I | collecting calibration activations in model.layers.0
25-01-23 09:09:23 | I | collecting calibration activations in model.layers.0
25-01-23 09:09:23 | I | collecting calibration activations in model.layers.0
25-01-23 09:09:23 | I | collecting calibration activations in model.layers.0
25-01-23 09:09:23 | I | collecting calibration activations in model.layers.0
25-01-23 09:09:24 | I | collecting calibration activations in model.layers.0
25-01-23 09:09:24 | I | collecting calibration activations in model.layers.0
25-01-23 09:09:24 | I | collecting calibration activations in model.layers.0
25-01-23 09:09:24 | I | collecting calibration activations in model.layers.0
25-01-23 09:09:24 | I | collecting calibration activations in model.layers.0
25-01-23 09:09:24 | I | collecting calibration activations in model.layers.0
25-01-23 09:09:24 | I | collecting calibration activations in model.layers.0
25-01-23 09:09:24 | I | collecting calibration activations in model.layers.0
25-01-23 09:09:24 | I | collecting calibration activations in model.layers.0
25-01-23 09:09:24 | I | collecting calibration activations in model.layers.0
25-01-23 09:09:24 | I | collecting calibration activations in model.layers.0
25-01-23 09:09:24 | I | collecting calibration activations in model.layers.0
25-01-23 09:09:24 | I | collecting calibration activations in model.layers.0
25-01-23 09:09:24 | I | collecting calibration activations in model.layers.0
25-01-23 09:09:24 | I | collecting calibration activations in model.layers.0
25-01-23 09:09:24 | I | collecting calibration activations in model.layers.0
25-01-23 09:09:25 | I | collecting calibration activations in model.layers.0
25-01-23 09:09:25 | I | collecting calibration activations in model.layers.0
25-01-23 09:09:25 | I | collecting calibration activations in model.layers.0
25-01-23 09:09:25 | I | collecting calibration activations in model.layers.0
25-01-23 09:09:25 | I | collecting calibration activations in model.layers.0
25-01-23 09:09:25 | I | collecting calibration activations in model.layers.0
25-01-23 09:09:25 | I | collecting calibration activations in model.layers.0
25-01-23 09:09:25 | I | collecting calibration activations in model.layers.0
25-01-23 09:09:25 | I | collecting calibration activations in model.layers.0
25-01-23 09:09:25 | I | collecting calibration activations in model.layers.0
25-01-23 09:09:25 | I | collecting calibration activations in model.layers.0
25-01-23 09:09:25 | I | collecting calibration activations in model.layers.0
25-01-23 09:09:25 | I | collecting calibration activations in model.layers.0
25-01-23 09:09:25 | I | collecting calibration activations in model.layers.0
25-01-23 09:09:25 | I | collecting calibration activations in model.layers.0
25-01-23 09:09:25 | I | collecting calibration activations in model.layers.0
25-01-23 09:09:26 | I | collecting calibration activations in model.layers.0
25-01-23 09:09:26 | I | collecting calibration activations in model.layers.0
25-01-23 09:09:26 | I | collecting calibration activations in model.layers.0
25-01-23 09:09:26 | I | collecting calibration activations in model.layers.0
25-01-23 09:09:26 | I | collecting calibration activations in model.layers.0
25-01-23 09:09:26 | I | collecting calibration activations in model.layers.0
25-01-23 09:09:26 | I | collecting calibration activations in model.layers.0
25-01-23 09:09:26 | I | collecting calibration activations in model.layers.0
25-01-23 09:09:26 | I | collecting calibration activations in model.layers.0
25-01-23 09:09:26 | I | collecting calibration activations in model.layers.0
25-01-23 09:09:26 | I | collecting calibration activations in model.layers.0
25-01-23 09:09:28 | I | use gptq_eval(lmquant) to calculate ppl, partly quanted
25-01-23 09:09:28 | W |   `pretrained` model kwarg is not of type `str`. Many other model arguments may be ignored. Please do not launch via accelerate or use `parallelize=True` if passing an existing model this way.
25-01-23 09:09:28 | W |   Passed an already-initialized model through `pretrained`, assuming single-process call to evaluate() or custom distributed integration
25-01-23 09:09:28 | I | - Evaluator: gptq
25-01-23 09:09:28 | I | - Tasks: ['wikitext', 'val_valid']
25-01-23 09:09:28 | I | - Batch_size: 8
25-01-23 09:09:28 | I |   + Max_seq_length: 2048
25-01-23 09:11:06 | I |     - Results:
25-01-23 09:11:06 | I |       |  Task   |Version|    Metric     |Value |   |Stderr|
25-01-23 09:11:06 | I |       |---------|------:|---------------|-----:|---|-----:|
25-01-23 09:11:06 | I |       |wikitext |      1|word_perplexity|5.4893|±  |5.4893|
25-01-23 09:11:06 | I |       |val_valid|      1|word_perplexity|5.0648|±  |5.0648|
25-01-23 09:11:06 | I |       
25-01-23 09:11:06 | I |   + Max_seq_length: 4096
25-01-23 09:12:41 | I |     - Results:
25-01-23 09:12:41 | I |       |  Task   |Version|    Metric     |Value |   |Stderr|
25-01-23 09:12:41 | I |       |---------|------:|---------------|-----:|---|-----:|
25-01-23 09:12:41 | I |       |wikitext |      1|word_perplexity|5.1286|±  |5.1286|
25-01-23 09:12:41 | I |       |val_valid|      1|word_perplexity|4.8612|±  |4.8612|
25-01-23 09:12:41 | I |       
25-01-23 09:13:41 | I | epoch 001 | valid on 'valid' subset:    102 / 819200000 loss_valid=2.15, lmquant_ppl_result_val=-1, lmquant_ppl_result_wikitext=-1, ppl=4.44, wps=7238.3, wpb=4058.2, bsz=2, num_updates=70, lmquant_ppl_wikitext_all_quanted=5.48459, lmquant_ppl_val_all_quanted=5.02891, lmquant_ppl_wikitext_partly_quanted=5.48931, lmquant_ppl_val_partly_quanted=5.06481
25-01-23 09:13:41 | I | epoch 001 | valid on 'valid' subset | loss_valid 2.15 | lmquant_ppl_result_val -1 | lmquant_ppl_result_wikitext -1 | ppl 4.44 | wps 7238.3 | wpb 4058.2 | bsz 2 | num_updates 70 | lmquant_ppl_wikitext_all_quanted 5.48459 | lmquant_ppl_val_all_quanted 5.02891 | lmquant_ppl_wikitext_partly_quanted 5.48931 | lmquant_ppl_val_partly_quanted 5.06481
25-01-23 09:13:41 | I | TRAIN CURRENT LAYER_IDX = 0
25-01-23 09:13:41 | I | in layer model.layers.0
25-01-23 09:13:41 | I | quantizing weights for layer model.layers.0
25-01-23 09:13:41 | I | collecting calibration activations in model.layers.0
25-01-23 09:13:41 | I | collecting calibration activations in model.layers.0
25-01-23 09:13:42 | I | - Quantizing decoder layer model.layers.0
25-01-23 09:13:42 | I |   - Calibrating model.layers.0.self_attn.q_proj.weight
25-01-23 09:13:42 | I |       - range scale = [    1.0000]
25-01-23 09:13:42 | I |         sum  error  = [    0.0454]
25-01-23 09:13:42 | I |         best error  = [    0.0454]
25-01-23 09:13:42 | I |     + error = [0.0454]
25-01-23 09:13:43 | I |       - range scale = [    1.0000]
25-01-23 09:13:43 | I |         sum  error  = [    0.7071]
25-01-23 09:13:43 | I |         best error  = [    0.7071]
25-01-23 09:13:43 | I |     + error = [0.7071]
25-01-23 09:13:43 | I |   - Calibrating model.layers.0.self_attn.k_proj.weight
25-01-23 09:13:44 | I |       - range scale = [    1.0000]
25-01-23 09:13:44 | I |         sum  error  = [    0.0589]
25-01-23 09:13:44 | I |         best error  = [    0.0589]
25-01-23 09:13:44 | I |     + error = [0.0589]
25-01-23 09:13:45 | I |       - range scale = [    1.0000]
25-01-23 09:13:45 | I |         sum  error  = [    0.9201]
25-01-23 09:13:45 | I |         best error  = [    0.9201]
25-01-23 09:13:45 | I |     + error = [0.9201]
25-01-23 09:13:45 | I |   - Calibrating model.layers.0.self_attn.v_proj.weight
25-01-23 09:13:46 | I |       - range scale = [    1.0000]
25-01-23 09:13:46 | I |         sum  error  = [    0.7052]
25-01-23 09:13:46 | I |         best error  = [    0.7052]
25-01-23 09:13:46 | I |     + error = [0.7052]
25-01-23 09:13:46 | I |       - range scale = [    1.0000]
25-01-23 09:13:46 | I |         sum  error  = [    6.5564]
25-01-23 09:13:46 | I |         best error  = [    6.5564]
25-01-23 09:13:46 | I |     + error = [6.5564]
25-01-23 09:13:47 | I |   - Calibrating model.layers.0.self_attn.o_proj.weight
25-01-23 09:13:47 | I |       - range scale = [    1.0000]
25-01-23 09:13:47 | I |         sum  error  = [    0.1900]
25-01-23 09:13:47 | I |         best error  = [    0.1900]
25-01-23 09:13:47 | I |     + error = [0.1900]
25-01-23 09:13:48 | I |       - range scale = [    1.0000]
25-01-23 09:13:48 | I |         sum  error  = [    1.5214]
25-01-23 09:13:48 | I |         best error  = [    1.5214]
25-01-23 09:13:48 | I |     + error = [1.5214]
25-01-23 09:13:48 | I |   - Calibrating model.layers.0.mlp.up_proj.weight
25-01-23 09:13:49 | I |       - range scale = [    1.0000]
25-01-23 09:13:49 | I |         sum  error  = [    1.0381]
25-01-23 09:13:49 | I |         best error  = [    1.0381]
25-01-23 09:13:49 | I |     + error = [1.0381]
25-01-23 09:13:50 | I |       - range scale = [    1.0000]
25-01-23 09:13:50 | I |         sum  error  = [   11.0467]
25-01-23 09:13:50 | I |         best error  = [   11.0467]
25-01-23 09:13:50 | I |     + error = [11.0467]
25-01-23 09:13:50 | I |   - Calibrating model.layers.0.mlp.gate_proj.weight
25-01-23 09:13:51 | I |       - range scale = [    1.0000]
25-01-23 09:13:51 | I |         sum  error  = [    1.0993]
25-01-23 09:13:51 | I |         best error  = [    1.0993]
25-01-23 09:13:51 | I |     + error = [1.0993]
25-01-23 09:13:52 | I |       - range scale = [    1.0000]
25-01-23 09:13:52 | I |         sum  error  = [   11.4166]
25-01-23 09:13:52 | I |         best error  = [   11.4166]
25-01-23 09:13:52 | I |     + error = [11.4166]
25-01-23 09:13:53 | I |   - Calibrating model.layers.0.mlp.down_proj.weight
25-01-23 09:13:53 | I |       - range scale = [    1.0000]
25-01-23 09:13:53 | I |         sum  error  = [    0.1617]
25-01-23 09:13:53 | I |         best error  = [    0.1617]
25-01-23 09:13:53 | I |     + error = [0.1617]
25-01-23 09:13:55 | I |       - range scale = [    1.0000]
25-01-23 09:13:55 | I |         sum  error  = [    1.5791]
25-01-23 09:13:55 | I |         best error  = [    1.5791]
25-01-23 09:13:55 | I |     + error = [1.5791]
25-01-23 09:13:55 | I |   - Quantizing model.layers.0.self_attn.q_proj.weight
25-01-23 09:13:57 | I |   - Quantizing model.layers.0.self_attn.k_proj.weight
25-01-23 09:13:59 | I |   - Quantizing model.layers.0.self_attn.v_proj.weight
25-01-23 09:14:02 | I |   - Quantizing model.layers.0.self_attn.o_proj.weight
25-01-23 09:14:04 | I |   - Quantizing model.layers.0.mlp.up_proj.weight
25-01-23 09:14:06 | I |   - Quantizing model.layers.0.mlp.gate_proj.weight
25-01-23 09:14:09 | I |   - Quantizing model.layers.0.mlp.down_proj.weight
25-01-23 09:14:15 | I | quantizing activations for layer model.layers.0
25-01-23 09:14:15 | I | collecting calibration activations in model.layers.0
25-01-23 09:14:15 | I | collecting calibration activations in model.layers.0
25-01-23 09:14:17 | I | forward this layer
25-01-23 09:14:17 | I | input_file: /data/gyy/lmquant-main/lmquant/data/data_without_preprocess_llama_7b_bs16/shard/val_input_args_layer_0/74.pt
25-01-23 09:14:17 | I | output_file: /data/gyy/lmquant-main/lmquant/data/data_without_preprocess_llama_7b_bs16/shard/val_teacher_output_layer_0/74.pt
25-01-23 09:14:17 | I | In layer model.layers.0, gradient of model.layers.0.self_attn.q_proj has nan: False
25-01-23 09:14:17 | I | In layer model.layers.0, gradient of model.layers.0.self_attn.q_proj has inf: False
25-01-23 09:14:17 | I | In layer model.layers.0, gradient of model.layers.0.self_attn.k_proj has nan: False
25-01-23 09:14:17 | I | In layer model.layers.0, gradient of model.layers.0.self_attn.k_proj has inf: False
25-01-23 09:14:17 | I | In layer model.layers.0, gradient of model.layers.0.self_attn.v_proj has nan: False
25-01-23 09:14:17 | I | In layer model.layers.0, gradient of model.layers.0.self_attn.v_proj has inf: False
25-01-23 09:14:17 | I | In layer model.layers.0, gradient of model.layers.0.self_attn.o_proj has nan: False
25-01-23 09:14:17 | I | In layer model.layers.0, gradient of model.layers.0.self_attn.o_proj has inf: False
25-01-23 09:14:17 | I | In layer model.layers.0, gradient of model.layers.0.mlp.up_proj has nan: False
25-01-23 09:14:17 | I | In layer model.layers.0, gradient of model.layers.0.mlp.up_proj has inf: False
25-01-23 09:14:17 | I | In layer model.layers.0, gradient of model.layers.0.mlp.gate_proj has nan: False
25-01-23 09:14:17 | I | In layer model.layers.0, gradient of model.layers.0.mlp.gate_proj has inf: False
25-01-23 09:14:17 | I | In layer model.layers.0, gradient of model.layers.0.mlp.down_proj has nan: False
25-01-23 09:14:17 | I | In layer model.layers.0, gradient of model.layers.0.mlp.down_proj has inf: False
25-01-23 09:14:17 | I | [70] done with optimizer step
25-01-23 09:14:17 | I | epoch 001:     75 / 819200000 loss=1.07894e-05, loss_per_token=0.0441934, loss_sum=362.032, wps=14.1, ups=0, wpb=8192, bsz=16, num_updates=71, lr=0.00499965, gnorm=0.175, clip=0, loss_scale=8, train_wall=37, cuda_gb_allocated=17.2, cuda_gb_reserved=21.3, cuda_gb_free=6.5, wall=6605
25-01-23 09:14:18 | I | TRAIN CURRENT LAYER_IDX = 0
25-01-23 09:14:18 | I | in layer model.layers.0
25-01-23 09:14:18 | I | quantizing weights for layer model.layers.0
25-01-23 09:14:18 | I | collecting calibration activations in model.layers.0
25-01-23 09:14:18 | I | collecting calibration activations in model.layers.0
25-01-23 09:14:18 | I | - Quantizing decoder layer model.layers.0
25-01-23 09:14:18 | I |   - Calibrating model.layers.0.self_attn.q_proj.weight
25-01-23 09:14:19 | I |       - range scale = [    1.0000]
25-01-23 09:14:19 | I |         sum  error  = [    0.0459]
25-01-23 09:14:19 | I |         best error  = [    0.0459]
25-01-23 09:14:19 | I |     + error = [0.0459]
25-01-23 09:14:20 | I |       - range scale = [    1.0000]
25-01-23 09:14:20 | I |         sum  error  = [    0.7091]
25-01-23 09:14:20 | I |         best error  = [    0.7091]
25-01-23 09:14:20 | I |     + error = [0.7091]
25-01-23 09:14:20 | I |   - Calibrating model.layers.0.self_attn.k_proj.weight
25-01-23 09:14:21 | I |       - range scale = [    1.0000]
25-01-23 09:14:21 | I |         sum  error  = [    0.0581]
25-01-23 09:14:21 | I |         best error  = [    0.0581]
25-01-23 09:14:21 | I |     + error = [0.0581]
25-01-23 09:14:21 | I |       - range scale = [    1.0000]
25-01-23 09:14:21 | I |         sum  error  = [    0.9125]
25-01-23 09:14:21 | I |         best error  = [    0.9125]
25-01-23 09:14:21 | I |     + error = [0.9125]
25-01-23 09:14:22 | I |   - Calibrating model.layers.0.self_attn.v_proj.weight
25-01-23 09:14:22 | I |       - range scale = [    1.0000]
25-01-23 09:14:22 | I |         sum  error  = [    0.7104]
25-01-23 09:14:22 | I |         best error  = [    0.7104]
25-01-23 09:14:22 | I |     + error = [0.7104]
25-01-23 09:14:23 | I |       - range scale = [    1.0000]
25-01-23 09:14:23 | I |         sum  error  = [    6.5969]
25-01-23 09:14:23 | I |         best error  = [    6.5969]
25-01-23 09:14:23 | I |     + error = [6.5969]
25-01-23 09:14:23 | I |   - Calibrating model.layers.0.self_attn.o_proj.weight
25-01-23 09:14:24 | I |       - range scale = [    1.0000]
25-01-23 09:14:24 | I |         sum  error  = [    0.1927]
25-01-23 09:14:24 | I |         best error  = [    0.1927]
25-01-23 09:14:24 | I |     + error = [0.1927]
25-01-23 09:14:25 | I |       - range scale = [    1.0000]
25-01-23 09:14:25 | I |         sum  error  = [    1.5489]
25-01-23 09:14:25 | I |         best error  = [    1.5489]
25-01-23 09:14:25 | I |     + error = [1.5489]
25-01-23 09:14:25 | I |   - Calibrating model.layers.0.mlp.up_proj.weight
25-01-23 09:14:26 | I |       - range scale = [    1.0000]
25-01-23 09:14:26 | I |         sum  error  = [    1.0417]
25-01-23 09:14:26 | I |         best error  = [    1.0417]
25-01-23 09:14:26 | I |     + error = [1.0417]
25-01-23 09:14:27 | I |       - range scale = [    1.0000]
25-01-23 09:14:27 | I |         sum  error  = [   11.0760]
25-01-23 09:14:27 | I |         best error  = [   11.0760]
25-01-23 09:14:27 | I |     + error = [11.0760]
25-01-23 09:14:27 | I |   - Calibrating model.layers.0.mlp.gate_proj.weight
25-01-23 09:14:28 | I |       - range scale = [    1.0000]
25-01-23 09:14:28 | I |         sum  error  = [    1.1038]
25-01-23 09:14:28 | I |         best error  = [    1.1038]
25-01-23 09:14:28 | I |     + error = [1.1038]
25-01-23 09:14:29 | I |       - range scale = [    1.0000]
25-01-23 09:14:29 | I |         sum  error  = [   11.4531]
25-01-23 09:14:29 | I |         best error  = [   11.4531]
25-01-23 09:14:29 | I |     + error = [11.4531]
25-01-23 09:14:29 | I |   - Calibrating model.layers.0.mlp.down_proj.weight
25-01-23 09:14:30 | I |       - range scale = [    1.0000]
25-01-23 09:14:30 | I |         sum  error  = [    0.1598]
25-01-23 09:14:30 | I |         best error  = [    0.1598]
25-01-23 09:14:30 | I |     + error = [0.1598]
25-01-23 09:14:31 | I |       - range scale = [    1.0000]
25-01-23 09:14:31 | I |         sum  error  = [    1.5572]
25-01-23 09:14:31 | I |         best error  = [    1.5572]
25-01-23 09:14:31 | I |     + error = [1.5572]
25-01-23 09:14:32 | I |   - Quantizing model.layers.0.self_attn.q_proj.weight
25-01-23 09:14:34 | I |   - Quantizing model.layers.0.self_attn.k_proj.weight
25-01-23 09:14:36 | I |   - Quantizing model.layers.0.self_attn.v_proj.weight
25-01-23 09:14:39 | I |   - Quantizing model.layers.0.self_attn.o_proj.weight
25-01-23 09:14:41 | I |   - Quantizing model.layers.0.mlp.up_proj.weight
25-01-23 09:14:43 | I |   - Quantizing model.layers.0.mlp.gate_proj.weight
25-01-23 09:14:46 | I |   - Quantizing model.layers.0.mlp.down_proj.weight
25-01-23 09:14:52 | I | quantizing activations for layer model.layers.0
25-01-23 09:14:52 | I | collecting calibration activations in model.layers.0
25-01-23 09:14:52 | I | collecting calibration activations in model.layers.0
25-01-23 09:14:54 | I | forward this layer
25-01-23 09:14:54 | I | input_file: /data/gyy/lmquant-main/lmquant/data/data_without_preprocess_llama_7b_bs16/shard/val_input_args_layer_0/75.pt
25-01-23 09:14:54 | I | output_file: /data/gyy/lmquant-main/lmquant/data/data_without_preprocess_llama_7b_bs16/shard/val_teacher_output_layer_0/75.pt
25-01-23 09:14:55 | I | In layer model.layers.0, gradient of model.layers.0.self_attn.q_proj has nan: False
25-01-23 09:14:55 | I | In layer model.layers.0, gradient of model.layers.0.self_attn.q_proj has inf: False
25-01-23 09:14:55 | I | In layer model.layers.0, gradient of model.layers.0.self_attn.k_proj has nan: False
25-01-23 09:14:55 | I | In layer model.layers.0, gradient of model.layers.0.self_attn.k_proj has inf: False
25-01-23 09:14:55 | I | In layer model.layers.0, gradient of model.layers.0.self_attn.v_proj has nan: False
25-01-23 09:14:55 | I | In layer model.layers.0, gradient of model.layers.0.self_attn.v_proj has inf: False
25-01-23 09:14:55 | I | In layer model.layers.0, gradient of model.layers.0.self_attn.o_proj has nan: False
25-01-23 09:14:55 | I | In layer model.layers.0, gradient of model.layers.0.self_attn.o_proj has inf: False
25-01-23 09:14:55 | I | In layer model.layers.0, gradient of model.layers.0.mlp.up_proj has nan: False
25-01-23 09:14:55 | I | In layer model.layers.0, gradient of model.layers.0.mlp.up_proj has inf: False
25-01-23 09:14:55 | I | In layer model.layers.0, gradient of model.layers.0.mlp.gate_proj has nan: False
25-01-23 09:14:55 | I | In layer model.layers.0, gradient of model.layers.0.mlp.gate_proj has inf: False
25-01-23 09:14:55 | I | In layer model.layers.0, gradient of model.layers.0.mlp.down_proj has nan: False
25-01-23 09:14:55 | I | In layer model.layers.0, gradient of model.layers.0.mlp.down_proj has inf: False
25-01-23 09:14:55 | I | [71] done with optimizer step
25-01-23 09:14:55 | I | epoch 001:     76 / 819200000 loss=1.06341e-05, loss_per_token=0.0435573, loss_sum=356.821, wps=220.1, ups=0.03, wpb=8192, bsz=16, num_updates=72, lr=0.00499963, gnorm=0.192, clip=0, loss_scale=8, train_wall=37, cuda_gb_allocated=17.1, cuda_gb_reserved=18.3, cuda_gb_free=6.6, wall=6642
25-01-23 09:14:55 | I | TRAIN CURRENT LAYER_IDX = 0
25-01-23 09:14:55 | I | in layer model.layers.0
25-01-23 09:14:55 | I | quantizing weights for layer model.layers.0
25-01-23 09:14:55 | I | collecting calibration activations in model.layers.0
25-01-23 09:14:55 | I | collecting calibration activations in model.layers.0
25-01-23 09:14:56 | I | - Quantizing decoder layer model.layers.0
25-01-23 09:14:56 | I |   - Calibrating model.layers.0.self_attn.q_proj.weight
25-01-23 09:14:56 | I |       - range scale = [    1.0000]
25-01-23 09:14:56 | I |         sum  error  = [    0.0463]
25-01-23 09:14:56 | I |         best error  = [    0.0463]
25-01-23 09:14:56 | I |     + error = [0.0463]
25-01-23 09:14:57 | I |       - range scale = [    1.0000]
25-01-23 09:14:57 | I |         sum  error  = [    0.7123]
25-01-23 09:14:57 | I |         best error  = [    0.7123]
25-01-23 09:14:57 | I |     + error = [0.7123]
25-01-23 09:14:57 | I |   - Calibrating model.layers.0.self_attn.k_proj.weight
25-01-23 09:14:58 | I |       - range scale = [    1.0000]
25-01-23 09:14:58 | I |         sum  error  = [    0.0590]
25-01-23 09:14:58 | I |         best error  = [    0.0590]
25-01-23 09:14:58 | I |     + error = [0.0590]
25-01-23 09:14:59 | I |       - range scale = [    1.0000]
25-01-23 09:14:59 | I |         sum  error  = [    0.9154]
25-01-23 09:14:59 | I |         best error  = [    0.9154]
25-01-23 09:14:59 | I |     + error = [0.9154]
25-01-23 09:14:59 | I |   - Calibrating model.layers.0.self_attn.v_proj.weight
25-01-23 09:15:00 | I |       - range scale = [    1.0000]
25-01-23 09:15:00 | I |         sum  error  = [    0.6841]
25-01-23 09:15:00 | I |         best error  = [    0.6841]
25-01-23 09:15:00 | I |     + error = [0.6841]
25-01-23 09:15:00 | I |       - range scale = [    1.0000]
25-01-23 09:15:00 | I |         sum  error  = [    6.3295]
25-01-23 09:15:00 | I |         best error  = [    6.3295]
25-01-23 09:15:00 | I |     + error = [6.3295]
25-01-23 09:15:01 | I |   - Calibrating model.layers.0.self_attn.o_proj.weight
25-01-23 09:15:01 | I |       - range scale = [    1.0000]
25-01-23 09:15:01 | I |         sum  error  = [    0.1884]
25-01-23 09:15:01 | I |         best error  = [    0.1884]
25-01-23 09:15:01 | I |     + error = [0.1884]
25-01-23 09:15:02 | I |       - range scale = [    1.0000]
25-01-23 09:15:02 | I |         sum  error  = [    1.5141]
25-01-23 09:15:02 | I |         best error  = [    1.5141]
25-01-23 09:15:02 | I |     + error = [1.5141]
25-01-23 09:15:02 | I |   - Calibrating model.layers.0.mlp.up_proj.weight
25-01-23 09:15:03 | I |       - range scale = [    1.0000]
25-01-23 09:15:03 | I |         sum  error  = [    1.0400]
25-01-23 09:15:03 | I |         best error  = [    1.0400]
25-01-23 09:15:03 | I |     + error = [1.0400]
25-01-23 09:15:04 | I |       - range scale = [    1.0000]
25-01-23 09:15:04 | I |         sum  error  = [   11.0657]
25-01-23 09:15:04 | I |         best error  = [   11.0657]
25-01-23 09:15:04 | I |     + error = [11.0657]
25-01-23 09:15:05 | I |   - Calibrating model.layers.0.mlp.gate_proj.weight
25-01-23 09:15:05 | I |       - range scale = [    1.0000]
25-01-23 09:15:05 | I |         sum  error  = [    1.1035]
25-01-23 09:15:05 | I |         best error  = [    1.1035]
25-01-23 09:15:05 | I |     + error = [1.1035]
25-01-23 09:15:07 | I |       - range scale = [    1.0000]
25-01-23 09:15:07 | I |         sum  error  = [   11.4424]
25-01-23 09:15:07 | I |         best error  = [   11.4424]
25-01-23 09:15:07 | I |     + error = [11.4424]
25-01-23 09:15:07 | I |   - Calibrating model.layers.0.mlp.down_proj.weight
25-01-23 09:15:08 | I |       - range scale = [    1.0000]
25-01-23 09:15:08 | I |         sum  error  = [    0.1592]
25-01-23 09:15:08 | I |         best error  = [    0.1592]
25-01-23 09:15:08 | I |     + error = [0.1592]
25-01-23 09:15:09 | I |       - range scale = [    1.0000]
25-01-23 09:15:09 | I |         sum  error  = [    1.5511]
25-01-23 09:15:09 | I |         best error  = [    1.5511]
25-01-23 09:15:09 | I |     + error = [1.5511]
25-01-23 09:15:09 | I |   - Quantizing model.layers.0.self_attn.q_proj.weight
25-01-23 09:15:11 | I |   - Quantizing model.layers.0.self_attn.k_proj.weight
25-01-23 09:15:14 | I |   - Quantizing model.layers.0.self_attn.v_proj.weight
25-01-23 09:15:16 | I |   - Quantizing model.layers.0.self_attn.o_proj.weight
25-01-23 09:15:18 | I |   - Quantizing model.layers.0.mlp.up_proj.weight
25-01-23 09:15:21 | I |   - Quantizing model.layers.0.mlp.gate_proj.weight
25-01-23 09:15:23 | I |   - Quantizing model.layers.0.mlp.down_proj.weight
25-01-23 09:15:29 | I | quantizing activations for layer model.layers.0
25-01-23 09:15:29 | I | collecting calibration activations in model.layers.0
25-01-23 09:15:29 | I | collecting calibration activations in model.layers.0
25-01-23 09:15:31 | I | forward this layer
25-01-23 09:15:31 | I | input_file: /data/gyy/lmquant-main/lmquant/data/data_without_preprocess_llama_7b_bs16/shard/val_input_args_layer_0/76.pt
25-01-23 09:15:31 | I | output_file: /data/gyy/lmquant-main/lmquant/data/data_without_preprocess_llama_7b_bs16/shard/val_teacher_output_layer_0/76.pt
25-01-23 09:15:32 | I | In layer model.layers.0, gradient of model.layers.0.self_attn.q_proj has nan: False
25-01-23 09:15:32 | I | In layer model.layers.0, gradient of model.layers.0.self_attn.q_proj has inf: False
25-01-23 09:15:32 | I | In layer model.layers.0, gradient of model.layers.0.self_attn.k_proj has nan: False
25-01-23 09:15:32 | I | In layer model.layers.0, gradient of model.layers.0.self_attn.k_proj has inf: False
25-01-23 09:15:32 | I | In layer model.layers.0, gradient of model.layers.0.self_attn.v_proj has nan: False
25-01-23 09:15:32 | I | In layer model.layers.0, gradient of model.layers.0.self_attn.v_proj has inf: False
25-01-23 09:15:32 | I | In layer model.layers.0, gradient of model.layers.0.self_attn.o_proj has nan: False
25-01-23 09:15:32 | I | In layer model.layers.0, gradient of model.layers.0.self_attn.o_proj has inf: False
25-01-23 09:15:32 | I | In layer model.layers.0, gradient of model.layers.0.mlp.up_proj has nan: False
25-01-23 09:15:32 | I | In layer model.layers.0, gradient of model.layers.0.mlp.up_proj has inf: False
25-01-23 09:15:32 | I | In layer model.layers.0, gradient of model.layers.0.mlp.gate_proj has nan: False
25-01-23 09:15:32 | I | In layer model.layers.0, gradient of model.layers.0.mlp.gate_proj has inf: False
25-01-23 09:15:32 | I | In layer model.layers.0, gradient of model.layers.0.mlp.down_proj has nan: False
25-01-23 09:15:32 | I | In layer model.layers.0, gradient of model.layers.0.mlp.down_proj has inf: False
25-01-23 09:15:32 | I | [72] done with optimizer step
25-01-23 09:15:32 | I | epoch 001:     77 / 819200000 loss=1.02364e-05, loss_per_token=0.0419282, loss_sum=343.476, wps=220.5, ups=0.03, wpb=8192, bsz=16, num_updates=73, lr=0.00499962, gnorm=0.08, clip=0, loss_scale=8, train_wall=37, cuda_gb_allocated=17.1, cuda_gb_reserved=18.4, cuda_gb_free=6.6, wall=6679
25-01-23 09:15:32 | I | TRAIN CURRENT LAYER_IDX = 0
25-01-23 09:15:32 | I | in layer model.layers.0
25-01-23 09:15:32 | I | quantizing weights for layer model.layers.0
25-01-23 09:15:32 | I | collecting calibration activations in model.layers.0
25-01-23 09:15:32 | I | collecting calibration activations in model.layers.0
25-01-23 09:15:33 | I | - Quantizing decoder layer model.layers.0
25-01-23 09:15:33 | I |   - Calibrating model.layers.0.self_attn.q_proj.weight
25-01-23 09:15:33 | I |       - range scale = [    1.0000]
25-01-23 09:15:33 | I |         sum  error  = [    0.0475]
25-01-23 09:15:33 | I |         best error  = [    0.0475]
25-01-23 09:15:33 | I |     + error = [0.0475]
25-01-23 09:15:34 | I |       - range scale = [    1.0000]
25-01-23 09:15:34 | I |         sum  error  = [    0.7319]
25-01-23 09:15:34 | I |         best error  = [    0.7319]
25-01-23 09:15:34 | I |     + error = [0.7319]
25-01-23 09:15:34 | I |   - Calibrating model.layers.0.self_attn.k_proj.weight
25-01-23 09:15:35 | I |       - range scale = [    1.0000]
25-01-23 09:15:35 | I |         sum  error  = [    0.0611]
25-01-23 09:15:35 | I |         best error  = [    0.0611]
25-01-23 09:15:35 | I |     + error = [0.0611]
25-01-23 09:15:36 | I |       - range scale = [    1.0000]
25-01-23 09:15:36 | I |         sum  error  = [    0.9418]
25-01-23 09:15:36 | I |         best error  = [    0.9418]
25-01-23 09:15:36 | I |     + error = [0.9418]
25-01-23 09:15:36 | I |   - Calibrating model.layers.0.self_attn.v_proj.weight
25-01-23 09:15:37 | I |       - range scale = [    1.0000]
25-01-23 09:15:37 | I |         sum  error  = [    0.6923]
25-01-23 09:15:37 | I |         best error  = [    0.6923]
25-01-23 09:15:37 | I |     + error = [0.6923]
25-01-23 09:15:38 | I |       - range scale = [    1.0000]
25-01-23 09:15:38 | I |         sum  error  = [    6.4551]
25-01-23 09:15:38 | I |         best error  = [    6.4551]
25-01-23 09:15:38 | I |     + error = [6.4551]
25-01-23 09:15:38 | I |   - Calibrating model.layers.0.self_attn.o_proj.weight
25-01-23 09:15:39 | I |       - range scale = [    1.0000]
25-01-23 09:15:39 | I |         sum  error  = [    0.1919]
25-01-23 09:15:39 | I |         best error  = [    0.1919]
25-01-23 09:15:39 | I |     + error = [0.1919]
25-01-23 09:15:39 | I |       - range scale = [    1.0000]
25-01-23 09:15:39 | I |         sum  error  = [    1.5457]
25-01-23 09:15:39 | I |         best error  = [    1.5457]
25-01-23 09:15:39 | I |     + error = [1.5457]
25-01-23 09:15:40 | I |   - Calibrating model.layers.0.mlp.up_proj.weight
25-01-23 09:15:40 | I |       - range scale = [    1.0000]
25-01-23 09:15:40 | I |         sum  error  = [    1.0431]
25-01-23 09:15:40 | I |         best error  = [    1.0431]
25-01-23 09:15:40 | I |     + error = [1.0431]
25-01-23 09:15:41 | I |       - range scale = [    1.0000]
25-01-23 09:15:41 | I |         sum  error  = [   11.0928]
25-01-23 09:15:41 | I |         best error  = [   11.0928]
25-01-23 09:15:41 | I |     + error = [11.0928]
25-01-23 09:15:42 | I |   - Calibrating model.layers.0.mlp.gate_proj.weight
25-01-23 09:15:42 | I |       - range scale = [    1.0000]
25-01-23 09:15:42 | I |         sum  error  = [    1.1063]
25-01-23 09:15:42 | I |         best error  = [    1.1063]
25-01-23 09:15:42 | I |     + error = [1.1063]
25-01-23 09:15:44 | I |       - range scale = [    1.0000]
25-01-23 09:15:44 | I |         sum  error  = [   11.4718]
25-01-23 09:15:44 | I |         best error  = [   11.4718]
25-01-23 09:15:44 | I |     + error = [11.4718]
25-01-23 09:15:44 | I |   - Calibrating model.layers.0.mlp.down_proj.weight
25-01-23 09:15:45 | I |       - range scale = [    1.0000]
25-01-23 09:15:45 | I |         sum  error  = [    0.1625]
25-01-23 09:15:45 | I |         best error  = [    0.1625]
25-01-23 09:15:45 | I |     + error = [0.1625]
25-01-23 09:15:46 | I |       - range scale = [    1.0000]
25-01-23 09:15:46 | I |         sum  error  = [    1.5850]
25-01-23 09:15:46 | I |         best error  = [    1.5850]
25-01-23 09:15:46 | I |     + error = [1.5850]
25-01-23 09:15:46 | I |   - Quantizing model.layers.0.self_attn.q_proj.weight
25-01-23 09:15:48 | I |   - Quantizing model.layers.0.self_attn.k_proj.weight
25-01-23 09:15:51 | I |   - Quantizing model.layers.0.self_attn.v_proj.weight
25-01-23 09:15:53 | I |   - Quantizing model.layers.0.self_attn.o_proj.weight
25-01-23 09:15:55 | I |   - Quantizing model.layers.0.mlp.up_proj.weight
25-01-23 09:15:58 | I |   - Quantizing model.layers.0.mlp.gate_proj.weight
25-01-23 09:16:00 | I |   - Quantizing model.layers.0.mlp.down_proj.weight
25-01-23 09:16:06 | I | quantizing activations for layer model.layers.0
25-01-23 09:16:07 | I | collecting calibration activations in model.layers.0
25-01-23 09:16:07 | I | collecting calibration activations in model.layers.0
25-01-23 09:16:08 | I | forward this layer
25-01-23 09:16:08 | I | input_file: /data/gyy/lmquant-main/lmquant/data/data_without_preprocess_llama_7b_bs16/shard/val_input_args_layer_0/77.pt
25-01-23 09:16:08 | I | output_file: /data/gyy/lmquant-main/lmquant/data/data_without_preprocess_llama_7b_bs16/shard/val_teacher_output_layer_0/77.pt
25-01-23 09:16:09 | I | In layer model.layers.0, gradient of model.layers.0.self_attn.q_proj has nan: False
25-01-23 09:16:09 | I | In layer model.layers.0, gradient of model.layers.0.self_attn.q_proj has inf: False
25-01-23 09:16:09 | I | In layer model.layers.0, gradient of model.layers.0.self_attn.k_proj has nan: False
25-01-23 09:16:09 | I | In layer model.layers.0, gradient of model.layers.0.self_attn.k_proj has inf: False
25-01-23 09:16:09 | I | In layer model.layers.0, gradient of model.layers.0.self_attn.v_proj has nan: False
25-01-23 09:16:09 | I | In layer model.layers.0, gradient of model.layers.0.self_attn.v_proj has inf: False
25-01-23 09:16:09 | I | In layer model.layers.0, gradient of model.layers.0.self_attn.o_proj has nan: False
25-01-23 09:16:09 | I | In layer model.layers.0, gradient of model.layers.0.self_attn.o_proj has inf: False
25-01-23 09:16:09 | I | In layer model.layers.0, gradient of model.layers.0.mlp.up_proj has nan: False
25-01-23 09:16:09 | I | In layer model.layers.0, gradient of model.layers.0.mlp.up_proj has inf: False
25-01-23 09:16:09 | I | In layer model.layers.0, gradient of model.layers.0.mlp.gate_proj has nan: False
25-01-23 09:16:09 | I | In layer model.layers.0, gradient of model.layers.0.mlp.gate_proj has inf: False
25-01-23 09:16:09 | I | In layer model.layers.0, gradient of model.layers.0.mlp.down_proj has nan: False
25-01-23 09:16:09 | I | In layer model.layers.0, gradient of model.layers.0.mlp.down_proj has inf: False
25-01-23 09:16:09 | I | [73] done with optimizer step
25-01-23 09:16:09 | I | epoch 001:     78 / 819200000 loss=1.07304e-05, loss_per_token=0.0439517, loss_sum=360.052, wps=220.8, ups=0.03, wpb=8192, bsz=16, num_updates=74, lr=0.0049996, gnorm=0.068, clip=0, loss_scale=8, train_wall=37, cuda_gb_allocated=17.1, cuda_gb_reserved=18.7, cuda_gb_free=6.6, wall=6717
25-01-23 09:16:09 | I | TRAIN CURRENT LAYER_IDX = 0
25-01-23 09:16:09 | I | in layer model.layers.0
25-01-23 09:16:09 | I | quantizing weights for layer model.layers.0
25-01-23 09:16:10 | I | collecting calibration activations in model.layers.0
25-01-23 09:16:10 | I | collecting calibration activations in model.layers.0
25-01-23 09:16:10 | I | - Quantizing decoder layer model.layers.0
25-01-23 09:16:10 | I |   - Calibrating model.layers.0.self_attn.q_proj.weight
25-01-23 09:16:11 | I |       - range scale = [    1.0000]
25-01-23 09:16:11 | I |         sum  error  = [    0.0390]
25-01-23 09:16:11 | I |         best error  = [    0.0390]
25-01-23 09:16:11 | I |     + error = [0.0390]
25-01-23 09:16:11 | I |       - range scale = [    1.0000]
25-01-23 09:16:11 | I |         sum  error  = [    0.6111]
25-01-23 09:16:11 | I |         best error  = [    0.6111]
25-01-23 09:16:11 | I |     + error = [0.6111]
25-01-23 09:16:11 | I |   - Calibrating model.layers.0.self_attn.k_proj.weight
25-01-23 09:16:12 | I |       - range scale = [    1.0000]
25-01-23 09:16:12 | I |         sum  error  = [    0.0452]
25-01-23 09:16:12 | I |         best error  = [    0.0452]
25-01-23 09:16:12 | I |     + error = [0.0452]
25-01-23 09:16:13 | I |       - range scale = [    1.0000]
25-01-23 09:16:13 | I |         sum  error  = [    0.8819]
25-01-23 09:16:13 | I |         best error  = [    0.8819]
25-01-23 09:16:13 | I |     + error = [0.8819]
25-01-23 09:16:13 | I |   - Calibrating model.layers.0.self_attn.v_proj.weight
25-01-23 09:16:14 | I |       - range scale = [    1.0000]
25-01-23 09:16:14 | I |         sum  error  = [    0.6511]
25-01-23 09:16:14 | I |         best error  = [    0.6511]
25-01-23 09:16:14 | I |     + error = [0.6511]
25-01-23 09:16:15 | I |       - range scale = [    1.0000]
25-01-23 09:16:15 | I |         sum  error  = [    6.0513]
25-01-23 09:16:15 | I |         best error  = [    6.0513]
25-01-23 09:16:15 | I |     + error = [6.0513]
25-01-23 09:16:15 | I |   - Calibrating model.layers.0.self_attn.o_proj.weight
25-01-23 09:16:16 | I |       - range scale = [    1.0000]
25-01-23 09:16:16 | I |         sum  error  = [    0.1853]
25-01-23 09:16:16 | I |         best error  = [    0.1853]
25-01-23 09:16:16 | I |     + error = [0.1853]
25-01-23 09:16:16 | I |       - range scale = [    1.0000]
25-01-23 09:16:16 | I |         sum  error  = [    1.4880]
25-01-23 09:16:16 | I |         best error  = [    1.4880]
25-01-23 09:16:16 | I |     + error = [1.4880]
25-01-23 09:16:17 | I |   - Calibrating model.layers.0.mlp.up_proj.weight
25-01-23 09:16:17 | I |       - range scale = [    1.0000]
25-01-23 09:16:17 | I |         sum  error  = [    1.0938]
25-01-23 09:16:17 | I |         best error  = [    1.0938]
25-01-23 09:16:17 | I |     + error = [1.0938]
25-01-23 09:16:19 | I |       - range scale = [    1.0000]
25-01-23 09:16:19 | I |         sum  error  = [   11.6414]
25-01-23 09:16:19 | I |         best error  = [   11.6414]
25-01-23 09:16:19 | I |     + error = [11.6414]
25-01-23 09:16:19 | I |   - Calibrating model.layers.0.mlp.gate_proj.weight
25-01-23 09:16:20 | I |       - range scale = [    1.0000]
25-01-23 09:16:20 | I |         sum  error  = [    1.1573]
25-01-23 09:16:20 | I |         best error  = [    1.1573]
25-01-23 09:16:20 | I |     + error = [1.1573]
25-01-23 09:16:21 | I |       - range scale = [    1.0000]
25-01-23 09:16:21 | I |         sum  error  = [   12.0345]
25-01-23 09:16:21 | I |         best error  = [   12.0345]
25-01-23 09:16:21 | I |     + error = [12.0345]
25-01-23 09:16:21 | I |   - Calibrating model.layers.0.mlp.down_proj.weight
25-01-23 09:16:22 | I |       - range scale = [    1.0000]
25-01-23 09:16:22 | I |         sum  error  = [    0.1582]
25-01-23 09:16:22 | I |         best error  = [    0.1582]
25-01-23 09:16:22 | I |     + error = [0.1582]
25-01-23 09:16:23 | I |       - range scale = [    1.0000]
25-01-23 09:16:23 | I |         sum  error  = [    1.5350]
25-01-23 09:16:23 | I |         best error  = [    1.5350]
25-01-23 09:16:23 | I |     + error = [1.5350]
25-01-23 09:16:23 | I |   - Quantizing model.layers.0.self_attn.q_proj.weight
25-01-23 09:16:26 | I |   - Quantizing model.layers.0.self_attn.k_proj.weight
25-01-23 09:16:28 | I |   - Quantizing model.layers.0.self_attn.v_proj.weight
25-01-23 09:16:30 | I |   - Quantizing model.layers.0.self_attn.o_proj.weight
25-01-23 09:16:32 | I |   - Quantizing model.layers.0.mlp.up_proj.weight
25-01-23 09:16:35 | I |   - Quantizing model.layers.0.mlp.gate_proj.weight
25-01-23 09:16:37 | I |   - Quantizing model.layers.0.mlp.down_proj.weight
25-01-23 09:16:43 | I | quantizing activations for layer model.layers.0
25-01-23 09:16:44 | I | collecting calibration activations in model.layers.0
25-01-23 09:16:44 | I | collecting calibration activations in model.layers.0
25-01-23 09:16:46 | I | forward this layer
25-01-23 09:16:46 | I | input_file: /data/gyy/lmquant-main/lmquant/data/data_without_preprocess_llama_7b_bs16/shard/val_input_args_layer_0/78.pt
25-01-23 09:16:46 | I | output_file: /data/gyy/lmquant-main/lmquant/data/data_without_preprocess_llama_7b_bs16/shard/val_teacher_output_layer_0/78.pt
25-01-23 09:16:46 | I | In layer model.layers.0, gradient of model.layers.0.self_attn.q_proj has nan: False
25-01-23 09:16:46 | I | In layer model.layers.0, gradient of model.layers.0.self_attn.q_proj has inf: False
25-01-23 09:16:46 | I | In layer model.layers.0, gradient of model.layers.0.self_attn.k_proj has nan: False
25-01-23 09:16:46 | I | In layer model.layers.0, gradient of model.layers.0.self_attn.k_proj has inf: False
25-01-23 09:16:46 | I | In layer model.layers.0, gradient of model.layers.0.self_attn.v_proj has nan: False
25-01-23 09:16:46 | I | In layer model.layers.0, gradient of model.layers.0.self_attn.v_proj has inf: False
25-01-23 09:16:46 | I | In layer model.layers.0, gradient of model.layers.0.self_attn.o_proj has nan: False
25-01-23 09:16:46 | I | In layer model.layers.0, gradient of model.layers.0.self_attn.o_proj has inf: False
25-01-23 09:16:46 | I | In layer model.layers.0, gradient of model.layers.0.mlp.up_proj has nan: False
25-01-23 09:16:46 | I | In layer model.layers.0, gradient of model.layers.0.mlp.up_proj has inf: False
25-01-23 09:16:46 | I | In layer model.layers.0, gradient of model.layers.0.mlp.gate_proj has nan: False
25-01-23 09:16:46 | I | In layer model.layers.0, gradient of model.layers.0.mlp.gate_proj has inf: False
25-01-23 09:16:46 | I | In layer model.layers.0, gradient of model.layers.0.mlp.down_proj has nan: False
25-01-23 09:16:46 | I | In layer model.layers.0, gradient of model.layers.0.mlp.down_proj has inf: False
25-01-23 09:16:46 | I | [74] done with optimizer step
25-01-23 09:16:46 | I | epoch 001:     79 / 819200000 loss=1.49724e-05, loss_per_token=0.0613271, loss_sum=502.391, wps=220.3, ups=0.03, wpb=8192, bsz=16, num_updates=75, lr=0.00499958, gnorm=0.184, clip=0, loss_scale=8, train_wall=37, cuda_gb_allocated=17.1, cuda_gb_reserved=18.5, cuda_gb_free=6.6, wall=6754
25-01-23 09:16:46 | I | TRAIN CURRENT LAYER_IDX = 0
25-01-23 09:16:46 | I | in layer model.layers.0
25-01-23 09:16:46 | I | quantizing weights for layer model.layers.0
25-01-23 09:16:47 | I | collecting calibration activations in model.layers.0
25-01-23 09:16:47 | I | collecting calibration activations in model.layers.0
25-01-23 09:16:47 | I | - Quantizing decoder layer model.layers.0
25-01-23 09:16:47 | I |   - Calibrating model.layers.0.self_attn.q_proj.weight
25-01-23 09:16:48 | I |       - range scale = [    1.0000]
25-01-23 09:16:48 | I |         sum  error  = [    0.0433]
25-01-23 09:16:48 | I |         best error  = [    0.0433]
25-01-23 09:16:48 | I |     + error = [0.0433]
25-01-23 09:16:48 | I |       - range scale = [    1.0000]
25-01-23 09:16:48 | I |         sum  error  = [    0.6776]
25-01-23 09:16:48 | I |         best error  = [    0.6776]
25-01-23 09:16:48 | I |     + error = [0.6776]
25-01-23 09:16:49 | I |   - Calibrating model.layers.0.self_attn.k_proj.weight
25-01-23 09:16:49 | I |       - range scale = [    1.0000]
25-01-23 09:16:49 | I |         sum  error  = [    0.0520]
25-01-23 09:16:49 | I |         best error  = [    0.0520]
25-01-23 09:16:49 | I |     + error = [0.0520]
25-01-23 09:16:50 | I |       - range scale = [    1.0000]
25-01-23 09:16:50 | I |         sum  error  = [    0.8987]
25-01-23 09:16:50 | I |         best error  = [    0.8987]
25-01-23 09:16:50 | I |     + error = [0.8987]
25-01-23 09:16:50 | I |   - Calibrating model.layers.0.self_attn.v_proj.weight
25-01-23 09:16:51 | I |       - range scale = [    1.0000]
25-01-23 09:16:51 | I |         sum  error  = [    0.6733]
25-01-23 09:16:51 | I |         best error  = [    0.6733]
25-01-23 09:16:51 | I |     + error = [0.6733]
25-01-23 09:16:52 | I |       - range scale = [    1.0000]
25-01-23 09:16:52 | I |         sum  error  = [    6.2573]
25-01-23 09:16:52 | I |         best error  = [    6.2573]
25-01-23 09:16:52 | I |     + error = [6.2573]
25-01-23 09:16:52 | I |   - Calibrating model.layers.0.self_attn.o_proj.weight
25-01-23 09:16:53 | I |       - range scale = [    1.0000]
25-01-23 09:16:53 | I |         sum  error  = [    0.1916]
25-01-23 09:16:53 | I |         best error  = [    0.1916]
25-01-23 09:16:53 | I |     + error = [0.1916]
25-01-23 09:16:54 | I |       - range scale = [    1.0000]
25-01-23 09:16:54 | I |         sum  error  = [    1.5417]
25-01-23 09:16:54 | I |         best error  = [    1.5417]
25-01-23 09:16:54 | I |     + error = [1.5417]
25-01-23 09:16:54 | I |   - Calibrating model.layers.0.mlp.up_proj.weight
25-01-23 09:16:55 | I |       - range scale = [    1.0000]
25-01-23 09:16:55 | I |         sum  error  = [    1.0703]
25-01-23 09:16:55 | I |         best error  = [    1.0703]
25-01-23 09:16:55 | I |     + error = [1.0703]
25-01-23 09:16:56 | I |       - range scale = [    1.0000]
25-01-23 09:16:56 | I |         sum  error  = [   11.3859]
25-01-23 09:16:56 | I |         best error  = [   11.3859]
25-01-23 09:16:56 | I |     + error = [11.3859]
25-01-23 09:16:56 | I |   - Calibrating model.layers.0.mlp.gate_proj.weight
25-01-23 09:16:57 | I |       - range scale = [    1.0000]
25-01-23 09:16:57 | I |         sum  error  = [    1.1341]
25-01-23 09:16:57 | I |         best error  = [    1.1341]
25-01-23 09:16:57 | I |     + error = [1.1341]
25-01-23 09:16:58 | I |       - range scale = [    1.0000]
25-01-23 09:16:58 | I |         sum  error  = [   11.7703]
25-01-23 09:16:58 | I |         best error  = [   11.7703]
25-01-23 09:16:58 | I |     + error = [11.7703]
25-01-23 09:16:58 | I |   - Calibrating model.layers.0.mlp.down_proj.weight
25-01-23 09:16:59 | I |       - range scale = [    1.0000]
25-01-23 09:16:59 | I |         sum  error  = [    0.1644]
25-01-23 09:16:59 | I |         best error  = [    0.1644]
25-01-23 09:16:59 | I |     + error = [0.1644]
25-01-23 09:17:00 | I |       - range scale = [    1.0000]
25-01-23 09:17:00 | I |         sum  error  = [    1.6002]
25-01-23 09:17:00 | I |         best error  = [    1.6002]
25-01-23 09:17:00 | I |     + error = [1.6002]
25-01-23 09:17:00 | I |   - Quantizing model.layers.0.self_attn.q_proj.weight
25-01-23 09:17:03 | I |   - Quantizing model.layers.0.self_attn.k_proj.weight
25-01-23 09:17:05 | I |   - Quantizing model.layers.0.self_attn.v_proj.weight
25-01-23 09:17:07 | I |   - Quantizing model.layers.0.self_attn.o_proj.weight
25-01-23 09:17:09 | I |   - Quantizing model.layers.0.mlp.up_proj.weight
25-01-23 09:17:12 | I |   - Quantizing model.layers.0.mlp.gate_proj.weight
25-01-23 09:17:14 | I |   - Quantizing model.layers.0.mlp.down_proj.weight
25-01-23 09:17:20 | I | quantizing activations for layer model.layers.0
25-01-23 09:17:20 | I | collecting calibration activations in model.layers.0
25-01-23 09:17:21 | I | collecting calibration activations in model.layers.0
25-01-23 09:17:22 | I | forward this layer
25-01-23 09:17:22 | I | input_file: /data/gyy/lmquant-main/lmquant/data/data_without_preprocess_llama_7b_bs16/shard/val_input_args_layer_0/79.pt
25-01-23 09:17:22 | I | output_file: /data/gyy/lmquant-main/lmquant/data/data_without_preprocess_llama_7b_bs16/shard/val_teacher_output_layer_0/79.pt
25-01-23 09:17:23 | I | In layer model.layers.0, gradient of model.layers.0.self_attn.q_proj has nan: False
25-01-23 09:17:23 | I | In layer model.layers.0, gradient of model.layers.0.self_attn.q_proj has inf: False
25-01-23 09:17:23 | I | In layer model.layers.0, gradient of model.layers.0.self_attn.k_proj has nan: False
25-01-23 09:17:23 | I | In layer model.layers.0, gradient of model.layers.0.self_attn.k_proj has inf: False
25-01-23 09:17:23 | I | In layer model.layers.0, gradient of model.layers.0.self_attn.v_proj has nan: False
25-01-23 09:17:23 | I | In layer model.layers.0, gradient of model.layers.0.self_attn.v_proj has inf: False
25-01-23 09:17:23 | I | In layer model.layers.0, gradient of model.layers.0.self_attn.o_proj has nan: False
25-01-23 09:17:23 | I | In layer model.layers.0, gradient of model.layers.0.self_attn.o_proj has inf: False
25-01-23 09:17:23 | I | In layer model.layers.0, gradient of model.layers.0.mlp.up_proj has nan: False
25-01-23 09:17:23 | I | In layer model.layers.0, gradient of model.layers.0.mlp.up_proj has inf: False
25-01-23 09:17:23 | I | In layer model.layers.0, gradient of model.layers.0.mlp.gate_proj has nan: False
25-01-23 09:17:23 | I | In layer model.layers.0, gradient of model.layers.0.mlp.gate_proj has inf: False
25-01-23 09:17:23 | I | In layer model.layers.0, gradient of model.layers.0.mlp.down_proj has nan: False
25-01-23 09:17:23 | I | In layer model.layers.0, gradient of model.layers.0.mlp.down_proj has inf: False
25-01-23 09:17:23 | I | [75] done with optimizer step
25-01-23 09:17:23 | I | epoch 001:     80 / 819200000 loss=1.3146e-05, loss_per_token=0.053846, loss_sum=441.106, wps=223.2, ups=0.03, wpb=8192, bsz=16, num_updates=76, lr=0.00499957, gnorm=0.173, clip=0, loss_scale=8, train_wall=37, cuda_gb_allocated=17.1, cuda_gb_reserved=18.7, cuda_gb_free=6.6, wall=6790
25-01-23 09:17:23 | I | TRAIN CURRENT LAYER_IDX = 0
25-01-23 09:17:23 | I | in layer model.layers.0
25-01-23 09:17:23 | I | quantizing weights for layer model.layers.0
25-01-23 09:17:23 | I | collecting calibration activations in model.layers.0
25-01-23 09:17:23 | I | collecting calibration activations in model.layers.0
25-01-23 09:17:24 | I | - Quantizing decoder layer model.layers.0
25-01-23 09:17:24 | I |   - Calibrating model.layers.0.self_attn.q_proj.weight
25-01-23 09:17:24 | I |       - range scale = [    1.0000]
25-01-23 09:17:24 | I |         sum  error  = [    0.0446]
25-01-23 09:17:24 | I |         best error  = [    0.0446]
25-01-23 09:17:24 | I |     + error = [0.0446]
25-01-23 09:17:25 | I |       - range scale = [    1.0000]
25-01-23 09:17:25 | I |         sum  error  = [    0.6953]
25-01-23 09:17:25 | I |         best error  = [    0.6953]
25-01-23 09:17:25 | I |     + error = [0.6953]
25-01-23 09:17:25 | I |   - Calibrating model.layers.0.self_attn.k_proj.weight
25-01-23 09:17:26 | I |       - range scale = [    1.0000]
25-01-23 09:17:26 | I |         sum  error  = [    0.0542]
25-01-23 09:17:26 | I |         best error  = [    0.0542]
25-01-23 09:17:26 | I |     + error = [0.0542]
25-01-23 09:17:27 | I |       - range scale = [    1.0000]
25-01-23 09:17:27 | I |         sum  error  = [    0.9164]
25-01-23 09:17:27 | I |         best error  = [    0.9164]
25-01-23 09:17:27 | I |     + error = [0.9164]
25-01-23 09:17:27 | I |   - Calibrating model.layers.0.self_attn.v_proj.weight
25-01-23 09:17:28 | I |       - range scale = [    1.0000]
25-01-23 09:17:28 | I |         sum  error  = [    0.6682]
25-01-23 09:17:28 | I |         best error  = [    0.6682]
25-01-23 09:17:28 | I |     + error = [0.6682]
25-01-23 09:17:29 | I |       - range scale = [    1.0000]
25-01-23 09:17:29 | I |         sum  error  = [    6.2535]
25-01-23 09:17:29 | I |         best error  = [    6.2535]
25-01-23 09:17:29 | I |     + error = [6.2535]
25-01-23 09:17:29 | I |   - Calibrating model.layers.0.self_attn.o_proj.weight
25-01-23 09:17:29 | I |       - range scale = [    1.0000]
25-01-23 09:17:29 | I |         sum  error  = [    0.1916]
25-01-23 09:17:29 | I |         best error  = [    0.1916]
25-01-23 09:17:29 | I |     + error = [0.1916]
25-01-23 09:17:30 | I |       - range scale = [    1.0000]
25-01-23 09:17:30 | I |         sum  error  = [    1.5328]
25-01-23 09:17:30 | I |         best error  = [    1.5328]
25-01-23 09:17:30 | I |     + error = [1.5328]
25-01-23 09:17:31 | I |   - Calibrating model.layers.0.mlp.up_proj.weight
25-01-23 09:17:31 | I |       - range scale = [    1.0000]
25-01-23 09:17:31 | I |         sum  error  = [    1.0994]
25-01-23 09:17:31 | I |         best error  = [    1.0994]
25-01-23 09:17:31 | I |     + error = [1.0994]
25-01-23 09:17:32 | I |       - range scale = [    1.0000]
25-01-23 09:17:32 | I |         sum  error  = [   11.7032]
25-01-23 09:17:32 | I |         best error  = [   11.7032]
25-01-23 09:17:32 | I |     + error = [11.7032]
25-01-23 09:17:33 | I |   - Calibrating model.layers.0.mlp.gate_proj.weight
25-01-23 09:17:33 | I |       - range scale = [    1.0000]
25-01-23 09:17:33 | I |         sum  error  = [    1.1630]
25-01-23 09:17:33 | I |         best error  = [    1.1630]
25-01-23 09:17:33 | I |     + error = [1.1630]
25-01-23 09:17:35 | I |       - range scale = [    1.0000]
25-01-23 09:17:35 | I |         sum  error  = [   12.0943]
25-01-23 09:17:35 | I |         best error  = [   12.0943]
25-01-23 09:17:35 | I |     + error = [12.0943]
25-01-23 09:17:35 | I |   - Calibrating model.layers.0.mlp.down_proj.weight
25-01-23 09:17:36 | I |       - range scale = [    1.0000]
25-01-23 09:17:36 | I |         sum  error  = [    0.1673]
25-01-23 09:17:36 | I |         best error  = [    0.1673]
25-01-23 09:17:36 | I |     + error = [0.1673]
25-01-23 09:17:37 | I |       - range scale = [    1.0000]
25-01-23 09:17:37 | I |         sum  error  = [    1.6249]
25-01-23 09:17:37 | I |         best error  = [    1.6249]
25-01-23 09:17:37 | I |     + error = [1.6249]
25-01-23 09:17:37 | I |   - Quantizing model.layers.0.self_attn.q_proj.weight
25-01-23 09:17:39 | I |   - Quantizing model.layers.0.self_attn.k_proj.weight
25-01-23 09:17:42 | I |   - Quantizing model.layers.0.self_attn.v_proj.weight
25-01-23 09:17:44 | I |   - Quantizing model.layers.0.self_attn.o_proj.weight
25-01-23 09:17:46 | I |   - Quantizing model.layers.0.mlp.up_proj.weight
25-01-23 09:17:49 | I |   - Quantizing model.layers.0.mlp.gate_proj.weight
25-01-23 09:17:51 | I |   - Quantizing model.layers.0.mlp.down_proj.weight
25-01-23 09:17:57 | I | quantizing activations for layer model.layers.0
25-01-23 09:17:57 | I | collecting calibration activations in model.layers.0
25-01-23 09:17:57 | I | collecting calibration activations in model.layers.0
25-01-23 09:17:59 | I | forward this layer
25-01-23 09:17:59 | I | input_file: /data/gyy/lmquant-main/lmquant/data/data_without_preprocess_llama_7b_bs16/shard/val_input_args_layer_0/80.pt
25-01-23 09:17:59 | I | output_file: /data/gyy/lmquant-main/lmquant/data/data_without_preprocess_llama_7b_bs16/shard/val_teacher_output_layer_0/80.pt
25-01-23 09:18:00 | I | In layer model.layers.0, gradient of model.layers.0.self_attn.q_proj has nan: False
25-01-23 09:18:00 | I | In layer model.layers.0, gradient of model.layers.0.self_attn.q_proj has inf: False
25-01-23 09:18:00 | I | In layer model.layers.0, gradient of model.layers.0.self_attn.k_proj has nan: False
25-01-23 09:18:00 | I | In layer model.layers.0, gradient of model.layers.0.self_attn.k_proj has inf: False
25-01-23 09:18:00 | I | In layer model.layers.0, gradient of model.layers.0.self_attn.v_proj has nan: False
25-01-23 09:18:00 | I | In layer model.layers.0, gradient of model.layers.0.self_attn.v_proj has inf: False
25-01-23 09:18:00 | I | In layer model.layers.0, gradient of model.layers.0.self_attn.o_proj has nan: False
25-01-23 09:18:00 | I | In layer model.layers.0, gradient of model.layers.0.self_attn.o_proj has inf: False
25-01-23 09:18:00 | I | In layer model.layers.0, gradient of model.layers.0.mlp.up_proj has nan: False
25-01-23 09:18:00 | I | In layer model.layers.0, gradient of model.layers.0.mlp.up_proj has inf: False
25-01-23 09:18:00 | I | In layer model.layers.0, gradient of model.layers.0.mlp.gate_proj has nan: False
25-01-23 09:18:00 | I | In layer model.layers.0, gradient of model.layers.0.mlp.gate_proj has inf: False
25-01-23 09:18:00 | I | In layer model.layers.0, gradient of model.layers.0.mlp.down_proj has nan: False
25-01-23 09:18:00 | I | In layer model.layers.0, gradient of model.layers.0.mlp.down_proj has inf: False
25-01-23 09:18:00 | I | [76] done with optimizer step
25-01-23 09:18:00 | I | epoch 001:     81 / 819200000 loss=1.43898e-05, loss_per_token=0.0589405, loss_sum=482.84, wps=221.8, ups=0.03, wpb=8192, bsz=16, num_updates=77, lr=0.00499955, gnorm=0.195, clip=0, loss_scale=8, train_wall=37, cuda_gb_allocated=17.1, cuda_gb_reserved=18.5, cuda_gb_free=6.6, wall=6827
25-01-23 09:18:00 | I | TRAIN CURRENT LAYER_IDX = 0
25-01-23 09:18:00 | I | in layer model.layers.0
25-01-23 09:18:00 | I | quantizing weights for layer model.layers.0
25-01-23 09:18:00 | I | collecting calibration activations in model.layers.0
25-01-23 09:18:00 | I | collecting calibration activations in model.layers.0
25-01-23 09:18:01 | I | - Quantizing decoder layer model.layers.0
25-01-23 09:18:01 | I |   - Calibrating model.layers.0.self_attn.q_proj.weight
25-01-23 09:18:01 | I |       - range scale = [    1.0000]
25-01-23 09:18:01 | I |         sum  error  = [    0.0452]
25-01-23 09:18:01 | I |         best error  = [    0.0452]
25-01-23 09:18:01 | I |     + error = [0.0452]
25-01-23 09:18:02 | I |       - range scale = [    1.0000]
25-01-23 09:18:02 | I |         sum  error  = [    0.6846]
25-01-23 09:18:02 | I |         best error  = [    0.6846]
25-01-23 09:18:02 | I |     + error = [0.6846]
25-01-23 09:18:02 | I |   - Calibrating model.layers.0.self_attn.k_proj.weight
25-01-23 09:18:03 | I |       - range scale = [    1.0000]
25-01-23 09:18:03 | I |         sum  error  = [    0.0537]
25-01-23 09:18:03 | I |         best error  = [    0.0537]
25-01-23 09:18:03 | I |     + error = [0.0537]
25-01-23 09:18:04 | I |       - range scale = [    1.0000]
25-01-23 09:18:04 | I |         sum  error  = [    0.8979]
25-01-23 09:18:04 | I |         best error  = [    0.8979]
25-01-23 09:18:04 | I |     + error = [0.8979]
25-01-23 09:18:04 | I |   - Calibrating model.layers.0.self_attn.v_proj.weight
25-01-23 09:18:05 | I |       - range scale = [    1.0000]
25-01-23 09:18:05 | I |         sum  error  = [    0.6795]
25-01-23 09:18:05 | I |         best error  = [    0.6795]
25-01-23 09:18:05 | I |     + error = [0.6795]
25-01-23 09:18:06 | I |       - range scale = [    1.0000]
25-01-23 09:18:06 | I |         sum  error  = [    6.3800]
25-01-23 09:18:06 | I |         best error  = [    6.3800]
25-01-23 09:18:06 | I |     + error = [6.3800]
25-01-23 09:18:06 | I |   - Calibrating model.layers.0.self_attn.o_proj.weight
25-01-23 09:18:06 | I |       - range scale = [    1.0000]
25-01-23 09:18:06 | I |         sum  error  = [    0.1920]
25-01-23 09:18:06 | I |         best error  = [    0.1920]
25-01-23 09:18:06 | I |     + error = [0.1920]
25-01-23 09:18:07 | I |       - range scale = [    1.0000]
25-01-23 09:18:07 | I |         sum  error  = [    1.5350]
25-01-23 09:18:07 | I |         best error  = [    1.5350]
25-01-23 09:18:07 | I |     + error = [1.5350]
25-01-23 09:18:07 | I |   - Calibrating model.layers.0.mlp.up_proj.weight
25-01-23 09:18:08 | I |       - range scale = [    1.0000]
25-01-23 09:18:08 | I |         sum  error  = [    1.1056]
25-01-23 09:18:08 | I |         best error  = [    1.1056]
25-01-23 09:18:08 | I |     + error = [1.1056]
25-01-23 09:18:09 | I |       - range scale = [    1.0000]
25-01-23 09:18:09 | I |         sum  error  = [   11.7788]
25-01-23 09:18:09 | I |         best error  = [   11.7788]
25-01-23 09:18:09 | I |     + error = [11.7788]
25-01-23 09:18:10 | I |   - Calibrating model.layers.0.mlp.gate_proj.weight
25-01-23 09:18:10 | I |       - range scale = [    1.0000]
25-01-23 09:18:10 | I |         sum  error  = [    1.1715]
25-01-23 09:18:10 | I |         best error  = [    1.1715]
25-01-23 09:18:10 | I |     + error = [1.1715]
25-01-23 09:18:11 | I |       - range scale = [    1.0000]
25-01-23 09:18:11 | I |         sum  error  = [   12.1653]
25-01-23 09:18:11 | I |         best error  = [   12.1653]
25-01-23 09:18:11 | I |     + error = [12.1653]
25-01-23 09:18:12 | I |   - Calibrating model.layers.0.mlp.down_proj.weight
25-01-23 09:18:12 | I |       - range scale = [    1.0000]
25-01-23 09:18:12 | I |         sum  error  = [    0.1724]
25-01-23 09:18:12 | I |         best error  = [    0.1724]
25-01-23 09:18:12 | I |     + error = [0.1724]
25-01-23 09:18:14 | I |       - range scale = [    1.0000]
25-01-23 09:18:14 | I |         sum  error  = [    1.6719]
25-01-23 09:18:14 | I |         best error  = [    1.6719]
25-01-23 09:18:14 | I |     + error = [1.6719]
25-01-23 09:18:14 | I |   - Quantizing model.layers.0.self_attn.q_proj.weight
25-01-23 09:18:16 | I |   - Quantizing model.layers.0.self_attn.k_proj.weight
25-01-23 09:18:18 | I |   - Quantizing model.layers.0.self_attn.v_proj.weight
25-01-23 09:18:21 | I |   - Quantizing model.layers.0.self_attn.o_proj.weight
25-01-23 09:18:23 | I |   - Quantizing model.layers.0.mlp.up_proj.weight
25-01-23 09:18:25 | I |   - Quantizing model.layers.0.mlp.gate_proj.weight
25-01-23 09:18:28 | I |   - Quantizing model.layers.0.mlp.down_proj.weight
25-01-23 09:18:34 | I | quantizing activations for layer model.layers.0
25-01-23 09:18:34 | I | collecting calibration activations in model.layers.0
25-01-23 09:18:34 | I | collecting calibration activations in model.layers.0
25-01-23 09:18:36 | I | forward this layer
25-01-23 09:18:36 | I | input_file: /data/gyy/lmquant-main/lmquant/data/data_without_preprocess_llama_7b_bs16/shard/val_input_args_layer_0/81.pt
25-01-23 09:18:36 | I | output_file: /data/gyy/lmquant-main/lmquant/data/data_without_preprocess_llama_7b_bs16/shard/val_teacher_output_layer_0/81.pt
25-01-23 09:18:36 | I | In layer model.layers.0, gradient of model.layers.0.self_attn.q_proj has nan: False
25-01-23 09:18:36 | I | In layer model.layers.0, gradient of model.layers.0.self_attn.q_proj has inf: False
25-01-23 09:18:36 | I | In layer model.layers.0, gradient of model.layers.0.self_attn.k_proj has nan: False
25-01-23 09:18:36 | I | In layer model.layers.0, gradient of model.layers.0.self_attn.k_proj has inf: False
25-01-23 09:18:36 | I | In layer model.layers.0, gradient of model.layers.0.self_attn.v_proj has nan: False
25-01-23 09:18:36 | I | In layer model.layers.0, gradient of model.layers.0.self_attn.v_proj has inf: False
25-01-23 09:18:36 | I | In layer model.layers.0, gradient of model.layers.0.self_attn.o_proj has nan: False
25-01-23 09:18:36 | I | In layer model.layers.0, gradient of model.layers.0.self_attn.o_proj has inf: False
25-01-23 09:18:36 | I | In layer model.layers.0, gradient of model.layers.0.mlp.up_proj has nan: False
25-01-23 09:18:36 | I | In layer model.layers.0, gradient of model.layers.0.mlp.up_proj has inf: False
25-01-23 09:18:36 | I | In layer model.layers.0, gradient of model.layers.0.mlp.gate_proj has nan: False
25-01-23 09:18:36 | I | In layer model.layers.0, gradient of model.layers.0.mlp.gate_proj has inf: False
25-01-23 09:18:36 | I | In layer model.layers.0, gradient of model.layers.0.mlp.down_proj has nan: False
25-01-23 09:18:36 | I | In layer model.layers.0, gradient of model.layers.0.mlp.down_proj has inf: False
25-01-23 09:18:36 | I | [77] done with optimizer step
25-01-23 09:18:36 | I | epoch 001:     82 / 819200000 loss=1.40808e-05, loss_per_token=0.0576749, loss_sum=472.473, wps=223.9, ups=0.03, wpb=8192, bsz=16, num_updates=78, lr=0.00499953, gnorm=0.151, clip=0, loss_scale=8, train_wall=36, cuda_gb_allocated=17.1, cuda_gb_reserved=18.5, cuda_gb_free=6.6, wall=6864
25-01-23 09:18:36 | I | TRAIN CURRENT LAYER_IDX = 0
25-01-23 09:18:36 | I | in layer model.layers.0
25-01-23 09:18:36 | I | quantizing weights for layer model.layers.0
25-01-23 09:18:37 | I | collecting calibration activations in model.layers.0
25-01-23 09:18:37 | I | collecting calibration activations in model.layers.0
25-01-23 09:18:37 | I | - Quantizing decoder layer model.layers.0
25-01-23 09:18:37 | I |   - Calibrating model.layers.0.self_attn.q_proj.weight
25-01-23 09:18:38 | I |       - range scale = [    1.0000]
25-01-23 09:18:38 | I |         sum  error  = [    0.0443]
25-01-23 09:18:38 | I |         best error  = [    0.0443]
25-01-23 09:18:38 | I |     + error = [0.0443]
25-01-23 09:18:39 | I |       - range scale = [    1.0000]
25-01-23 09:18:39 | I |         sum  error  = [    0.6991]
25-01-23 09:18:39 | I |         best error  = [    0.6991]
25-01-23 09:18:39 | I |     + error = [0.6991]
25-01-23 09:18:39 | I |   - Calibrating model.layers.0.self_attn.k_proj.weight
25-01-23 09:18:40 | I |       - range scale = [    1.0000]
25-01-23 09:18:40 | I |         sum  error  = [    0.0544]
25-01-23 09:18:40 | I |         best error  = [    0.0544]
25-01-23 09:18:40 | I |     + error = [0.0544]
25-01-23 09:18:40 | I |       - range scale = [    1.0000]
25-01-23 09:18:40 | I |         sum  error  = [    0.9330]
25-01-23 09:18:40 | I |         best error  = [    0.9330]
25-01-23 09:18:40 | I |     + error = [0.9330]
25-01-23 09:18:41 | I |   - Calibrating model.layers.0.self_attn.v_proj.weight
25-01-23 09:18:41 | I |       - range scale = [    1.0000]
25-01-23 09:18:41 | I |         sum  error  = [    0.6704]
25-01-23 09:18:41 | I |         best error  = [    0.6704]
25-01-23 09:18:41 | I |     + error = [0.6704]
25-01-23 09:18:42 | I |       - range scale = [    1.0000]
25-01-23 09:18:42 | I |         sum  error  = [    6.2486]
25-01-23 09:18:42 | I |         best error  = [    6.2486]
25-01-23 09:18:42 | I |     + error = [6.2486]
25-01-23 09:18:42 | I |   - Calibrating model.layers.0.self_attn.o_proj.weight
25-01-23 09:18:43 | I |       - range scale = [    1.0000]
25-01-23 09:18:43 | I |         sum  error  = [    0.1927]
25-01-23 09:18:43 | I |         best error  = [    0.1927]
25-01-23 09:18:43 | I |     + error = [0.1927]
25-01-23 09:18:44 | I |       - range scale = [    1.0000]
25-01-23 09:18:44 | I |         sum  error  = [    1.5440]
25-01-23 09:18:44 | I |         best error  = [    1.5440]
25-01-23 09:18:44 | I |     + error = [1.5440]
25-01-23 09:18:44 | I |   - Calibrating model.layers.0.mlp.up_proj.weight
25-01-23 09:18:45 | I |       - range scale = [    1.0000]
25-01-23 09:18:45 | I |         sum  error  = [    1.0843]
25-01-23 09:18:45 | I |         best error  = [    1.0843]
25-01-23 09:18:45 | I |     + error = [1.0843]
25-01-23 09:18:46 | I |       - range scale = [    1.0000]
25-01-23 09:18:46 | I |         sum  error  = [   11.5337]
25-01-23 09:18:46 | I |         best error  = [   11.5337]
25-01-23 09:18:46 | I |     + error = [11.5337]
25-01-23 09:18:46 | I |   - Calibrating model.layers.0.mlp.gate_proj.weight
25-01-23 09:18:47 | I |       - range scale = [    1.0000]
25-01-23 09:18:47 | I |         sum  error  = [    1.1447]
25-01-23 09:18:47 | I |         best error  = [    1.1447]
25-01-23 09:18:47 | I |     + error = [1.1447]
25-01-23 09:18:48 | I |       - range scale = [    1.0000]
25-01-23 09:18:48 | I |         sum  error  = [   11.9218]
25-01-23 09:18:48 | I |         best error  = [   11.9218]
25-01-23 09:18:48 | I |     + error = [11.9218]
25-01-23 09:18:48 | I |   - Calibrating model.layers.0.mlp.down_proj.weight
25-01-23 09:18:49 | I |       - range scale = [    1.0000]
25-01-23 09:18:49 | I |         sum  error  = [    0.1670]
25-01-23 09:18:49 | I |         best error  = [    0.1670]
25-01-23 09:18:49 | I |     + error = [0.1670]
25-01-23 09:18:50 | I |       - range scale = [    1.0000]
25-01-23 09:18:50 | I |         sum  error  = [    1.6279]
25-01-23 09:18:50 | I |         best error  = [    1.6279]
25-01-23 09:18:50 | I |     + error = [1.6279]
25-01-23 09:18:50 | I |   - Quantizing model.layers.0.self_attn.q_proj.weight
25-01-23 09:18:53 | I |   - Quantizing model.layers.0.self_attn.k_proj.weight
25-01-23 09:18:55 | I |   - Quantizing model.layers.0.self_attn.v_proj.weight
25-01-23 09:18:57 | I |   - Quantizing model.layers.0.self_attn.o_proj.weight
25-01-23 09:19:00 | I |   - Quantizing model.layers.0.mlp.up_proj.weight
25-01-23 09:19:02 | I |   - Quantizing model.layers.0.mlp.gate_proj.weight
25-01-23 09:19:04 | I |   - Quantizing model.layers.0.mlp.down_proj.weight
25-01-23 09:19:10 | I | quantizing activations for layer model.layers.0
25-01-23 09:19:11 | I | collecting calibration activations in model.layers.0
25-01-23 09:19:11 | I | collecting calibration activations in model.layers.0
25-01-23 09:19:13 | I | forward this layer
25-01-23 09:19:13 | I | input_file: /data/gyy/lmquant-main/lmquant/data/data_without_preprocess_llama_7b_bs16/shard/val_input_args_layer_0/82.pt
25-01-23 09:19:13 | I | output_file: /data/gyy/lmquant-main/lmquant/data/data_without_preprocess_llama_7b_bs16/shard/val_teacher_output_layer_0/82.pt
25-01-23 09:19:13 | I | In layer model.layers.0, gradient of model.layers.0.self_attn.q_proj has nan: False
25-01-23 09:19:13 | I | In layer model.layers.0, gradient of model.layers.0.self_attn.q_proj has inf: False
25-01-23 09:19:13 | I | In layer model.layers.0, gradient of model.layers.0.self_attn.k_proj has nan: False
25-01-23 09:19:13 | I | In layer model.layers.0, gradient of model.layers.0.self_attn.k_proj has inf: False
25-01-23 09:19:13 | I | In layer model.layers.0, gradient of model.layers.0.self_attn.v_proj has nan: False
25-01-23 09:19:13 | I | In layer model.layers.0, gradient of model.layers.0.self_attn.v_proj has inf: False
25-01-23 09:19:13 | I | In layer model.layers.0, gradient of model.layers.0.self_attn.o_proj has nan: False
25-01-23 09:19:13 | I | In layer model.layers.0, gradient of model.layers.0.self_attn.o_proj has inf: False
25-01-23 09:19:13 | I | In layer model.layers.0, gradient of model.layers.0.mlp.up_proj has nan: False
25-01-23 09:19:13 | I | In layer model.layers.0, gradient of model.layers.0.mlp.up_proj has inf: False
25-01-23 09:19:13 | I | In layer model.layers.0, gradient of model.layers.0.mlp.gate_proj has nan: False
25-01-23 09:19:13 | I | In layer model.layers.0, gradient of model.layers.0.mlp.gate_proj has inf: False
25-01-23 09:19:13 | I | In layer model.layers.0, gradient of model.layers.0.mlp.down_proj has nan: False
25-01-23 09:19:13 | I | In layer model.layers.0, gradient of model.layers.0.mlp.down_proj has inf: False
25-01-23 09:19:13 | I | [78] done with optimizer step
25-01-23 09:19:13 | I | epoch 001:     83 / 819200000 loss=1.24921e-05, loss_per_token=0.0511676, loss_sum=419.165, wps=223.8, ups=0.03, wpb=8192, bsz=16, num_updates=79, lr=0.00499952, gnorm=0.115, clip=0, loss_scale=8, train_wall=37, cuda_gb_allocated=17.1, cuda_gb_reserved=18.7, cuda_gb_free=6.6, wall=6901
25-01-23 09:19:13 | I | TRAIN CURRENT LAYER_IDX = 0
25-01-23 09:19:13 | I | in layer model.layers.0
25-01-23 09:19:13 | I | quantizing weights for layer model.layers.0
25-01-23 09:19:14 | I | collecting calibration activations in model.layers.0
25-01-23 09:19:14 | I | collecting calibration activations in model.layers.0
25-01-23 09:19:14 | I | - Quantizing decoder layer model.layers.0
25-01-23 09:19:14 | I |   - Calibrating model.layers.0.self_attn.q_proj.weight
25-01-23 09:19:15 | I |       - range scale = [    1.0000]
25-01-23 09:19:15 | I |         sum  error  = [    0.0440]
25-01-23 09:19:15 | I |         best error  = [    0.0440]
25-01-23 09:19:15 | I |     + error = [0.0440]
25-01-23 09:19:15 | I |       - range scale = [    1.0000]
25-01-23 09:19:15 | I |         sum  error  = [    0.6885]
25-01-23 09:19:15 | I |         best error  = [    0.6885]
25-01-23 09:19:15 | I |     + error = [0.6885]
25-01-23 09:19:16 | I |   - Calibrating model.layers.0.self_attn.k_proj.weight
25-01-23 09:19:16 | I |       - range scale = [    1.0000]
25-01-23 09:19:16 | I |         sum  error  = [    0.0545]
25-01-23 09:19:16 | I |         best error  = [    0.0545]
25-01-23 09:19:16 | I |     + error = [0.0545]
25-01-23 09:19:17 | I |       - range scale = [    1.0000]
25-01-23 09:19:17 | I |         sum  error  = [    0.9459]
25-01-23 09:19:17 | I |         best error  = [    0.9459]
25-01-23 09:19:17 | I |     + error = [0.9459]
25-01-23 09:19:17 | I |   - Calibrating model.layers.0.self_attn.v_proj.weight
25-01-23 09:19:18 | I |       - range scale = [    1.0000]
25-01-23 09:19:18 | I |         sum  error  = [    0.6866]
25-01-23 09:19:18 | I |         best error  = [    0.6866]
25-01-23 09:19:18 | I |     + error = [0.6866]
25-01-23 09:19:19 | I |       - range scale = [    1.0000]
25-01-23 09:19:19 | I |         sum  error  = [    6.3990]
25-01-23 09:19:19 | I |         best error  = [    6.3990]
25-01-23 09:19:19 | I |     + error = [6.3990]
25-01-23 09:19:19 | I |   - Calibrating model.layers.0.self_attn.o_proj.weight
25-01-23 09:19:20 | I |       - range scale = [    1.0000]
25-01-23 09:19:20 | I |         sum  error  = [    0.1950]
25-01-23 09:19:20 | I |         best error  = [    0.1950]
25-01-23 09:19:20 | I |     + error = [0.1950]
25-01-23 09:19:21 | I |       - range scale = [    1.0000]
25-01-23 09:19:21 | I |         sum  error  = [    1.5587]
25-01-23 09:19:21 | I |         best error  = [    1.5587]
25-01-23 09:19:21 | I |     + error = [1.5587]
25-01-23 09:19:21 | I |   - Calibrating model.layers.0.mlp.up_proj.weight
25-01-23 09:19:22 | I |       - range scale = [    1.0000]
25-01-23 09:19:22 | I |         sum  error  = [    1.0756]
25-01-23 09:19:22 | I |         best error  = [    1.0756]
25-01-23 09:19:22 | I |     + error = [1.0756]
25-01-23 09:19:23 | I |       - range scale = [    1.0000]
25-01-23 09:19:23 | I |         sum  error  = [   11.4573]
25-01-23 09:19:23 | I |         best error  = [   11.4573]
25-01-23 09:19:23 | I |     + error = [11.4573]
25-01-23 09:19:23 | I |   - Calibrating model.layers.0.mlp.gate_proj.weight
25-01-23 09:19:24 | I |       - range scale = [    1.0000]
25-01-23 09:19:24 | I |         sum  error  = [    1.1363]
25-01-23 09:19:24 | I |         best error  = [    1.1363]
25-01-23 09:19:24 | I |     + error = [1.1363]
25-01-23 09:19:25 | I |       - range scale = [    1.0000]
25-01-23 09:19:25 | I |         sum  error  = [   11.8440]
25-01-23 09:19:25 | I |         best error  = [   11.8440]
25-01-23 09:19:25 | I |     + error = [11.8440]
25-01-23 09:19:25 | I |   - Calibrating model.layers.0.mlp.down_proj.weight
25-01-23 09:19:26 | I |       - range scale = [    1.0000]
25-01-23 09:19:26 | I |         sum  error  = [    0.1614]
25-01-23 09:19:26 | I |         best error  = [    0.1614]
25-01-23 09:19:26 | I |     + error = [0.1614]
25-01-23 09:19:27 | I |       - range scale = [    1.0000]
25-01-23 09:19:27 | I |         sum  error  = [    1.5729]
25-01-23 09:19:27 | I |         best error  = [    1.5729]
25-01-23 09:19:27 | I |     + error = [1.5729]
25-01-23 09:19:27 | I |   - Quantizing model.layers.0.self_attn.q_proj.weight
25-01-23 09:19:30 | I |   - Quantizing model.layers.0.self_attn.k_proj.weight
25-01-23 09:19:32 | I |   - Quantizing model.layers.0.self_attn.v_proj.weight
25-01-23 09:19:34 | I |   - Quantizing model.layers.0.self_attn.o_proj.weight
25-01-23 09:19:37 | I |   - Quantizing model.layers.0.mlp.up_proj.weight
25-01-23 09:19:39 | I |   - Quantizing model.layers.0.mlp.gate_proj.weight
25-01-23 09:19:41 | I |   - Quantizing model.layers.0.mlp.down_proj.weight
25-01-23 09:19:47 | I | quantizing activations for layer model.layers.0
25-01-23 09:19:48 | I | collecting calibration activations in model.layers.0
25-01-23 09:19:48 | I | collecting calibration activations in model.layers.0
25-01-23 09:19:50 | I | forward this layer
25-01-23 09:19:50 | I | input_file: /data/gyy/lmquant-main/lmquant/data/data_without_preprocess_llama_7b_bs16/shard/val_input_args_layer_0/83.pt
25-01-23 09:19:50 | I | output_file: /data/gyy/lmquant-main/lmquant/data/data_without_preprocess_llama_7b_bs16/shard/val_teacher_output_layer_0/83.pt
25-01-23 09:19:50 | I | In layer model.layers.0, gradient of model.layers.0.self_attn.q_proj has nan: False
25-01-23 09:19:50 | I | In layer model.layers.0, gradient of model.layers.0.self_attn.q_proj has inf: False
25-01-23 09:19:50 | I | In layer model.layers.0, gradient of model.layers.0.self_attn.k_proj has nan: False
25-01-23 09:19:50 | I | In layer model.layers.0, gradient of model.layers.0.self_attn.k_proj has inf: False
25-01-23 09:19:50 | I | In layer model.layers.0, gradient of model.layers.0.self_attn.v_proj has nan: False
25-01-23 09:19:50 | I | In layer model.layers.0, gradient of model.layers.0.self_attn.v_proj has inf: False
25-01-23 09:19:50 | I | In layer model.layers.0, gradient of model.layers.0.self_attn.o_proj has nan: False
25-01-23 09:19:50 | I | In layer model.layers.0, gradient of model.layers.0.self_attn.o_proj has inf: False
25-01-23 09:19:50 | I | In layer model.layers.0, gradient of model.layers.0.mlp.up_proj has nan: False
25-01-23 09:19:50 | I | In layer model.layers.0, gradient of model.layers.0.mlp.up_proj has inf: False
25-01-23 09:19:50 | I | In layer model.layers.0, gradient of model.layers.0.mlp.gate_proj has nan: False
25-01-23 09:19:50 | I | In layer model.layers.0, gradient of model.layers.0.mlp.gate_proj has inf: False
25-01-23 09:19:50 | I | In layer model.layers.0, gradient of model.layers.0.mlp.down_proj has nan: False
25-01-23 09:19:50 | I | In layer model.layers.0, gradient of model.layers.0.mlp.down_proj has inf: False
25-01-23 09:19:50 | I | [79] done with optimizer step
25-01-23 09:19:50 | I | epoch 001:     84 / 819200000 loss=1.23252e-05, loss_per_token=0.0504839, loss_sum=413.564, wps=221.4, ups=0.03, wpb=8192, bsz=16, num_updates=80, lr=0.0049995, gnorm=0.151, clip=0, loss_scale=8, train_wall=37, cuda_gb_allocated=17.1, cuda_gb_reserved=18.8, cuda_gb_free=6.6, wall=6938
25-01-23 09:19:50 | I | begin validation on "valid" subset on rank 0
25-01-23 09:19:50 | I | got valid iterator on "valid" subset on rank 0
25-01-23 09:19:50 | I | Valid: Start iterating over samples
25-01-23 09:19:50 | W |   `pretrained` model kwarg is not of type `str`. Many other model arguments may be ignored. Please do not launch via accelerate or use `parallelize=True` if passing an existing model this way.
25-01-23 09:19:50 | W |   Passed an already-initialized model through `pretrained`, assuming single-process call to evaluate() or custom distributed integration
25-01-23 09:19:50 | I | - Evaluator: gptq
25-01-23 09:19:50 | I | - Tasks: ['wikitext', 'val_valid']
25-01-23 09:19:50 | I | - Batch_size: 8
25-01-23 09:19:50 | I |   + Max_seq_length: 2048
25-01-23 09:21:25 | I |     - Results:
25-01-23 09:21:25 | I |       |  Task   |Version|    Metric     |Value |   |Stderr|
25-01-23 09:21:25 | I |       |---------|------:|---------------|-----:|---|-----:|
25-01-23 09:21:25 | I |       |wikitext |      1|word_perplexity|5.4862|±  |5.4862|
25-01-23 09:21:25 | I |       |val_valid|      1|word_perplexity|5.0300|±  |5.0300|
25-01-23 09:21:25 | I |       
25-01-23 09:21:25 | I |   + Max_seq_length: 4096
25-01-23 09:22:59 | I |     - Results:
25-01-23 09:22:59 | I |       |  Task   |Version|    Metric     |Value |   |Stderr|
25-01-23 09:22:59 | I |       |---------|------:|---------------|-----:|---|-----:|
25-01-23 09:22:59 | I |       |wikitext |      1|word_perplexity|5.1262|±  |5.1262|
25-01-23 09:22:59 | I |       |val_valid|      1|word_perplexity|4.8256|±  |4.8256|
25-01-23 09:22:59 | I |       
25-01-23 09:22:59 | I | in valid, quantize current layer weights
25-01-23 09:22:59 | W | Repo card metadata block was not found. Setting CardData to empty.
25-01-23 09:23:44 | I | collecting calibration activations in model.layers.0
25-01-23 09:23:44 | I | collecting calibration activations in model.layers.0
25-01-23 09:23:44 | I | collecting calibration activations in model.layers.0
25-01-23 09:23:44 | I | collecting calibration activations in model.layers.0
25-01-23 09:23:44 | I | collecting calibration activations in model.layers.0
25-01-23 09:23:44 | I | collecting calibration activations in model.layers.0
25-01-23 09:23:44 | I | collecting calibration activations in model.layers.0
25-01-23 09:23:44 | I | collecting calibration activations in model.layers.0
25-01-23 09:23:44 | I | collecting calibration activations in model.layers.0
25-01-23 09:23:44 | I | collecting calibration activations in model.layers.0
25-01-23 09:23:44 | I | collecting calibration activations in model.layers.0
25-01-23 09:23:44 | I | collecting calibration activations in model.layers.0
25-01-23 09:23:44 | I | collecting calibration activations in model.layers.0
25-01-23 09:23:44 | I | collecting calibration activations in model.layers.0
25-01-23 09:23:44 | I | collecting calibration activations in model.layers.0
25-01-23 09:23:44 | I | collecting calibration activations in model.layers.0
25-01-23 09:23:44 | I | collecting calibration activations in model.layers.0
25-01-23 09:23:44 | I | collecting calibration activations in model.layers.0
25-01-23 09:23:44 | I | collecting calibration activations in model.layers.0
25-01-23 09:23:45 | I | collecting calibration activations in model.layers.0
25-01-23 09:23:45 | I | collecting calibration activations in model.layers.0
25-01-23 09:23:45 | I | collecting calibration activations in model.layers.0
25-01-23 09:23:45 | I | collecting calibration activations in model.layers.0
25-01-23 09:23:45 | I | collecting calibration activations in model.layers.0
25-01-23 09:23:45 | I | collecting calibration activations in model.layers.0
25-01-23 09:23:45 | I | collecting calibration activations in model.layers.0
25-01-23 09:23:45 | I | collecting calibration activations in model.layers.0
25-01-23 09:23:45 | I | collecting calibration activations in model.layers.0
25-01-23 09:23:45 | I | collecting calibration activations in model.layers.0
25-01-23 09:23:45 | I | collecting calibration activations in model.layers.0
25-01-23 09:23:45 | I | collecting calibration activations in model.layers.0
25-01-23 09:23:45 | I | collecting calibration activations in model.layers.0
25-01-23 09:23:45 | I | collecting calibration activations in model.layers.0
25-01-23 09:23:45 | I | collecting calibration activations in model.layers.0
25-01-23 09:23:45 | I | collecting calibration activations in model.layers.0
25-01-23 09:23:45 | I | collecting calibration activations in model.layers.0
25-01-23 09:23:45 | I | collecting calibration activations in model.layers.0
25-01-23 09:23:45 | I | collecting calibration activations in model.layers.0
25-01-23 09:23:46 | I | collecting calibration activations in model.layers.0
25-01-23 09:23:46 | I | collecting calibration activations in model.layers.0
25-01-23 09:23:46 | I | collecting calibration activations in model.layers.0
25-01-23 09:23:46 | I | collecting calibration activations in model.layers.0
25-01-23 09:23:46 | I | collecting calibration activations in model.layers.0
25-01-23 09:23:46 | I | collecting calibration activations in model.layers.0
25-01-23 09:23:46 | I | collecting calibration activations in model.layers.0
25-01-23 09:23:46 | I | collecting calibration activations in model.layers.0
25-01-23 09:23:46 | I | collecting calibration activations in model.layers.0
25-01-23 09:23:46 | I | collecting calibration activations in model.layers.0
25-01-23 09:23:46 | I | collecting calibration activations in model.layers.0
25-01-23 09:23:46 | I | collecting calibration activations in model.layers.0
25-01-23 09:23:46 | I | collecting calibration activations in model.layers.0
25-01-23 09:23:46 | I | collecting calibration activations in model.layers.0
25-01-23 09:23:46 | I | collecting calibration activations in model.layers.0
25-01-23 09:23:46 | I | collecting calibration activations in model.layers.0
25-01-23 09:23:46 | I | collecting calibration activations in model.layers.0
25-01-23 09:23:46 | I | collecting calibration activations in model.layers.0
25-01-23 09:23:46 | I | collecting calibration activations in model.layers.0
25-01-23 09:23:47 | I | collecting calibration activations in model.layers.0
25-01-23 09:23:47 | I | collecting calibration activations in model.layers.0
25-01-23 09:23:47 | I | collecting calibration activations in model.layers.0
25-01-23 09:23:47 | I | collecting calibration activations in model.layers.0
25-01-23 09:23:47 | I | collecting calibration activations in model.layers.0
25-01-23 09:23:47 | I | collecting calibration activations in model.layers.0
25-01-23 09:23:47 | I | collecting calibration activations in model.layers.0
25-01-23 09:23:47 | I | collecting calibration activations in model.layers.0
25-01-23 09:23:47 | I | collecting calibration activations in model.layers.0
25-01-23 09:23:47 | I | collecting calibration activations in model.layers.0
25-01-23 09:23:47 | I | collecting calibration activations in model.layers.0
25-01-23 09:23:47 | I | collecting calibration activations in model.layers.0
25-01-23 09:23:47 | I | collecting calibration activations in model.layers.0
25-01-23 09:23:47 | I | collecting calibration activations in model.layers.0
25-01-23 09:23:47 | I | collecting calibration activations in model.layers.0
25-01-23 09:23:47 | I | collecting calibration activations in model.layers.0
25-01-23 09:23:47 | I | collecting calibration activations in model.layers.0
25-01-23 09:23:47 | I | collecting calibration activations in model.layers.0
25-01-23 09:23:47 | I | collecting calibration activations in model.layers.0
25-01-23 09:23:48 | I | collecting calibration activations in model.layers.0
25-01-23 09:23:48 | I | collecting calibration activations in model.layers.0
25-01-23 09:23:48 | I | collecting calibration activations in model.layers.0
25-01-23 09:23:48 | I | collecting calibration activations in model.layers.0
25-01-23 09:23:48 | I | collecting calibration activations in model.layers.0
25-01-23 09:23:48 | I | collecting calibration activations in model.layers.0
25-01-23 09:23:48 | I | collecting calibration activations in model.layers.0
25-01-23 09:23:48 | I | collecting calibration activations in model.layers.0
25-01-23 09:23:48 | I | collecting calibration activations in model.layers.0
25-01-23 09:23:48 | I | collecting calibration activations in model.layers.0
25-01-23 09:23:48 | I | collecting calibration activations in model.layers.0
25-01-23 09:23:48 | I | collecting calibration activations in model.layers.0
25-01-23 09:23:48 | I | collecting calibration activations in model.layers.0
25-01-23 09:23:48 | I | collecting calibration activations in model.layers.0
25-01-23 09:23:48 | I | collecting calibration activations in model.layers.0
25-01-23 09:23:48 | I | collecting calibration activations in model.layers.0
25-01-23 09:23:48 | I | collecting calibration activations in model.layers.0
25-01-23 09:23:48 | I | collecting calibration activations in model.layers.0
25-01-23 09:23:48 | I | collecting calibration activations in model.layers.0
25-01-23 09:23:49 | I | collecting calibration activations in model.layers.0
25-01-23 09:23:49 | I | collecting calibration activations in model.layers.0
25-01-23 09:23:49 | I | collecting calibration activations in model.layers.0
25-01-23 09:23:49 | I | collecting calibration activations in model.layers.0
25-01-23 09:23:49 | I | collecting calibration activations in model.layers.0
25-01-23 09:23:49 | I | collecting calibration activations in model.layers.0
25-01-23 09:23:49 | I | collecting calibration activations in model.layers.0
25-01-23 09:23:49 | I | collecting calibration activations in model.layers.0
25-01-23 09:23:49 | I | collecting calibration activations in model.layers.0
25-01-23 09:23:49 | I | collecting calibration activations in model.layers.0
25-01-23 09:23:49 | I | collecting calibration activations in model.layers.0
25-01-23 09:23:49 | I | collecting calibration activations in model.layers.0
25-01-23 09:23:49 | I | collecting calibration activations in model.layers.0
25-01-23 09:23:49 | I | collecting calibration activations in model.layers.0
25-01-23 09:23:49 | I | collecting calibration activations in model.layers.0
25-01-23 09:23:49 | I | collecting calibration activations in model.layers.0
25-01-23 09:23:49 | I | collecting calibration activations in model.layers.0
25-01-23 09:23:49 | I | collecting calibration activations in model.layers.0
25-01-23 09:23:49 | I | collecting calibration activations in model.layers.0
25-01-23 09:23:50 | I | collecting calibration activations in model.layers.0
25-01-23 09:23:50 | I | collecting calibration activations in model.layers.0
25-01-23 09:23:50 | I | collecting calibration activations in model.layers.0
25-01-23 09:23:50 | I | collecting calibration activations in model.layers.0
25-01-23 09:23:50 | I | collecting calibration activations in model.layers.0
25-01-23 09:23:50 | I | collecting calibration activations in model.layers.0
25-01-23 09:23:50 | I | collecting calibration activations in model.layers.0
25-01-23 09:23:50 | I | collecting calibration activations in model.layers.0
25-01-23 09:23:50 | I | collecting calibration activations in model.layers.0
25-01-23 09:23:50 | I | collecting calibration activations in model.layers.0
25-01-23 09:23:50 | I | collecting calibration activations in model.layers.0
25-01-23 09:23:50 | I | collecting calibration activations in model.layers.0
25-01-23 09:23:50 | I | collecting calibration activations in model.layers.0
25-01-23 09:23:50 | I | collecting calibration activations in model.layers.0
25-01-23 09:23:52 | I |       - range scale = [    1.0000]
25-01-23 09:23:52 | I |         sum  error  = [    0.1586]
25-01-23 09:23:52 | I |         best error  = [    0.1586]
25-01-23 09:23:52 | I |     + error = [0.1586]
25-01-23 09:23:53 | I |       - range scale = [    1.0000]
25-01-23 09:23:53 | I |         sum  error  = [    2.7540]
25-01-23 09:23:53 | I |         best error  = [    2.7540]
25-01-23 09:23:53 | I |     + error = [2.7540]
25-01-23 09:23:55 | I |       - range scale = [    1.0000]
25-01-23 09:23:55 | I |         sum  error  = [    0.1971]
25-01-23 09:23:55 | I |         best error  = [    0.1971]
25-01-23 09:23:55 | I |     + error = [0.1971]
25-01-23 09:23:57 | I |       - range scale = [    1.0000]
25-01-23 09:23:57 | I |         sum  error  = [    3.7152]
25-01-23 09:23:57 | I |         best error  = [    3.7152]
25-01-23 09:23:57 | I |     + error = [3.7152]
25-01-23 09:23:58 | I |       - range scale = [    1.0000]
25-01-23 09:23:58 | I |         sum  error  = [    0.6467]
25-01-23 09:23:58 | I |         best error  = [    0.6467]
25-01-23 09:23:58 | I |     + error = [0.6467]
25-01-23 09:23:58 | I |       - range scale = [    1.0000]
25-01-23 09:23:58 | I |         sum  error  = [    5.9557]
25-01-23 09:23:58 | I |         best error  = [    5.9557]
25-01-23 09:23:58 | I |     + error = [5.9557]
25-01-23 09:23:59 | I |       - range scale = [    1.0000]
25-01-23 09:23:59 | I |         sum  error  = [    0.1498]
25-01-23 09:23:59 | I |         best error  = [    0.1498]
25-01-23 09:23:59 | I |     + error = [0.1498]
25-01-23 09:24:00 | I |       - range scale = [    1.0000]
25-01-23 09:24:00 | I |         sum  error  = [    1.2348]
25-01-23 09:24:00 | I |         best error  = [    1.2348]
25-01-23 09:24:00 | I |     + error = [1.2348]
25-01-23 09:24:01 | I |       - range scale = [    1.0000]
25-01-23 09:24:01 | I |         sum  error  = [    1.0858]
25-01-23 09:24:01 | I |         best error  = [    1.0858]
25-01-23 09:24:01 | I |     + error = [1.0858]
25-01-23 09:24:02 | I |       - range scale = [    1.0000]
25-01-23 09:24:02 | I |         sum  error  = [   11.5732]
25-01-23 09:24:02 | I |         best error  = [   11.5732]
25-01-23 09:24:02 | I |     + error = [11.5732]
25-01-23 09:24:03 | I |       - range scale = [    1.0000]
25-01-23 09:24:03 | I |         sum  error  = [    1.1518]
25-01-23 09:24:03 | I |         best error  = [    1.1518]
25-01-23 09:24:03 | I |     + error = [1.1518]
25-01-23 09:24:04 | I |       - range scale = [    1.0000]
25-01-23 09:24:04 | I |         sum  error  = [   11.9476]
25-01-23 09:24:04 | I |         best error  = [   11.9476]
25-01-23 09:24:04 | I |     + error = [11.9476]
25-01-23 09:24:05 | I |       - range scale = [    1.0000]
25-01-23 09:24:05 | I |         sum  error  = [    0.1834]
25-01-23 09:24:05 | I |         best error  = [    0.1834]
25-01-23 09:24:05 | I |     + error = [0.1834]
25-01-23 09:24:07 | I |       - range scale = [    1.0000]
25-01-23 09:24:07 | I |         sum  error  = [    1.8081]
25-01-23 09:24:07 | I |         best error  = [    1.8081]
25-01-23 09:24:07 | I |     + error = [1.8081]
25-01-23 09:24:32 | I | in valid, quantize current layer acts
25-01-23 09:24:34 | I | collecting calibration activations in model.layers.0
25-01-23 09:24:34 | I | collecting calibration activations in model.layers.0
25-01-23 09:24:34 | I | collecting calibration activations in model.layers.0
25-01-23 09:24:34 | I | collecting calibration activations in model.layers.0
25-01-23 09:24:34 | I | collecting calibration activations in model.layers.0
25-01-23 09:24:34 | I | collecting calibration activations in model.layers.0
25-01-23 09:24:34 | I | collecting calibration activations in model.layers.0
25-01-23 09:24:34 | I | collecting calibration activations in model.layers.0
25-01-23 09:24:34 | I | collecting calibration activations in model.layers.0
25-01-23 09:24:34 | I | collecting calibration activations in model.layers.0
25-01-23 09:24:35 | I | collecting calibration activations in model.layers.0
25-01-23 09:24:35 | I | collecting calibration activations in model.layers.0
25-01-23 09:24:35 | I | collecting calibration activations in model.layers.0
25-01-23 09:24:35 | I | collecting calibration activations in model.layers.0
25-01-23 09:24:35 | I | collecting calibration activations in model.layers.0
25-01-23 09:24:35 | I | collecting calibration activations in model.layers.0
25-01-23 09:24:35 | I | collecting calibration activations in model.layers.0
25-01-23 09:24:35 | I | collecting calibration activations in model.layers.0
25-01-23 09:24:35 | I | collecting calibration activations in model.layers.0
25-01-23 09:24:35 | I | collecting calibration activations in model.layers.0
25-01-23 09:24:35 | I | collecting calibration activations in model.layers.0
25-01-23 09:24:35 | I | collecting calibration activations in model.layers.0
25-01-23 09:24:35 | I | collecting calibration activations in model.layers.0
25-01-23 09:24:35 | I | collecting calibration activations in model.layers.0
25-01-23 09:24:35 | I | collecting calibration activations in model.layers.0
25-01-23 09:24:36 | I | collecting calibration activations in model.layers.0
25-01-23 09:24:36 | I | collecting calibration activations in model.layers.0
25-01-23 09:24:36 | I | collecting calibration activations in model.layers.0
25-01-23 09:24:36 | I | collecting calibration activations in model.layers.0
25-01-23 09:24:36 | I | collecting calibration activations in model.layers.0
25-01-23 09:24:36 | I | collecting calibration activations in model.layers.0
25-01-23 09:24:36 | I | collecting calibration activations in model.layers.0
25-01-23 09:24:36 | I | collecting calibration activations in model.layers.0
25-01-23 09:24:36 | I | collecting calibration activations in model.layers.0
25-01-23 09:24:36 | I | collecting calibration activations in model.layers.0
25-01-23 09:24:36 | I | collecting calibration activations in model.layers.0
25-01-23 09:24:36 | I | collecting calibration activations in model.layers.0
25-01-23 09:24:36 | I | collecting calibration activations in model.layers.0
25-01-23 09:24:36 | I | collecting calibration activations in model.layers.0
25-01-23 09:24:36 | I | collecting calibration activations in model.layers.0
25-01-23 09:24:36 | I | collecting calibration activations in model.layers.0
25-01-23 09:24:37 | I | collecting calibration activations in model.layers.0
25-01-23 09:24:37 | I | collecting calibration activations in model.layers.0
25-01-23 09:24:37 | I | collecting calibration activations in model.layers.0
25-01-23 09:24:37 | I | collecting calibration activations in model.layers.0
25-01-23 09:24:37 | I | collecting calibration activations in model.layers.0
25-01-23 09:24:37 | I | collecting calibration activations in model.layers.0
25-01-23 09:24:37 | I | collecting calibration activations in model.layers.0
25-01-23 09:24:37 | I | collecting calibration activations in model.layers.0
25-01-23 09:24:37 | I | collecting calibration activations in model.layers.0
25-01-23 09:24:37 | I | collecting calibration activations in model.layers.0
25-01-23 09:24:37 | I | collecting calibration activations in model.layers.0
25-01-23 09:24:37 | I | collecting calibration activations in model.layers.0
25-01-23 09:24:37 | I | collecting calibration activations in model.layers.0
25-01-23 09:24:37 | I | collecting calibration activations in model.layers.0
25-01-23 09:24:37 | I | collecting calibration activations in model.layers.0
25-01-23 09:24:38 | I | collecting calibration activations in model.layers.0
25-01-23 09:24:38 | I | collecting calibration activations in model.layers.0
25-01-23 09:24:38 | I | collecting calibration activations in model.layers.0
25-01-23 09:24:38 | I | collecting calibration activations in model.layers.0
25-01-23 09:24:38 | I | collecting calibration activations in model.layers.0
25-01-23 09:24:38 | I | collecting calibration activations in model.layers.0
25-01-23 09:24:38 | I | collecting calibration activations in model.layers.0
25-01-23 09:24:38 | I | collecting calibration activations in model.layers.0
25-01-23 09:24:38 | I | collecting calibration activations in model.layers.0
25-01-23 09:24:38 | I | collecting calibration activations in model.layers.0
25-01-23 09:24:38 | I | collecting calibration activations in model.layers.0
25-01-23 09:24:38 | I | collecting calibration activations in model.layers.0
25-01-23 09:24:38 | I | collecting calibration activations in model.layers.0
25-01-23 09:24:38 | I | collecting calibration activations in model.layers.0
25-01-23 09:24:38 | I | collecting calibration activations in model.layers.0
25-01-23 09:24:38 | I | collecting calibration activations in model.layers.0
25-01-23 09:24:39 | I | collecting calibration activations in model.layers.0
25-01-23 09:24:39 | I | collecting calibration activations in model.layers.0
25-01-23 09:24:39 | I | collecting calibration activations in model.layers.0
25-01-23 09:24:39 | I | collecting calibration activations in model.layers.0
25-01-23 09:24:39 | I | collecting calibration activations in model.layers.0
25-01-23 09:24:39 | I | collecting calibration activations in model.layers.0
25-01-23 09:24:39 | I | collecting calibration activations in model.layers.0
25-01-23 09:24:39 | I | collecting calibration activations in model.layers.0
25-01-23 09:24:39 | I | collecting calibration activations in model.layers.0
25-01-23 09:24:39 | I | collecting calibration activations in model.layers.0
25-01-23 09:24:39 | I | collecting calibration activations in model.layers.0
25-01-23 09:24:39 | I | collecting calibration activations in model.layers.0
25-01-23 09:24:39 | I | collecting calibration activations in model.layers.0
25-01-23 09:24:39 | I | collecting calibration activations in model.layers.0
25-01-23 09:24:39 | I | collecting calibration activations in model.layers.0
25-01-23 09:24:40 | I | collecting calibration activations in model.layers.0
25-01-23 09:24:40 | I | collecting calibration activations in model.layers.0
25-01-23 09:24:40 | I | collecting calibration activations in model.layers.0
25-01-23 09:24:40 | I | collecting calibration activations in model.layers.0
25-01-23 09:24:40 | I | collecting calibration activations in model.layers.0
25-01-23 09:24:40 | I | collecting calibration activations in model.layers.0
25-01-23 09:24:40 | I | collecting calibration activations in model.layers.0
25-01-23 09:24:40 | I | collecting calibration activations in model.layers.0
25-01-23 09:24:40 | I | collecting calibration activations in model.layers.0
25-01-23 09:24:40 | I | collecting calibration activations in model.layers.0
25-01-23 09:24:40 | I | collecting calibration activations in model.layers.0
25-01-23 09:24:40 | I | collecting calibration activations in model.layers.0
25-01-23 09:24:40 | I | collecting calibration activations in model.layers.0
25-01-23 09:24:40 | I | collecting calibration activations in model.layers.0
25-01-23 09:24:40 | I | collecting calibration activations in model.layers.0
25-01-23 09:24:41 | I | collecting calibration activations in model.layers.0
25-01-23 09:24:41 | I | collecting calibration activations in model.layers.0
25-01-23 09:24:41 | I | collecting calibration activations in model.layers.0
25-01-23 09:24:41 | I | collecting calibration activations in model.layers.0
25-01-23 09:24:41 | I | collecting calibration activations in model.layers.0
25-01-23 09:24:41 | I | collecting calibration activations in model.layers.0
25-01-23 09:24:41 | I | collecting calibration activations in model.layers.0
25-01-23 09:24:41 | I | collecting calibration activations in model.layers.0
25-01-23 09:24:41 | I | collecting calibration activations in model.layers.0
25-01-23 09:24:41 | I | collecting calibration activations in model.layers.0
25-01-23 09:24:41 | I | collecting calibration activations in model.layers.0
25-01-23 09:24:41 | I | collecting calibration activations in model.layers.0
25-01-23 09:24:41 | I | collecting calibration activations in model.layers.0
25-01-23 09:24:41 | I | collecting calibration activations in model.layers.0
25-01-23 09:24:41 | I | collecting calibration activations in model.layers.0
25-01-23 09:24:42 | I | collecting calibration activations in model.layers.0
25-01-23 09:24:42 | I | collecting calibration activations in model.layers.0
25-01-23 09:24:42 | I | collecting calibration activations in model.layers.0
25-01-23 09:24:42 | I | collecting calibration activations in model.layers.0
25-01-23 09:24:42 | I | collecting calibration activations in model.layers.0
25-01-23 09:24:42 | I | collecting calibration activations in model.layers.0
25-01-23 09:24:42 | I | collecting calibration activations in model.layers.0
25-01-23 09:24:42 | I | collecting calibration activations in model.layers.0
25-01-23 09:24:42 | I | collecting calibration activations in model.layers.0
25-01-23 09:24:42 | I | collecting calibration activations in model.layers.0
25-01-23 09:24:42 | I | collecting calibration activations in model.layers.0
25-01-23 09:24:44 | I | use gptq_eval(lmquant) to calculate ppl, partly quanted
25-01-23 09:24:44 | W |   `pretrained` model kwarg is not of type `str`. Many other model arguments may be ignored. Please do not launch via accelerate or use `parallelize=True` if passing an existing model this way.
25-01-23 09:24:44 | W |   Passed an already-initialized model through `pretrained`, assuming single-process call to evaluate() or custom distributed integration
25-01-23 09:24:44 | I | - Evaluator: gptq
25-01-23 09:24:44 | I | - Tasks: ['wikitext', 'val_valid']
25-01-23 09:24:44 | I | - Batch_size: 8
25-01-23 09:24:44 | I |   + Max_seq_length: 2048
25-01-23 09:26:22 | I |     - Results:
25-01-23 09:26:22 | I |       |  Task   |Version|    Metric     |Value |   |Stderr|
25-01-23 09:26:22 | I |       |---------|------:|---------------|-----:|---|-----:|
25-01-23 09:26:22 | I |       |wikitext |      1|word_perplexity|5.4917|±  |5.4917|
25-01-23 09:26:22 | I |       |val_valid|      1|word_perplexity|5.0618|±  |5.0618|
25-01-23 09:26:22 | I |       
25-01-23 09:26:22 | I |   + Max_seq_length: 4096
25-01-23 09:27:57 | I |     - Results:
25-01-23 09:27:57 | I |       |  Task   |Version|    Metric     |Value |   |Stderr|
25-01-23 09:27:57 | I |       |---------|------:|---------------|-----:|---|-----:|
25-01-23 09:27:57 | I |       |wikitext |      1|word_perplexity|5.1292|±  |5.1292|
25-01-23 09:27:57 | I |       |val_valid|      1|word_perplexity|4.8571|±  |4.8571|
25-01-23 09:27:57 | I |       
25-01-23 09:28:57 | I | epoch 001 | valid on 'valid' subset:    102 / 819200000 loss_valid=2.15, lmquant_ppl_result_val=-1, lmquant_ppl_result_wikitext=-1, ppl=4.44, wps=7238, wpb=4058.2, bsz=2, num_updates=80, lmquant_ppl_wikitext_all_quanted=5.48616, lmquant_ppl_val_all_quanted=5.03004, lmquant_ppl_wikitext_partly_quanted=5.49172, lmquant_ppl_val_partly_quanted=5.06178
25-01-23 09:28:57 | I | epoch 001 | valid on 'valid' subset | loss_valid 2.15 | lmquant_ppl_result_val -1 | lmquant_ppl_result_wikitext -1 | ppl 4.44 | wps 7238 | wpb 4058.2 | bsz 2 | num_updates 80 | lmquant_ppl_wikitext_all_quanted 5.48616 | lmquant_ppl_val_all_quanted 5.03004 | lmquant_ppl_wikitext_partly_quanted 5.49172 | lmquant_ppl_val_partly_quanted 5.06178
25-01-23 09:28:57 | I | TRAIN CURRENT LAYER_IDX = 0
25-01-23 09:28:57 | I | in layer model.layers.0
25-01-23 09:28:57 | I | quantizing weights for layer model.layers.0
25-01-23 09:28:57 | I | collecting calibration activations in model.layers.0
25-01-23 09:28:57 | I | collecting calibration activations in model.layers.0
25-01-23 09:28:58 | I | - Quantizing decoder layer model.layers.0
25-01-23 09:28:58 | I |   - Calibrating model.layers.0.self_attn.q_proj.weight
25-01-23 09:28:58 | I |       - range scale = [    1.0000]
25-01-23 09:28:58 | I |         sum  error  = [    0.0426]
25-01-23 09:28:58 | I |         best error  = [    0.0426]
25-01-23 09:28:58 | I |     + error = [0.0426]
25-01-23 09:28:59 | I |       - range scale = [    1.0000]
25-01-23 09:28:59 | I |         sum  error  = [    0.6683]
25-01-23 09:28:59 | I |         best error  = [    0.6683]
25-01-23 09:28:59 | I |     + error = [0.6683]
25-01-23 09:28:59 | I |   - Calibrating model.layers.0.self_attn.k_proj.weight
25-01-23 09:29:00 | I |       - range scale = [    1.0000]
25-01-23 09:29:00 | I |         sum  error  = [    0.0531]
25-01-23 09:29:00 | I |         best error  = [    0.0531]
25-01-23 09:29:00 | I |     + error = [0.0531]
25-01-23 09:29:01 | I |       - range scale = [    1.0000]
25-01-23 09:29:01 | I |         sum  error  = [    0.9188]
25-01-23 09:29:01 | I |         best error  = [    0.9188]
25-01-23 09:29:01 | I |     + error = [0.9188]
25-01-23 09:29:01 | I |   - Calibrating model.layers.0.self_attn.v_proj.weight
25-01-23 09:29:01 | I |       - range scale = [    1.0000]
25-01-23 09:29:01 | I |         sum  error  = [    0.6849]
25-01-23 09:29:01 | I |         best error  = [    0.6849]
25-01-23 09:29:01 | I |     + error = [0.6849]
25-01-23 09:29:02 | I |       - range scale = [    1.0000]
25-01-23 09:29:02 | I |         sum  error  = [    6.3420]
25-01-23 09:29:02 | I |         best error  = [    6.3420]
25-01-23 09:29:02 | I |     + error = [6.3420]
25-01-23 09:29:02 | I |   - Calibrating model.layers.0.self_attn.o_proj.weight
25-01-23 09:29:03 | I |       - range scale = [    1.0000]
25-01-23 09:29:03 | I |         sum  error  = [    0.1997]
25-01-23 09:29:03 | I |         best error  = [    0.1997]
25-01-23 09:29:03 | I |     + error = [0.1997]
25-01-23 09:29:04 | I |       - range scale = [    1.0000]
25-01-23 09:29:04 | I |         sum  error  = [    1.5916]
25-01-23 09:29:04 | I |         best error  = [    1.5916]
25-01-23 09:29:04 | I |     + error = [1.5916]
25-01-23 09:29:04 | I |   - Calibrating model.layers.0.mlp.up_proj.weight
25-01-23 09:29:05 | I |       - range scale = [    1.0000]
25-01-23 09:29:05 | I |         sum  error  = [    1.0500]
25-01-23 09:29:05 | I |         best error  = [    1.0500]
25-01-23 09:29:05 | I |     + error = [1.0500]
25-01-23 09:29:06 | I |       - range scale = [    1.0000]
25-01-23 09:29:06 | I |         sum  error  = [   11.1676]
25-01-23 09:29:06 | I |         best error  = [   11.1676]
25-01-23 09:29:06 | I |     + error = [11.1676]
25-01-23 09:29:06 | I |   - Calibrating model.layers.0.mlp.gate_proj.weight
25-01-23 09:29:07 | I |       - range scale = [    1.0000]
25-01-23 09:29:07 | I |         sum  error  = [    1.1135]
25-01-23 09:29:07 | I |         best error  = [    1.1135]
25-01-23 09:29:07 | I |     + error = [1.1135]
25-01-23 09:29:08 | I |       - range scale = [    1.0000]
25-01-23 09:29:08 | I |         sum  error  = [   11.5515]
25-01-23 09:29:08 | I |         best error  = [   11.5515]
25-01-23 09:29:08 | I |     + error = [11.5515]
25-01-23 09:29:08 | I |   - Calibrating model.layers.0.mlp.down_proj.weight
25-01-23 09:29:09 | I |       - range scale = [    1.0000]
25-01-23 09:29:09 | I |         sum  error  = [    0.1562]
25-01-23 09:29:09 | I |         best error  = [    0.1562]
25-01-23 09:29:09 | I |     + error = [0.1562]
25-01-23 09:29:10 | I |       - range scale = [    1.0000]
25-01-23 09:29:10 | I |         sum  error  = [    1.5198]
25-01-23 09:29:10 | I |         best error  = [    1.5198]
25-01-23 09:29:10 | I |     + error = [1.5198]
25-01-23 09:29:10 | I |   - Quantizing model.layers.0.self_attn.q_proj.weight
25-01-23 09:29:13 | I |   - Quantizing model.layers.0.self_attn.k_proj.weight
25-01-23 09:29:16 | I |   - Quantizing model.layers.0.self_attn.v_proj.weight
25-01-23 09:29:19 | I |   - Quantizing model.layers.0.self_attn.o_proj.weight
25-01-23 09:29:22 | I |   - Quantizing model.layers.0.mlp.up_proj.weight
25-01-23 09:29:25 | I |   - Quantizing model.layers.0.mlp.gate_proj.weight
25-01-23 09:29:28 | I |   - Quantizing model.layers.0.mlp.down_proj.weight
25-01-23 09:29:36 | I | quantizing activations for layer model.layers.0
25-01-23 09:29:36 | I | collecting calibration activations in model.layers.0
25-01-23 09:29:36 | I | collecting calibration activations in model.layers.0
25-01-23 09:29:38 | I | forward this layer
25-01-23 09:29:38 | I | input_file: /data/gyy/lmquant-main/lmquant/data/data_without_preprocess_llama_7b_bs16/shard/val_input_args_layer_0/84.pt
25-01-23 09:29:38 | I | output_file: /data/gyy/lmquant-main/lmquant/data/data_without_preprocess_llama_7b_bs16/shard/val_teacher_output_layer_0/84.pt
25-01-23 09:29:39 | I | In layer model.layers.0, gradient of model.layers.0.self_attn.q_proj has nan: False
25-01-23 09:29:39 | I | In layer model.layers.0, gradient of model.layers.0.self_attn.q_proj has inf: False
25-01-23 09:29:39 | I | In layer model.layers.0, gradient of model.layers.0.self_attn.k_proj has nan: False
25-01-23 09:29:39 | I | In layer model.layers.0, gradient of model.layers.0.self_attn.k_proj has inf: False
25-01-23 09:29:39 | I | In layer model.layers.0, gradient of model.layers.0.self_attn.v_proj has nan: False
25-01-23 09:29:39 | I | In layer model.layers.0, gradient of model.layers.0.self_attn.v_proj has inf: False
25-01-23 09:29:39 | I | In layer model.layers.0, gradient of model.layers.0.self_attn.o_proj has nan: False
25-01-23 09:29:39 | I | In layer model.layers.0, gradient of model.layers.0.self_attn.o_proj has inf: False
25-01-23 09:29:39 | I | In layer model.layers.0, gradient of model.layers.0.mlp.up_proj has nan: False
25-01-23 09:29:39 | I | In layer model.layers.0, gradient of model.layers.0.mlp.up_proj has inf: False
25-01-23 09:29:39 | I | In layer model.layers.0, gradient of model.layers.0.mlp.gate_proj has nan: False
25-01-23 09:29:39 | I | In layer model.layers.0, gradient of model.layers.0.mlp.gate_proj has inf: False
25-01-23 09:29:39 | I | In layer model.layers.0, gradient of model.layers.0.mlp.down_proj has nan: False
25-01-23 09:29:39 | I | In layer model.layers.0, gradient of model.layers.0.mlp.down_proj has inf: False
25-01-23 09:29:39 | I | [80] done with optimizer step
25-01-23 09:29:39 | I | epoch 001:     85 / 819200000 loss=1.06824e-05, loss_per_token=0.0437551, loss_sum=358.442, wps=13.9, ups=0, wpb=8192, bsz=16, num_updates=81, lr=0.00499948, gnorm=0.147, clip=0, loss_scale=8, train_wall=42, cuda_gb_allocated=17.2, cuda_gb_reserved=21.3, cuda_gb_free=6.5, wall=7526
25-01-23 09:29:39 | I | TRAIN CURRENT LAYER_IDX = 0
25-01-23 09:29:39 | I | in layer model.layers.0
25-01-23 09:29:39 | I | quantizing weights for layer model.layers.0
25-01-23 09:29:39 | I | collecting calibration activations in model.layers.0
25-01-23 09:29:39 | I | collecting calibration activations in model.layers.0
25-01-23 09:29:40 | I | - Quantizing decoder layer model.layers.0
25-01-23 09:29:40 | I |   - Calibrating model.layers.0.self_attn.q_proj.weight
25-01-23 09:29:40 | I |       - range scale = [    1.0000]
25-01-23 09:29:40 | I |         sum  error  = [    0.0424]
25-01-23 09:29:40 | I |         best error  = [    0.0424]
25-01-23 09:29:40 | I |     + error = [0.0424]
25-01-23 09:29:41 | I |       - range scale = [    1.0000]
25-01-23 09:29:41 | I |         sum  error  = [    0.6516]
25-01-23 09:29:41 | I |         best error  = [    0.6516]
25-01-23 09:29:41 | I |     + error = [0.6516]
25-01-23 09:29:41 | I |   - Calibrating model.layers.0.self_attn.k_proj.weight
25-01-23 09:29:42 | I |       - range scale = [    1.0000]
25-01-23 09:29:42 | I |         sum  error  = [    0.0525]
25-01-23 09:29:42 | I |         best error  = [    0.0525]
25-01-23 09:29:42 | I |     + error = [0.0525]
25-01-23 09:29:43 | I |       - range scale = [    1.0000]
25-01-23 09:29:43 | I |         sum  error  = [    0.9063]
25-01-23 09:29:43 | I |         best error  = [    0.9063]
25-01-23 09:29:43 | I |     + error = [0.9063]
25-01-23 09:29:43 | I |   - Calibrating model.layers.0.self_attn.v_proj.weight
25-01-23 09:29:43 | I |       - range scale = [    1.0000]
25-01-23 09:29:43 | I |         sum  error  = [    0.6818]
25-01-23 09:29:43 | I |         best error  = [    0.6818]
25-01-23 09:29:43 | I |     + error = [0.6818]
25-01-23 09:29:44 | I |       - range scale = [    1.0000]
25-01-23 09:29:44 | I |         sum  error  = [    6.2895]
25-01-23 09:29:44 | I |         best error  = [    6.2895]
25-01-23 09:29:44 | I |     + error = [6.2895]
25-01-23 09:29:45 | I |   - Calibrating model.layers.0.self_attn.o_proj.weight
25-01-23 09:29:45 | I |       - range scale = [    1.0000]
25-01-23 09:29:45 | I |         sum  error  = [    0.1949]
25-01-23 09:29:45 | I |         best error  = [    0.1949]
25-01-23 09:29:45 | I |     + error = [0.1949]
25-01-23 09:29:46 | I |       - range scale = [    1.0000]
25-01-23 09:29:46 | I |         sum  error  = [    1.5515]
25-01-23 09:29:46 | I |         best error  = [    1.5515]
25-01-23 09:29:46 | I |     + error = [1.5515]
25-01-23 09:29:46 | I |   - Calibrating model.layers.0.mlp.up_proj.weight
25-01-23 09:29:47 | I |       - range scale = [    1.0000]
25-01-23 09:29:47 | I |         sum  error  = [    1.0540]
25-01-23 09:29:47 | I |         best error  = [    1.0540]
25-01-23 09:29:47 | I |     + error = [1.0540]
25-01-23 09:29:48 | I |       - range scale = [    1.0000]
25-01-23 09:29:48 | I |         sum  error  = [   11.2116]
25-01-23 09:29:48 | I |         best error  = [   11.2116]
25-01-23 09:29:48 | I |     + error = [11.2116]
25-01-23 09:29:48 | I |   - Calibrating model.layers.0.mlp.gate_proj.weight
25-01-23 09:29:49 | I |       - range scale = [    1.0000]
25-01-23 09:29:49 | I |         sum  error  = [    1.1167]
25-01-23 09:29:49 | I |         best error  = [    1.1167]
25-01-23 09:29:49 | I |     + error = [1.1167]
25-01-23 09:29:50 | I |       - range scale = [    1.0000]
25-01-23 09:29:50 | I |         sum  error  = [   11.5970]
25-01-23 09:29:50 | I |         best error  = [   11.5970]
25-01-23 09:29:50 | I |     + error = [11.5970]
25-01-23 09:29:50 | I |   - Calibrating model.layers.0.mlp.down_proj.weight
25-01-23 09:29:51 | I |       - range scale = [    1.0000]
25-01-23 09:29:51 | I |         sum  error  = [    0.1580]
25-01-23 09:29:51 | I |         best error  = [    0.1580]
25-01-23 09:29:51 | I |     + error = [0.1580]
25-01-23 09:29:52 | I |       - range scale = [    1.0000]
25-01-23 09:29:52 | I |         sum  error  = [    1.5431]
25-01-23 09:29:52 | I |         best error  = [    1.5431]
25-01-23 09:29:52 | I |     + error = [1.5431]
25-01-23 09:29:53 | I |   - Quantizing model.layers.0.self_attn.q_proj.weight
25-01-23 09:29:56 | I |   - Quantizing model.layers.0.self_attn.k_proj.weight
25-01-23 09:29:59 | I |   - Quantizing model.layers.0.self_attn.v_proj.weight
25-01-23 09:30:01 | I |   - Quantizing model.layers.0.self_attn.o_proj.weight
25-01-23 09:30:04 | I |   - Quantizing model.layers.0.mlp.up_proj.weight
25-01-23 09:30:07 | I |   - Quantizing model.layers.0.mlp.gate_proj.weight
25-01-23 09:30:10 | I |   - Quantizing model.layers.0.mlp.down_proj.weight
25-01-23 09:30:18 | I | quantizing activations for layer model.layers.0
25-01-23 09:30:18 | I | collecting calibration activations in model.layers.0
25-01-23 09:30:18 | I | collecting calibration activations in model.layers.0
25-01-23 09:30:20 | I | forward this layer
25-01-23 09:30:20 | I | input_file: /data/gyy/lmquant-main/lmquant/data/data_without_preprocess_llama_7b_bs16/shard/val_input_args_layer_0/85.pt
25-01-23 09:30:20 | I | output_file: /data/gyy/lmquant-main/lmquant/data/data_without_preprocess_llama_7b_bs16/shard/val_teacher_output_layer_0/85.pt
25-01-23 09:30:21 | I | In layer model.layers.0, gradient of model.layers.0.self_attn.q_proj has nan: False
25-01-23 09:30:21 | I | In layer model.layers.0, gradient of model.layers.0.self_attn.q_proj has inf: False
25-01-23 09:30:21 | I | In layer model.layers.0, gradient of model.layers.0.self_attn.k_proj has nan: False
25-01-23 09:30:21 | I | In layer model.layers.0, gradient of model.layers.0.self_attn.k_proj has inf: False
25-01-23 09:30:21 | I | In layer model.layers.0, gradient of model.layers.0.self_attn.v_proj has nan: False
25-01-23 09:30:21 | I | In layer model.layers.0, gradient of model.layers.0.self_attn.v_proj has inf: False
25-01-23 09:30:21 | I | In layer model.layers.0, gradient of model.layers.0.self_attn.o_proj has nan: False
25-01-23 09:30:21 | I | In layer model.layers.0, gradient of model.layers.0.self_attn.o_proj has inf: False
25-01-23 09:30:21 | I | In layer model.layers.0, gradient of model.layers.0.mlp.up_proj has nan: False
25-01-23 09:30:21 | I | In layer model.layers.0, gradient of model.layers.0.mlp.up_proj has inf: False
25-01-23 09:30:21 | I | In layer model.layers.0, gradient of model.layers.0.mlp.gate_proj has nan: False
25-01-23 09:30:21 | I | In layer model.layers.0, gradient of model.layers.0.mlp.gate_proj has inf: False
25-01-23 09:30:21 | I | In layer model.layers.0, gradient of model.layers.0.mlp.down_proj has nan: False
25-01-23 09:30:21 | I | In layer model.layers.0, gradient of model.layers.0.mlp.down_proj has inf: False
25-01-23 09:30:21 | I | [81] done with optimizer step
25-01-23 09:30:21 | I | epoch 001:     86 / 819200000 loss=1.16021e-05, loss_per_token=0.047522, loss_sum=389.3, wps=194.7, ups=0.02, wpb=8192, bsz=16, num_updates=82, lr=0.00499947, gnorm=0.153, clip=0, loss_scale=8, train_wall=42, cuda_gb_allocated=17.1, cuda_gb_reserved=18.4, cuda_gb_free=6.6, wall=7568
25-01-23 09:30:21 | I | TRAIN CURRENT LAYER_IDX = 0
25-01-23 09:30:21 | I | in layer model.layers.0
25-01-23 09:30:21 | I | quantizing weights for layer model.layers.0
25-01-23 09:30:21 | I | collecting calibration activations in model.layers.0
25-01-23 09:30:21 | I | collecting calibration activations in model.layers.0
25-01-23 09:30:22 | I | - Quantizing decoder layer model.layers.0
25-01-23 09:30:22 | I |   - Calibrating model.layers.0.self_attn.q_proj.weight
25-01-23 09:30:22 | I |       - range scale = [    1.0000]
25-01-23 09:30:22 | I |         sum  error  = [    0.0435]
25-01-23 09:30:22 | I |         best error  = [    0.0435]
25-01-23 09:30:22 | I |     + error = [0.0435]
25-01-23 09:30:23 | I |       - range scale = [    1.0000]
25-01-23 09:30:23 | I |         sum  error  = [    0.6743]
25-01-23 09:30:23 | I |         best error  = [    0.6743]
25-01-23 09:30:23 | I |     + error = [0.6743]
25-01-23 09:30:23 | I |   - Calibrating model.layers.0.self_attn.k_proj.weight
25-01-23 09:30:24 | I |       - range scale = [    1.0000]
25-01-23 09:30:24 | I |         sum  error  = [    0.0534]
25-01-23 09:30:24 | I |         best error  = [    0.0534]
25-01-23 09:30:24 | I |     + error = [0.0534]
25-01-23 09:30:25 | I |       - range scale = [    1.0000]
25-01-23 09:30:25 | I |         sum  error  = [    0.9058]
25-01-23 09:30:25 | I |         best error  = [    0.9058]
25-01-23 09:30:25 | I |     + error = [0.9058]
25-01-23 09:30:25 | I |   - Calibrating model.layers.0.self_attn.v_proj.weight
25-01-23 09:30:26 | I |       - range scale = [    1.0000]
25-01-23 09:30:26 | I |         sum  error  = [    0.6892]
25-01-23 09:30:26 | I |         best error  = [    0.6892]
25-01-23 09:30:26 | I |     + error = [0.6892]
25-01-23 09:30:26 | I |       - range scale = [    1.0000]
25-01-23 09:30:26 | I |         sum  error  = [    6.4042]
25-01-23 09:30:26 | I |         best error  = [    6.4042]
25-01-23 09:30:26 | I |     + error = [6.4042]
25-01-23 09:30:27 | I |   - Calibrating model.layers.0.self_attn.o_proj.weight
25-01-23 09:30:27 | I |       - range scale = [    1.0000]
25-01-23 09:30:27 | I |         sum  error  = [    0.1963]
25-01-23 09:30:27 | I |         best error  = [    0.1963]
25-01-23 09:30:27 | I |     + error = [0.1963]
25-01-23 09:30:28 | I |       - range scale = [    1.0000]
25-01-23 09:30:28 | I |         sum  error  = [    1.5784]
25-01-23 09:30:28 | I |         best error  = [    1.5784]
25-01-23 09:30:28 | I |     + error = [1.5784]
25-01-23 09:30:28 | I |   - Calibrating model.layers.0.mlp.up_proj.weight
25-01-23 09:30:29 | I |       - range scale = [    1.0000]
25-01-23 09:30:29 | I |         sum  error  = [    1.0862]
25-01-23 09:30:29 | I |         best error  = [    1.0862]
25-01-23 09:30:29 | I |     + error = [1.0862]
25-01-23 09:30:30 | I |       - range scale = [    1.0000]
25-01-23 09:30:30 | I |         sum  error  = [   11.5536]
25-01-23 09:30:30 | I |         best error  = [   11.5536]
25-01-23 09:30:30 | I |     + error = [11.5536]
25-01-23 09:30:30 | I |   - Calibrating model.layers.0.mlp.gate_proj.weight
25-01-23 09:30:31 | I |       - range scale = [    1.0000]
25-01-23 09:30:31 | I |         sum  error  = [    1.1453]
25-01-23 09:30:31 | I |         best error  = [    1.1453]
25-01-23 09:30:31 | I |     + error = [1.1453]
25-01-23 09:30:32 | I |       - range scale = [    1.0000]
25-01-23 09:30:32 | I |         sum  error  = [   11.9513]
25-01-23 09:30:32 | I |         best error  = [   11.9513]
25-01-23 09:30:32 | I |     + error = [11.9513]
25-01-23 09:30:32 | I |   - Calibrating model.layers.0.mlp.down_proj.weight
25-01-23 09:30:33 | I |       - range scale = [    1.0000]
25-01-23 09:30:33 | I |         sum  error  = [    0.1661]
25-01-23 09:30:33 | I |         best error  = [    0.1661]
25-01-23 09:30:33 | I |     + error = [0.1661]
25-01-23 09:30:34 | I |       - range scale = [    1.0000]
25-01-23 09:30:34 | I |         sum  error  = [    1.6214]
25-01-23 09:30:34 | I |         best error  = [    1.6214]
25-01-23 09:30:34 | I |     + error = [1.6214]
25-01-23 09:30:35 | I |   - Quantizing model.layers.0.self_attn.q_proj.weight
25-01-23 09:30:38 | I |   - Quantizing model.layers.0.self_attn.k_proj.weight
25-01-23 09:30:40 | I |   - Quantizing model.layers.0.self_attn.v_proj.weight
25-01-23 09:30:43 | I |   - Quantizing model.layers.0.self_attn.o_proj.weight
25-01-23 09:30:46 | I |   - Quantizing model.layers.0.mlp.up_proj.weight
25-01-23 09:30:49 | I |   - Quantizing model.layers.0.mlp.gate_proj.weight
25-01-23 09:30:52 | I |   - Quantizing model.layers.0.mlp.down_proj.weight
25-01-23 09:31:00 | I | quantizing activations for layer model.layers.0
25-01-23 09:31:00 | I | collecting calibration activations in model.layers.0
25-01-23 09:31:01 | I | collecting calibration activations in model.layers.0
25-01-23 09:31:02 | I | forward this layer
25-01-23 09:31:02 | I | input_file: /data/gyy/lmquant-main/lmquant/data/data_without_preprocess_llama_7b_bs16/shard/val_input_args_layer_0/86.pt
25-01-23 09:31:02 | I | output_file: /data/gyy/lmquant-main/lmquant/data/data_without_preprocess_llama_7b_bs16/shard/val_teacher_output_layer_0/86.pt
25-01-23 09:31:03 | I | In layer model.layers.0, gradient of model.layers.0.self_attn.q_proj has nan: False
25-01-23 09:31:03 | I | In layer model.layers.0, gradient of model.layers.0.self_attn.q_proj has inf: False
25-01-23 09:31:03 | I | In layer model.layers.0, gradient of model.layers.0.self_attn.k_proj has nan: False
25-01-23 09:31:03 | I | In layer model.layers.0, gradient of model.layers.0.self_attn.k_proj has inf: False
25-01-23 09:31:03 | I | In layer model.layers.0, gradient of model.layers.0.self_attn.v_proj has nan: False
25-01-23 09:31:03 | I | In layer model.layers.0, gradient of model.layers.0.self_attn.v_proj has inf: False
25-01-23 09:31:03 | I | In layer model.layers.0, gradient of model.layers.0.self_attn.o_proj has nan: False
25-01-23 09:31:03 | I | In layer model.layers.0, gradient of model.layers.0.self_attn.o_proj has inf: False
25-01-23 09:31:03 | I | In layer model.layers.0, gradient of model.layers.0.mlp.up_proj has nan: False
25-01-23 09:31:03 | I | In layer model.layers.0, gradient of model.layers.0.mlp.up_proj has inf: False
25-01-23 09:31:03 | I | In layer model.layers.0, gradient of model.layers.0.mlp.gate_proj has nan: False
25-01-23 09:31:03 | I | In layer model.layers.0, gradient of model.layers.0.mlp.gate_proj has inf: False
25-01-23 09:31:03 | I | In layer model.layers.0, gradient of model.layers.0.mlp.down_proj has nan: False
25-01-23 09:31:03 | I | In layer model.layers.0, gradient of model.layers.0.mlp.down_proj has inf: False
25-01-23 09:31:03 | I | [82] done with optimizer step
25-01-23 09:31:03 | I | epoch 001:     87 / 819200000 loss=1.32361e-05, loss_per_token=0.0542151, loss_sum=444.13, wps=194.3, ups=0.02, wpb=8192, bsz=16, num_updates=83, lr=0.00499945, gnorm=0.109, clip=0, loss_scale=8, train_wall=42, cuda_gb_allocated=17.1, cuda_gb_reserved=18.8, cuda_gb_free=6.6, wall=7610
25-01-23 09:31:03 | I | TRAIN CURRENT LAYER_IDX = 0
25-01-23 09:31:03 | I | in layer model.layers.0
25-01-23 09:31:03 | I | quantizing weights for layer model.layers.0
25-01-23 09:31:03 | I | collecting calibration activations in model.layers.0
25-01-23 09:31:03 | I | collecting calibration activations in model.layers.0
25-01-23 09:31:04 | I | - Quantizing decoder layer model.layers.0
25-01-23 09:31:04 | I |   - Calibrating model.layers.0.self_attn.q_proj.weight
25-01-23 09:31:04 | I |       - range scale = [    1.0000]
25-01-23 09:31:04 | I |         sum  error  = [    0.0438]
25-01-23 09:31:04 | I |         best error  = [    0.0438]
25-01-23 09:31:04 | I |     + error = [0.0438]
25-01-23 09:31:05 | I |       - range scale = [    1.0000]
25-01-23 09:31:05 | I |         sum  error  = [    0.6789]
25-01-23 09:31:05 | I |         best error  = [    0.6789]
25-01-23 09:31:05 | I |     + error = [0.6789]
25-01-23 09:31:05 | I |   - Calibrating model.layers.0.self_attn.k_proj.weight
25-01-23 09:31:06 | I |       - range scale = [    1.0000]
25-01-23 09:31:06 | I |         sum  error  = [    0.0542]
25-01-23 09:31:06 | I |         best error  = [    0.0542]
25-01-23 09:31:06 | I |     + error = [0.0542]
25-01-23 09:31:07 | I |       - range scale = [    1.0000]
25-01-23 09:31:07 | I |         sum  error  = [    0.8642]
25-01-23 09:31:07 | I |         best error  = [    0.8642]
25-01-23 09:31:07 | I |     + error = [0.8642]
25-01-23 09:31:07 | I |   - Calibrating model.layers.0.self_attn.v_proj.weight
25-01-23 09:31:08 | I |       - range scale = [    1.0000]
25-01-23 09:31:08 | I |         sum  error  = [    0.6983]
25-01-23 09:31:08 | I |         best error  = [    0.6983]
25-01-23 09:31:08 | I |     + error = [0.6983]
25-01-23 09:31:09 | I |       - range scale = [    1.0000]
25-01-23 09:31:09 | I |         sum  error  = [    6.5046]
25-01-23 09:31:09 | I |         best error  = [    6.5046]
25-01-23 09:31:09 | I |     + error = [6.5046]
25-01-23 09:31:09 | I |   - Calibrating model.layers.0.self_attn.o_proj.weight
25-01-23 09:31:09 | I |       - range scale = [    1.0000]
25-01-23 09:31:09 | I |         sum  error  = [    0.2080]
25-01-23 09:31:09 | I |         best error  = [    0.2080]
25-01-23 09:31:09 | I |     + error = [0.2080]
25-01-23 09:31:10 | I |       - range scale = [    1.0000]
25-01-23 09:31:10 | I |         sum  error  = [    1.6546]
25-01-23 09:31:10 | I |         best error  = [    1.6546]
25-01-23 09:31:10 | I |     + error = [1.6546]
25-01-23 09:31:10 | I |   - Calibrating model.layers.0.mlp.up_proj.weight
25-01-23 09:31:11 | I |       - range scale = [    1.0000]
25-01-23 09:31:11 | I |         sum  error  = [    1.1161]
25-01-23 09:31:11 | I |         best error  = [    1.1161]
25-01-23 09:31:11 | I |     + error = [1.1161]
25-01-23 09:31:12 | I |       - range scale = [    1.0000]
25-01-23 09:31:12 | I |         sum  error  = [   11.8854]
25-01-23 09:31:12 | I |         best error  = [   11.8854]
25-01-23 09:31:12 | I |     + error = [11.8854]
25-01-23 09:31:13 | I |   - Calibrating model.layers.0.mlp.gate_proj.weight
25-01-23 09:31:13 | I |       - range scale = [    1.0000]
25-01-23 09:31:13 | I |         sum  error  = [    1.1785]
25-01-23 09:31:13 | I |         best error  = [    1.1785]
25-01-23 09:31:13 | I |     + error = [1.1785]
25-01-23 09:31:14 | I |       - range scale = [    1.0000]
25-01-23 09:31:14 | I |         sum  error  = [   12.2892]
25-01-23 09:31:14 | I |         best error  = [   12.2892]
25-01-23 09:31:14 | I |     + error = [12.2892]
25-01-23 09:31:15 | I |   - Calibrating model.layers.0.mlp.down_proj.weight
25-01-23 09:31:15 | I |       - range scale = [    1.0000]
25-01-23 09:31:15 | I |         sum  error  = [    0.1795]
25-01-23 09:31:15 | I |         best error  = [    0.1795]
25-01-23 09:31:15 | I |     + error = [0.1795]
25-01-23 09:31:17 | I |       - range scale = [    1.0000]
25-01-23 09:31:17 | I |         sum  error  = [    1.7452]
25-01-23 09:31:17 | I |         best error  = [    1.7452]
25-01-23 09:31:17 | I |     + error = [1.7452]
25-01-23 09:31:17 | I |   - Quantizing model.layers.0.self_attn.q_proj.weight
25-01-23 09:31:20 | I |   - Quantizing model.layers.0.self_attn.k_proj.weight
25-01-23 09:31:23 | I |   - Quantizing model.layers.0.self_attn.v_proj.weight
25-01-23 09:31:26 | I |   - Quantizing model.layers.0.self_attn.o_proj.weight
25-01-23 09:31:29 | I |   - Quantizing model.layers.0.mlp.up_proj.weight
25-01-23 09:31:32 | I |   - Quantizing model.layers.0.mlp.gate_proj.weight
25-01-23 09:31:35 | I |   - Quantizing model.layers.0.mlp.down_proj.weight
25-01-23 09:31:42 | I | quantizing activations for layer model.layers.0
25-01-23 09:31:42 | I | collecting calibration activations in model.layers.0
25-01-23 09:31:43 | I | collecting calibration activations in model.layers.0
25-01-23 09:31:44 | I | forward this layer
25-01-23 09:31:44 | I | input_file: /data/gyy/lmquant-main/lmquant/data/data_without_preprocess_llama_7b_bs16/shard/val_input_args_layer_0/87.pt
25-01-23 09:31:44 | I | output_file: /data/gyy/lmquant-main/lmquant/data/data_without_preprocess_llama_7b_bs16/shard/val_teacher_output_layer_0/87.pt
25-01-23 09:31:45 | I | In layer model.layers.0, gradient of model.layers.0.self_attn.q_proj has nan: False
25-01-23 09:31:45 | I | In layer model.layers.0, gradient of model.layers.0.self_attn.q_proj has inf: False
25-01-23 09:31:45 | I | In layer model.layers.0, gradient of model.layers.0.self_attn.k_proj has nan: False
25-01-23 09:31:45 | I | In layer model.layers.0, gradient of model.layers.0.self_attn.k_proj has inf: False
25-01-23 09:31:45 | I | In layer model.layers.0, gradient of model.layers.0.self_attn.v_proj has nan: False
25-01-23 09:31:45 | I | In layer model.layers.0, gradient of model.layers.0.self_attn.v_proj has inf: False
25-01-23 09:31:45 | I | In layer model.layers.0, gradient of model.layers.0.self_attn.o_proj has nan: False
25-01-23 09:31:45 | I | In layer model.layers.0, gradient of model.layers.0.self_attn.o_proj has inf: False
25-01-23 09:31:45 | I | In layer model.layers.0, gradient of model.layers.0.mlp.up_proj has nan: False
25-01-23 09:31:45 | I | In layer model.layers.0, gradient of model.layers.0.mlp.up_proj has inf: False
25-01-23 09:31:45 | I | In layer model.layers.0, gradient of model.layers.0.mlp.gate_proj has nan: False
25-01-23 09:31:45 | I | In layer model.layers.0, gradient of model.layers.0.mlp.gate_proj has inf: False
25-01-23 09:31:45 | I | In layer model.layers.0, gradient of model.layers.0.mlp.down_proj has nan: False
25-01-23 09:31:45 | I | In layer model.layers.0, gradient of model.layers.0.mlp.down_proj has inf: False
25-01-23 09:31:45 | I | [83] done with optimizer step
25-01-23 09:31:45 | I | epoch 001:     88 / 819200000 loss=1.29644e-05, loss_per_token=0.0531021, loss_sum=435.012, wps=195.2, ups=0.02, wpb=8192, bsz=16, num_updates=84, lr=0.00499943, gnorm=0.117, clip=0, loss_scale=8, train_wall=42, cuda_gb_allocated=17.1, cuda_gb_reserved=18.5, cuda_gb_free=6.6, wall=7652
25-01-23 09:31:45 | I | TRAIN CURRENT LAYER_IDX = 0
25-01-23 09:31:45 | I | in layer model.layers.0
25-01-23 09:31:45 | I | quantizing weights for layer model.layers.0
25-01-23 09:31:45 | I | collecting calibration activations in model.layers.0
25-01-23 09:31:45 | I | collecting calibration activations in model.layers.0
25-01-23 09:31:46 | I | - Quantizing decoder layer model.layers.0
25-01-23 09:31:46 | I |   - Calibrating model.layers.0.self_attn.q_proj.weight
25-01-23 09:31:46 | I |       - range scale = [    1.0000]
25-01-23 09:31:46 | I |         sum  error  = [    0.0379]
25-01-23 09:31:46 | I |         best error  = [    0.0379]
25-01-23 09:31:46 | I |     + error = [0.0379]
25-01-23 09:31:47 | I |       - range scale = [    1.0000]
25-01-23 09:31:47 | I |         sum  error  = [    0.6054]
25-01-23 09:31:47 | I |         best error  = [    0.6054]
25-01-23 09:31:47 | I |     + error = [0.6054]
25-01-23 09:31:47 | I |   - Calibrating model.layers.0.self_attn.k_proj.weight
25-01-23 09:31:48 | I |       - range scale = [    1.0000]
25-01-23 09:31:48 | I |         sum  error  = [    0.0431]
25-01-23 09:31:48 | I |         best error  = [    0.0431]
25-01-23 09:31:48 | I |     + error = [0.0431]
25-01-23 09:31:49 | I |       - range scale = [    1.0000]
25-01-23 09:31:49 | I |         sum  error  = [    0.8105]
25-01-23 09:31:49 | I |         best error  = [    0.8105]
25-01-23 09:31:49 | I |     + error = [0.8105]
25-01-23 09:31:49 | I |   - Calibrating model.layers.0.self_attn.v_proj.weight
25-01-23 09:31:50 | I |       - range scale = [    1.0000]
25-01-23 09:31:50 | I |         sum  error  = [    0.6455]
25-01-23 09:31:50 | I |         best error  = [    0.6455]
25-01-23 09:31:50 | I |     + error = [0.6455]
25-01-23 09:31:50 | I |       - range scale = [    1.0000]
25-01-23 09:31:50 | I |         sum  error  = [    5.9694]
25-01-23 09:31:50 | I |         best error  = [    5.9694]
25-01-23 09:31:50 | I |     + error = [5.9694]
25-01-23 09:31:51 | I |   - Calibrating model.layers.0.self_attn.o_proj.weight
25-01-23 09:31:51 | I |       - range scale = [    1.0000]
25-01-23 09:31:51 | I |         sum  error  = [    0.1829]
25-01-23 09:31:51 | I |         best error  = [    0.1829]
25-01-23 09:31:51 | I |     + error = [0.1829]
25-01-23 09:31:52 | I |       - range scale = [    1.0000]
25-01-23 09:31:52 | I |         sum  error  = [    1.4489]
25-01-23 09:31:52 | I |         best error  = [    1.4489]
25-01-23 09:31:52 | I |     + error = [1.4489]
25-01-23 09:31:52 | I |   - Calibrating model.layers.0.mlp.up_proj.weight
25-01-23 09:31:53 | I |       - range scale = [    1.0000]
25-01-23 09:31:53 | I |         sum  error  = [    1.0697]
25-01-23 09:31:53 | I |         best error  = [    1.0697]
25-01-23 09:31:53 | I |     + error = [1.0697]
25-01-23 09:31:54 | I |       - range scale = [    1.0000]
25-01-23 09:31:54 | I |         sum  error  = [   11.3722]
25-01-23 09:31:54 | I |         best error  = [   11.3722]
25-01-23 09:31:54 | I |     + error = [11.3722]
25-01-23 09:31:54 | I |   - Calibrating model.layers.0.mlp.gate_proj.weight
25-01-23 09:31:55 | I |       - range scale = [    1.0000]
25-01-23 09:31:55 | I |         sum  error  = [    1.1334]
25-01-23 09:31:55 | I |         best error  = [    1.1334]
25-01-23 09:31:55 | I |     + error = [1.1334]
25-01-23 09:31:56 | I |       - range scale = [    1.0000]
25-01-23 09:31:56 | I |         sum  error  = [   11.7584]
25-01-23 09:31:56 | I |         best error  = [   11.7584]
25-01-23 09:31:56 | I |     + error = [11.7584]
25-01-23 09:31:57 | I |   - Calibrating model.layers.0.mlp.down_proj.weight
25-01-23 09:31:57 | I |       - range scale = [    1.0000]
25-01-23 09:31:57 | I |         sum  error  = [    0.1663]
25-01-23 09:31:57 | I |         best error  = [    0.1663]
25-01-23 09:31:57 | I |     + error = [0.1663]
25-01-23 09:31:58 | I |       - range scale = [    1.0000]
25-01-23 09:31:58 | I |         sum  error  = [    1.6243]
25-01-23 09:31:58 | I |         best error  = [    1.6243]
25-01-23 09:31:58 | I |     + error = [1.6243]
25-01-23 09:31:59 | I |   - Quantizing model.layers.0.self_attn.q_proj.weight
25-01-23 09:32:02 | I |   - Quantizing model.layers.0.self_attn.k_proj.weight
25-01-23 09:32:04 | I |   - Quantizing model.layers.0.self_attn.v_proj.weight
25-01-23 09:32:07 | I |   - Quantizing model.layers.0.self_attn.o_proj.weight
25-01-23 09:32:10 | I |   - Quantizing model.layers.0.mlp.up_proj.weight
25-01-23 09:32:13 | I |   - Quantizing model.layers.0.mlp.gate_proj.weight
25-01-23 09:32:16 | I |   - Quantizing model.layers.0.mlp.down_proj.weight
25-01-23 09:32:24 | I | quantizing activations for layer model.layers.0
25-01-23 09:32:24 | I | collecting calibration activations in model.layers.0
25-01-23 09:32:24 | I | collecting calibration activations in model.layers.0
25-01-23 09:32:26 | I | forward this layer
25-01-23 09:32:26 | I | input_file: /data/gyy/lmquant-main/lmquant/data/data_without_preprocess_llama_7b_bs16/shard/val_input_args_layer_0/88.pt
25-01-23 09:32:26 | I | output_file: /data/gyy/lmquant-main/lmquant/data/data_without_preprocess_llama_7b_bs16/shard/val_teacher_output_layer_0/88.pt
25-01-23 09:32:26 | I | In layer model.layers.0, gradient of model.layers.0.self_attn.q_proj has nan: False
25-01-23 09:32:26 | I | In layer model.layers.0, gradient of model.layers.0.self_attn.q_proj has inf: False
25-01-23 09:32:26 | I | In layer model.layers.0, gradient of model.layers.0.self_attn.k_proj has nan: False
25-01-23 09:32:26 | I | In layer model.layers.0, gradient of model.layers.0.self_attn.k_proj has inf: False
25-01-23 09:32:26 | I | In layer model.layers.0, gradient of model.layers.0.self_attn.v_proj has nan: False
25-01-23 09:32:26 | I | In layer model.layers.0, gradient of model.layers.0.self_attn.v_proj has inf: False
25-01-23 09:32:26 | I | In layer model.layers.0, gradient of model.layers.0.self_attn.o_proj has nan: False
25-01-23 09:32:26 | I | In layer model.layers.0, gradient of model.layers.0.self_attn.o_proj has inf: False
25-01-23 09:32:26 | I | In layer model.layers.0, gradient of model.layers.0.mlp.up_proj has nan: False
25-01-23 09:32:26 | I | In layer model.layers.0, gradient of model.layers.0.mlp.up_proj has inf: False
25-01-23 09:32:26 | I | In layer model.layers.0, gradient of model.layers.0.mlp.gate_proj has nan: False
25-01-23 09:32:26 | I | In layer model.layers.0, gradient of model.layers.0.mlp.gate_proj has inf: False
25-01-23 09:32:26 | I | In layer model.layers.0, gradient of model.layers.0.mlp.down_proj has nan: False
25-01-23 09:32:26 | I | In layer model.layers.0, gradient of model.layers.0.mlp.down_proj has inf: False
25-01-23 09:32:26 | I | [84] done with optimizer step
25-01-23 09:32:26 | I | epoch 001:     89 / 819200000 loss=1.04678e-05, loss_per_token=0.042876, loss_sum=351.24, wps=197.4, ups=0.02, wpb=8192, bsz=16, num_updates=85, lr=0.00499942, gnorm=0.172, clip=0, loss_scale=8, train_wall=41, cuda_gb_allocated=17.1, cuda_gb_reserved=18.5, cuda_gb_free=6.6, wall=7694
25-01-23 09:32:26 | I | TRAIN CURRENT LAYER_IDX = 0
25-01-23 09:32:26 | I | in layer model.layers.0
25-01-23 09:32:26 | I | quantizing weights for layer model.layers.0
25-01-23 09:32:27 | I | collecting calibration activations in model.layers.0
25-01-23 09:32:27 | I | collecting calibration activations in model.layers.0
25-01-23 09:32:27 | I | - Quantizing decoder layer model.layers.0
25-01-23 09:32:27 | I |   - Calibrating model.layers.0.self_attn.q_proj.weight
25-01-23 09:32:28 | I |       - range scale = [    1.0000]
25-01-23 09:32:28 | I |         sum  error  = [    0.0402]
25-01-23 09:32:28 | I |         best error  = [    0.0402]
25-01-23 09:32:28 | I |     + error = [0.0402]
25-01-23 09:32:29 | I |       - range scale = [    1.0000]
25-01-23 09:32:29 | I |         sum  error  = [    0.6331]
25-01-23 09:32:29 | I |         best error  = [    0.6331]
25-01-23 09:32:29 | I |     + error = [0.6331]
25-01-23 09:32:29 | I |   - Calibrating model.layers.0.self_attn.k_proj.weight
25-01-23 09:32:29 | I |       - range scale = [    1.0000]
25-01-23 09:32:29 | I |         sum  error  = [    0.0477]
25-01-23 09:32:29 | I |         best error  = [    0.0477]
25-01-23 09:32:29 | I |     + error = [0.0477]
25-01-23 09:32:30 | I |       - range scale = [    1.0000]
25-01-23 09:32:30 | I |         sum  error  = [    0.8537]
25-01-23 09:32:30 | I |         best error  = [    0.8537]
25-01-23 09:32:30 | I |     + error = [0.8537]
25-01-23 09:32:30 | I |   - Calibrating model.layers.0.self_attn.v_proj.weight
25-01-23 09:32:31 | I |       - range scale = [    1.0000]
25-01-23 09:32:31 | I |         sum  error  = [    0.6738]
25-01-23 09:32:31 | I |         best error  = [    0.6738]
25-01-23 09:32:31 | I |     + error = [0.6738]
25-01-23 09:32:32 | I |       - range scale = [    1.0000]
25-01-23 09:32:32 | I |         sum  error  = [    6.2707]
25-01-23 09:32:32 | I |         best error  = [    6.2707]
25-01-23 09:32:32 | I |     + error = [6.2707]
25-01-23 09:32:32 | I |   - Calibrating model.layers.0.self_attn.o_proj.weight
25-01-23 09:32:33 | I |       - range scale = [    1.0000]
25-01-23 09:32:33 | I |         sum  error  = [    0.1965]
25-01-23 09:32:33 | I |         best error  = [    0.1965]
25-01-23 09:32:33 | I |     + error = [0.1965]
25-01-23 09:32:33 | I |       - range scale = [    1.0000]
25-01-23 09:32:33 | I |         sum  error  = [    1.5577]
25-01-23 09:32:33 | I |         best error  = [    1.5577]
25-01-23 09:32:33 | I |     + error = [1.5577]
25-01-23 09:32:34 | I |   - Calibrating model.layers.0.mlp.up_proj.weight
25-01-23 09:32:34 | I |       - range scale = [    1.0000]
25-01-23 09:32:34 | I |         sum  error  = [    1.1036]
25-01-23 09:32:34 | I |         best error  = [    1.1036]
25-01-23 09:32:34 | I |     + error = [1.1036]
25-01-23 09:32:36 | I |       - range scale = [    1.0000]
25-01-23 09:32:36 | I |         sum  error  = [   11.7298]
25-01-23 09:32:36 | I |         best error  = [   11.7298]
25-01-23 09:32:36 | I |     + error = [11.7298]
25-01-23 09:32:36 | I |   - Calibrating model.layers.0.mlp.gate_proj.weight
25-01-23 09:32:36 | I |       - range scale = [    1.0000]
25-01-23 09:32:36 | I |         sum  error  = [    1.1661]
25-01-23 09:32:36 | I |         best error  = [    1.1661]
25-01-23 09:32:36 | I |     + error = [1.1661]
25-01-23 09:32:38 | I |       - range scale = [    1.0000]
25-01-23 09:32:38 | I |         sum  error  = [   12.1314]
25-01-23 09:32:38 | I |         best error  = [   12.1314]
25-01-23 09:32:38 | I |     + error = [12.1314]
25-01-23 09:32:38 | I |   - Calibrating model.layers.0.mlp.down_proj.weight
25-01-23 09:32:39 | I |       - range scale = [    1.0000]
25-01-23 09:32:39 | I |         sum  error  = [    0.1758]
25-01-23 09:32:39 | I |         best error  = [    0.1758]
25-01-23 09:32:39 | I |     + error = [0.1758]
25-01-23 09:32:40 | I |       - range scale = [    1.0000]
25-01-23 09:32:40 | I |         sum  error  = [    1.7178]
25-01-23 09:32:40 | I |         best error  = [    1.7178]
25-01-23 09:32:40 | I |     + error = [1.7178]
25-01-23 09:32:40 | I |   - Quantizing model.layers.0.self_attn.q_proj.weight
25-01-23 09:32:43 | I |   - Quantizing model.layers.0.self_attn.k_proj.weight
25-01-23 09:32:46 | I |   - Quantizing model.layers.0.self_attn.v_proj.weight
25-01-23 09:32:48 | I |   - Quantizing model.layers.0.self_attn.o_proj.weight
25-01-23 09:32:51 | I |   - Quantizing model.layers.0.mlp.up_proj.weight
25-01-23 09:32:54 | I |   - Quantizing model.layers.0.mlp.gate_proj.weight
25-01-23 09:32:57 | I |   - Quantizing model.layers.0.mlp.down_proj.weight
25-01-23 09:33:05 | I | quantizing activations for layer model.layers.0
25-01-23 09:33:05 | I | collecting calibration activations in model.layers.0
25-01-23 09:33:05 | I | collecting calibration activations in model.layers.0
25-01-23 09:33:07 | I | forward this layer
25-01-23 09:33:07 | I | input_file: /data/gyy/lmquant-main/lmquant/data/data_without_preprocess_llama_7b_bs16/shard/val_input_args_layer_0/89.pt
25-01-23 09:33:07 | I | output_file: /data/gyy/lmquant-main/lmquant/data/data_without_preprocess_llama_7b_bs16/shard/val_teacher_output_layer_0/89.pt
25-01-23 09:33:08 | I | In layer model.layers.0, gradient of model.layers.0.self_attn.q_proj has nan: False
25-01-23 09:33:08 | I | In layer model.layers.0, gradient of model.layers.0.self_attn.q_proj has inf: False
25-01-23 09:33:08 | I | In layer model.layers.0, gradient of model.layers.0.self_attn.k_proj has nan: False
25-01-23 09:33:08 | I | In layer model.layers.0, gradient of model.layers.0.self_attn.k_proj has inf: False
25-01-23 09:33:08 | I | In layer model.layers.0, gradient of model.layers.0.self_attn.v_proj has nan: False
25-01-23 09:33:08 | I | In layer model.layers.0, gradient of model.layers.0.self_attn.v_proj has inf: False
25-01-23 09:33:08 | I | In layer model.layers.0, gradient of model.layers.0.self_attn.o_proj has nan: False
25-01-23 09:33:08 | I | In layer model.layers.0, gradient of model.layers.0.self_attn.o_proj has inf: False
25-01-23 09:33:08 | I | In layer model.layers.0, gradient of model.layers.0.mlp.up_proj has nan: False
25-01-23 09:33:08 | I | In layer model.layers.0, gradient of model.layers.0.mlp.up_proj has inf: False
25-01-23 09:33:08 | I | In layer model.layers.0, gradient of model.layers.0.mlp.gate_proj has nan: False
25-01-23 09:33:08 | I | In layer model.layers.0, gradient of model.layers.0.mlp.gate_proj has inf: False
25-01-23 09:33:08 | I | In layer model.layers.0, gradient of model.layers.0.mlp.down_proj has nan: False
25-01-23 09:33:08 | I | In layer model.layers.0, gradient of model.layers.0.mlp.down_proj has inf: False
25-01-23 09:33:08 | I | [85] done with optimizer step
25-01-23 09:33:08 | I | epoch 001:     90 / 819200000 loss=1.07979e-05, loss_per_token=0.0442283, loss_sum=362.318, wps=198.2, ups=0.02, wpb=8192, bsz=16, num_updates=86, lr=0.0049994, gnorm=0.143, clip=0, loss_scale=8, train_wall=41, cuda_gb_allocated=17.1, cuda_gb_reserved=18.7, cuda_gb_free=6.6, wall=7735
25-01-23 09:33:08 | I | TRAIN CURRENT LAYER_IDX = 0
25-01-23 09:33:08 | I | in layer model.layers.0
25-01-23 09:33:08 | I | quantizing weights for layer model.layers.0
25-01-23 09:33:08 | I | collecting calibration activations in model.layers.0
25-01-23 09:33:08 | I | collecting calibration activations in model.layers.0
25-01-23 09:33:09 | I | - Quantizing decoder layer model.layers.0
25-01-23 09:33:09 | I |   - Calibrating model.layers.0.self_attn.q_proj.weight
25-01-23 09:33:09 | I |       - range scale = [    1.0000]
25-01-23 09:33:09 | I |         sum  error  = [    0.0432]
25-01-23 09:33:09 | I |         best error  = [    0.0432]
25-01-23 09:33:09 | I |     + error = [0.0432]
25-01-23 09:33:10 | I |       - range scale = [    1.0000]
25-01-23 09:33:10 | I |         sum  error  = [    0.6756]
25-01-23 09:33:10 | I |         best error  = [    0.6756]
25-01-23 09:33:10 | I |     + error = [0.6756]
25-01-23 09:33:10 | I |   - Calibrating model.layers.0.self_attn.k_proj.weight
25-01-23 09:33:11 | I |       - range scale = [    1.0000]
25-01-23 09:33:11 | I |         sum  error  = [    0.0536]
25-01-23 09:33:11 | I |         best error  = [    0.0536]
25-01-23 09:33:11 | I |     + error = [0.0536]
25-01-23 09:33:11 | I |       - range scale = [    1.0000]
25-01-23 09:33:11 | I |         sum  error  = [    0.9585]
25-01-23 09:33:11 | I |         best error  = [    0.9585]
25-01-23 09:33:11 | I |     + error = [0.9585]
25-01-23 09:33:12 | I |   - Calibrating model.layers.0.self_attn.v_proj.weight
25-01-23 09:33:12 | I |       - range scale = [    1.0000]
25-01-23 09:33:12 | I |         sum  error  = [    0.6584]
25-01-23 09:33:12 | I |         best error  = [    0.6584]
25-01-23 09:33:12 | I |     + error = [0.6584]
25-01-23 09:33:13 | I |       - range scale = [    1.0000]
25-01-23 09:33:13 | I |         sum  error  = [    6.1270]
25-01-23 09:33:13 | I |         best error  = [    6.1270]
25-01-23 09:33:13 | I |     + error = [6.1270]
25-01-23 09:33:13 | I |   - Calibrating model.layers.0.self_attn.o_proj.weight
25-01-23 09:33:14 | I |       - range scale = [    1.0000]
25-01-23 09:33:14 | I |         sum  error  = [    0.1874]
25-01-23 09:33:14 | I |         best error  = [    0.1874]
25-01-23 09:33:14 | I |     + error = [0.1874]
25-01-23 09:33:15 | I |       - range scale = [    1.0000]
25-01-23 09:33:15 | I |         sum  error  = [    1.5091]
25-01-23 09:33:15 | I |         best error  = [    1.5091]
25-01-23 09:33:15 | I |     + error = [1.5091]
25-01-23 09:33:15 | I |   - Calibrating model.layers.0.mlp.up_proj.weight
25-01-23 09:33:16 | I |       - range scale = [    1.0000]
25-01-23 09:33:16 | I |         sum  error  = [    1.0894]
25-01-23 09:33:16 | I |         best error  = [    1.0894]
25-01-23 09:33:16 | I |     + error = [1.0894]
25-01-23 09:33:17 | I |       - range scale = [    1.0000]
25-01-23 09:33:17 | I |         sum  error  = [   11.5839]
25-01-23 09:33:17 | I |         best error  = [   11.5839]
25-01-23 09:33:17 | I |     + error = [11.5839]
25-01-23 09:33:17 | I |   - Calibrating model.layers.0.mlp.gate_proj.weight
25-01-23 09:33:18 | I |       - range scale = [    1.0000]
25-01-23 09:33:18 | I |         sum  error  = [    1.1498]
25-01-23 09:33:18 | I |         best error  = [    1.1498]
25-01-23 09:33:18 | I |     + error = [1.1498]
25-01-23 09:33:19 | I |       - range scale = [    1.0000]
25-01-23 09:33:19 | I |         sum  error  = [   11.9770]
25-01-23 09:33:19 | I |         best error  = [   11.9770]
25-01-23 09:33:19 | I |     + error = [11.9770]
25-01-23 09:33:19 | I |   - Calibrating model.layers.0.mlp.down_proj.weight
25-01-23 09:33:20 | I |       - range scale = [    1.0000]
25-01-23 09:33:20 | I |         sum  error  = [    0.1673]
25-01-23 09:33:20 | I |         best error  = [    0.1673]
25-01-23 09:33:20 | I |     + error = [0.1673]
25-01-23 09:33:21 | I |       - range scale = [    1.0000]
25-01-23 09:33:21 | I |         sum  error  = [    1.6399]
25-01-23 09:33:21 | I |         best error  = [    1.6399]
25-01-23 09:33:21 | I |     + error = [1.6399]
25-01-23 09:33:21 | I |   - Quantizing model.layers.0.self_attn.q_proj.weight
25-01-23 09:33:24 | I |   - Quantizing model.layers.0.self_attn.k_proj.weight
25-01-23 09:33:27 | I |   - Quantizing model.layers.0.self_attn.v_proj.weight
25-01-23 09:33:30 | I |   - Quantizing model.layers.0.self_attn.o_proj.weight
25-01-23 09:33:33 | I |   - Quantizing model.layers.0.mlp.up_proj.weight
25-01-23 09:33:36 | I |   - Quantizing model.layers.0.mlp.gate_proj.weight
25-01-23 09:33:39 | I |   - Quantizing model.layers.0.mlp.down_proj.weight
25-01-23 09:33:45 | I | quantizing activations for layer model.layers.0
25-01-23 09:33:45 | I | collecting calibration activations in model.layers.0
25-01-23 09:33:45 | I | collecting calibration activations in model.layers.0
25-01-23 09:33:47 | I | forward this layer
25-01-23 09:33:47 | I | input_file: /data/gyy/lmquant-main/lmquant/data/data_without_preprocess_llama_7b_bs16/shard/val_input_args_layer_0/90.pt
25-01-23 09:33:47 | I | output_file: /data/gyy/lmquant-main/lmquant/data/data_without_preprocess_llama_7b_bs16/shard/val_teacher_output_layer_0/90.pt
25-01-23 09:33:47 | I | In layer model.layers.0, gradient of model.layers.0.self_attn.q_proj has nan: False
25-01-23 09:33:47 | I | In layer model.layers.0, gradient of model.layers.0.self_attn.q_proj has inf: False
25-01-23 09:33:47 | I | In layer model.layers.0, gradient of model.layers.0.self_attn.k_proj has nan: False
25-01-23 09:33:47 | I | In layer model.layers.0, gradient of model.layers.0.self_attn.k_proj has inf: False
25-01-23 09:33:47 | I | In layer model.layers.0, gradient of model.layers.0.self_attn.v_proj has nan: False
25-01-23 09:33:47 | I | In layer model.layers.0, gradient of model.layers.0.self_attn.v_proj has inf: False
25-01-23 09:33:47 | I | In layer model.layers.0, gradient of model.layers.0.self_attn.o_proj has nan: False
25-01-23 09:33:47 | I | In layer model.layers.0, gradient of model.layers.0.self_attn.o_proj has inf: False
25-01-23 09:33:47 | I | In layer model.layers.0, gradient of model.layers.0.mlp.up_proj has nan: False
25-01-23 09:33:47 | I | In layer model.layers.0, gradient of model.layers.0.mlp.up_proj has inf: False
25-01-23 09:33:47 | I | In layer model.layers.0, gradient of model.layers.0.mlp.gate_proj has nan: False
25-01-23 09:33:47 | I | In layer model.layers.0, gradient of model.layers.0.mlp.gate_proj has inf: False
25-01-23 09:33:47 | I | In layer model.layers.0, gradient of model.layers.0.mlp.down_proj has nan: False
25-01-23 09:33:47 | I | In layer model.layers.0, gradient of model.layers.0.mlp.down_proj has inf: False
25-01-23 09:33:47 | I | [86] done with optimizer step
25-01-23 09:33:47 | I | epoch 001:     91 / 819200000 loss=1.08426e-05, loss_per_token=0.0444115, loss_sum=363.819, wps=206, ups=0.03, wpb=8192, bsz=16, num_updates=87, lr=0.00499938, gnorm=0.062, clip=0, loss_scale=8, train_wall=40, cuda_gb_allocated=17.1, cuda_gb_reserved=18.5, cuda_gb_free=6.6, wall=7775
25-01-23 09:33:48 | I | TRAIN CURRENT LAYER_IDX = 0
25-01-23 09:33:48 | I | in layer model.layers.0
25-01-23 09:33:48 | I | quantizing weights for layer model.layers.0
25-01-23 09:33:48 | I | collecting calibration activations in model.layers.0
25-01-23 09:33:48 | I | collecting calibration activations in model.layers.0
25-01-23 09:33:48 | I | - Quantizing decoder layer model.layers.0
25-01-23 09:33:48 | I |   - Calibrating model.layers.0.self_attn.q_proj.weight
25-01-23 09:33:49 | I |       - range scale = [    1.0000]
25-01-23 09:33:49 | I |         sum  error  = [    0.0435]
25-01-23 09:33:49 | I |         best error  = [    0.0435]
25-01-23 09:33:49 | I |     + error = [0.0435]
25-01-23 09:33:50 | I |       - range scale = [    1.0000]
25-01-23 09:33:50 | I |         sum  error  = [    0.6674]
25-01-23 09:33:50 | I |         best error  = [    0.6674]
25-01-23 09:33:50 | I |     + error = [0.6674]
25-01-23 09:33:50 | I |   - Calibrating model.layers.0.self_attn.k_proj.weight
25-01-23 09:33:51 | I |       - range scale = [    1.0000]
25-01-23 09:33:51 | I |         sum  error  = [    0.0513]
25-01-23 09:33:51 | I |         best error  = [    0.0513]
25-01-23 09:33:51 | I |     + error = [0.0513]
25-01-23 09:33:51 | I |       - range scale = [    1.0000]
25-01-23 09:33:51 | I |         sum  error  = [    0.8564]
25-01-23 09:33:51 | I |         best error  = [    0.8564]
25-01-23 09:33:51 | I |     + error = [0.8564]
25-01-23 09:33:52 | I |   - Calibrating model.layers.0.self_attn.v_proj.weight
25-01-23 09:33:52 | I |       - range scale = [    1.0000]
25-01-23 09:33:52 | I |         sum  error  = [    0.6776]
25-01-23 09:33:52 | I |         best error  = [    0.6776]
25-01-23 09:33:52 | I |     + error = [0.6776]
25-01-23 09:33:53 | I |       - range scale = [    1.0000]
25-01-23 09:33:53 | I |         sum  error  = [    6.3001]
25-01-23 09:33:53 | I |         best error  = [    6.3001]
25-01-23 09:33:53 | I |     + error = [6.3001]
25-01-23 09:33:53 | I |   - Calibrating model.layers.0.self_attn.o_proj.weight
25-01-23 09:33:54 | I |       - range scale = [    1.0000]
25-01-23 09:33:54 | I |         sum  error  = [    0.1948]
25-01-23 09:33:54 | I |         best error  = [    0.1948]
25-01-23 09:33:54 | I |     + error = [0.1948]
25-01-23 09:33:55 | I |       - range scale = [    1.0000]
25-01-23 09:33:55 | I |         sum  error  = [    1.5525]
25-01-23 09:33:55 | I |         best error  = [    1.5525]
25-01-23 09:33:55 | I |     + error = [1.5525]
25-01-23 09:33:55 | I |   - Calibrating model.layers.0.mlp.up_proj.weight
25-01-23 09:33:56 | I |       - range scale = [    1.0000]
25-01-23 09:33:56 | I |         sum  error  = [    1.0733]
25-01-23 09:33:56 | I |         best error  = [    1.0733]
25-01-23 09:33:56 | I |     + error = [1.0733]
25-01-23 09:33:57 | I |       - range scale = [    1.0000]
25-01-23 09:33:57 | I |         sum  error  = [   11.4252]
25-01-23 09:33:57 | I |         best error  = [   11.4252]
25-01-23 09:33:57 | I |     + error = [11.4252]
25-01-23 09:33:57 | I |   - Calibrating model.layers.0.mlp.gate_proj.weight
25-01-23 09:33:58 | I |       - range scale = [    1.0000]
25-01-23 09:33:58 | I |         sum  error  = [    1.1336]
25-01-23 09:33:58 | I |         best error  = [    1.1336]
25-01-23 09:33:58 | I |     + error = [1.1336]
25-01-23 09:33:59 | I |       - range scale = [    1.0000]
25-01-23 09:33:59 | I |         sum  error  = [   11.8106]
25-01-23 09:33:59 | I |         best error  = [   11.8106]
25-01-23 09:33:59 | I |     + error = [11.8106]
25-01-23 09:33:59 | I |   - Calibrating model.layers.0.mlp.down_proj.weight
25-01-23 09:34:00 | I |       - range scale = [    1.0000]
25-01-23 09:34:00 | I |         sum  error  = [    0.1636]
25-01-23 09:34:00 | I |         best error  = [    0.1636]
25-01-23 09:34:00 | I |     + error = [0.1636]
25-01-23 09:34:01 | I |       - range scale = [    1.0000]
25-01-23 09:34:01 | I |         sum  error  = [    1.5945]
25-01-23 09:34:01 | I |         best error  = [    1.5945]
25-01-23 09:34:01 | I |     + error = [1.5945]
25-01-23 09:34:02 | I |   - Quantizing model.layers.0.self_attn.q_proj.weight
25-01-23 09:34:04 | I |   - Quantizing model.layers.0.self_attn.k_proj.weight
25-01-23 09:34:06 | I |   - Quantizing model.layers.0.self_attn.v_proj.weight
25-01-23 09:34:08 | I |   - Quantizing model.layers.0.self_attn.o_proj.weight
25-01-23 09:34:11 | I |   - Quantizing model.layers.0.mlp.up_proj.weight
25-01-23 09:34:13 | I |   - Quantizing model.layers.0.mlp.gate_proj.weight
25-01-23 09:34:15 | I |   - Quantizing model.layers.0.mlp.down_proj.weight
25-01-23 09:34:21 | I | quantizing activations for layer model.layers.0
25-01-23 09:34:22 | I | collecting calibration activations in model.layers.0
25-01-23 09:34:22 | I | collecting calibration activations in model.layers.0
25-01-23 09:34:24 | I | forward this layer
25-01-23 09:34:24 | I | input_file: /data/gyy/lmquant-main/lmquant/data/data_without_preprocess_llama_7b_bs16/shard/val_input_args_layer_0/91.pt
25-01-23 09:34:24 | I | output_file: /data/gyy/lmquant-main/lmquant/data/data_without_preprocess_llama_7b_bs16/shard/val_teacher_output_layer_0/91.pt
25-01-23 09:34:24 | I | In layer model.layers.0, gradient of model.layers.0.self_attn.q_proj has nan: False
25-01-23 09:34:24 | I | In layer model.layers.0, gradient of model.layers.0.self_attn.q_proj has inf: False
25-01-23 09:34:24 | I | In layer model.layers.0, gradient of model.layers.0.self_attn.k_proj has nan: False
25-01-23 09:34:24 | I | In layer model.layers.0, gradient of model.layers.0.self_attn.k_proj has inf: False
25-01-23 09:34:24 | I | In layer model.layers.0, gradient of model.layers.0.self_attn.v_proj has nan: False
25-01-23 09:34:24 | I | In layer model.layers.0, gradient of model.layers.0.self_attn.v_proj has inf: False
25-01-23 09:34:24 | I | In layer model.layers.0, gradient of model.layers.0.self_attn.o_proj has nan: False
25-01-23 09:34:24 | I | In layer model.layers.0, gradient of model.layers.0.self_attn.o_proj has inf: False
25-01-23 09:34:24 | I | In layer model.layers.0, gradient of model.layers.0.mlp.up_proj has nan: False
25-01-23 09:34:24 | I | In layer model.layers.0, gradient of model.layers.0.mlp.up_proj has inf: False
25-01-23 09:34:24 | I | In layer model.layers.0, gradient of model.layers.0.mlp.gate_proj has nan: False
25-01-23 09:34:24 | I | In layer model.layers.0, gradient of model.layers.0.mlp.gate_proj has inf: False
25-01-23 09:34:24 | I | In layer model.layers.0, gradient of model.layers.0.mlp.down_proj has nan: False
25-01-23 09:34:24 | I | In layer model.layers.0, gradient of model.layers.0.mlp.down_proj has inf: False
25-01-23 09:34:24 | I | [87] done with optimizer step
25-01-23 09:34:24 | I | epoch 001:     92 / 819200000 loss=1.27698e-05, loss_per_token=0.0523049, loss_sum=428.482, wps=223.6, ups=0.03, wpb=8192, bsz=16, num_updates=88, lr=0.00499937, gnorm=0.144, clip=0, loss_scale=8, train_wall=37, cuda_gb_allocated=17.1, cuda_gb_reserved=18.5, cuda_gb_free=6.6, wall=7812
25-01-23 09:34:24 | I | TRAIN CURRENT LAYER_IDX = 0
25-01-23 09:34:24 | I | in layer model.layers.0
25-01-23 09:34:24 | I | quantizing weights for layer model.layers.0
25-01-23 09:34:25 | I | collecting calibration activations in model.layers.0
25-01-23 09:34:25 | I | collecting calibration activations in model.layers.0
25-01-23 09:34:25 | I | - Quantizing decoder layer model.layers.0
25-01-23 09:34:25 | I |   - Calibrating model.layers.0.self_attn.q_proj.weight
25-01-23 09:34:26 | I |       - range scale = [    1.0000]
25-01-23 09:34:26 | I |         sum  error  = [    0.0445]
25-01-23 09:34:26 | I |         best error  = [    0.0445]
25-01-23 09:34:26 | I |     + error = [0.0445]
25-01-23 09:34:26 | I |       - range scale = [    1.0000]
25-01-23 09:34:26 | I |         sum  error  = [    0.6845]
25-01-23 09:34:26 | I |         best error  = [    0.6845]
25-01-23 09:34:26 | I |     + error = [0.6845]
25-01-23 09:34:27 | I |   - Calibrating model.layers.0.self_attn.k_proj.weight
25-01-23 09:34:27 | I |       - range scale = [    1.0000]
25-01-23 09:34:27 | I |         sum  error  = [    0.0552]
25-01-23 09:34:27 | I |         best error  = [    0.0552]
25-01-23 09:34:27 | I |     + error = [0.0552]
25-01-23 09:34:28 | I |       - range scale = [    1.0000]
25-01-23 09:34:28 | I |         sum  error  = [    0.9328]
25-01-23 09:34:28 | I |         best error  = [    0.9328]
25-01-23 09:34:28 | I |     + error = [0.9328]
25-01-23 09:34:28 | I |   - Calibrating model.layers.0.self_attn.v_proj.weight
25-01-23 09:34:29 | I |       - range scale = [    1.0000]
25-01-23 09:34:29 | I |         sum  error  = [    0.6639]
25-01-23 09:34:29 | I |         best error  = [    0.6639]
25-01-23 09:34:29 | I |     + error = [0.6639]
25-01-23 09:34:30 | I |       - range scale = [    1.0000]
25-01-23 09:34:30 | I |         sum  error  = [    6.1460]
25-01-23 09:34:30 | I |         best error  = [    6.1460]
25-01-23 09:34:30 | I |     + error = [6.1460]
25-01-23 09:34:30 | I |   - Calibrating model.layers.0.self_attn.o_proj.weight
25-01-23 09:34:31 | I |       - range scale = [    1.0000]
25-01-23 09:34:31 | I |         sum  error  = [    0.1897]
25-01-23 09:34:31 | I |         best error  = [    0.1897]
25-01-23 09:34:31 | I |     + error = [0.1897]
25-01-23 09:34:32 | I |       - range scale = [    1.0000]
25-01-23 09:34:32 | I |         sum  error  = [    1.5030]
25-01-23 09:34:32 | I |         best error  = [    1.5030]
25-01-23 09:34:32 | I |     + error = [1.5030]
25-01-23 09:34:32 | I |   - Calibrating model.layers.0.mlp.up_proj.weight
25-01-23 09:34:32 | I |       - range scale = [    1.0000]
25-01-23 09:34:32 | I |         sum  error  = [    1.0846]
25-01-23 09:34:32 | I |         best error  = [    1.0846]
25-01-23 09:34:32 | I |     + error = [1.0846]
25-01-23 09:34:34 | I |       - range scale = [    1.0000]
25-01-23 09:34:34 | I |         sum  error  = [   11.5437]
25-01-23 09:34:34 | I |         best error  = [   11.5437]
25-01-23 09:34:34 | I |     + error = [11.5437]
25-01-23 09:34:34 | I |   - Calibrating model.layers.0.mlp.gate_proj.weight
25-01-23 09:34:35 | I |       - range scale = [    1.0000]
25-01-23 09:34:35 | I |         sum  error  = [    1.1475]
25-01-23 09:34:35 | I |         best error  = [    1.1475]
25-01-23 09:34:35 | I |     + error = [1.1475]
25-01-23 09:34:36 | I |       - range scale = [    1.0000]
25-01-23 09:34:36 | I |         sum  error  = [   11.9337]
25-01-23 09:34:36 | I |         best error  = [   11.9337]
25-01-23 09:34:36 | I |     + error = [11.9337]
25-01-23 09:34:36 | I |   - Calibrating model.layers.0.mlp.down_proj.weight
25-01-23 09:34:37 | I |       - range scale = [    1.0000]
25-01-23 09:34:37 | I |         sum  error  = [    0.1662]
25-01-23 09:34:37 | I |         best error  = [    0.1662]
25-01-23 09:34:37 | I |     + error = [0.1662]
25-01-23 09:34:38 | I |       - range scale = [    1.0000]
25-01-23 09:34:38 | I |         sum  error  = [    1.6171]
25-01-23 09:34:38 | I |         best error  = [    1.6171]
25-01-23 09:34:38 | I |     + error = [1.6171]
25-01-23 09:34:38 | I |   - Quantizing model.layers.0.self_attn.q_proj.weight
25-01-23 09:34:40 | I |   - Quantizing model.layers.0.self_attn.k_proj.weight
25-01-23 09:34:43 | I |   - Quantizing model.layers.0.self_attn.v_proj.weight
25-01-23 09:34:45 | I |   - Quantizing model.layers.0.self_attn.o_proj.weight
25-01-23 09:34:47 | I |   - Quantizing model.layers.0.mlp.up_proj.weight
25-01-23 09:34:50 | I |   - Quantizing model.layers.0.mlp.gate_proj.weight
25-01-23 09:34:52 | I |   - Quantizing model.layers.0.mlp.down_proj.weight
25-01-23 09:34:58 | I | quantizing activations for layer model.layers.0
25-01-23 09:34:58 | I | collecting calibration activations in model.layers.0
25-01-23 09:34:58 | I | collecting calibration activations in model.layers.0
25-01-23 09:35:00 | I | forward this layer
25-01-23 09:35:00 | I | input_file: /data/gyy/lmquant-main/lmquant/data/data_without_preprocess_llama_7b_bs16/shard/val_input_args_layer_0/92.pt
25-01-23 09:35:00 | I | output_file: /data/gyy/lmquant-main/lmquant/data/data_without_preprocess_llama_7b_bs16/shard/val_teacher_output_layer_0/92.pt
25-01-23 09:35:01 | I | In layer model.layers.0, gradient of model.layers.0.self_attn.q_proj has nan: False
25-01-23 09:35:01 | I | In layer model.layers.0, gradient of model.layers.0.self_attn.q_proj has inf: False
25-01-23 09:35:01 | I | In layer model.layers.0, gradient of model.layers.0.self_attn.k_proj has nan: False
25-01-23 09:35:01 | I | In layer model.layers.0, gradient of model.layers.0.self_attn.k_proj has inf: False
25-01-23 09:35:01 | I | In layer model.layers.0, gradient of model.layers.0.self_attn.v_proj has nan: False
25-01-23 09:35:01 | I | In layer model.layers.0, gradient of model.layers.0.self_attn.v_proj has inf: False
25-01-23 09:35:01 | I | In layer model.layers.0, gradient of model.layers.0.self_attn.o_proj has nan: False
25-01-23 09:35:01 | I | In layer model.layers.0, gradient of model.layers.0.self_attn.o_proj has inf: False
25-01-23 09:35:01 | I | In layer model.layers.0, gradient of model.layers.0.mlp.up_proj has nan: False
25-01-23 09:35:01 | I | In layer model.layers.0, gradient of model.layers.0.mlp.up_proj has inf: False
25-01-23 09:35:01 | I | In layer model.layers.0, gradient of model.layers.0.mlp.gate_proj has nan: False
25-01-23 09:35:01 | I | In layer model.layers.0, gradient of model.layers.0.mlp.gate_proj has inf: False
25-01-23 09:35:01 | I | In layer model.layers.0, gradient of model.layers.0.mlp.down_proj has nan: False
25-01-23 09:35:01 | I | In layer model.layers.0, gradient of model.layers.0.mlp.down_proj has inf: False
25-01-23 09:35:01 | I | [88] done with optimizer step
25-01-23 09:35:01 | I | epoch 001:     93 / 819200000 loss=1.19892e-05, loss_per_token=0.0491078, loss_sum=402.291, wps=223.6, ups=0.03, wpb=8192, bsz=16, num_updates=89, lr=0.00499935, gnorm=0.117, clip=0, loss_scale=8, train_wall=37, cuda_gb_allocated=17.1, cuda_gb_reserved=18.7, cuda_gb_free=6.6, wall=7848
25-01-23 09:35:01 | I | TRAIN CURRENT LAYER_IDX = 0
25-01-23 09:35:01 | I | in layer model.layers.0
25-01-23 09:35:01 | I | quantizing weights for layer model.layers.0
25-01-23 09:35:01 | I | collecting calibration activations in model.layers.0
25-01-23 09:35:01 | I | collecting calibration activations in model.layers.0
25-01-23 09:35:02 | I | - Quantizing decoder layer model.layers.0
25-01-23 09:35:02 | I |   - Calibrating model.layers.0.self_attn.q_proj.weight
25-01-23 09:35:02 | I |       - range scale = [    1.0000]
25-01-23 09:35:02 | I |         sum  error  = [    0.0434]
25-01-23 09:35:02 | I |         best error  = [    0.0434]
25-01-23 09:35:02 | I |     + error = [0.0434]
25-01-23 09:35:03 | I |       - range scale = [    1.0000]
25-01-23 09:35:03 | I |         sum  error  = [    0.6713]
25-01-23 09:35:03 | I |         best error  = [    0.6713]
25-01-23 09:35:03 | I |     + error = [0.6713]
25-01-23 09:35:03 | I |   - Calibrating model.layers.0.self_attn.k_proj.weight
25-01-23 09:35:04 | I |       - range scale = [    1.0000]
25-01-23 09:35:04 | I |         sum  error  = [    0.0515]
25-01-23 09:35:04 | I |         best error  = [    0.0515]
25-01-23 09:35:04 | I |     + error = [0.0515]
25-01-23 09:35:05 | I |       - range scale = [    1.0000]
25-01-23 09:35:05 | I |         sum  error  = [    0.7882]
25-01-23 09:35:05 | I |         best error  = [    0.7882]
25-01-23 09:35:05 | I |     + error = [0.7882]
25-01-23 09:35:05 | I |   - Calibrating model.layers.0.self_attn.v_proj.weight
25-01-23 09:35:06 | I |       - range scale = [    1.0000]
25-01-23 09:35:06 | I |         sum  error  = [    0.7014]
25-01-23 09:35:06 | I |         best error  = [    0.7014]
25-01-23 09:35:06 | I |     + error = [0.7014]
25-01-23 09:35:06 | I |       - range scale = [    1.0000]
25-01-23 09:35:06 | I |         sum  error  = [    6.4370]
25-01-23 09:35:06 | I |         best error  = [    6.4370]
25-01-23 09:35:06 | I |     + error = [6.4370]
25-01-23 09:35:07 | I |   - Calibrating model.layers.0.self_attn.o_proj.weight
25-01-23 09:35:07 | I |       - range scale = [    1.0000]
25-01-23 09:35:07 | I |         sum  error  = [    0.2013]
25-01-23 09:35:07 | I |         best error  = [    0.2013]
25-01-23 09:35:07 | I |     + error = [0.2013]
25-01-23 09:35:08 | I |       - range scale = [    1.0000]
25-01-23 09:35:08 | I |         sum  error  = [    1.6088]
25-01-23 09:35:08 | I |         best error  = [    1.6088]
25-01-23 09:35:08 | I |     + error = [1.6088]
25-01-23 09:35:08 | I |   - Calibrating model.layers.0.mlp.up_proj.weight
25-01-23 09:35:09 | I |       - range scale = [    1.0000]
25-01-23 09:35:09 | I |         sum  error  = [    1.0426]
25-01-23 09:35:09 | I |         best error  = [    1.0426]
25-01-23 09:35:09 | I |     + error = [1.0426]
25-01-23 09:35:10 | I |       - range scale = [    1.0000]
25-01-23 09:35:10 | I |         sum  error  = [   11.0817]
25-01-23 09:35:10 | I |         best error  = [   11.0817]
25-01-23 09:35:10 | I |     + error = [11.0817]
25-01-23 09:35:10 | I |   - Calibrating model.layers.0.mlp.gate_proj.weight
25-01-23 09:35:11 | I |       - range scale = [    1.0000]
25-01-23 09:35:11 | I |         sum  error  = [    1.1063]
25-01-23 09:35:11 | I |         best error  = [    1.1063]
25-01-23 09:35:11 | I |     + error = [1.1063]
25-01-23 09:35:12 | I |       - range scale = [    1.0000]
25-01-23 09:35:12 | I |         sum  error  = [   11.4606]
25-01-23 09:35:12 | I |         best error  = [   11.4606]
25-01-23 09:35:12 | I |     + error = [11.4606]
25-01-23 09:35:13 | I |   - Calibrating model.layers.0.mlp.down_proj.weight
25-01-23 09:35:13 | I |       - range scale = [    1.0000]
25-01-23 09:35:13 | I |         sum  error  = [    0.1627]
25-01-23 09:35:13 | I |         best error  = [    0.1627]
25-01-23 09:35:13 | I |     + error = [0.1627]
25-01-23 09:35:14 | I |       - range scale = [    1.0000]
25-01-23 09:35:14 | I |         sum  error  = [    1.5881]
25-01-23 09:35:14 | I |         best error  = [    1.5881]
25-01-23 09:35:14 | I |     + error = [1.5881]
25-01-23 09:35:15 | I |   - Quantizing model.layers.0.self_attn.q_proj.weight
25-01-23 09:35:17 | I |   - Quantizing model.layers.0.self_attn.k_proj.weight
25-01-23 09:35:19 | I |   - Quantizing model.layers.0.self_attn.v_proj.weight
25-01-23 09:35:22 | I |   - Quantizing model.layers.0.self_attn.o_proj.weight
25-01-23 09:35:24 | I |   - Quantizing model.layers.0.mlp.up_proj.weight
25-01-23 09:35:26 | I |   - Quantizing model.layers.0.mlp.gate_proj.weight
25-01-23 09:35:29 | I |   - Quantizing model.layers.0.mlp.down_proj.weight
25-01-23 09:35:35 | I | quantizing activations for layer model.layers.0
25-01-23 09:35:35 | I | collecting calibration activations in model.layers.0
25-01-23 09:35:35 | I | collecting calibration activations in model.layers.0
25-01-23 09:35:37 | I | forward this layer
25-01-23 09:35:37 | I | input_file: /data/gyy/lmquant-main/lmquant/data/data_without_preprocess_llama_7b_bs16/shard/val_input_args_layer_0/93.pt
25-01-23 09:35:37 | I | output_file: /data/gyy/lmquant-main/lmquant/data/data_without_preprocess_llama_7b_bs16/shard/val_teacher_output_layer_0/93.pt
25-01-23 09:35:37 | I | In layer model.layers.0, gradient of model.layers.0.self_attn.q_proj has nan: False
25-01-23 09:35:37 | I | In layer model.layers.0, gradient of model.layers.0.self_attn.q_proj has inf: False
25-01-23 09:35:37 | I | In layer model.layers.0, gradient of model.layers.0.self_attn.k_proj has nan: False
25-01-23 09:35:37 | I | In layer model.layers.0, gradient of model.layers.0.self_attn.k_proj has inf: False
25-01-23 09:35:37 | I | In layer model.layers.0, gradient of model.layers.0.self_attn.v_proj has nan: False
25-01-23 09:35:37 | I | In layer model.layers.0, gradient of model.layers.0.self_attn.v_proj has inf: False
25-01-23 09:35:37 | I | In layer model.layers.0, gradient of model.layers.0.self_attn.o_proj has nan: False
25-01-23 09:35:37 | I | In layer model.layers.0, gradient of model.layers.0.self_attn.o_proj has inf: False
25-01-23 09:35:37 | I | In layer model.layers.0, gradient of model.layers.0.mlp.up_proj has nan: False
25-01-23 09:35:37 | I | In layer model.layers.0, gradient of model.layers.0.mlp.up_proj has inf: False
25-01-23 09:35:37 | I | In layer model.layers.0, gradient of model.layers.0.mlp.gate_proj has nan: False
25-01-23 09:35:37 | I | In layer model.layers.0, gradient of model.layers.0.mlp.gate_proj has inf: False
25-01-23 09:35:37 | I | In layer model.layers.0, gradient of model.layers.0.mlp.down_proj has nan: False
25-01-23 09:35:37 | I | In layer model.layers.0, gradient of model.layers.0.mlp.down_proj has inf: False
25-01-23 09:35:37 | I | [89] done with optimizer step
25-01-23 09:35:37 | I | epoch 001:     94 / 819200000 loss=1.22953e-05, loss_per_token=0.0503617, loss_sum=412.563, wps=224.3, ups=0.03, wpb=8192, bsz=16, num_updates=90, lr=0.00499933, gnorm=0.208, clip=0, loss_scale=8, train_wall=36, cuda_gb_allocated=17.1, cuda_gb_reserved=18.5, cuda_gb_free=6.6, wall=7885
25-01-23 09:35:37 | I | begin validation on "valid" subset on rank 0
25-01-23 09:35:37 | I | got valid iterator on "valid" subset on rank 0
25-01-23 09:35:37 | I | Valid: Start iterating over samples
25-01-23 09:35:37 | W |   `pretrained` model kwarg is not of type `str`. Many other model arguments may be ignored. Please do not launch via accelerate or use `parallelize=True` if passing an existing model this way.
25-01-23 09:35:37 | W |   Passed an already-initialized model through `pretrained`, assuming single-process call to evaluate() or custom distributed integration
25-01-23 09:35:37 | I | - Evaluator: gptq
25-01-23 09:35:37 | I | - Tasks: ['wikitext', 'val_valid']
25-01-23 09:35:37 | I | - Batch_size: 8
25-01-23 09:35:37 | I |   + Max_seq_length: 2048
25-01-23 09:37:12 | I |     - Results:
25-01-23 09:37:12 | I |       |  Task   |Version|    Metric     |Value |   |Stderr|
25-01-23 09:37:12 | I |       |---------|------:|---------------|-----:|---|-----:|
25-01-23 09:37:12 | I |       |wikitext |      1|word_perplexity|5.4834|±  |5.4834|
25-01-23 09:37:12 | I |       |val_valid|      1|word_perplexity|5.0362|±  |5.0362|
25-01-23 09:37:12 | I |       
25-01-23 09:37:12 | I |   + Max_seq_length: 4096
25-01-23 09:38:46 | I |     - Results:
25-01-23 09:38:46 | I |       |  Task   |Version|    Metric     |Value |   |Stderr|
25-01-23 09:38:46 | I |       |---------|------:|---------------|-----:|---|-----:|
25-01-23 09:38:46 | I |       |wikitext |      1|word_perplexity|5.1239|±  |5.1239|
25-01-23 09:38:46 | I |       |val_valid|      1|word_perplexity|4.8320|±  |4.8320|
25-01-23 09:38:46 | I |       
25-01-23 09:38:46 | I | in valid, quantize current layer weights
25-01-23 09:38:46 | W | Repo card metadata block was not found. Setting CardData to empty.
25-01-23 09:39:31 | I | collecting calibration activations in model.layers.0
25-01-23 09:39:31 | I | collecting calibration activations in model.layers.0
25-01-23 09:39:31 | I | collecting calibration activations in model.layers.0
25-01-23 09:39:31 | I | collecting calibration activations in model.layers.0
25-01-23 09:39:31 | I | collecting calibration activations in model.layers.0
25-01-23 09:39:31 | I | collecting calibration activations in model.layers.0
25-01-23 09:39:31 | I | collecting calibration activations in model.layers.0
25-01-23 09:39:31 | I | collecting calibration activations in model.layers.0
25-01-23 09:39:32 | I | collecting calibration activations in model.layers.0
25-01-23 09:39:32 | I | collecting calibration activations in model.layers.0
25-01-23 09:39:32 | I | collecting calibration activations in model.layers.0
25-01-23 09:39:32 | I | collecting calibration activations in model.layers.0
25-01-23 09:39:32 | I | collecting calibration activations in model.layers.0
25-01-23 09:39:32 | I | collecting calibration activations in model.layers.0
25-01-23 09:39:32 | I | collecting calibration activations in model.layers.0
25-01-23 09:39:32 | I | collecting calibration activations in model.layers.0
25-01-23 09:39:32 | I | collecting calibration activations in model.layers.0
25-01-23 09:39:32 | I | collecting calibration activations in model.layers.0
25-01-23 09:39:32 | I | collecting calibration activations in model.layers.0
25-01-23 09:39:32 | I | collecting calibration activations in model.layers.0
25-01-23 09:39:32 | I | collecting calibration activations in model.layers.0
25-01-23 09:39:32 | I | collecting calibration activations in model.layers.0
25-01-23 09:39:32 | I | collecting calibration activations in model.layers.0
25-01-23 09:39:32 | I | collecting calibration activations in model.layers.0
25-01-23 09:39:32 | I | collecting calibration activations in model.layers.0
25-01-23 09:39:32 | I | collecting calibration activations in model.layers.0
25-01-23 09:39:32 | I | collecting calibration activations in model.layers.0
25-01-23 09:39:33 | I | collecting calibration activations in model.layers.0
25-01-23 09:39:33 | I | collecting calibration activations in model.layers.0
25-01-23 09:39:33 | I | collecting calibration activations in model.layers.0
25-01-23 09:39:33 | I | collecting calibration activations in model.layers.0
25-01-23 09:39:33 | I | collecting calibration activations in model.layers.0
25-01-23 09:39:33 | I | collecting calibration activations in model.layers.0
25-01-23 09:39:33 | I | collecting calibration activations in model.layers.0
25-01-23 09:39:33 | I | collecting calibration activations in model.layers.0
25-01-23 09:39:33 | I | collecting calibration activations in model.layers.0
25-01-23 09:39:33 | I | collecting calibration activations in model.layers.0
25-01-23 09:39:33 | I | collecting calibration activations in model.layers.0
25-01-23 09:39:33 | I | collecting calibration activations in model.layers.0
25-01-23 09:39:33 | I | collecting calibration activations in model.layers.0
25-01-23 09:39:33 | I | collecting calibration activations in model.layers.0
25-01-23 09:39:33 | I | collecting calibration activations in model.layers.0
25-01-23 09:39:33 | I | collecting calibration activations in model.layers.0
25-01-23 09:39:33 | I | collecting calibration activations in model.layers.0
25-01-23 09:39:33 | I | collecting calibration activations in model.layers.0
25-01-23 09:39:33 | I | collecting calibration activations in model.layers.0
25-01-23 09:39:34 | I | collecting calibration activations in model.layers.0
25-01-23 09:39:34 | I | collecting calibration activations in model.layers.0
25-01-23 09:39:34 | I | collecting calibration activations in model.layers.0
25-01-23 09:39:34 | I | collecting calibration activations in model.layers.0
25-01-23 09:39:34 | I | collecting calibration activations in model.layers.0
25-01-23 09:39:34 | I | collecting calibration activations in model.layers.0
25-01-23 09:39:34 | I | collecting calibration activations in model.layers.0
25-01-23 09:39:34 | I | collecting calibration activations in model.layers.0
25-01-23 09:39:34 | I | collecting calibration activations in model.layers.0
25-01-23 09:39:34 | I | collecting calibration activations in model.layers.0
25-01-23 09:39:34 | I | collecting calibration activations in model.layers.0
25-01-23 09:39:34 | I | collecting calibration activations in model.layers.0
25-01-23 09:39:34 | I | collecting calibration activations in model.layers.0
25-01-23 09:39:34 | I | collecting calibration activations in model.layers.0
25-01-23 09:39:34 | I | collecting calibration activations in model.layers.0
25-01-23 09:39:34 | I | collecting calibration activations in model.layers.0
25-01-23 09:39:34 | I | collecting calibration activations in model.layers.0
25-01-23 09:39:34 | I | collecting calibration activations in model.layers.0
25-01-23 09:39:35 | I | collecting calibration activations in model.layers.0
25-01-23 09:39:35 | I | collecting calibration activations in model.layers.0
25-01-23 09:39:35 | I | collecting calibration activations in model.layers.0
25-01-23 09:39:35 | I | collecting calibration activations in model.layers.0
25-01-23 09:39:35 | I | collecting calibration activations in model.layers.0
25-01-23 09:39:35 | I | collecting calibration activations in model.layers.0
25-01-23 09:39:35 | I | collecting calibration activations in model.layers.0
25-01-23 09:39:35 | I | collecting calibration activations in model.layers.0
25-01-23 09:39:35 | I | collecting calibration activations in model.layers.0
25-01-23 09:39:35 | I | collecting calibration activations in model.layers.0
25-01-23 09:39:35 | I | collecting calibration activations in model.layers.0
25-01-23 09:39:35 | I | collecting calibration activations in model.layers.0
25-01-23 09:39:35 | I | collecting calibration activations in model.layers.0
25-01-23 09:39:35 | I | collecting calibration activations in model.layers.0
25-01-23 09:39:35 | I | collecting calibration activations in model.layers.0
25-01-23 09:39:35 | I | collecting calibration activations in model.layers.0
25-01-23 09:39:35 | I | collecting calibration activations in model.layers.0
25-01-23 09:39:35 | I | collecting calibration activations in model.layers.0
25-01-23 09:39:35 | I | collecting calibration activations in model.layers.0
25-01-23 09:39:36 | I | collecting calibration activations in model.layers.0
25-01-23 09:39:36 | I | collecting calibration activations in model.layers.0
25-01-23 09:39:36 | I | collecting calibration activations in model.layers.0
25-01-23 09:39:36 | I | collecting calibration activations in model.layers.0
25-01-23 09:39:36 | I | collecting calibration activations in model.layers.0
25-01-23 09:39:36 | I | collecting calibration activations in model.layers.0
25-01-23 09:39:36 | I | collecting calibration activations in model.layers.0
25-01-23 09:39:36 | I | collecting calibration activations in model.layers.0
25-01-23 09:39:36 | I | collecting calibration activations in model.layers.0
25-01-23 09:39:36 | I | collecting calibration activations in model.layers.0
25-01-23 09:39:36 | I | collecting calibration activations in model.layers.0
25-01-23 09:39:36 | I | collecting calibration activations in model.layers.0
25-01-23 09:39:36 | I | collecting calibration activations in model.layers.0
25-01-23 09:39:36 | I | collecting calibration activations in model.layers.0
25-01-23 09:39:36 | I | collecting calibration activations in model.layers.0
25-01-23 09:39:36 | I | collecting calibration activations in model.layers.0
25-01-23 09:39:36 | I | collecting calibration activations in model.layers.0
25-01-23 09:39:36 | I | collecting calibration activations in model.layers.0
25-01-23 09:39:36 | I | collecting calibration activations in model.layers.0
25-01-23 09:39:37 | I | collecting calibration activations in model.layers.0
25-01-23 09:39:37 | I | collecting calibration activations in model.layers.0
25-01-23 09:39:37 | I | collecting calibration activations in model.layers.0
25-01-23 09:39:37 | I | collecting calibration activations in model.layers.0
25-01-23 09:39:37 | I | collecting calibration activations in model.layers.0
25-01-23 09:39:37 | I | collecting calibration activations in model.layers.0
25-01-23 09:39:37 | I | collecting calibration activations in model.layers.0
25-01-23 09:39:37 | I | collecting calibration activations in model.layers.0
25-01-23 09:39:37 | I | collecting calibration activations in model.layers.0
25-01-23 09:39:37 | I | collecting calibration activations in model.layers.0
25-01-23 09:39:37 | I | collecting calibration activations in model.layers.0
25-01-23 09:39:37 | I | collecting calibration activations in model.layers.0
25-01-23 09:39:37 | I | collecting calibration activations in model.layers.0
25-01-23 09:39:37 | I | collecting calibration activations in model.layers.0
25-01-23 09:39:37 | I | collecting calibration activations in model.layers.0
25-01-23 09:39:37 | I | collecting calibration activations in model.layers.0
25-01-23 09:39:37 | I | collecting calibration activations in model.layers.0
25-01-23 09:39:37 | I | collecting calibration activations in model.layers.0
25-01-23 09:39:37 | I | collecting calibration activations in model.layers.0
25-01-23 09:39:38 | I | collecting calibration activations in model.layers.0
25-01-23 09:39:38 | I | collecting calibration activations in model.layers.0
25-01-23 09:39:38 | I | collecting calibration activations in model.layers.0
25-01-23 09:39:38 | I | collecting calibration activations in model.layers.0
25-01-23 09:39:38 | I | collecting calibration activations in model.layers.0
25-01-23 09:39:38 | I | collecting calibration activations in model.layers.0
25-01-23 09:39:38 | I | collecting calibration activations in model.layers.0
25-01-23 09:39:40 | I |       - range scale = [    1.0000]
25-01-23 09:39:40 | I |         sum  error  = [    0.1580]
25-01-23 09:39:40 | I |         best error  = [    0.1580]
25-01-23 09:39:40 | I |     + error = [0.1580]
25-01-23 09:39:41 | I |       - range scale = [    1.0000]
25-01-23 09:39:41 | I |         sum  error  = [    2.7223]
25-01-23 09:39:41 | I |         best error  = [    2.7223]
25-01-23 09:39:41 | I |     + error = [2.7223]
25-01-23 09:39:43 | I |       - range scale = [    1.0000]
25-01-23 09:39:43 | I |         sum  error  = [    0.1850]
25-01-23 09:39:43 | I |         best error  = [    0.1850]
25-01-23 09:39:43 | I |     + error = [0.1850]
25-01-23 09:39:44 | I |       - range scale = [    1.0000]
25-01-23 09:39:44 | I |         sum  error  = [    3.7498]
25-01-23 09:39:44 | I |         best error  = [    3.7498]
25-01-23 09:39:44 | I |     + error = [3.7498]
25-01-23 09:39:45 | I |       - range scale = [    1.0000]
25-01-23 09:39:45 | I |         sum  error  = [    0.6461]
25-01-23 09:39:45 | I |         best error  = [    0.6461]
25-01-23 09:39:45 | I |     + error = [0.6461]
25-01-23 09:39:46 | I |       - range scale = [    1.0000]
25-01-23 09:39:46 | I |         sum  error  = [    5.9409]
25-01-23 09:39:46 | I |         best error  = [    5.9409]
25-01-23 09:39:46 | I |     + error = [5.9409]
25-01-23 09:39:47 | I |       - range scale = [    1.0000]
25-01-23 09:39:47 | I |         sum  error  = [    0.1488]
25-01-23 09:39:47 | I |         best error  = [    0.1488]
25-01-23 09:39:47 | I |     + error = [0.1488]
25-01-23 09:39:48 | I |       - range scale = [    1.0000]
25-01-23 09:39:48 | I |         sum  error  = [    1.2269]
25-01-23 09:39:48 | I |         best error  = [    1.2269]
25-01-23 09:39:48 | I |     + error = [1.2269]
25-01-23 09:39:49 | I |       - range scale = [    1.0000]
25-01-23 09:39:49 | I |         sum  error  = [    1.0988]
25-01-23 09:39:49 | I |         best error  = [    1.0988]
25-01-23 09:39:49 | I |     + error = [1.0988]
25-01-23 09:39:50 | I |       - range scale = [    1.0000]
25-01-23 09:39:50 | I |         sum  error  = [   11.7095]
25-01-23 09:39:50 | I |         best error  = [   11.7095]
25-01-23 09:39:50 | I |     + error = [11.7095]
25-01-23 09:39:51 | I |       - range scale = [    1.0000]
25-01-23 09:39:51 | I |         sum  error  = [    1.1666]
25-01-23 09:39:51 | I |         best error  = [    1.1666]
25-01-23 09:39:51 | I |     + error = [1.1666]
25-01-23 09:39:52 | I |       - range scale = [    1.0000]
25-01-23 09:39:52 | I |         sum  error  = [   12.0897]
25-01-23 09:39:52 | I |         best error  = [   12.0897]
25-01-23 09:39:52 | I |     + error = [12.0897]
25-01-23 09:39:53 | I |       - range scale = [    1.0000]
25-01-23 09:39:53 | I |         sum  error  = [    0.1932]
25-01-23 09:39:53 | I |         best error  = [    0.1932]
25-01-23 09:39:53 | I |     + error = [0.1932]
25-01-23 09:39:54 | I |       - range scale = [    1.0000]
25-01-23 09:39:54 | I |         sum  error  = [    1.9089]
25-01-23 09:39:54 | I |         best error  = [    1.9089]
25-01-23 09:39:54 | I |     + error = [1.9089]
25-01-23 09:40:19 | I | in valid, quantize current layer acts
25-01-23 09:40:20 | I | collecting calibration activations in model.layers.0
25-01-23 09:40:20 | I | collecting calibration activations in model.layers.0
25-01-23 09:40:20 | I | collecting calibration activations in model.layers.0
25-01-23 09:40:21 | I | collecting calibration activations in model.layers.0
25-01-23 09:40:21 | I | collecting calibration activations in model.layers.0
25-01-23 09:40:21 | I | collecting calibration activations in model.layers.0
25-01-23 09:40:21 | I | collecting calibration activations in model.layers.0
25-01-23 09:40:21 | I | collecting calibration activations in model.layers.0
25-01-23 09:40:21 | I | collecting calibration activations in model.layers.0
25-01-23 09:40:21 | I | collecting calibration activations in model.layers.0
25-01-23 09:40:21 | I | collecting calibration activations in model.layers.0
25-01-23 09:40:21 | I | collecting calibration activations in model.layers.0
25-01-23 09:40:21 | I | collecting calibration activations in model.layers.0
25-01-23 09:40:21 | I | collecting calibration activations in model.layers.0
25-01-23 09:40:21 | I | collecting calibration activations in model.layers.0
25-01-23 09:40:21 | I | collecting calibration activations in model.layers.0
25-01-23 09:40:21 | I | collecting calibration activations in model.layers.0
25-01-23 09:40:21 | I | collecting calibration activations in model.layers.0
25-01-23 09:40:22 | I | collecting calibration activations in model.layers.0
25-01-23 09:40:22 | I | collecting calibration activations in model.layers.0
25-01-23 09:40:22 | I | collecting calibration activations in model.layers.0
25-01-23 09:40:22 | I | collecting calibration activations in model.layers.0
25-01-23 09:40:22 | I | collecting calibration activations in model.layers.0
25-01-23 09:40:22 | I | collecting calibration activations in model.layers.0
25-01-23 09:40:22 | I | collecting calibration activations in model.layers.0
25-01-23 09:40:22 | I | collecting calibration activations in model.layers.0
25-01-23 09:40:22 | I | collecting calibration activations in model.layers.0
25-01-23 09:40:22 | I | collecting calibration activations in model.layers.0
25-01-23 09:40:22 | I | collecting calibration activations in model.layers.0
25-01-23 09:40:22 | I | collecting calibration activations in model.layers.0
25-01-23 09:40:22 | I | collecting calibration activations in model.layers.0
25-01-23 09:40:22 | I | collecting calibration activations in model.layers.0
25-01-23 09:40:22 | I | collecting calibration activations in model.layers.0
25-01-23 09:40:23 | I | collecting calibration activations in model.layers.0
25-01-23 09:40:23 | I | collecting calibration activations in model.layers.0
25-01-23 09:40:23 | I | collecting calibration activations in model.layers.0
25-01-23 09:40:23 | I | collecting calibration activations in model.layers.0
25-01-23 09:40:23 | I | collecting calibration activations in model.layers.0
25-01-23 09:40:23 | I | collecting calibration activations in model.layers.0
25-01-23 09:40:23 | I | collecting calibration activations in model.layers.0
25-01-23 09:40:23 | I | collecting calibration activations in model.layers.0
25-01-23 09:40:23 | I | collecting calibration activations in model.layers.0
25-01-23 09:40:23 | I | collecting calibration activations in model.layers.0
25-01-23 09:40:23 | I | collecting calibration activations in model.layers.0
25-01-23 09:40:23 | I | collecting calibration activations in model.layers.0
25-01-23 09:40:23 | I | collecting calibration activations in model.layers.0
25-01-23 09:40:23 | I | collecting calibration activations in model.layers.0
25-01-23 09:40:23 | I | collecting calibration activations in model.layers.0
25-01-23 09:40:23 | I | collecting calibration activations in model.layers.0
25-01-23 09:40:24 | I | collecting calibration activations in model.layers.0
25-01-23 09:40:24 | I | collecting calibration activations in model.layers.0
25-01-23 09:40:24 | I | collecting calibration activations in model.layers.0
25-01-23 09:40:24 | I | collecting calibration activations in model.layers.0
25-01-23 09:40:24 | I | collecting calibration activations in model.layers.0
25-01-23 09:40:24 | I | collecting calibration activations in model.layers.0
25-01-23 09:40:24 | I | collecting calibration activations in model.layers.0
25-01-23 09:40:24 | I | collecting calibration activations in model.layers.0
25-01-23 09:40:24 | I | collecting calibration activations in model.layers.0
25-01-23 09:40:24 | I | collecting calibration activations in model.layers.0
25-01-23 09:40:24 | I | collecting calibration activations in model.layers.0
25-01-23 09:40:24 | I | collecting calibration activations in model.layers.0
25-01-23 09:40:24 | I | collecting calibration activations in model.layers.0
25-01-23 09:40:24 | I | collecting calibration activations in model.layers.0
25-01-23 09:40:24 | I | collecting calibration activations in model.layers.0
25-01-23 09:40:25 | I | collecting calibration activations in model.layers.0
25-01-23 09:40:25 | I | collecting calibration activations in model.layers.0
25-01-23 09:40:25 | I | collecting calibration activations in model.layers.0
25-01-23 09:40:25 | I | collecting calibration activations in model.layers.0
25-01-23 09:40:25 | I | collecting calibration activations in model.layers.0
25-01-23 09:40:25 | I | collecting calibration activations in model.layers.0
25-01-23 09:40:25 | I | collecting calibration activations in model.layers.0
25-01-23 09:40:25 | I | collecting calibration activations in model.layers.0
25-01-23 09:40:25 | I | collecting calibration activations in model.layers.0
25-01-23 09:40:25 | I | collecting calibration activations in model.layers.0
25-01-23 09:40:25 | I | collecting calibration activations in model.layers.0
25-01-23 09:40:25 | I | collecting calibration activations in model.layers.0
25-01-23 09:40:25 | I | collecting calibration activations in model.layers.0
25-01-23 09:40:25 | I | collecting calibration activations in model.layers.0
25-01-23 09:40:25 | I | collecting calibration activations in model.layers.0
25-01-23 09:40:25 | I | collecting calibration activations in model.layers.0
25-01-23 09:40:26 | I | collecting calibration activations in model.layers.0
25-01-23 09:40:26 | I | collecting calibration activations in model.layers.0
25-01-23 09:40:26 | I | collecting calibration activations in model.layers.0
25-01-23 09:40:26 | I | collecting calibration activations in model.layers.0
25-01-23 09:40:26 | I | collecting calibration activations in model.layers.0
25-01-23 09:40:26 | I | collecting calibration activations in model.layers.0
25-01-23 09:40:26 | I | collecting calibration activations in model.layers.0
25-01-23 09:40:26 | I | collecting calibration activations in model.layers.0
25-01-23 09:40:26 | I | collecting calibration activations in model.layers.0
25-01-23 09:40:26 | I | collecting calibration activations in model.layers.0
25-01-23 09:40:26 | I | collecting calibration activations in model.layers.0
25-01-23 09:40:26 | I | collecting calibration activations in model.layers.0
25-01-23 09:40:26 | I | collecting calibration activations in model.layers.0
25-01-23 09:40:26 | I | collecting calibration activations in model.layers.0
25-01-23 09:40:26 | I | collecting calibration activations in model.layers.0
25-01-23 09:40:26 | I | collecting calibration activations in model.layers.0
25-01-23 09:40:27 | I | collecting calibration activations in model.layers.0
25-01-23 09:40:27 | I | collecting calibration activations in model.layers.0
25-01-23 09:40:27 | I | collecting calibration activations in model.layers.0
25-01-23 09:40:27 | I | collecting calibration activations in model.layers.0
25-01-23 09:40:27 | I | collecting calibration activations in model.layers.0
25-01-23 09:40:27 | I | collecting calibration activations in model.layers.0
25-01-23 09:40:27 | I | collecting calibration activations in model.layers.0
25-01-23 09:40:27 | I | collecting calibration activations in model.layers.0
25-01-23 09:40:27 | I | collecting calibration activations in model.layers.0
25-01-23 09:40:27 | I | collecting calibration activations in model.layers.0
25-01-23 09:40:27 | I | collecting calibration activations in model.layers.0
25-01-23 09:40:27 | I | collecting calibration activations in model.layers.0
25-01-23 09:40:27 | I | collecting calibration activations in model.layers.0
25-01-23 09:40:27 | I | collecting calibration activations in model.layers.0
25-01-23 09:40:27 | I | collecting calibration activations in model.layers.0
25-01-23 09:40:28 | I | collecting calibration activations in model.layers.0
25-01-23 09:40:28 | I | collecting calibration activations in model.layers.0
25-01-23 09:40:28 | I | collecting calibration activations in model.layers.0
25-01-23 09:40:28 | I | collecting calibration activations in model.layers.0
25-01-23 09:40:28 | I | collecting calibration activations in model.layers.0
25-01-23 09:40:28 | I | collecting calibration activations in model.layers.0
25-01-23 09:40:28 | I | collecting calibration activations in model.layers.0
25-01-23 09:40:28 | I | collecting calibration activations in model.layers.0
25-01-23 09:40:28 | I | collecting calibration activations in model.layers.0
25-01-23 09:40:28 | I | collecting calibration activations in model.layers.0
25-01-23 09:40:28 | I | collecting calibration activations in model.layers.0
25-01-23 09:40:28 | I | collecting calibration activations in model.layers.0
25-01-23 09:40:28 | I | collecting calibration activations in model.layers.0
25-01-23 09:40:28 | I | collecting calibration activations in model.layers.0
25-01-23 09:40:28 | I | collecting calibration activations in model.layers.0
25-01-23 09:40:28 | I | collecting calibration activations in model.layers.0
25-01-23 09:40:29 | I | collecting calibration activations in model.layers.0
25-01-23 09:40:31 | I | use gptq_eval(lmquant) to calculate ppl, partly quanted
25-01-23 09:40:31 | W |   `pretrained` model kwarg is not of type `str`. Many other model arguments may be ignored. Please do not launch via accelerate or use `parallelize=True` if passing an existing model this way.
25-01-23 09:40:31 | W |   Passed an already-initialized model through `pretrained`, assuming single-process call to evaluate() or custom distributed integration
25-01-23 09:40:31 | I | - Evaluator: gptq
25-01-23 09:40:31 | I | - Tasks: ['wikitext', 'val_valid']
25-01-23 09:40:31 | I | - Batch_size: 8
25-01-23 09:40:31 | I |   + Max_seq_length: 2048
25-01-23 09:42:09 | I |     - Results:
25-01-23 09:42:09 | I |       |  Task   |Version|    Metric     |Value |   |Stderr|
25-01-23 09:42:09 | I |       |---------|------:|---------------|-----:|---|-----:|
25-01-23 09:42:09 | I |       |wikitext |      1|word_perplexity|5.4893|±  |5.4893|
25-01-23 09:42:09 | I |       |val_valid|      1|word_perplexity|5.0625|±  |5.0625|
25-01-23 09:42:09 | I |       
25-01-23 09:42:09 | I |   + Max_seq_length: 4096
25-01-23 09:43:44 | I |     - Results:
25-01-23 09:43:44 | I |       |  Task   |Version|    Metric     |Value |   |Stderr|
25-01-23 09:43:44 | I |       |---------|------:|---------------|-----:|---|-----:|
25-01-23 09:43:44 | I |       |wikitext |      1|word_perplexity|5.1277|±  |5.1277|
25-01-23 09:43:44 | I |       |val_valid|      1|word_perplexity|4.8570|±  |4.8570|
25-01-23 09:43:44 | I |       
25-01-23 09:44:43 | I | epoch 001 | valid on 'valid' subset:    102 / 819200000 loss_valid=2.149, lmquant_ppl_result_val=-1, lmquant_ppl_result_wikitext=-1, ppl=4.44, wps=7230.6, wpb=4058.2, bsz=2, num_updates=90, lmquant_ppl_wikitext_all_quanted=5.48337, lmquant_ppl_val_all_quanted=5.03618, lmquant_ppl_wikitext_partly_quanted=5.48934, lmquant_ppl_val_partly_quanted=5.06246
25-01-23 09:44:43 | I | epoch 001 | valid on 'valid' subset | loss_valid 2.149 | lmquant_ppl_result_val -1 | lmquant_ppl_result_wikitext -1 | ppl 4.44 | wps 7230.6 | wpb 4058.2 | bsz 2 | num_updates 90 | lmquant_ppl_wikitext_all_quanted 5.48337 | lmquant_ppl_val_all_quanted 5.03618 | lmquant_ppl_wikitext_partly_quanted 5.48934 | lmquant_ppl_val_partly_quanted 5.06246
25-01-23 09:44:43 | I | TRAIN CURRENT LAYER_IDX = 0
25-01-23 09:44:43 | I | in layer model.layers.0
25-01-23 09:44:43 | I | quantizing weights for layer model.layers.0
25-01-23 09:44:44 | I | collecting calibration activations in model.layers.0
25-01-23 09:44:44 | I | collecting calibration activations in model.layers.0
25-01-23 09:44:44 | I | - Quantizing decoder layer model.layers.0
25-01-23 09:44:44 | I |   - Calibrating model.layers.0.self_attn.q_proj.weight
25-01-23 09:44:45 | I |       - range scale = [    1.0000]
25-01-23 09:44:45 | I |         sum  error  = [    0.0448]
25-01-23 09:44:45 | I |         best error  = [    0.0448]
25-01-23 09:44:45 | I |     + error = [0.0448]
25-01-23 09:44:46 | I |       - range scale = [    1.0000]
25-01-23 09:44:46 | I |         sum  error  = [    0.6856]
25-01-23 09:44:46 | I |         best error  = [    0.6856]
25-01-23 09:44:46 | I |     + error = [0.6856]
25-01-23 09:44:46 | I |   - Calibrating model.layers.0.self_attn.k_proj.weight
25-01-23 09:44:47 | I |       - range scale = [    1.0000]
25-01-23 09:44:47 | I |         sum  error  = [    0.0556]
25-01-23 09:44:47 | I |         best error  = [    0.0556]
25-01-23 09:44:47 | I |     + error = [0.0556]
25-01-23 09:44:47 | I |       - range scale = [    1.0000]
25-01-23 09:44:47 | I |         sum  error  = [    0.9400]
25-01-23 09:44:47 | I |         best error  = [    0.9400]
25-01-23 09:44:47 | I |     + error = [0.9400]
25-01-23 09:44:48 | I |   - Calibrating model.layers.0.self_attn.v_proj.weight
25-01-23 09:44:48 | I |       - range scale = [    1.0000]
25-01-23 09:44:48 | I |         sum  error  = [    0.6935]
25-01-23 09:44:48 | I |         best error  = [    0.6935]
25-01-23 09:44:48 | I |     + error = [0.6935]
25-01-23 09:44:49 | I |       - range scale = [    1.0000]
25-01-23 09:44:49 | I |         sum  error  = [    6.4581]
25-01-23 09:44:49 | I |         best error  = [    6.4581]
25-01-23 09:44:49 | I |     + error = [6.4581]
25-01-23 09:44:49 | I |   - Calibrating model.layers.0.self_attn.o_proj.weight
25-01-23 09:44:50 | I |       - range scale = [    1.0000]
25-01-23 09:44:50 | I |         sum  error  = [    0.2010]
25-01-23 09:44:50 | I |         best error  = [    0.2010]
25-01-23 09:44:50 | I |     + error = [0.2010]
25-01-23 09:44:51 | I |       - range scale = [    1.0000]
25-01-23 09:44:51 | I |         sum  error  = [    1.6023]
25-01-23 09:44:51 | I |         best error  = [    1.6023]
25-01-23 09:44:51 | I |     + error = [1.6023]
25-01-23 09:44:51 | I |   - Calibrating model.layers.0.mlp.up_proj.weight
25-01-23 09:44:52 | I |       - range scale = [    1.0000]
25-01-23 09:44:52 | I |         sum  error  = [    1.0638]
25-01-23 09:44:52 | I |         best error  = [    1.0638]
25-01-23 09:44:52 | I |     + error = [1.0638]
25-01-23 09:44:53 | I |       - range scale = [    1.0000]
25-01-23 09:44:53 | I |         sum  error  = [   11.3296]
25-01-23 09:44:53 | I |         best error  = [   11.3296]
25-01-23 09:44:53 | I |     + error = [11.3296]
25-01-23 09:44:53 | I |   - Calibrating model.layers.0.mlp.gate_proj.weight
25-01-23 09:44:54 | I |       - range scale = [    1.0000]
25-01-23 09:44:54 | I |         sum  error  = [    1.1316]
25-01-23 09:44:54 | I |         best error  = [    1.1316]
25-01-23 09:44:54 | I |     + error = [1.1316]
25-01-23 09:44:55 | I |       - range scale = [    1.0000]
25-01-23 09:44:55 | I |         sum  error  = [   11.7086]
25-01-23 09:44:55 | I |         best error  = [   11.7086]
25-01-23 09:44:55 | I |     + error = [11.7086]
25-01-23 09:44:55 | I |   - Calibrating model.layers.0.mlp.down_proj.weight
25-01-23 09:44:56 | I |       - range scale = [    1.0000]
25-01-23 09:44:56 | I |         sum  error  = [    0.1652]
25-01-23 09:44:56 | I |         best error  = [    0.1652]
25-01-23 09:44:56 | I |     + error = [0.1652]
25-01-23 09:44:57 | I |       - range scale = [    1.0000]
25-01-23 09:44:57 | I |         sum  error  = [    1.6113]
25-01-23 09:44:57 | I |         best error  = [    1.6113]
25-01-23 09:44:57 | I |     + error = [1.6113]
25-01-23 09:44:57 | I |   - Quantizing model.layers.0.self_attn.q_proj.weight
25-01-23 09:45:00 | I |   - Quantizing model.layers.0.self_attn.k_proj.weight
25-01-23 09:45:02 | I |   - Quantizing model.layers.0.self_attn.v_proj.weight
25-01-23 09:45:04 | I |   - Quantizing model.layers.0.self_attn.o_proj.weight
25-01-23 09:45:07 | I |   - Quantizing model.layers.0.mlp.up_proj.weight
25-01-23 09:45:09 | I |   - Quantizing model.layers.0.mlp.gate_proj.weight
25-01-23 09:45:11 | I |   - Quantizing model.layers.0.mlp.down_proj.weight
25-01-23 09:45:17 | I | quantizing activations for layer model.layers.0
25-01-23 09:45:18 | I | collecting calibration activations in model.layers.0
25-01-23 09:45:18 | I | collecting calibration activations in model.layers.0
25-01-23 09:45:20 | I | forward this layer
25-01-23 09:45:20 | I | input_file: /data/gyy/lmquant-main/lmquant/data/data_without_preprocess_llama_7b_bs16/shard/val_input_args_layer_0/94.pt
25-01-23 09:45:20 | I | output_file: /data/gyy/lmquant-main/lmquant/data/data_without_preprocess_llama_7b_bs16/shard/val_teacher_output_layer_0/94.pt
25-01-23 09:45:20 | I | In layer model.layers.0, gradient of model.layers.0.self_attn.q_proj has nan: False
25-01-23 09:45:20 | I | In layer model.layers.0, gradient of model.layers.0.self_attn.q_proj has inf: False
25-01-23 09:45:20 | I | In layer model.layers.0, gradient of model.layers.0.self_attn.k_proj has nan: False
25-01-23 09:45:20 | I | In layer model.layers.0, gradient of model.layers.0.self_attn.k_proj has inf: False
25-01-23 09:45:20 | I | In layer model.layers.0, gradient of model.layers.0.self_attn.v_proj has nan: False
25-01-23 09:45:20 | I | In layer model.layers.0, gradient of model.layers.0.self_attn.v_proj has inf: False
25-01-23 09:45:20 | I | In layer model.layers.0, gradient of model.layers.0.self_attn.o_proj has nan: False
25-01-23 09:45:20 | I | In layer model.layers.0, gradient of model.layers.0.self_attn.o_proj has inf: False
25-01-23 09:45:20 | I | In layer model.layers.0, gradient of model.layers.0.mlp.up_proj has nan: False
25-01-23 09:45:20 | I | In layer model.layers.0, gradient of model.layers.0.mlp.up_proj has inf: False
25-01-23 09:45:20 | I | In layer model.layers.0, gradient of model.layers.0.mlp.gate_proj has nan: False
25-01-23 09:45:20 | I | In layer model.layers.0, gradient of model.layers.0.mlp.gate_proj has inf: False
25-01-23 09:45:20 | I | In layer model.layers.0, gradient of model.layers.0.mlp.down_proj has nan: False
25-01-23 09:45:20 | I | In layer model.layers.0, gradient of model.layers.0.mlp.down_proj has inf: False
25-01-23 09:45:20 | I | [90] done with optimizer step
25-01-23 09:45:20 | I | epoch 001:     95 / 819200000 loss=1.10125e-05, loss_per_token=0.045107, loss_sum=369.517, wps=14.1, ups=0, wpb=8192, bsz=16, num_updates=91, lr=0.00499932, gnorm=0.097, clip=0, loss_scale=8, train_wall=37, cuda_gb_allocated=17.2, cuda_gb_reserved=20.9, cuda_gb_free=6.5, wall=8468
25-01-23 09:45:20 | I | TRAIN CURRENT LAYER_IDX = 0
25-01-23 09:45:20 | I | in layer model.layers.0
25-01-23 09:45:20 | I | quantizing weights for layer model.layers.0
25-01-23 09:45:21 | I | collecting calibration activations in model.layers.0
25-01-23 09:45:21 | I | collecting calibration activations in model.layers.0
25-01-23 09:45:21 | I | - Quantizing decoder layer model.layers.0
25-01-23 09:45:21 | I |   - Calibrating model.layers.0.self_attn.q_proj.weight
25-01-23 09:45:22 | I |       - range scale = [    1.0000]
25-01-23 09:45:22 | I |         sum  error  = [    0.0434]
25-01-23 09:45:22 | I |         best error  = [    0.0434]
25-01-23 09:45:22 | I |     + error = [0.0434]
25-01-23 09:45:22 | I |       - range scale = [    1.0000]
25-01-23 09:45:22 | I |         sum  error  = [    0.6830]
25-01-23 09:45:22 | I |         best error  = [    0.6830]
25-01-23 09:45:22 | I |     + error = [0.6830]
25-01-23 09:45:23 | I |   - Calibrating model.layers.0.self_attn.k_proj.weight
25-01-23 09:45:23 | I |       - range scale = [    1.0000]
25-01-23 09:45:23 | I |         sum  error  = [    0.0535]
25-01-23 09:45:23 | I |         best error  = [    0.0535]
25-01-23 09:45:23 | I |     + error = [0.0535]
25-01-23 09:45:24 | I |       - range scale = [    1.0000]
25-01-23 09:45:24 | I |         sum  error  = [    0.8753]
25-01-23 09:45:24 | I |         best error  = [    0.8753]
25-01-23 09:45:24 | I |     + error = [0.8753]
25-01-23 09:45:24 | I |   - Calibrating model.layers.0.self_attn.v_proj.weight
25-01-23 09:45:25 | I |       - range scale = [    1.0000]
25-01-23 09:45:25 | I |         sum  error  = [    0.6707]
25-01-23 09:45:25 | I |         best error  = [    0.6707]
25-01-23 09:45:25 | I |     + error = [0.6707]
25-01-23 09:45:26 | I |       - range scale = [    1.0000]
25-01-23 09:45:26 | I |         sum  error  = [    6.2437]
25-01-23 09:45:26 | I |         best error  = [    6.2437]
25-01-23 09:45:26 | I |     + error = [6.2437]
25-01-23 09:45:26 | I |   - Calibrating model.layers.0.self_attn.o_proj.weight
25-01-23 09:45:27 | I |       - range scale = [    1.0000]
25-01-23 09:45:27 | I |         sum  error  = [    0.1997]
25-01-23 09:45:27 | I |         best error  = [    0.1997]
25-01-23 09:45:27 | I |     + error = [0.1997]
25-01-23 09:45:27 | I |       - range scale = [    1.0000]
25-01-23 09:45:27 | I |         sum  error  = [    1.5965]
25-01-23 09:45:27 | I |         best error  = [    1.5965]
25-01-23 09:45:27 | I |     + error = [1.5965]
25-01-23 09:45:28 | I |   - Calibrating model.layers.0.mlp.up_proj.weight
25-01-23 09:45:28 | I |       - range scale = [    1.0000]
25-01-23 09:45:28 | I |         sum  error  = [    1.1206]
25-01-23 09:45:28 | I |         best error  = [    1.1206]
25-01-23 09:45:28 | I |     + error = [1.1206]
25-01-23 09:45:30 | I |       - range scale = [    1.0000]
25-01-23 09:45:30 | I |         sum  error  = [   11.9132]
25-01-23 09:45:30 | I |         best error  = [   11.9132]
25-01-23 09:45:30 | I |     + error = [11.9132]
25-01-23 09:45:30 | I |   - Calibrating model.layers.0.mlp.gate_proj.weight
25-01-23 09:45:30 | I |       - range scale = [    1.0000]
25-01-23 09:45:30 | I |         sum  error  = [    1.1825]
25-01-23 09:45:30 | I |         best error  = [    1.1825]
25-01-23 09:45:30 | I |     + error = [1.1825]
25-01-23 09:45:32 | I |       - range scale = [    1.0000]
25-01-23 09:45:32 | I |         sum  error  = [   12.3154]
25-01-23 09:45:32 | I |         best error  = [   12.3154]
25-01-23 09:45:32 | I |     + error = [12.3154]
25-01-23 09:45:32 | I |   - Calibrating model.layers.0.mlp.down_proj.weight
25-01-23 09:45:33 | I |       - range scale = [    1.0000]
25-01-23 09:45:33 | I |         sum  error  = [    0.1739]
25-01-23 09:45:33 | I |         best error  = [    0.1739]
25-01-23 09:45:33 | I |     + error = [0.1739]
25-01-23 09:45:34 | I |       - range scale = [    1.0000]
25-01-23 09:45:34 | I |         sum  error  = [    1.6942]
25-01-23 09:45:34 | I |         best error  = [    1.6942]
25-01-23 09:45:34 | I |     + error = [1.6942]
25-01-23 09:45:34 | I |   - Quantizing model.layers.0.self_attn.q_proj.weight
25-01-23 09:45:36 | I |   - Quantizing model.layers.0.self_attn.k_proj.weight
25-01-23 09:45:39 | I |   - Quantizing model.layers.0.self_attn.v_proj.weight
25-01-23 09:45:41 | I |   - Quantizing model.layers.0.self_attn.o_proj.weight
25-01-23 09:45:43 | I |   - Quantizing model.layers.0.mlp.up_proj.weight
25-01-23 09:45:45 | I |   - Quantizing model.layers.0.mlp.gate_proj.weight
25-01-23 09:45:48 | I |   - Quantizing model.layers.0.mlp.down_proj.weight
25-01-23 09:45:54 | I | quantizing activations for layer model.layers.0
25-01-23 09:45:54 | I | collecting calibration activations in model.layers.0
25-01-23 09:45:54 | I | collecting calibration activations in model.layers.0
25-01-23 09:45:56 | I | forward this layer
25-01-23 09:45:56 | I | input_file: /data/gyy/lmquant-main/lmquant/data/data_without_preprocess_llama_7b_bs16/shard/val_input_args_layer_0/95.pt
25-01-23 09:45:56 | I | output_file: /data/gyy/lmquant-main/lmquant/data/data_without_preprocess_llama_7b_bs16/shard/val_teacher_output_layer_0/95.pt
25-01-23 09:45:56 | I | In layer model.layers.0, gradient of model.layers.0.self_attn.q_proj has nan: False
25-01-23 09:45:56 | I | In layer model.layers.0, gradient of model.layers.0.self_attn.q_proj has inf: False
25-01-23 09:45:56 | I | In layer model.layers.0, gradient of model.layers.0.self_attn.k_proj has nan: False
25-01-23 09:45:56 | I | In layer model.layers.0, gradient of model.layers.0.self_attn.k_proj has inf: False
25-01-23 09:45:56 | I | In layer model.layers.0, gradient of model.layers.0.self_attn.v_proj has nan: False
25-01-23 09:45:56 | I | In layer model.layers.0, gradient of model.layers.0.self_attn.v_proj has inf: False
25-01-23 09:45:56 | I | In layer model.layers.0, gradient of model.layers.0.self_attn.o_proj has nan: False
25-01-23 09:45:56 | I | In layer model.layers.0, gradient of model.layers.0.self_attn.o_proj has inf: False
25-01-23 09:45:56 | I | In layer model.layers.0, gradient of model.layers.0.mlp.up_proj has nan: False
25-01-23 09:45:56 | I | In layer model.layers.0, gradient of model.layers.0.mlp.up_proj has inf: False
25-01-23 09:45:56 | I | In layer model.layers.0, gradient of model.layers.0.mlp.gate_proj has nan: False
25-01-23 09:45:56 | I | In layer model.layers.0, gradient of model.layers.0.mlp.gate_proj has inf: False
25-01-23 09:45:56 | I | In layer model.layers.0, gradient of model.layers.0.mlp.down_proj has nan: False
25-01-23 09:45:56 | I | In layer model.layers.0, gradient of model.layers.0.mlp.down_proj has inf: False
25-01-23 09:45:57 | I | [91] done with optimizer step
25-01-23 09:45:57 | I | epoch 001:     96 / 819200000 loss=1.21377e-05, loss_per_token=0.0497162, loss_sum=407.275, wps=224.6, ups=0.03, wpb=8192, bsz=16, num_updates=92, lr=0.0049993, gnorm=0.163, clip=0, loss_scale=8, train_wall=36, cuda_gb_allocated=17.1, cuda_gb_reserved=18.3, cuda_gb_free=6.6, wall=8504
25-01-23 09:45:57 | I | TRAIN CURRENT LAYER_IDX = 0
25-01-23 09:45:57 | I | in layer model.layers.0
25-01-23 09:45:57 | I | quantizing weights for layer model.layers.0
25-01-23 09:45:57 | I | collecting calibration activations in model.layers.0
25-01-23 09:45:57 | I | collecting calibration activations in model.layers.0
25-01-23 09:45:57 | I | - Quantizing decoder layer model.layers.0
25-01-23 09:45:57 | I |   - Calibrating model.layers.0.self_attn.q_proj.weight
25-01-23 09:45:58 | I |       - range scale = [    1.0000]
25-01-23 09:45:58 | I |         sum  error  = [    0.0432]
25-01-23 09:45:58 | I |         best error  = [    0.0432]
25-01-23 09:45:58 | I |     + error = [0.0432]
25-01-23 09:45:59 | I |       - range scale = [    1.0000]
25-01-23 09:45:59 | I |         sum  error  = [    0.6463]
25-01-23 09:45:59 | I |         best error  = [    0.6463]
25-01-23 09:45:59 | I |     + error = [0.6463]
25-01-23 09:45:59 | I |   - Calibrating model.layers.0.self_attn.k_proj.weight
25-01-23 09:46:00 | I |       - range scale = [    1.0000]
25-01-23 09:46:00 | I |         sum  error  = [    0.0536]
25-01-23 09:46:00 | I |         best error  = [    0.0536]
25-01-23 09:46:00 | I |     + error = [0.0536]
25-01-23 09:46:00 | I |       - range scale = [    1.0000]
25-01-23 09:46:00 | I |         sum  error  = [    0.8250]
25-01-23 09:46:00 | I |         best error  = [    0.8250]
25-01-23 09:46:00 | I |     + error = [0.8250]
25-01-23 09:46:01 | I |   - Calibrating model.layers.0.self_attn.v_proj.weight
25-01-23 09:46:01 | I |       - range scale = [    1.0000]
25-01-23 09:46:01 | I |         sum  error  = [    0.6869]
25-01-23 09:46:01 | I |         best error  = [    0.6869]
25-01-23 09:46:01 | I |     + error = [0.6869]
25-01-23 09:46:02 | I |       - range scale = [    1.0000]
25-01-23 09:46:02 | I |         sum  error  = [    6.3799]
25-01-23 09:46:02 | I |         best error  = [    6.3799]
25-01-23 09:46:02 | I |     + error = [6.3799]
25-01-23 09:46:02 | I |   - Calibrating model.layers.0.self_attn.o_proj.weight
25-01-23 09:46:03 | I |       - range scale = [    1.0000]
25-01-23 09:46:03 | I |         sum  error  = [    0.1983]
25-01-23 09:46:03 | I |         best error  = [    0.1983]
25-01-23 09:46:03 | I |     + error = [0.1983]
25-01-23 09:46:04 | I |       - range scale = [    1.0000]
25-01-23 09:46:04 | I |         sum  error  = [    1.5942]
25-01-23 09:46:04 | I |         best error  = [    1.5942]
25-01-23 09:46:04 | I |     + error = [1.5942]
25-01-23 09:46:04 | I |   - Calibrating model.layers.0.mlp.up_proj.weight
25-01-23 09:46:05 | I |       - range scale = [    1.0000]
25-01-23 09:46:05 | I |         sum  error  = [    1.0744]
25-01-23 09:46:05 | I |         best error  = [    1.0744]
25-01-23 09:46:05 | I |     + error = [1.0744]
25-01-23 09:46:06 | I |       - range scale = [    1.0000]
25-01-23 09:46:06 | I |         sum  error  = [   11.4258]
25-01-23 09:46:06 | I |         best error  = [   11.4258]
25-01-23 09:46:06 | I |     + error = [11.4258]
25-01-23 09:46:06 | I |   - Calibrating model.layers.0.mlp.gate_proj.weight
25-01-23 09:46:07 | I |       - range scale = [    1.0000]
25-01-23 09:46:07 | I |         sum  error  = [    1.1364]
25-01-23 09:46:07 | I |         best error  = [    1.1364]
25-01-23 09:46:07 | I |     + error = [1.1364]
25-01-23 09:46:08 | I |       - range scale = [    1.0000]
25-01-23 09:46:08 | I |         sum  error  = [   11.8163]
25-01-23 09:46:08 | I |         best error  = [   11.8163]
25-01-23 09:46:08 | I |     + error = [11.8163]
25-01-23 09:46:08 | I |   - Calibrating model.layers.0.mlp.down_proj.weight
25-01-23 09:46:09 | I |       - range scale = [    1.0000]
25-01-23 09:46:09 | I |         sum  error  = [    0.1647]
25-01-23 09:46:09 | I |         best error  = [    0.1647]
25-01-23 09:46:09 | I |     + error = [0.1647]
25-01-23 09:46:10 | I |       - range scale = [    1.0000]
25-01-23 09:46:10 | I |         sum  error  = [    1.6071]
25-01-23 09:46:10 | I |         best error  = [    1.6071]
25-01-23 09:46:10 | I |     + error = [1.6071]
25-01-23 09:46:10 | I |   - Quantizing model.layers.0.self_attn.q_proj.weight
25-01-23 09:46:13 | I |   - Quantizing model.layers.0.self_attn.k_proj.weight
25-01-23 09:46:15 | I |   - Quantizing model.layers.0.self_attn.v_proj.weight
25-01-23 09:46:17 | I |   - Quantizing model.layers.0.self_attn.o_proj.weight
25-01-23 09:46:20 | I |   - Quantizing model.layers.0.mlp.up_proj.weight
25-01-23 09:46:22 | I |   - Quantizing model.layers.0.mlp.gate_proj.weight
25-01-23 09:46:24 | I |   - Quantizing model.layers.0.mlp.down_proj.weight
25-01-23 09:46:30 | I | quantizing activations for layer model.layers.0
25-01-23 09:46:31 | I | collecting calibration activations in model.layers.0
25-01-23 09:46:31 | I | collecting calibration activations in model.layers.0
25-01-23 09:46:33 | I | forward this layer
25-01-23 09:46:33 | I | input_file: /data/gyy/lmquant-main/lmquant/data/data_without_preprocess_llama_7b_bs16/shard/val_input_args_layer_0/96.pt
25-01-23 09:46:33 | I | output_file: /data/gyy/lmquant-main/lmquant/data/data_without_preprocess_llama_7b_bs16/shard/val_teacher_output_layer_0/96.pt
25-01-23 09:46:33 | I | In layer model.layers.0, gradient of model.layers.0.self_attn.q_proj has nan: False
25-01-23 09:46:33 | I | In layer model.layers.0, gradient of model.layers.0.self_attn.q_proj has inf: False
25-01-23 09:46:33 | I | In layer model.layers.0, gradient of model.layers.0.self_attn.k_proj has nan: False
25-01-23 09:46:33 | I | In layer model.layers.0, gradient of model.layers.0.self_attn.k_proj has inf: False
25-01-23 09:46:33 | I | In layer model.layers.0, gradient of model.layers.0.self_attn.v_proj has nan: False
25-01-23 09:46:33 | I | In layer model.layers.0, gradient of model.layers.0.self_attn.v_proj has inf: False
25-01-23 09:46:33 | I | In layer model.layers.0, gradient of model.layers.0.self_attn.o_proj has nan: False
25-01-23 09:46:33 | I | In layer model.layers.0, gradient of model.layers.0.self_attn.o_proj has inf: False
25-01-23 09:46:33 | I | In layer model.layers.0, gradient of model.layers.0.mlp.up_proj has nan: False
25-01-23 09:46:33 | I | In layer model.layers.0, gradient of model.layers.0.mlp.up_proj has inf: False
25-01-23 09:46:33 | I | In layer model.layers.0, gradient of model.layers.0.mlp.gate_proj has nan: False
25-01-23 09:46:33 | I | In layer model.layers.0, gradient of model.layers.0.mlp.gate_proj has inf: False
25-01-23 09:46:33 | I | In layer model.layers.0, gradient of model.layers.0.mlp.down_proj has nan: False
25-01-23 09:46:33 | I | In layer model.layers.0, gradient of model.layers.0.mlp.down_proj has inf: False
25-01-23 09:46:33 | I | [92] done with optimizer step
25-01-23 09:46:33 | I | epoch 001:     97 / 819200000 loss=1.14314e-05, loss_per_token=0.046823, loss_sum=383.574, wps=223.2, ups=0.03, wpb=8192, bsz=16, num_updates=93, lr=0.00499928, gnorm=0.141, clip=0, loss_scale=8, train_wall=37, cuda_gb_allocated=17.1, cuda_gb_reserved=18.3, cuda_gb_free=6.6, wall=8541
25-01-23 09:46:33 | I | TRAIN CURRENT LAYER_IDX = 0
25-01-23 09:46:33 | I | in layer model.layers.0
25-01-23 09:46:33 | I | quantizing weights for layer model.layers.0
25-01-23 09:46:34 | I | collecting calibration activations in model.layers.0
25-01-23 09:46:34 | I | collecting calibration activations in model.layers.0
25-01-23 09:46:34 | I | - Quantizing decoder layer model.layers.0
25-01-23 09:46:34 | I |   - Calibrating model.layers.0.self_attn.q_proj.weight
25-01-23 09:46:35 | I |       - range scale = [    1.0000]
25-01-23 09:46:35 | I |         sum  error  = [    0.0463]
25-01-23 09:46:35 | I |         best error  = [    0.0463]
25-01-23 09:46:35 | I |     + error = [0.0463]
25-01-23 09:46:36 | I |       - range scale = [    1.0000]
25-01-23 09:46:36 | I |         sum  error  = [    0.7125]
25-01-23 09:46:36 | I |         best error  = [    0.7125]
25-01-23 09:46:36 | I |     + error = [0.7125]
25-01-23 09:46:36 | I |   - Calibrating model.layers.0.self_attn.k_proj.weight
25-01-23 09:46:37 | I |       - range scale = [    1.0000]
25-01-23 09:46:37 | I |         sum  error  = [    0.0590]
25-01-23 09:46:37 | I |         best error  = [    0.0590]
25-01-23 09:46:37 | I |     + error = [0.0590]
25-01-23 09:46:37 | I |       - range scale = [    1.0000]
25-01-23 09:46:37 | I |         sum  error  = [    0.9280]
25-01-23 09:46:37 | I |         best error  = [    0.9280]
25-01-23 09:46:37 | I |     + error = [0.9280]
25-01-23 09:46:37 | I |   - Calibrating model.layers.0.self_attn.v_proj.weight
25-01-23 09:46:38 | I |       - range scale = [    1.0000]
25-01-23 09:46:38 | I |         sum  error  = [    0.6912]
25-01-23 09:46:38 | I |         best error  = [    0.6912]
25-01-23 09:46:38 | I |     + error = [0.6912]
25-01-23 09:46:39 | I |       - range scale = [    1.0000]
25-01-23 09:46:39 | I |         sum  error  = [    6.4934]
25-01-23 09:46:39 | I |         best error  = [    6.4934]
25-01-23 09:46:39 | I |     + error = [6.4934]
25-01-23 09:46:39 | I |   - Calibrating model.layers.0.self_attn.o_proj.weight
25-01-23 09:46:40 | I |       - range scale = [    1.0000]
25-01-23 09:46:40 | I |         sum  error  = [    0.2057]
25-01-23 09:46:40 | I |         best error  = [    0.2057]
25-01-23 09:46:40 | I |     + error = [0.2057]
25-01-23 09:46:41 | I |       - range scale = [    1.0000]
25-01-23 09:46:41 | I |         sum  error  = [    1.6518]
25-01-23 09:46:41 | I |         best error  = [    1.6518]
25-01-23 09:46:41 | I |     + error = [1.6518]
25-01-23 09:46:41 | I |   - Calibrating model.layers.0.mlp.up_proj.weight
25-01-23 09:46:42 | I |       - range scale = [    1.0000]
25-01-23 09:46:42 | I |         sum  error  = [    1.1181]
25-01-23 09:46:42 | I |         best error  = [    1.1181]
25-01-23 09:46:42 | I |     + error = [1.1181]
25-01-23 09:46:43 | I |       - range scale = [    1.0000]
25-01-23 09:46:43 | I |         sum  error  = [   11.9003]
25-01-23 09:46:43 | I |         best error  = [   11.9003]
25-01-23 09:46:43 | I |     + error = [11.9003]
25-01-23 09:46:43 | I |   - Calibrating model.layers.0.mlp.gate_proj.weight
25-01-23 09:46:44 | I |       - range scale = [    1.0000]
25-01-23 09:46:44 | I |         sum  error  = [    1.1818]
25-01-23 09:46:44 | I |         best error  = [    1.1818]
25-01-23 09:46:44 | I |     + error = [1.1818]
25-01-23 09:46:45 | I |       - range scale = [    1.0000]
25-01-23 09:46:45 | I |         sum  error  = [   12.2996]
25-01-23 09:46:45 | I |         best error  = [   12.2996]
25-01-23 09:46:45 | I |     + error = [12.2996]
25-01-23 09:46:45 | I |   - Calibrating model.layers.0.mlp.down_proj.weight
25-01-23 09:46:46 | I |       - range scale = [    1.0000]
25-01-23 09:46:46 | I |         sum  error  = [    0.1725]
25-01-23 09:46:46 | I |         best error  = [    0.1725]
25-01-23 09:46:46 | I |     + error = [0.1725]
25-01-23 09:46:47 | I |       - range scale = [    1.0000]
25-01-23 09:46:47 | I |         sum  error  = [    1.6797]
25-01-23 09:46:47 | I |         best error  = [    1.6797]
25-01-23 09:46:47 | I |     + error = [1.6797]
25-01-23 09:46:47 | I |   - Quantizing model.layers.0.self_attn.q_proj.weight
25-01-23 09:46:50 | I |   - Quantizing model.layers.0.self_attn.k_proj.weight
25-01-23 09:46:52 | I |   - Quantizing model.layers.0.self_attn.v_proj.weight
25-01-23 09:46:54 | I |   - Quantizing model.layers.0.self_attn.o_proj.weight
25-01-23 09:46:57 | I |   - Quantizing model.layers.0.mlp.up_proj.weight
25-01-23 09:46:59 | I |   - Quantizing model.layers.0.mlp.gate_proj.weight
25-01-23 09:47:01 | I |   - Quantizing model.layers.0.mlp.down_proj.weight
25-01-23 09:47:07 | I | quantizing activations for layer model.layers.0
25-01-23 09:47:07 | I | collecting calibration activations in model.layers.0
25-01-23 09:47:08 | I | collecting calibration activations in model.layers.0
25-01-23 09:47:09 | I | forward this layer
25-01-23 09:47:09 | I | input_file: /data/gyy/lmquant-main/lmquant/data/data_without_preprocess_llama_7b_bs16/shard/val_input_args_layer_0/97.pt
25-01-23 09:47:09 | I | output_file: /data/gyy/lmquant-main/lmquant/data/data_without_preprocess_llama_7b_bs16/shard/val_teacher_output_layer_0/97.pt
25-01-23 09:47:10 | I | In layer model.layers.0, gradient of model.layers.0.self_attn.q_proj has nan: False
25-01-23 09:47:10 | I | In layer model.layers.0, gradient of model.layers.0.self_attn.q_proj has inf: False
25-01-23 09:47:10 | I | In layer model.layers.0, gradient of model.layers.0.self_attn.k_proj has nan: False
25-01-23 09:47:10 | I | In layer model.layers.0, gradient of model.layers.0.self_attn.k_proj has inf: False
25-01-23 09:47:10 | I | In layer model.layers.0, gradient of model.layers.0.self_attn.v_proj has nan: False
25-01-23 09:47:10 | I | In layer model.layers.0, gradient of model.layers.0.self_attn.v_proj has inf: False
25-01-23 09:47:10 | I | In layer model.layers.0, gradient of model.layers.0.self_attn.o_proj has nan: False
25-01-23 09:47:10 | I | In layer model.layers.0, gradient of model.layers.0.self_attn.o_proj has inf: False
25-01-23 09:47:10 | I | In layer model.layers.0, gradient of model.layers.0.mlp.up_proj has nan: False
25-01-23 09:47:10 | I | In layer model.layers.0, gradient of model.layers.0.mlp.up_proj has inf: False
25-01-23 09:47:10 | I | In layer model.layers.0, gradient of model.layers.0.mlp.gate_proj has nan: False
25-01-23 09:47:10 | I | In layer model.layers.0, gradient of model.layers.0.mlp.gate_proj has inf: False
25-01-23 09:47:10 | I | In layer model.layers.0, gradient of model.layers.0.mlp.down_proj has nan: False
25-01-23 09:47:10 | I | In layer model.layers.0, gradient of model.layers.0.mlp.down_proj has inf: False
25-01-23 09:47:10 | I | [93] done with optimizer step
25-01-23 09:47:10 | I | epoch 001:     98 / 819200000 loss=1.32925e-05, loss_per_token=0.0544461, loss_sum=446.023, wps=223.8, ups=0.03, wpb=8192, bsz=16, num_updates=94, lr=0.00499927, gnorm=0.258, clip=0, loss_scale=8, train_wall=37, cuda_gb_allocated=17.1, cuda_gb_reserved=18.7, cuda_gb_free=6.6, wall=8577
25-01-23 09:47:10 | I | TRAIN CURRENT LAYER_IDX = 0
25-01-23 09:47:10 | I | in layer model.layers.0
25-01-23 09:47:10 | I | quantizing weights for layer model.layers.0
25-01-23 09:47:10 | I | collecting calibration activations in model.layers.0
25-01-23 09:47:10 | I | collecting calibration activations in model.layers.0
25-01-23 09:47:11 | I | - Quantizing decoder layer model.layers.0
25-01-23 09:47:11 | I |   - Calibrating model.layers.0.self_attn.q_proj.weight
25-01-23 09:47:11 | I |       - range scale = [    1.0000]
25-01-23 09:47:11 | I |         sum  error  = [    0.0456]
25-01-23 09:47:11 | I |         best error  = [    0.0456]
25-01-23 09:47:11 | I |     + error = [0.0456]
25-01-23 09:47:12 | I |       - range scale = [    1.0000]
25-01-23 09:47:12 | I |         sum  error  = [    0.7294]
25-01-23 09:47:12 | I |         best error  = [    0.7294]
25-01-23 09:47:12 | I |     + error = [0.7294]
25-01-23 09:47:12 | I |   - Calibrating model.layers.0.self_attn.k_proj.weight
25-01-23 09:47:13 | I |       - range scale = [    1.0000]
25-01-23 09:47:13 | I |         sum  error  = [    0.0572]
25-01-23 09:47:13 | I |         best error  = [    0.0572]
25-01-23 09:47:13 | I |     + error = [0.0572]
25-01-23 09:47:14 | I |       - range scale = [    1.0000]
25-01-23 09:47:14 | I |         sum  error  = [    0.9367]
25-01-23 09:47:14 | I |         best error  = [    0.9367]
25-01-23 09:47:14 | I |     + error = [0.9367]
25-01-23 09:47:14 | I |   - Calibrating model.layers.0.self_attn.v_proj.weight
25-01-23 09:47:15 | I |       - range scale = [    1.0000]
25-01-23 09:47:15 | I |         sum  error  = [    0.6703]
25-01-23 09:47:15 | I |         best error  = [    0.6703]
25-01-23 09:47:15 | I |     + error = [0.6703]
25-01-23 09:47:15 | I |       - range scale = [    1.0000]
25-01-23 09:47:15 | I |         sum  error  = [    6.2645]
25-01-23 09:47:15 | I |         best error  = [    6.2645]
25-01-23 09:47:15 | I |     + error = [6.2645]
25-01-23 09:47:16 | I |   - Calibrating model.layers.0.self_attn.o_proj.weight
25-01-23 09:47:16 | I |       - range scale = [    1.0000]
25-01-23 09:47:16 | I |         sum  error  = [    0.2032]
25-01-23 09:47:16 | I |         best error  = [    0.2032]
25-01-23 09:47:16 | I |     + error = [0.2032]
25-01-23 09:47:17 | I |       - range scale = [    1.0000]
25-01-23 09:47:17 | I |         sum  error  = [    1.6221]
25-01-23 09:47:17 | I |         best error  = [    1.6221]
25-01-23 09:47:17 | I |     + error = [1.6221]
25-01-23 09:47:17 | I |   - Calibrating model.layers.0.mlp.up_proj.weight
25-01-23 09:47:18 | I |       - range scale = [    1.0000]
25-01-23 09:47:18 | I |         sum  error  = [    1.0987]
25-01-23 09:47:18 | I |         best error  = [    1.0987]
25-01-23 09:47:18 | I |     + error = [1.0987]
25-01-23 09:47:19 | I |       - range scale = [    1.0000]
25-01-23 09:47:19 | I |         sum  error  = [   11.6764]
25-01-23 09:47:19 | I |         best error  = [   11.6764]
25-01-23 09:47:19 | I |     + error = [11.6764]
25-01-23 09:47:20 | I |   - Calibrating model.layers.0.mlp.gate_proj.weight
25-01-23 09:47:20 | I |       - range scale = [    1.0000]
25-01-23 09:47:20 | I |         sum  error  = [    1.1594]
25-01-23 09:47:20 | I |         best error  = [    1.1594]
25-01-23 09:47:20 | I |     + error = [1.1594]
25-01-23 09:47:21 | I |       - range scale = [    1.0000]
25-01-23 09:47:21 | I |         sum  error  = [   12.0665]
25-01-23 09:47:21 | I |         best error  = [   12.0665]
25-01-23 09:47:21 | I |     + error = [12.0665]
25-01-23 09:47:22 | I |   - Calibrating model.layers.0.mlp.down_proj.weight
25-01-23 09:47:22 | I |       - range scale = [    1.0000]
25-01-23 09:47:22 | I |         sum  error  = [    0.1683]
25-01-23 09:47:22 | I |         best error  = [    0.1683]
25-01-23 09:47:22 | I |     + error = [0.1683]
25-01-23 09:47:24 | I |       - range scale = [    1.0000]
25-01-23 09:47:24 | I |         sum  error  = [    1.6403]
25-01-23 09:47:24 | I |         best error  = [    1.6403]
25-01-23 09:47:24 | I |     + error = [1.6403]
25-01-23 09:47:24 | I |   - Quantizing model.layers.0.self_attn.q_proj.weight
25-01-23 09:47:26 | I |   - Quantizing model.layers.0.self_attn.k_proj.weight
25-01-23 09:47:28 | I |   - Quantizing model.layers.0.self_attn.v_proj.weight
25-01-23 09:47:31 | I |   - Quantizing model.layers.0.self_attn.o_proj.weight
25-01-23 09:47:33 | I |   - Quantizing model.layers.0.mlp.up_proj.weight
25-01-23 09:47:35 | I |   - Quantizing model.layers.0.mlp.gate_proj.weight
25-01-23 09:47:38 | I |   - Quantizing model.layers.0.mlp.down_proj.weight
25-01-23 09:47:44 | I | quantizing activations for layer model.layers.0
25-01-23 09:47:44 | I | collecting calibration activations in model.layers.0
25-01-23 09:47:44 | I | collecting calibration activations in model.layers.0
25-01-23 09:47:46 | I | forward this layer
25-01-23 09:47:46 | I | input_file: /data/gyy/lmquant-main/lmquant/data/data_without_preprocess_llama_7b_bs16/shard/val_input_args_layer_0/98.pt
25-01-23 09:47:46 | I | output_file: /data/gyy/lmquant-main/lmquant/data/data_without_preprocess_llama_7b_bs16/shard/val_teacher_output_layer_0/98.pt
25-01-23 09:47:46 | I | In layer model.layers.0, gradient of model.layers.0.self_attn.q_proj has nan: False
25-01-23 09:47:46 | I | In layer model.layers.0, gradient of model.layers.0.self_attn.q_proj has inf: False
25-01-23 09:47:46 | I | In layer model.layers.0, gradient of model.layers.0.self_attn.k_proj has nan: False
25-01-23 09:47:46 | I | In layer model.layers.0, gradient of model.layers.0.self_attn.k_proj has inf: False
25-01-23 09:47:46 | I | In layer model.layers.0, gradient of model.layers.0.self_attn.v_proj has nan: False
25-01-23 09:47:46 | I | In layer model.layers.0, gradient of model.layers.0.self_attn.v_proj has inf: False
25-01-23 09:47:46 | I | In layer model.layers.0, gradient of model.layers.0.self_attn.o_proj has nan: False
25-01-23 09:47:46 | I | In layer model.layers.0, gradient of model.layers.0.self_attn.o_proj has inf: False
25-01-23 09:47:46 | I | In layer model.layers.0, gradient of model.layers.0.mlp.up_proj has nan: False
25-01-23 09:47:46 | I | In layer model.layers.0, gradient of model.layers.0.mlp.up_proj has inf: False
25-01-23 09:47:46 | I | In layer model.layers.0, gradient of model.layers.0.mlp.gate_proj has nan: False
25-01-23 09:47:46 | I | In layer model.layers.0, gradient of model.layers.0.mlp.gate_proj has inf: False
25-01-23 09:47:46 | I | In layer model.layers.0, gradient of model.layers.0.mlp.down_proj has nan: False
25-01-23 09:47:46 | I | In layer model.layers.0, gradient of model.layers.0.mlp.down_proj has inf: False
25-01-23 09:47:46 | I | [94] done with optimizer step
25-01-23 09:47:46 | I | epoch 001:     99 / 819200000 loss=1.21169e-05, loss_per_token=0.0496309, loss_sum=406.576, wps=224.4, ups=0.03, wpb=8192, bsz=16, num_updates=95, lr=0.00499925, gnorm=0.132, clip=0, loss_scale=8, train_wall=36, cuda_gb_allocated=17.1, cuda_gb_reserved=18.7, cuda_gb_free=6.6, wall=8614
25-01-23 09:47:46 | I | TRAIN CURRENT LAYER_IDX = 0
25-01-23 09:47:46 | I | in layer model.layers.0
25-01-23 09:47:46 | I | quantizing weights for layer model.layers.0
25-01-23 09:47:47 | I | collecting calibration activations in model.layers.0
25-01-23 09:47:47 | I | collecting calibration activations in model.layers.0
25-01-23 09:47:47 | I | - Quantizing decoder layer model.layers.0
25-01-23 09:47:47 | I |   - Calibrating model.layers.0.self_attn.q_proj.weight
25-01-23 09:47:48 | I |       - range scale = [    1.0000]
25-01-23 09:47:48 | I |         sum  error  = [    0.0476]
25-01-23 09:47:48 | I |         best error  = [    0.0476]
25-01-23 09:47:48 | I |     + error = [0.0476]
25-01-23 09:47:49 | I |       - range scale = [    1.0000]
25-01-23 09:47:49 | I |         sum  error  = [    0.7452]
25-01-23 09:47:49 | I |         best error  = [    0.7452]
25-01-23 09:47:49 | I |     + error = [0.7452]
25-01-23 09:47:49 | I |   - Calibrating model.layers.0.self_attn.k_proj.weight
25-01-23 09:47:50 | I |       - range scale = [    1.0000]
25-01-23 09:47:50 | I |         sum  error  = [    0.0621]
25-01-23 09:47:50 | I |         best error  = [    0.0621]
25-01-23 09:47:50 | I |     + error = [0.0621]
25-01-23 09:47:50 | I |       - range scale = [    1.0000]
25-01-23 09:47:50 | I |         sum  error  = [    0.9307]
25-01-23 09:47:50 | I |         best error  = [    0.9307]
25-01-23 09:47:50 | I |     + error = [0.9307]
25-01-23 09:47:51 | I |   - Calibrating model.layers.0.self_attn.v_proj.weight
25-01-23 09:47:51 | I |       - range scale = [    1.0000]
25-01-23 09:47:51 | I |         sum  error  = [    0.7081]
25-01-23 09:47:51 | I |         best error  = [    0.7081]
25-01-23 09:47:51 | I |     + error = [0.7081]
25-01-23 09:47:52 | I |       - range scale = [    1.0000]
25-01-23 09:47:52 | I |         sum  error  = [    6.5431]
25-01-23 09:47:52 | I |         best error  = [    6.5431]
25-01-23 09:47:52 | I |     + error = [6.5431]
25-01-23 09:47:52 | I |   - Calibrating model.layers.0.self_attn.o_proj.weight
25-01-23 09:47:53 | I |       - range scale = [    1.0000]
25-01-23 09:47:53 | I |         sum  error  = [    0.2064]
25-01-23 09:47:53 | I |         best error  = [    0.2064]
25-01-23 09:47:53 | I |     + error = [0.2064]
25-01-23 09:47:54 | I |       - range scale = [    1.0000]
25-01-23 09:47:54 | I |         sum  error  = [    1.6460]
25-01-23 09:47:54 | I |         best error  = [    1.6460]
25-01-23 09:47:54 | I |     + error = [1.6460]
25-01-23 09:47:54 | I |   - Calibrating model.layers.0.mlp.up_proj.weight
25-01-23 09:47:55 | I |       - range scale = [    1.0000]
25-01-23 09:47:55 | I |         sum  error  = [    1.0546]
25-01-23 09:47:55 | I |         best error  = [    1.0546]
25-01-23 09:47:55 | I |     + error = [1.0546]
25-01-23 09:47:56 | I |       - range scale = [    1.0000]
25-01-23 09:47:56 | I |         sum  error  = [   11.2176]
25-01-23 09:47:56 | I |         best error  = [   11.2176]
25-01-23 09:47:56 | I |     + error = [11.2176]
25-01-23 09:47:56 | I |   - Calibrating model.layers.0.mlp.gate_proj.weight
25-01-23 09:47:57 | I |       - range scale = [    1.0000]
25-01-23 09:47:57 | I |         sum  error  = [    1.1154]
25-01-23 09:47:57 | I |         best error  = [    1.1154]
25-01-23 09:47:57 | I |     + error = [1.1154]
25-01-23 09:47:58 | I |       - range scale = [    1.0000]
25-01-23 09:47:58 | I |         sum  error  = [   11.5941]
25-01-23 09:47:58 | I |         best error  = [   11.5941]
25-01-23 09:47:58 | I |     + error = [11.5941]
25-01-23 09:47:58 | I |   - Calibrating model.layers.0.mlp.down_proj.weight
25-01-23 09:47:59 | I |       - range scale = [    1.0000]
25-01-23 09:47:59 | I |         sum  error  = [    0.1586]
25-01-23 09:47:59 | I |         best error  = [    0.1586]
25-01-23 09:47:59 | I |     + error = [0.1586]
25-01-23 09:48:00 | I |       - range scale = [    1.0000]
25-01-23 09:48:00 | I |         sum  error  = [    1.5443]
25-01-23 09:48:00 | I |         best error  = [    1.5443]
25-01-23 09:48:00 | I |     + error = [1.5443]
25-01-23 09:48:00 | I |   - Quantizing model.layers.0.self_attn.q_proj.weight
25-01-23 09:48:03 | I |   - Quantizing model.layers.0.self_attn.k_proj.weight
25-01-23 09:48:05 | I |   - Quantizing model.layers.0.self_attn.v_proj.weight
25-01-23 09:48:07 | I |   - Quantizing model.layers.0.self_attn.o_proj.weight
25-01-23 09:48:09 | I |   - Quantizing model.layers.0.mlp.up_proj.weight
25-01-23 09:48:12 | I |   - Quantizing model.layers.0.mlp.gate_proj.weight
25-01-23 09:48:14 | I |   - Quantizing model.layers.0.mlp.down_proj.weight
25-01-23 09:48:20 | I | quantizing activations for layer model.layers.0
25-01-23 09:48:20 | I | collecting calibration activations in model.layers.0
25-01-23 09:48:20 | I | collecting calibration activations in model.layers.0
25-01-23 09:48:22 | I | forward this layer
25-01-23 09:48:22 | I | input_file: /data/gyy/lmquant-main/lmquant/data/data_without_preprocess_llama_7b_bs16/shard/val_input_args_layer_0/99.pt
25-01-23 09:48:22 | I | output_file: /data/gyy/lmquant-main/lmquant/data/data_without_preprocess_llama_7b_bs16/shard/val_teacher_output_layer_0/99.pt
25-01-23 09:48:23 | I | In layer model.layers.0, gradient of model.layers.0.self_attn.q_proj has nan: False
25-01-23 09:48:23 | I | In layer model.layers.0, gradient of model.layers.0.self_attn.q_proj has inf: False
25-01-23 09:48:23 | I | In layer model.layers.0, gradient of model.layers.0.self_attn.k_proj has nan: False
25-01-23 09:48:23 | I | In layer model.layers.0, gradient of model.layers.0.self_attn.k_proj has inf: False
25-01-23 09:48:23 | I | In layer model.layers.0, gradient of model.layers.0.self_attn.v_proj has nan: False
25-01-23 09:48:23 | I | In layer model.layers.0, gradient of model.layers.0.self_attn.v_proj has inf: False
25-01-23 09:48:23 | I | In layer model.layers.0, gradient of model.layers.0.self_attn.o_proj has nan: False
25-01-23 09:48:23 | I | In layer model.layers.0, gradient of model.layers.0.self_attn.o_proj has inf: False
25-01-23 09:48:23 | I | In layer model.layers.0, gradient of model.layers.0.mlp.up_proj has nan: False
25-01-23 09:48:23 | I | In layer model.layers.0, gradient of model.layers.0.mlp.up_proj has inf: False
25-01-23 09:48:23 | I | In layer model.layers.0, gradient of model.layers.0.mlp.gate_proj has nan: False
25-01-23 09:48:23 | I | In layer model.layers.0, gradient of model.layers.0.mlp.gate_proj has inf: False
25-01-23 09:48:23 | I | In layer model.layers.0, gradient of model.layers.0.mlp.down_proj has nan: False
25-01-23 09:48:23 | I | In layer model.layers.0, gradient of model.layers.0.mlp.down_proj has inf: False
25-01-23 09:48:23 | I | [95] done with optimizer step
25-01-23 09:48:23 | I | epoch 001:    100 / 819200000 loss=1.12317e-05, loss_per_token=0.0460051, loss_sum=376.874, wps=225.2, ups=0.03, wpb=8192, bsz=16, num_updates=96, lr=0.00499923, gnorm=0.133, clip=0, loss_scale=8, train_wall=36, cuda_gb_allocated=17.1, cuda_gb_reserved=18.7, cuda_gb_free=6.6, wall=8650
25-01-23 09:48:23 | I | TRAIN CURRENT LAYER_IDX = 0
25-01-23 09:48:23 | I | in layer model.layers.0
25-01-23 09:48:23 | I | quantizing weights for layer model.layers.0
25-01-23 09:48:23 | I | collecting calibration activations in model.layers.0
25-01-23 09:48:23 | I | collecting calibration activations in model.layers.0
25-01-23 09:48:24 | I | - Quantizing decoder layer model.layers.0
25-01-23 09:48:24 | I |   - Calibrating model.layers.0.self_attn.q_proj.weight
25-01-23 09:48:24 | I |       - range scale = [    1.0000]
25-01-23 09:48:24 | I |         sum  error  = [    0.0464]
25-01-23 09:48:24 | I |         best error  = [    0.0464]
25-01-23 09:48:24 | I |     + error = [0.0464]
25-01-23 09:48:25 | I |       - range scale = [    1.0000]
25-01-23 09:48:25 | I |         sum  error  = [    0.7060]
25-01-23 09:48:25 | I |         best error  = [    0.7060]
25-01-23 09:48:25 | I |     + error = [0.7060]
25-01-23 09:48:25 | I |   - Calibrating model.layers.0.self_attn.k_proj.weight
25-01-23 09:48:26 | I |       - range scale = [    1.0000]
25-01-23 09:48:26 | I |         sum  error  = [    0.0574]
25-01-23 09:48:26 | I |         best error  = [    0.0574]
25-01-23 09:48:26 | I |     + error = [0.0574]
25-01-23 09:48:27 | I |       - range scale = [    1.0000]
25-01-23 09:48:27 | I |         sum  error  = [    0.8559]
25-01-23 09:48:27 | I |         best error  = [    0.8559]
25-01-23 09:48:27 | I |     + error = [0.8559]
25-01-23 09:48:27 | I |   - Calibrating model.layers.0.self_attn.v_proj.weight
25-01-23 09:48:28 | I |       - range scale = [    1.0000]
25-01-23 09:48:28 | I |         sum  error  = [    0.7084]
25-01-23 09:48:28 | I |         best error  = [    0.7084]
25-01-23 09:48:28 | I |     + error = [0.7084]
25-01-23 09:48:28 | I |       - range scale = [    1.0000]
25-01-23 09:48:28 | I |         sum  error  = [    6.5538]
25-01-23 09:48:28 | I |         best error  = [    6.5538]
25-01-23 09:48:28 | I |     + error = [6.5538]
25-01-23 09:48:29 | I |   - Calibrating model.layers.0.self_attn.o_proj.weight
25-01-23 09:48:29 | I |       - range scale = [    1.0000]
25-01-23 09:48:29 | I |         sum  error  = [    0.2025]
25-01-23 09:48:29 | I |         best error  = [    0.2025]
25-01-23 09:48:29 | I |     + error = [0.2025]
25-01-23 09:48:30 | I |       - range scale = [    1.0000]
25-01-23 09:48:30 | I |         sum  error  = [    1.6073]
25-01-23 09:48:30 | I |         best error  = [    1.6073]
25-01-23 09:48:30 | I |     + error = [1.6073]
25-01-23 09:48:30 | I |   - Calibrating model.layers.0.mlp.up_proj.weight
25-01-23 09:48:31 | I |       - range scale = [    1.0000]
25-01-23 09:48:31 | I |         sum  error  = [    1.0508]
25-01-23 09:48:31 | I |         best error  = [    1.0508]
25-01-23 09:48:31 | I |     + error = [1.0508]
25-01-23 09:48:32 | I |       - range scale = [    1.0000]
25-01-23 09:48:32 | I |         sum  error  = [   11.1874]
25-01-23 09:48:32 | I |         best error  = [   11.1874]
25-01-23 09:48:32 | I |     + error = [11.1874]
25-01-23 09:48:32 | I |   - Calibrating model.layers.0.mlp.gate_proj.weight
25-01-23 09:48:33 | I |       - range scale = [    1.0000]
25-01-23 09:48:33 | I |         sum  error  = [    1.1156]
25-01-23 09:48:33 | I |         best error  = [    1.1156]
25-01-23 09:48:33 | I |     + error = [1.1156]
25-01-23 09:48:34 | I |       - range scale = [    1.0000]
25-01-23 09:48:34 | I |         sum  error  = [   11.5632]
25-01-23 09:48:34 | I |         best error  = [   11.5632]
25-01-23 09:48:34 | I |     + error = [11.5632]
25-01-23 09:48:35 | I |   - Calibrating model.layers.0.mlp.down_proj.weight
25-01-23 09:48:35 | I |       - range scale = [    1.0000]
25-01-23 09:48:35 | I |         sum  error  = [    0.1661]
25-01-23 09:48:35 | I |         best error  = [    0.1661]
25-01-23 09:48:35 | I |     + error = [0.1661]
25-01-23 09:48:36 | I |       - range scale = [    1.0000]
25-01-23 09:48:36 | I |         sum  error  = [    1.6150]
25-01-23 09:48:36 | I |         best error  = [    1.6150]
25-01-23 09:48:36 | I |     + error = [1.6150]
25-01-23 09:48:37 | I |   - Quantizing model.layers.0.self_attn.q_proj.weight
25-01-23 09:48:39 | I |   - Quantizing model.layers.0.self_attn.k_proj.weight
25-01-23 09:48:41 | I |   - Quantizing model.layers.0.self_attn.v_proj.weight
25-01-23 09:48:43 | I |   - Quantizing model.layers.0.self_attn.o_proj.weight
25-01-23 09:48:46 | I |   - Quantizing model.layers.0.mlp.up_proj.weight
25-01-23 09:48:48 | I |   - Quantizing model.layers.0.mlp.gate_proj.weight
25-01-23 09:48:50 | I |   - Quantizing model.layers.0.mlp.down_proj.weight
25-01-23 09:48:56 | I | quantizing activations for layer model.layers.0
25-01-23 09:48:57 | I | collecting calibration activations in model.layers.0
25-01-23 09:48:57 | I | collecting calibration activations in model.layers.0
25-01-23 09:48:59 | I | forward this layer
25-01-23 09:48:59 | I | input_file: /data/gyy/lmquant-main/lmquant/data/data_without_preprocess_llama_7b_bs16/shard/val_input_args_layer_0/100.pt
25-01-23 09:48:59 | I | output_file: /data/gyy/lmquant-main/lmquant/data/data_without_preprocess_llama_7b_bs16/shard/val_teacher_output_layer_0/100.pt
25-01-23 09:48:59 | I | In layer model.layers.0, gradient of model.layers.0.self_attn.q_proj has nan: False
25-01-23 09:48:59 | I | In layer model.layers.0, gradient of model.layers.0.self_attn.q_proj has inf: False
25-01-23 09:48:59 | I | In layer model.layers.0, gradient of model.layers.0.self_attn.k_proj has nan: False
25-01-23 09:48:59 | I | In layer model.layers.0, gradient of model.layers.0.self_attn.k_proj has inf: False
25-01-23 09:48:59 | I | In layer model.layers.0, gradient of model.layers.0.self_attn.v_proj has nan: False
25-01-23 09:48:59 | I | In layer model.layers.0, gradient of model.layers.0.self_attn.v_proj has inf: False
25-01-23 09:48:59 | I | In layer model.layers.0, gradient of model.layers.0.self_attn.o_proj has nan: False
25-01-23 09:48:59 | I | In layer model.layers.0, gradient of model.layers.0.self_attn.o_proj has inf: False
25-01-23 09:48:59 | I | In layer model.layers.0, gradient of model.layers.0.mlp.up_proj has nan: False
25-01-23 09:48:59 | I | In layer model.layers.0, gradient of model.layers.0.mlp.up_proj has inf: False
25-01-23 09:48:59 | I | In layer model.layers.0, gradient of model.layers.0.mlp.gate_proj has nan: False
25-01-23 09:48:59 | I | In layer model.layers.0, gradient of model.layers.0.mlp.gate_proj has inf: False
25-01-23 09:48:59 | I | In layer model.layers.0, gradient of model.layers.0.mlp.down_proj has nan: False
25-01-23 09:48:59 | I | In layer model.layers.0, gradient of model.layers.0.mlp.down_proj has inf: False
25-01-23 09:48:59 | I | [96] done with optimizer step
25-01-23 09:48:59 | I | epoch 001:    101 / 819200000 loss=1.25249e-05, loss_per_token=0.0513018, loss_sum=420.264, wps=224.8, ups=0.03, wpb=8192, bsz=16, num_updates=97, lr=0.00499922, gnorm=0.211, clip=0, loss_scale=8, train_wall=36, cuda_gb_allocated=17.1, cuda_gb_reserved=18.7, cuda_gb_free=6.6, wall=8687
25-01-23 09:48:59 | I | TRAIN CURRENT LAYER_IDX = 0
25-01-23 09:48:59 | I | in layer model.layers.0
25-01-23 09:48:59 | I | quantizing weights for layer model.layers.0
25-01-23 09:49:00 | I | collecting calibration activations in model.layers.0
25-01-23 09:49:00 | I | collecting calibration activations in model.layers.0
25-01-23 09:49:00 | I | - Quantizing decoder layer model.layers.0
25-01-23 09:49:00 | I |   - Calibrating model.layers.0.self_attn.q_proj.weight
25-01-23 09:49:01 | I |       - range scale = [    1.0000]
25-01-23 09:49:01 | I |         sum  error  = [    0.0534]
25-01-23 09:49:01 | I |         best error  = [    0.0534]
25-01-23 09:49:01 | I |     + error = [0.0534]
25-01-23 09:49:01 | I |       - range scale = [    1.0000]
25-01-23 09:49:01 | I |         sum  error  = [    0.7816]
25-01-23 09:49:01 | I |         best error  = [    0.7816]
25-01-23 09:49:01 | I |     + error = [0.7816]
25-01-23 09:49:02 | I |   - Calibrating model.layers.0.self_attn.k_proj.weight
25-01-23 09:49:02 | I |       - range scale = [    1.0000]
25-01-23 09:49:02 | I |         sum  error  = [    0.0736]
25-01-23 09:49:02 | I |         best error  = [    0.0736]
25-01-23 09:49:02 | I |     + error = [0.0736]
25-01-23 09:49:03 | I |       - range scale = [    1.0000]
25-01-23 09:49:03 | I |         sum  error  = [    0.9135]
25-01-23 09:49:03 | I |         best error  = [    0.9135]
25-01-23 09:49:03 | I |     + error = [0.9135]
25-01-23 09:49:03 | I |   - Calibrating model.layers.0.self_attn.v_proj.weight
25-01-23 09:49:04 | I |       - range scale = [    1.0000]
25-01-23 09:49:04 | I |         sum  error  = [    0.7334]
25-01-23 09:49:04 | I |         best error  = [    0.7334]
25-01-23 09:49:04 | I |     + error = [0.7334]
25-01-23 09:49:05 | I |       - range scale = [    1.0000]
25-01-23 09:49:05 | I |         sum  error  = [    6.9076]
25-01-23 09:49:05 | I |         best error  = [    6.9076]
25-01-23 09:49:05 | I |     + error = [6.9076]
25-01-23 09:49:05 | I |   - Calibrating model.layers.0.self_attn.o_proj.weight
25-01-23 09:49:06 | I |       - range scale = [    1.0000]
25-01-23 09:49:06 | I |         sum  error  = [    0.2086]
25-01-23 09:49:06 | I |         best error  = [    0.2086]
25-01-23 09:49:06 | I |     + error = [0.2086]
25-01-23 09:49:07 | I |       - range scale = [    1.0000]
25-01-23 09:49:07 | I |         sum  error  = [    1.6543]
25-01-23 09:49:07 | I |         best error  = [    1.6543]
25-01-23 09:49:07 | I |     + error = [1.6543]
25-01-23 09:49:07 | I |   - Calibrating model.layers.0.mlp.up_proj.weight
25-01-23 09:49:07 | I |       - range scale = [    1.0000]
25-01-23 09:49:07 | I |         sum  error  = [    1.1382]
25-01-23 09:49:07 | I |         best error  = [    1.1382]
25-01-23 09:49:07 | I |     + error = [1.1382]
25-01-23 09:49:09 | I |       - range scale = [    1.0000]
25-01-23 09:49:09 | I |         sum  error  = [   12.1339]
25-01-23 09:49:09 | I |         best error  = [   12.1339]
25-01-23 09:49:09 | I |     + error = [12.1339]
25-01-23 09:49:09 | I |   - Calibrating model.layers.0.mlp.gate_proj.weight
25-01-23 09:49:10 | I |       - range scale = [    1.0000]
25-01-23 09:49:10 | I |         sum  error  = [    1.2062]
25-01-23 09:49:10 | I |         best error  = [    1.2062]
25-01-23 09:49:10 | I |     + error = [1.2062]
25-01-23 09:49:11 | I |       - range scale = [    1.0000]
25-01-23 09:49:11 | I |         sum  error  = [   12.5395]
25-01-23 09:49:11 | I |         best error  = [   12.5395]
25-01-23 09:49:11 | I |     + error = [12.5395]
25-01-23 09:49:11 | I |   - Calibrating model.layers.0.mlp.down_proj.weight
25-01-23 09:49:12 | I |       - range scale = [    1.0000]
25-01-23 09:49:12 | I |         sum  error  = [    0.1863]
25-01-23 09:49:12 | I |         best error  = [    0.1863]
25-01-23 09:49:12 | I |     + error = [0.1863]
25-01-23 09:49:13 | I |       - range scale = [    1.0000]
25-01-23 09:49:13 | I |         sum  error  = [    1.7941]
25-01-23 09:49:13 | I |         best error  = [    1.7941]
25-01-23 09:49:13 | I |     + error = [1.7941]
25-01-23 09:49:13 | I |   - Quantizing model.layers.0.self_attn.q_proj.weight
25-01-23 09:49:15 | I |   - Quantizing model.layers.0.self_attn.k_proj.weight
25-01-23 09:49:18 | I |   - Quantizing model.layers.0.self_attn.v_proj.weight
25-01-23 09:49:20 | I |   - Quantizing model.layers.0.self_attn.o_proj.weight
25-01-23 09:49:22 | I |   - Quantizing model.layers.0.mlp.up_proj.weight
25-01-23 09:49:25 | I |   - Quantizing model.layers.0.mlp.gate_proj.weight
25-01-23 09:49:27 | I |   - Quantizing model.layers.0.mlp.down_proj.weight
25-01-23 09:49:33 | I | quantizing activations for layer model.layers.0
25-01-23 09:49:33 | I | collecting calibration activations in model.layers.0
25-01-23 09:49:33 | I | collecting calibration activations in model.layers.0
25-01-23 09:49:35 | I | forward this layer
25-01-23 09:49:35 | I | input_file: /data/gyy/lmquant-main/lmquant/data/data_without_preprocess_llama_7b_bs16/shard/val_input_args_layer_0/101.pt
25-01-23 09:49:35 | I | output_file: /data/gyy/lmquant-main/lmquant/data/data_without_preprocess_llama_7b_bs16/shard/val_teacher_output_layer_0/101.pt
25-01-23 09:49:36 | I | In layer model.layers.0, gradient of model.layers.0.self_attn.q_proj has nan: False
25-01-23 09:49:36 | I | In layer model.layers.0, gradient of model.layers.0.self_attn.q_proj has inf: False
25-01-23 09:49:36 | I | In layer model.layers.0, gradient of model.layers.0.self_attn.k_proj has nan: False
25-01-23 09:49:36 | I | In layer model.layers.0, gradient of model.layers.0.self_attn.k_proj has inf: False
25-01-23 09:49:36 | I | In layer model.layers.0, gradient of model.layers.0.self_attn.v_proj has nan: False
25-01-23 09:49:36 | I | In layer model.layers.0, gradient of model.layers.0.self_attn.v_proj has inf: False
25-01-23 09:49:36 | I | In layer model.layers.0, gradient of model.layers.0.self_attn.o_proj has nan: False
25-01-23 09:49:36 | I | In layer model.layers.0, gradient of model.layers.0.self_attn.o_proj has inf: False
25-01-23 09:49:36 | I | In layer model.layers.0, gradient of model.layers.0.mlp.up_proj has nan: False
25-01-23 09:49:36 | I | In layer model.layers.0, gradient of model.layers.0.mlp.up_proj has inf: False
25-01-23 09:49:36 | I | In layer model.layers.0, gradient of model.layers.0.mlp.gate_proj has nan: False
25-01-23 09:49:36 | I | In layer model.layers.0, gradient of model.layers.0.mlp.gate_proj has inf: False
25-01-23 09:49:36 | I | In layer model.layers.0, gradient of model.layers.0.mlp.down_proj has nan: False
25-01-23 09:49:36 | I | In layer model.layers.0, gradient of model.layers.0.mlp.down_proj has inf: False
25-01-23 09:49:36 | I | [97] done with optimizer step
25-01-23 09:49:36 | I | epoch 001:    102 / 819200000 loss=1.45825e-05, loss_per_token=0.05973, loss_sum=489.308, wps=223.5, ups=0.03, wpb=8192, bsz=16, num_updates=98, lr=0.0049992, gnorm=0.334, clip=0, loss_scale=8, train_wall=37, cuda_gb_allocated=17.1, cuda_gb_reserved=18.8, cuda_gb_free=6.6, wall=8723
25-01-23 09:49:36 | I | TRAIN CURRENT LAYER_IDX = 0
25-01-23 09:49:36 | I | in layer model.layers.0
25-01-23 09:49:36 | I | quantizing weights for layer model.layers.0
25-01-23 09:49:36 | I | collecting calibration activations in model.layers.0
25-01-23 09:49:36 | I | collecting calibration activations in model.layers.0
25-01-23 09:49:37 | I | - Quantizing decoder layer model.layers.0
25-01-23 09:49:37 | I |   - Calibrating model.layers.0.self_attn.q_proj.weight
25-01-23 09:49:37 | I |       - range scale = [    1.0000]
25-01-23 09:49:37 | I |         sum  error  = [    0.0511]
25-01-23 09:49:37 | I |         best error  = [    0.0511]
25-01-23 09:49:37 | I |     + error = [0.0511]
25-01-23 09:49:38 | I |       - range scale = [    1.0000]
25-01-23 09:49:38 | I |         sum  error  = [    0.7680]
25-01-23 09:49:38 | I |         best error  = [    0.7680]
25-01-23 09:49:38 | I |     + error = [0.7680]
25-01-23 09:49:38 | I |   - Calibrating model.layers.0.self_attn.k_proj.weight
25-01-23 09:49:39 | I |       - range scale = [    1.0000]
25-01-23 09:49:39 | I |         sum  error  = [    0.0708]
25-01-23 09:49:39 | I |         best error  = [    0.0708]
25-01-23 09:49:39 | I |     + error = [0.0708]
25-01-23 09:49:40 | I |       - range scale = [    1.0000]
25-01-23 09:49:40 | I |         sum  error  = [    0.8539]
25-01-23 09:49:40 | I |         best error  = [    0.8539]
25-01-23 09:49:40 | I |     + error = [0.8539]
25-01-23 09:49:40 | I |   - Calibrating model.layers.0.self_attn.v_proj.weight
25-01-23 09:49:41 | I |       - range scale = [    1.0000]
25-01-23 09:49:41 | I |         sum  error  = [    0.7171]
25-01-23 09:49:41 | I |         best error  = [    0.7171]
25-01-23 09:49:41 | I |     + error = [0.7171]
25-01-23 09:49:42 | I |       - range scale = [    1.0000]
25-01-23 09:49:42 | I |         sum  error  = [    6.7769]
25-01-23 09:49:42 | I |         best error  = [    6.7769]
25-01-23 09:49:42 | I |     + error = [6.7769]
25-01-23 09:49:42 | I |   - Calibrating model.layers.0.self_attn.o_proj.weight
25-01-23 09:49:43 | I |       - range scale = [    1.0000]
25-01-23 09:49:43 | I |         sum  error  = [    0.2055]
25-01-23 09:49:43 | I |         best error  = [    0.2055]
25-01-23 09:49:43 | I |     + error = [0.2055]
25-01-23 09:49:43 | I |       - range scale = [    1.0000]
25-01-23 09:49:43 | I |         sum  error  = [    1.6405]
25-01-23 09:49:43 | I |         best error  = [    1.6405]
25-01-23 09:49:43 | I |     + error = [1.6405]
25-01-23 09:49:44 | I |   - Calibrating model.layers.0.mlp.up_proj.weight
25-01-23 09:49:44 | I |       - range scale = [    1.0000]
25-01-23 09:49:44 | I |         sum  error  = [    1.1599]
25-01-23 09:49:44 | I |         best error  = [    1.1599]
25-01-23 09:49:44 | I |     + error = [1.1599]
25-01-23 09:49:45 | I |       - range scale = [    1.0000]
25-01-23 09:49:45 | I |         sum  error  = [   12.3563]
25-01-23 09:49:45 | I |         best error  = [   12.3563]
25-01-23 09:49:45 | I |     + error = [12.3563]
25-01-23 09:49:46 | I |   - Calibrating model.layers.0.mlp.gate_proj.weight
25-01-23 09:49:46 | I |       - range scale = [    1.0000]
25-01-23 09:49:46 | I |         sum  error  = [    1.2279]
25-01-23 09:49:46 | I |         best error  = [    1.2279]
25-01-23 09:49:46 | I |     + error = [1.2279]
25-01-23 09:49:48 | I |       - range scale = [    1.0000]
25-01-23 09:49:48 | I |         sum  error  = [   12.7734]
25-01-23 09:49:48 | I |         best error  = [   12.7734]
25-01-23 09:49:48 | I |     + error = [12.7734]
25-01-23 09:49:48 | I |   - Calibrating model.layers.0.mlp.down_proj.weight
25-01-23 09:49:49 | I |       - range scale = [    1.0000]
25-01-23 09:49:49 | I |         sum  error  = [    0.1945]
25-01-23 09:49:49 | I |         best error  = [    0.1945]
25-01-23 09:49:49 | I |     + error = [0.1945]
25-01-23 09:49:50 | I |       - range scale = [    1.0000]
25-01-23 09:49:50 | I |         sum  error  = [    1.8731]
25-01-23 09:49:50 | I |         best error  = [    1.8731]
25-01-23 09:49:50 | I |     + error = [1.8731]
25-01-23 09:49:50 | I |   - Quantizing model.layers.0.self_attn.q_proj.weight
25-01-23 09:49:52 | I |   - Quantizing model.layers.0.self_attn.k_proj.weight
25-01-23 09:49:55 | I |   - Quantizing model.layers.0.self_attn.v_proj.weight
25-01-23 09:49:57 | I |   - Quantizing model.layers.0.self_attn.o_proj.weight
25-01-23 09:49:59 | I |   - Quantizing model.layers.0.mlp.up_proj.weight
25-01-23 09:50:01 | I |   - Quantizing model.layers.0.mlp.gate_proj.weight
25-01-23 09:50:04 | I |   - Quantizing model.layers.0.mlp.down_proj.weight
25-01-23 09:50:10 | I | quantizing activations for layer model.layers.0
25-01-23 09:50:10 | I | collecting calibration activations in model.layers.0
25-01-23 09:50:10 | I | collecting calibration activations in model.layers.0
25-01-23 09:50:12 | I | forward this layer
25-01-23 09:50:12 | I | input_file: /data/gyy/lmquant-main/lmquant/data/data_without_preprocess_llama_7b_bs16/shard/val_input_args_layer_0/102.pt
25-01-23 09:50:12 | I | output_file: /data/gyy/lmquant-main/lmquant/data/data_without_preprocess_llama_7b_bs16/shard/val_teacher_output_layer_0/102.pt
25-01-23 09:50:12 | I | In layer model.layers.0, gradient of model.layers.0.self_attn.q_proj has nan: False
25-01-23 09:50:12 | I | In layer model.layers.0, gradient of model.layers.0.self_attn.q_proj has inf: False
25-01-23 09:50:12 | I | In layer model.layers.0, gradient of model.layers.0.self_attn.k_proj has nan: False
25-01-23 09:50:12 | I | In layer model.layers.0, gradient of model.layers.0.self_attn.k_proj has inf: False
25-01-23 09:50:12 | I | In layer model.layers.0, gradient of model.layers.0.self_attn.v_proj has nan: False
25-01-23 09:50:12 | I | In layer model.layers.0, gradient of model.layers.0.self_attn.v_proj has inf: False
25-01-23 09:50:12 | I | In layer model.layers.0, gradient of model.layers.0.self_attn.o_proj has nan: False
25-01-23 09:50:12 | I | In layer model.layers.0, gradient of model.layers.0.self_attn.o_proj has inf: False
25-01-23 09:50:12 | I | In layer model.layers.0, gradient of model.layers.0.mlp.up_proj has nan: False
25-01-23 09:50:12 | I | In layer model.layers.0, gradient of model.layers.0.mlp.up_proj has inf: False
25-01-23 09:50:12 | I | In layer model.layers.0, gradient of model.layers.0.mlp.gate_proj has nan: False
25-01-23 09:50:12 | I | In layer model.layers.0, gradient of model.layers.0.mlp.gate_proj has inf: False
25-01-23 09:50:12 | I | In layer model.layers.0, gradient of model.layers.0.mlp.down_proj has nan: False
25-01-23 09:50:12 | I | In layer model.layers.0, gradient of model.layers.0.mlp.down_proj has inf: False
25-01-23 09:50:12 | I | [98] done with optimizer step
25-01-23 09:50:12 | I | epoch 001:    103 / 819200000 loss=1.33756e-05, loss_per_token=0.0547863, loss_sum=448.809, wps=223.9, ups=0.03, wpb=8192, bsz=16, num_updates=99, lr=0.00499918, gnorm=0.227, clip=0, loss_scale=8, train_wall=37, cuda_gb_allocated=17.1, cuda_gb_reserved=18.4, cuda_gb_free=6.6, wall=8760
25-01-23 09:50:13 | I | TRAIN CURRENT LAYER_IDX = 0
25-01-23 09:50:13 | I | in layer model.layers.0
25-01-23 09:50:13 | I | quantizing weights for layer model.layers.0
25-01-23 09:50:13 | I | collecting calibration activations in model.layers.0
25-01-23 09:50:13 | I | collecting calibration activations in model.layers.0
25-01-23 09:50:13 | I | - Quantizing decoder layer model.layers.0
25-01-23 09:50:13 | I |   - Calibrating model.layers.0.self_attn.q_proj.weight
25-01-23 09:50:14 | I |       - range scale = [    1.0000]
25-01-23 09:50:14 | I |         sum  error  = [    0.0450]
25-01-23 09:50:14 | I |         best error  = [    0.0450]
25-01-23 09:50:14 | I |     + error = [0.0450]
25-01-23 09:50:15 | I |       - range scale = [    1.0000]
25-01-23 09:50:15 | I |         sum  error  = [    0.6992]
25-01-23 09:50:15 | I |         best error  = [    0.6992]
25-01-23 09:50:15 | I |     + error = [0.6992]
25-01-23 09:50:15 | I |   - Calibrating model.layers.0.self_attn.k_proj.weight
25-01-23 09:50:16 | I |       - range scale = [    1.0000]
25-01-23 09:50:16 | I |         sum  error  = [    0.0543]
25-01-23 09:50:16 | I |         best error  = [    0.0543]
25-01-23 09:50:16 | I |     + error = [0.0543]
25-01-23 09:50:16 | I |       - range scale = [    1.0000]
25-01-23 09:50:16 | I |         sum  error  = [    0.8802]
25-01-23 09:50:16 | I |         best error  = [    0.8802]
25-01-23 09:50:16 | I |     + error = [0.8802]
25-01-23 09:50:17 | I |   - Calibrating model.layers.0.self_attn.v_proj.weight
25-01-23 09:50:17 | I |       - range scale = [    1.0000]
25-01-23 09:50:17 | I |         sum  error  = [    0.6944]
25-01-23 09:50:17 | I |         best error  = [    0.6944]
25-01-23 09:50:17 | I |     + error = [0.6944]
25-01-23 09:50:18 | I |       - range scale = [    1.0000]
25-01-23 09:50:18 | I |         sum  error  = [    6.5382]
25-01-23 09:50:18 | I |         best error  = [    6.5382]
25-01-23 09:50:18 | I |     + error = [6.5382]
25-01-23 09:50:18 | I |   - Calibrating model.layers.0.self_attn.o_proj.weight
25-01-23 09:50:19 | I |       - range scale = [    1.0000]
25-01-23 09:50:19 | I |         sum  error  = [    0.1987]
25-01-23 09:50:19 | I |         best error  = [    0.1987]
25-01-23 09:50:19 | I |     + error = [0.1987]
25-01-23 09:50:20 | I |       - range scale = [    1.0000]
25-01-23 09:50:20 | I |         sum  error  = [    1.5879]
25-01-23 09:50:20 | I |         best error  = [    1.5879]
25-01-23 09:50:20 | I |     + error = [1.5879]
25-01-23 09:50:20 | I |   - Calibrating model.layers.0.mlp.up_proj.weight
25-01-23 09:50:21 | I |       - range scale = [    1.0000]
25-01-23 09:50:21 | I |         sum  error  = [    1.1393]
25-01-23 09:50:21 | I |         best error  = [    1.1393]
25-01-23 09:50:21 | I |     + error = [1.1393]
25-01-23 09:50:22 | I |       - range scale = [    1.0000]
25-01-23 09:50:22 | I |         sum  error  = [   12.1314]
25-01-23 09:50:22 | I |         best error  = [   12.1314]
25-01-23 09:50:22 | I |     + error = [12.1314]
25-01-23 09:50:22 | I |   - Calibrating model.layers.0.mlp.gate_proj.weight
25-01-23 09:50:23 | I |       - range scale = [    1.0000]
25-01-23 09:50:23 | I |         sum  error  = [    1.2038]
25-01-23 09:50:23 | I |         best error  = [    1.2038]
25-01-23 09:50:23 | I |     + error = [1.2038]
25-01-23 09:50:24 | I |       - range scale = [    1.0000]
25-01-23 09:50:24 | I |         sum  error  = [   12.5377]
25-01-23 09:50:24 | I |         best error  = [   12.5377]
25-01-23 09:50:24 | I |     + error = [12.5377]
25-01-23 09:50:24 | I |   - Calibrating model.layers.0.mlp.down_proj.weight
25-01-23 09:50:25 | I |       - range scale = [    1.0000]
25-01-23 09:50:25 | I |         sum  error  = [    0.1778]
25-01-23 09:50:25 | I |         best error  = [    0.1778]
25-01-23 09:50:25 | I |     + error = [0.1778]
25-01-23 09:50:26 | I |       - range scale = [    1.0000]
25-01-23 09:50:26 | I |         sum  error  = [    1.7245]
25-01-23 09:50:26 | I |         best error  = [    1.7245]
25-01-23 09:50:26 | I |     + error = [1.7245]
25-01-23 09:50:26 | I |   - Quantizing model.layers.0.self_attn.q_proj.weight
25-01-23 09:50:29 | I |   - Quantizing model.layers.0.self_attn.k_proj.weight
25-01-23 09:50:31 | I |   - Quantizing model.layers.0.self_attn.v_proj.weight
25-01-23 09:50:33 | I |   - Quantizing model.layers.0.self_attn.o_proj.weight
25-01-23 09:50:36 | I |   - Quantizing model.layers.0.mlp.up_proj.weight
25-01-23 09:50:38 | I |   - Quantizing model.layers.0.mlp.gate_proj.weight
25-01-23 09:50:40 | I |   - Quantizing model.layers.0.mlp.down_proj.weight
25-01-23 09:50:46 | I | quantizing activations for layer model.layers.0
25-01-23 09:50:47 | I | collecting calibration activations in model.layers.0
25-01-23 09:50:47 | I | collecting calibration activations in model.layers.0
25-01-23 09:50:48 | I | forward this layer
25-01-23 09:50:48 | I | input_file: /data/gyy/lmquant-main/lmquant/data/data_without_preprocess_llama_7b_bs16/shard/val_input_args_layer_0/103.pt
25-01-23 09:50:48 | I | output_file: /data/gyy/lmquant-main/lmquant/data/data_without_preprocess_llama_7b_bs16/shard/val_teacher_output_layer_0/103.pt
25-01-23 09:50:49 | I | In layer model.layers.0, gradient of model.layers.0.self_attn.q_proj has nan: False
25-01-23 09:50:49 | I | In layer model.layers.0, gradient of model.layers.0.self_attn.q_proj has inf: False
25-01-23 09:50:49 | I | In layer model.layers.0, gradient of model.layers.0.self_attn.k_proj has nan: False
25-01-23 09:50:49 | I | In layer model.layers.0, gradient of model.layers.0.self_attn.k_proj has inf: False
25-01-23 09:50:49 | I | In layer model.layers.0, gradient of model.layers.0.self_attn.v_proj has nan: False
25-01-23 09:50:49 | I | In layer model.layers.0, gradient of model.layers.0.self_attn.v_proj has inf: False
25-01-23 09:50:49 | I | In layer model.layers.0, gradient of model.layers.0.self_attn.o_proj has nan: False
25-01-23 09:50:49 | I | In layer model.layers.0, gradient of model.layers.0.self_attn.o_proj has inf: False
25-01-23 09:50:49 | I | In layer model.layers.0, gradient of model.layers.0.mlp.up_proj has nan: False
25-01-23 09:50:49 | I | In layer model.layers.0, gradient of model.layers.0.mlp.up_proj has inf: False
25-01-23 09:50:49 | I | In layer model.layers.0, gradient of model.layers.0.mlp.gate_proj has nan: False
25-01-23 09:50:49 | I | In layer model.layers.0, gradient of model.layers.0.mlp.gate_proj has inf: False
25-01-23 09:50:49 | I | In layer model.layers.0, gradient of model.layers.0.mlp.down_proj has nan: False
25-01-23 09:50:49 | I | In layer model.layers.0, gradient of model.layers.0.mlp.down_proj has inf: False
25-01-23 09:50:49 | I | [99] done with optimizer step
25-01-23 09:50:49 | I | epoch 001:    104 / 819200000 loss=1.32316e-05, loss_per_token=0.0541966, loss_sum=443.979, wps=225.2, ups=0.03, wpb=8192, bsz=16, num_updates=100, lr=0.00499917, gnorm=0.215, clip=0, loss_scale=8, train_wall=36, cuda_gb_allocated=17.1, cuda_gb_reserved=18.7, cuda_gb_free=6.6, wall=8796
25-01-23 09:50:49 | I | Stopping training due to num_updates: 100 >= max_update: 100
25-01-23 09:50:49 | I | begin validation on "valid" subset on rank 0
25-01-23 09:50:49 | I | got valid iterator on "valid" subset on rank 0
25-01-23 09:50:49 | I | Valid: Start iterating over samples
25-01-23 09:50:49 | W |   `pretrained` model kwarg is not of type `str`. Many other model arguments may be ignored. Please do not launch via accelerate or use `parallelize=True` if passing an existing model this way.
25-01-23 09:50:49 | W |   Passed an already-initialized model through `pretrained`, assuming single-process call to evaluate() or custom distributed integration
25-01-23 09:50:49 | I | - Evaluator: gptq
25-01-23 09:50:49 | I | - Tasks: ['wikitext', 'val_valid']
25-01-23 09:50:49 | I | - Batch_size: 8
25-01-23 09:50:49 | I |   + Max_seq_length: 2048
25-01-23 09:52:24 | I |     - Results:
25-01-23 09:52:24 | I |       |  Task   |Version|    Metric     |Value |   |Stderr|
25-01-23 09:52:24 | I |       |---------|------:|---------------|-----:|---|-----:|
25-01-23 09:52:24 | I |       |wikitext |      1|word_perplexity|5.4825|±  |5.4825|
25-01-23 09:52:24 | I |       |val_valid|      1|word_perplexity|5.0230|±  |5.0230|
25-01-23 09:52:24 | I |       
25-01-23 09:52:24 | I |   + Max_seq_length: 4096
25-01-23 09:53:57 | I |     - Results:
25-01-23 09:53:57 | I |       |  Task   |Version|    Metric     |Value |   |Stderr|
25-01-23 09:53:57 | I |       |---------|------:|---------------|-----:|---|-----:|
25-01-23 09:53:57 | I |       |wikitext |      1|word_perplexity|5.1235|±  |5.1235|
25-01-23 09:53:57 | I |       |val_valid|      1|word_perplexity|4.8196|±  |4.8196|
25-01-23 09:53:57 | I |       
25-01-23 09:53:57 | I | in valid, quantize current layer weights
25-01-23 09:53:58 | W | Repo card metadata block was not found. Setting CardData to empty.
25-01-23 09:54:42 | I | collecting calibration activations in model.layers.0
25-01-23 09:54:42 | I | collecting calibration activations in model.layers.0
25-01-23 09:54:42 | I | collecting calibration activations in model.layers.0
25-01-23 09:54:42 | I | collecting calibration activations in model.layers.0
25-01-23 09:54:42 | I | collecting calibration activations in model.layers.0
25-01-23 09:54:43 | I | collecting calibration activations in model.layers.0
25-01-23 09:54:43 | I | collecting calibration activations in model.layers.0
25-01-23 09:54:43 | I | collecting calibration activations in model.layers.0
25-01-23 09:54:43 | I | collecting calibration activations in model.layers.0
25-01-23 09:54:43 | I | collecting calibration activations in model.layers.0
25-01-23 09:54:43 | I | collecting calibration activations in model.layers.0
25-01-23 09:54:43 | I | collecting calibration activations in model.layers.0
25-01-23 09:54:43 | I | collecting calibration activations in model.layers.0
25-01-23 09:54:43 | I | collecting calibration activations in model.layers.0
25-01-23 09:54:43 | I | collecting calibration activations in model.layers.0
25-01-23 09:54:43 | I | collecting calibration activations in model.layers.0
25-01-23 09:54:43 | I | collecting calibration activations in model.layers.0
25-01-23 09:54:43 | I | collecting calibration activations in model.layers.0
25-01-23 09:54:43 | I | collecting calibration activations in model.layers.0
25-01-23 09:54:43 | I | collecting calibration activations in model.layers.0
25-01-23 09:54:43 | I | collecting calibration activations in model.layers.0
25-01-23 09:54:43 | I | collecting calibration activations in model.layers.0
25-01-23 09:54:43 | I | collecting calibration activations in model.layers.0
25-01-23 09:54:43 | I | collecting calibration activations in model.layers.0
25-01-23 09:54:44 | I | collecting calibration activations in model.layers.0
25-01-23 09:54:44 | I | collecting calibration activations in model.layers.0
25-01-23 09:54:44 | I | collecting calibration activations in model.layers.0
25-01-23 09:54:44 | I | collecting calibration activations in model.layers.0
25-01-23 09:54:44 | I | collecting calibration activations in model.layers.0
25-01-23 09:54:44 | I | collecting calibration activations in model.layers.0
25-01-23 09:54:44 | I | collecting calibration activations in model.layers.0
25-01-23 09:54:44 | I | collecting calibration activations in model.layers.0
25-01-23 09:54:44 | I | collecting calibration activations in model.layers.0
25-01-23 09:54:44 | I | collecting calibration activations in model.layers.0
25-01-23 09:54:44 | I | collecting calibration activations in model.layers.0
25-01-23 09:54:44 | I | collecting calibration activations in model.layers.0
25-01-23 09:54:44 | I | collecting calibration activations in model.layers.0
25-01-23 09:54:44 | I | collecting calibration activations in model.layers.0
25-01-23 09:54:44 | I | collecting calibration activations in model.layers.0
25-01-23 09:54:44 | I | collecting calibration activations in model.layers.0
25-01-23 09:54:44 | I | collecting calibration activations in model.layers.0
25-01-23 09:54:44 | I | collecting calibration activations in model.layers.0
25-01-23 09:54:44 | I | collecting calibration activations in model.layers.0
25-01-23 09:54:45 | I | collecting calibration activations in model.layers.0
25-01-23 09:54:45 | I | collecting calibration activations in model.layers.0
25-01-23 09:54:45 | I | collecting calibration activations in model.layers.0
25-01-23 09:54:45 | I | collecting calibration activations in model.layers.0
25-01-23 09:54:45 | I | collecting calibration activations in model.layers.0
25-01-23 09:54:45 | I | collecting calibration activations in model.layers.0
25-01-23 09:54:45 | I | collecting calibration activations in model.layers.0
25-01-23 09:54:45 | I | collecting calibration activations in model.layers.0
25-01-23 09:54:45 | I | collecting calibration activations in model.layers.0
25-01-23 09:54:45 | I | collecting calibration activations in model.layers.0
25-01-23 09:54:45 | I | collecting calibration activations in model.layers.0
25-01-23 09:54:45 | I | collecting calibration activations in model.layers.0
25-01-23 09:54:45 | I | collecting calibration activations in model.layers.0
25-01-23 09:54:45 | I | collecting calibration activations in model.layers.0
25-01-23 09:54:45 | I | collecting calibration activations in model.layers.0
25-01-23 09:54:45 | I | collecting calibration activations in model.layers.0
25-01-23 09:54:45 | I | collecting calibration activations in model.layers.0
25-01-23 09:54:45 | I | collecting calibration activations in model.layers.0
25-01-23 09:54:45 | I | collecting calibration activations in model.layers.0
25-01-23 09:54:46 | I | collecting calibration activations in model.layers.0
25-01-23 09:54:46 | I | collecting calibration activations in model.layers.0
25-01-23 09:54:46 | I | collecting calibration activations in model.layers.0
25-01-23 09:54:46 | I | collecting calibration activations in model.layers.0
25-01-23 09:54:46 | I | collecting calibration activations in model.layers.0
25-01-23 09:54:46 | I | collecting calibration activations in model.layers.0
25-01-23 09:54:46 | I | collecting calibration activations in model.layers.0
25-01-23 09:54:46 | I | collecting calibration activations in model.layers.0
25-01-23 09:54:46 | I | collecting calibration activations in model.layers.0
25-01-23 09:54:46 | I | collecting calibration activations in model.layers.0
25-01-23 09:54:46 | I | collecting calibration activations in model.layers.0
25-01-23 09:54:46 | I | collecting calibration activations in model.layers.0
25-01-23 09:54:46 | I | collecting calibration activations in model.layers.0
25-01-23 09:54:46 | I | collecting calibration activations in model.layers.0
25-01-23 09:54:46 | I | collecting calibration activations in model.layers.0
25-01-23 09:54:46 | I | collecting calibration activations in model.layers.0
25-01-23 09:54:46 | I | collecting calibration activations in model.layers.0
25-01-23 09:54:46 | I | collecting calibration activations in model.layers.0
25-01-23 09:54:46 | I | collecting calibration activations in model.layers.0
25-01-23 09:54:47 | I | collecting calibration activations in model.layers.0
25-01-23 09:54:47 | I | collecting calibration activations in model.layers.0
25-01-23 09:54:47 | I | collecting calibration activations in model.layers.0
25-01-23 09:54:47 | I | collecting calibration activations in model.layers.0
25-01-23 09:54:47 | I | collecting calibration activations in model.layers.0
25-01-23 09:54:47 | I | collecting calibration activations in model.layers.0
25-01-23 09:54:47 | I | collecting calibration activations in model.layers.0
25-01-23 09:54:47 | I | collecting calibration activations in model.layers.0
25-01-23 09:54:47 | I | collecting calibration activations in model.layers.0
25-01-23 09:54:47 | I | collecting calibration activations in model.layers.0
25-01-23 09:54:47 | I | collecting calibration activations in model.layers.0
25-01-23 09:54:47 | I | collecting calibration activations in model.layers.0
25-01-23 09:54:47 | I | collecting calibration activations in model.layers.0
25-01-23 09:54:47 | I | collecting calibration activations in model.layers.0
25-01-23 09:54:47 | I | collecting calibration activations in model.layers.0
25-01-23 09:54:47 | I | collecting calibration activations in model.layers.0
25-01-23 09:54:47 | I | collecting calibration activations in model.layers.0
25-01-23 09:54:47 | I | collecting calibration activations in model.layers.0
25-01-23 09:54:47 | I | collecting calibration activations in model.layers.0
25-01-23 09:54:48 | I | collecting calibration activations in model.layers.0
25-01-23 09:54:48 | I | collecting calibration activations in model.layers.0
25-01-23 09:54:48 | I | collecting calibration activations in model.layers.0
25-01-23 09:54:48 | I | collecting calibration activations in model.layers.0
25-01-23 09:54:48 | I | collecting calibration activations in model.layers.0
25-01-23 09:54:48 | I | collecting calibration activations in model.layers.0
25-01-23 09:54:48 | I | collecting calibration activations in model.layers.0
25-01-23 09:54:48 | I | collecting calibration activations in model.layers.0
25-01-23 09:54:48 | I | collecting calibration activations in model.layers.0
25-01-23 09:54:48 | I | collecting calibration activations in model.layers.0
25-01-23 09:54:48 | I | collecting calibration activations in model.layers.0
25-01-23 09:54:48 | I | collecting calibration activations in model.layers.0
25-01-23 09:54:48 | I | collecting calibration activations in model.layers.0
25-01-23 09:54:48 | I | collecting calibration activations in model.layers.0
25-01-23 09:54:48 | I | collecting calibration activations in model.layers.0
25-01-23 09:54:48 | I | collecting calibration activations in model.layers.0
25-01-23 09:54:48 | I | collecting calibration activations in model.layers.0
25-01-23 09:54:48 | I | collecting calibration activations in model.layers.0
25-01-23 09:54:48 | I | collecting calibration activations in model.layers.0
25-01-23 09:54:49 | I | collecting calibration activations in model.layers.0
25-01-23 09:54:49 | I | collecting calibration activations in model.layers.0
25-01-23 09:54:49 | I | collecting calibration activations in model.layers.0
25-01-23 09:54:49 | I | collecting calibration activations in model.layers.0
25-01-23 09:54:49 | I | collecting calibration activations in model.layers.0
25-01-23 09:54:49 | I | collecting calibration activations in model.layers.0
25-01-23 09:54:49 | I | collecting calibration activations in model.layers.0
25-01-23 09:54:49 | I | collecting calibration activations in model.layers.0
25-01-23 09:54:49 | I | collecting calibration activations in model.layers.0
25-01-23 09:54:51 | I |       - range scale = [    1.0000]
25-01-23 09:54:51 | I |         sum  error  = [    0.1562]
25-01-23 09:54:51 | I |         best error  = [    0.1562]
25-01-23 09:54:51 | I |     + error = [0.1562]
25-01-23 09:54:52 | I |       - range scale = [    1.0000]
25-01-23 09:54:52 | I |         sum  error  = [    2.6738]
25-01-23 09:54:52 | I |         best error  = [    2.6738]
25-01-23 09:54:52 | I |     + error = [2.6738]
25-01-23 09:54:54 | I |       - range scale = [    1.0000]
25-01-23 09:54:54 | I |         sum  error  = [    0.1865]
25-01-23 09:54:54 | I |         best error  = [    0.1865]
25-01-23 09:54:54 | I |     + error = [0.1865]
25-01-23 09:54:55 | I |       - range scale = [    1.0000]
25-01-23 09:54:55 | I |         sum  error  = [    3.6993]
25-01-23 09:54:55 | I |         best error  = [    3.6993]
25-01-23 09:54:55 | I |     + error = [3.6993]
25-01-23 09:54:56 | I |       - range scale = [    1.0000]
25-01-23 09:54:56 | I |         sum  error  = [    0.6442]
25-01-23 09:54:56 | I |         best error  = [    0.6442]
25-01-23 09:54:56 | I |     + error = [0.6442]
25-01-23 09:54:57 | I |       - range scale = [    1.0000]
25-01-23 09:54:57 | I |         sum  error  = [    5.9245]
25-01-23 09:54:57 | I |         best error  = [    5.9245]
25-01-23 09:54:57 | I |     + error = [5.9245]
25-01-23 09:54:58 | I |       - range scale = [    1.0000]
25-01-23 09:54:58 | I |         sum  error  = [    0.1459]
25-01-23 09:54:58 | I |         best error  = [    0.1459]
25-01-23 09:54:58 | I |     + error = [0.1459]
25-01-23 09:54:59 | I |       - range scale = [    1.0000]
25-01-23 09:54:59 | I |         sum  error  = [    1.2002]
25-01-23 09:54:59 | I |         best error  = [    1.2002]
25-01-23 09:54:59 | I |     + error = [1.2002]
25-01-23 09:55:00 | I |       - range scale = [    1.0000]
25-01-23 09:55:00 | I |         sum  error  = [    1.0935]
25-01-23 09:55:00 | I |         best error  = [    1.0935]
25-01-23 09:55:00 | I |     + error = [1.0935]
25-01-23 09:55:01 | I |       - range scale = [    1.0000]
25-01-23 09:55:01 | I |         sum  error  = [   11.6511]
25-01-23 09:55:01 | I |         best error  = [   11.6511]
25-01-23 09:55:01 | I |     + error = [11.6511]
25-01-23 09:55:02 | I |       - range scale = [    1.0000]
25-01-23 09:55:02 | I |         sum  error  = [    1.1586]
25-01-23 09:55:02 | I |         best error  = [    1.1586]
25-01-23 09:55:02 | I |     + error = [1.1586]
25-01-23 09:55:03 | I |       - range scale = [    1.0000]
25-01-23 09:55:03 | I |         sum  error  = [   12.0345]
25-01-23 09:55:03 | I |         best error  = [   12.0345]
25-01-23 09:55:03 | I |     + error = [12.0345]
25-01-23 09:55:04 | I |       - range scale = [    1.0000]
25-01-23 09:55:04 | I |         sum  error  = [    0.1842]
25-01-23 09:55:04 | I |         best error  = [    0.1842]
25-01-23 09:55:04 | I |     + error = [0.1842]
25-01-23 09:55:05 | I |       - range scale = [    1.0000]
25-01-23 09:55:05 | I |         sum  error  = [    1.8164]
25-01-23 09:55:05 | I |         best error  = [    1.8164]
25-01-23 09:55:05 | I |     + error = [1.8164]
25-01-23 09:55:29 | I | in valid, quantize current layer acts
25-01-23 09:55:31 | I | collecting calibration activations in model.layers.0
25-01-23 09:55:31 | I | collecting calibration activations in model.layers.0
25-01-23 09:55:31 | I | collecting calibration activations in model.layers.0
25-01-23 09:55:31 | I | collecting calibration activations in model.layers.0
25-01-23 09:55:31 | I | collecting calibration activations in model.layers.0
25-01-23 09:55:31 | I | collecting calibration activations in model.layers.0
25-01-23 09:55:32 | I | collecting calibration activations in model.layers.0
25-01-23 09:55:32 | I | collecting calibration activations in model.layers.0
25-01-23 09:55:32 | I | collecting calibration activations in model.layers.0
25-01-23 09:55:32 | I | collecting calibration activations in model.layers.0
25-01-23 09:55:32 | I | collecting calibration activations in model.layers.0
25-01-23 09:55:32 | I | collecting calibration activations in model.layers.0
25-01-23 09:55:32 | I | collecting calibration activations in model.layers.0
25-01-23 09:55:32 | I | collecting calibration activations in model.layers.0
25-01-23 09:55:32 | I | collecting calibration activations in model.layers.0
25-01-23 09:55:32 | I | collecting calibration activations in model.layers.0
25-01-23 09:55:32 | I | collecting calibration activations in model.layers.0
25-01-23 09:55:32 | I | collecting calibration activations in model.layers.0
25-01-23 09:55:32 | I | collecting calibration activations in model.layers.0
25-01-23 09:55:32 | I | collecting calibration activations in model.layers.0
25-01-23 09:55:32 | I | collecting calibration activations in model.layers.0
25-01-23 09:55:33 | I | collecting calibration activations in model.layers.0
25-01-23 09:55:33 | I | collecting calibration activations in model.layers.0
25-01-23 09:55:33 | I | collecting calibration activations in model.layers.0
25-01-23 09:55:33 | I | collecting calibration activations in model.layers.0
25-01-23 09:55:33 | I | collecting calibration activations in model.layers.0
25-01-23 09:55:33 | I | collecting calibration activations in model.layers.0
25-01-23 09:55:33 | I | collecting calibration activations in model.layers.0
25-01-23 09:55:33 | I | collecting calibration activations in model.layers.0
25-01-23 09:55:33 | I | collecting calibration activations in model.layers.0
25-01-23 09:55:33 | I | collecting calibration activations in model.layers.0
25-01-23 09:55:33 | I | collecting calibration activations in model.layers.0
25-01-23 09:55:33 | I | collecting calibration activations in model.layers.0
25-01-23 09:55:33 | I | collecting calibration activations in model.layers.0
25-01-23 09:55:33 | I | collecting calibration activations in model.layers.0
25-01-23 09:55:33 | I | collecting calibration activations in model.layers.0
25-01-23 09:55:33 | I | collecting calibration activations in model.layers.0
25-01-23 09:55:34 | I | collecting calibration activations in model.layers.0
25-01-23 09:55:34 | I | collecting calibration activations in model.layers.0
25-01-23 09:55:34 | I | collecting calibration activations in model.layers.0
25-01-23 09:55:34 | I | collecting calibration activations in model.layers.0
25-01-23 09:55:34 | I | collecting calibration activations in model.layers.0
25-01-23 09:55:34 | I | collecting calibration activations in model.layers.0
25-01-23 09:55:34 | I | collecting calibration activations in model.layers.0
25-01-23 09:55:34 | I | collecting calibration activations in model.layers.0
25-01-23 09:55:34 | I | collecting calibration activations in model.layers.0
25-01-23 09:55:34 | I | collecting calibration activations in model.layers.0
25-01-23 09:55:34 | I | collecting calibration activations in model.layers.0
25-01-23 09:55:34 | I | collecting calibration activations in model.layers.0
25-01-23 09:55:34 | I | collecting calibration activations in model.layers.0
25-01-23 09:55:34 | I | collecting calibration activations in model.layers.0
25-01-23 09:55:34 | I | collecting calibration activations in model.layers.0
25-01-23 09:55:35 | I | collecting calibration activations in model.layers.0
25-01-23 09:55:35 | I | collecting calibration activations in model.layers.0
25-01-23 09:55:35 | I | collecting calibration activations in model.layers.0
25-01-23 09:55:35 | I | collecting calibration activations in model.layers.0
25-01-23 09:55:35 | I | collecting calibration activations in model.layers.0
25-01-23 09:55:35 | I | collecting calibration activations in model.layers.0
25-01-23 09:55:35 | I | collecting calibration activations in model.layers.0
25-01-23 09:55:35 | I | collecting calibration activations in model.layers.0
25-01-23 09:55:35 | I | collecting calibration activations in model.layers.0
25-01-23 09:55:35 | I | collecting calibration activations in model.layers.0
25-01-23 09:55:35 | I | collecting calibration activations in model.layers.0
25-01-23 09:55:35 | I | collecting calibration activations in model.layers.0
25-01-23 09:55:35 | I | collecting calibration activations in model.layers.0
25-01-23 09:55:35 | I | collecting calibration activations in model.layers.0
25-01-23 09:55:35 | I | collecting calibration activations in model.layers.0
25-01-23 09:55:35 | I | collecting calibration activations in model.layers.0
25-01-23 09:55:36 | I | collecting calibration activations in model.layers.0
25-01-23 09:55:36 | I | collecting calibration activations in model.layers.0
25-01-23 09:55:36 | I | collecting calibration activations in model.layers.0
25-01-23 09:55:36 | I | collecting calibration activations in model.layers.0
25-01-23 09:55:36 | I | collecting calibration activations in model.layers.0
25-01-23 09:55:36 | I | collecting calibration activations in model.layers.0
25-01-23 09:55:36 | I | collecting calibration activations in model.layers.0
25-01-23 09:55:36 | I | collecting calibration activations in model.layers.0
25-01-23 09:55:36 | I | collecting calibration activations in model.layers.0
25-01-23 09:55:36 | I | collecting calibration activations in model.layers.0
25-01-23 09:55:36 | I | collecting calibration activations in model.layers.0
25-01-23 09:55:36 | I | collecting calibration activations in model.layers.0
25-01-23 09:55:36 | I | collecting calibration activations in model.layers.0
25-01-23 09:55:36 | I | collecting calibration activations in model.layers.0
25-01-23 09:55:36 | I | collecting calibration activations in model.layers.0
25-01-23 09:55:37 | I | collecting calibration activations in model.layers.0
25-01-23 09:55:37 | I | collecting calibration activations in model.layers.0
25-01-23 09:55:37 | I | collecting calibration activations in model.layers.0
25-01-23 09:55:37 | I | collecting calibration activations in model.layers.0
25-01-23 09:55:37 | I | collecting calibration activations in model.layers.0
25-01-23 09:55:37 | I | collecting calibration activations in model.layers.0
25-01-23 09:55:37 | I | collecting calibration activations in model.layers.0
25-01-23 09:55:37 | I | collecting calibration activations in model.layers.0
25-01-23 09:55:37 | I | collecting calibration activations in model.layers.0
25-01-23 09:55:37 | I | collecting calibration activations in model.layers.0
25-01-23 09:55:37 | I | collecting calibration activations in model.layers.0
25-01-23 09:55:37 | I | collecting calibration activations in model.layers.0
25-01-23 09:55:37 | I | collecting calibration activations in model.layers.0
25-01-23 09:55:37 | I | collecting calibration activations in model.layers.0
25-01-23 09:55:37 | I | collecting calibration activations in model.layers.0
25-01-23 09:55:38 | I | collecting calibration activations in model.layers.0
25-01-23 09:55:38 | I | collecting calibration activations in model.layers.0
25-01-23 09:55:38 | I | collecting calibration activations in model.layers.0
25-01-23 09:55:38 | I | collecting calibration activations in model.layers.0
25-01-23 09:55:38 | I | collecting calibration activations in model.layers.0
25-01-23 09:55:38 | I | collecting calibration activations in model.layers.0
25-01-23 09:55:38 | I | collecting calibration activations in model.layers.0
25-01-23 09:55:38 | I | collecting calibration activations in model.layers.0
25-01-23 09:55:38 | I | collecting calibration activations in model.layers.0
25-01-23 09:55:38 | I | collecting calibration activations in model.layers.0
25-01-23 09:55:38 | I | collecting calibration activations in model.layers.0
25-01-23 09:55:38 | I | collecting calibration activations in model.layers.0
25-01-23 09:55:38 | I | collecting calibration activations in model.layers.0
25-01-23 09:55:38 | I | collecting calibration activations in model.layers.0
25-01-23 09:55:38 | I | collecting calibration activations in model.layers.0
25-01-23 09:55:38 | I | collecting calibration activations in model.layers.0
25-01-23 09:55:39 | I | collecting calibration activations in model.layers.0
25-01-23 09:55:39 | I | collecting calibration activations in model.layers.0
25-01-23 09:55:39 | I | collecting calibration activations in model.layers.0
25-01-23 09:55:39 | I | collecting calibration activations in model.layers.0
25-01-23 09:55:39 | I | collecting calibration activations in model.layers.0
25-01-23 09:55:39 | I | collecting calibration activations in model.layers.0
25-01-23 09:55:39 | I | collecting calibration activations in model.layers.0
25-01-23 09:55:39 | I | collecting calibration activations in model.layers.0
25-01-23 09:55:39 | I | collecting calibration activations in model.layers.0
25-01-23 09:55:39 | I | collecting calibration activations in model.layers.0
25-01-23 09:55:39 | I | collecting calibration activations in model.layers.0
25-01-23 09:55:39 | I | collecting calibration activations in model.layers.0
25-01-23 09:55:39 | I | collecting calibration activations in model.layers.0
25-01-23 09:55:39 | I | collecting calibration activations in model.layers.0
25-01-23 09:55:42 | I | use gptq_eval(lmquant) to calculate ppl, partly quanted
25-01-23 09:55:42 | W |   `pretrained` model kwarg is not of type `str`. Many other model arguments may be ignored. Please do not launch via accelerate or use `parallelize=True` if passing an existing model this way.
25-01-23 09:55:42 | W |   Passed an already-initialized model through `pretrained`, assuming single-process call to evaluate() or custom distributed integration
25-01-23 09:55:42 | I | - Evaluator: gptq
25-01-23 09:55:42 | I | - Tasks: ['wikitext', 'val_valid']
25-01-23 09:55:42 | I | - Batch_size: 8
25-01-23 09:55:42 | I |   + Max_seq_length: 2048
25-01-23 09:57:20 | I |     - Results:
25-01-23 09:57:20 | I |       |  Task   |Version|    Metric     |Value |   |Stderr|
25-01-23 09:57:20 | I |       |---------|------:|---------------|-----:|---|-----:|
25-01-23 09:57:20 | I |       |wikitext |      1|word_perplexity|5.4863|±  |5.4863|
25-01-23 09:57:20 | I |       |val_valid|      1|word_perplexity|5.0461|±  |5.0461|
25-01-23 09:57:20 | I |       
25-01-23 09:57:20 | I |   + Max_seq_length: 4096
25-01-23 09:58:55 | I |     - Results:
25-01-23 09:58:55 | I |       |  Task   |Version|    Metric     |Value |   |Stderr|
25-01-23 09:58:55 | I |       |---------|------:|---------------|-----:|---|-----:|
25-01-23 09:58:55 | I |       |wikitext |      1|word_perplexity|5.1264|±  |5.1264|
25-01-23 09:58:55 | I |       |val_valid|      1|word_perplexity|4.8419|±  |4.8419|
25-01-23 09:58:55 | I |       
25-01-23 09:59:54 | I | epoch 001 | valid on 'valid' subset:    102 / 819200000 loss_valid=2.15, lmquant_ppl_result_val=-1, lmquant_ppl_result_wikitext=-1, ppl=4.44, wps=7233.6, wpb=4058.2, bsz=2, num_updates=100, lmquant_ppl_wikitext_all_quanted=5.48248, lmquant_ppl_val_all_quanted=5.023, lmquant_ppl_wikitext_partly_quanted=5.48632, lmquant_ppl_val_partly_quanted=5.04609
25-01-23 09:59:54 | I | epoch 001 | valid on 'valid' subset | loss_valid 2.15 | lmquant_ppl_result_val -1 | lmquant_ppl_result_wikitext -1 | ppl 4.44 | wps 7233.6 | wpb 4058.2 | bsz 2 | num_updates 100 | lmquant_ppl_wikitext_all_quanted 5.48248 | lmquant_ppl_val_all_quanted 5.023 | lmquant_ppl_wikitext_partly_quanted 5.48632 | lmquant_ppl_val_partly_quanted 5.04609
25-01-23 09:59:54 | I | end of epoch 1 (average epoch stats below)
25-01-23 09:59:54 | I | epoch 001 | loss 1.2752e-05 | loss_per_token 0.0522322 | loss_sum 427.886 | wps 88.7 | ups 0.01 | wpb 8192 | bsz 16 | num_updates 100 | lr 0.00499917 | gnorm 0.523 | clip 8 | loss_scale 8 | train_wall 3865 | cuda_gb_allocated 17.1 | cuda_gb_reserved 18.7 | cuda_gb_free 6.6 | wall 9342
25-01-23 09:59:54 | I | done training in 9338.9 seconds
25-01-23 09:59:55 | W | Repo card metadata block was not found. Setting CardData to empty.
25-01-23 10:00:39 | I | collecting calibration activations in model.layers.0
25-01-23 10:00:39 | I | collecting calibration activations in model.layers.0
25-01-23 10:00:39 | I | collecting calibration activations in model.layers.0
25-01-23 10:00:39 | I | collecting calibration activations in model.layers.0
25-01-23 10:00:39 | I | collecting calibration activations in model.layers.0
25-01-23 10:00:39 | I | collecting calibration activations in model.layers.0
25-01-23 10:00:39 | I | collecting calibration activations in model.layers.0
25-01-23 10:00:40 | I | collecting calibration activations in model.layers.0
25-01-23 10:00:40 | I | collecting calibration activations in model.layers.0
25-01-23 10:00:40 | I | collecting calibration activations in model.layers.0
25-01-23 10:00:40 | I | collecting calibration activations in model.layers.0
25-01-23 10:00:40 | I | collecting calibration activations in model.layers.0
25-01-23 10:00:40 | I | collecting calibration activations in model.layers.0
25-01-23 10:00:40 | I | collecting calibration activations in model.layers.0
25-01-23 10:00:40 | I | collecting calibration activations in model.layers.0
25-01-23 10:00:40 | I | collecting calibration activations in model.layers.0
25-01-23 10:00:40 | I | collecting calibration activations in model.layers.0
25-01-23 10:00:40 | I | collecting calibration activations in model.layers.0
25-01-23 10:00:40 | I | collecting calibration activations in model.layers.0
25-01-23 10:00:40 | I | collecting calibration activations in model.layers.0
25-01-23 10:00:40 | I | collecting calibration activations in model.layers.0
25-01-23 10:00:40 | I | collecting calibration activations in model.layers.0
25-01-23 10:00:40 | I | collecting calibration activations in model.layers.0
25-01-23 10:00:40 | I | collecting calibration activations in model.layers.0
25-01-23 10:00:40 | I | collecting calibration activations in model.layers.0
25-01-23 10:00:40 | I | collecting calibration activations in model.layers.0
25-01-23 10:00:41 | I | collecting calibration activations in model.layers.0
25-01-23 10:00:41 | I | collecting calibration activations in model.layers.0
25-01-23 10:00:41 | I | collecting calibration activations in model.layers.0
25-01-23 10:00:41 | I | collecting calibration activations in model.layers.0
25-01-23 10:00:41 | I | collecting calibration activations in model.layers.0
25-01-23 10:00:41 | I | collecting calibration activations in model.layers.0
25-01-23 10:00:41 | I | collecting calibration activations in model.layers.0
25-01-23 10:00:41 | I | collecting calibration activations in model.layers.0
25-01-23 10:00:41 | I | collecting calibration activations in model.layers.0
25-01-23 10:00:41 | I | collecting calibration activations in model.layers.0
25-01-23 10:00:41 | I | collecting calibration activations in model.layers.0
25-01-23 10:00:41 | I | collecting calibration activations in model.layers.0
25-01-23 10:00:41 | I | collecting calibration activations in model.layers.0
25-01-23 10:00:41 | I | collecting calibration activations in model.layers.0
25-01-23 10:00:41 | I | collecting calibration activations in model.layers.0
25-01-23 10:00:41 | I | collecting calibration activations in model.layers.0
25-01-23 10:00:41 | I | collecting calibration activations in model.layers.0
25-01-23 10:00:41 | I | collecting calibration activations in model.layers.0
25-01-23 10:00:41 | I | collecting calibration activations in model.layers.0
25-01-23 10:00:42 | I | collecting calibration activations in model.layers.0
25-01-23 10:00:42 | I | collecting calibration activations in model.layers.0
25-01-23 10:00:42 | I | collecting calibration activations in model.layers.0
25-01-23 10:00:42 | I | collecting calibration activations in model.layers.0
25-01-23 10:00:42 | I | collecting calibration activations in model.layers.0
25-01-23 10:00:42 | I | collecting calibration activations in model.layers.0
25-01-23 10:00:42 | I | collecting calibration activations in model.layers.0
25-01-23 10:00:42 | I | collecting calibration activations in model.layers.0
25-01-23 10:00:42 | I | collecting calibration activations in model.layers.0
25-01-23 10:00:42 | I | collecting calibration activations in model.layers.0
25-01-23 10:00:42 | I | collecting calibration activations in model.layers.0
25-01-23 10:00:42 | I | collecting calibration activations in model.layers.0
25-01-23 10:00:42 | I | collecting calibration activations in model.layers.0
25-01-23 10:00:42 | I | collecting calibration activations in model.layers.0
25-01-23 10:00:42 | I | collecting calibration activations in model.layers.0
25-01-23 10:00:42 | I | collecting calibration activations in model.layers.0
25-01-23 10:00:42 | I | collecting calibration activations in model.layers.0
25-01-23 10:00:42 | I | collecting calibration activations in model.layers.0
25-01-23 10:00:42 | I | collecting calibration activations in model.layers.0
25-01-23 10:00:43 | I | collecting calibration activations in model.layers.0
25-01-23 10:00:43 | I | collecting calibration activations in model.layers.0
25-01-23 10:00:43 | I | collecting calibration activations in model.layers.0
25-01-23 10:00:43 | I | collecting calibration activations in model.layers.0
25-01-23 10:00:43 | I | collecting calibration activations in model.layers.0
25-01-23 10:00:43 | I | collecting calibration activations in model.layers.0
25-01-23 10:00:43 | I | collecting calibration activations in model.layers.0
25-01-23 10:00:43 | I | collecting calibration activations in model.layers.0
25-01-23 10:00:43 | I | collecting calibration activations in model.layers.0
25-01-23 10:00:43 | I | collecting calibration activations in model.layers.0
25-01-23 10:00:43 | I | collecting calibration activations in model.layers.0
25-01-23 10:00:43 | I | collecting calibration activations in model.layers.0
25-01-23 10:00:43 | I | collecting calibration activations in model.layers.0
25-01-23 10:00:43 | I | collecting calibration activations in model.layers.0
25-01-23 10:00:43 | I | collecting calibration activations in model.layers.0
25-01-23 10:00:43 | I | collecting calibration activations in model.layers.0
25-01-23 10:00:43 | I | collecting calibration activations in model.layers.0
25-01-23 10:00:43 | I | collecting calibration activations in model.layers.0
25-01-23 10:00:43 | I | collecting calibration activations in model.layers.0
25-01-23 10:00:44 | I | collecting calibration activations in model.layers.0
25-01-23 10:00:44 | I | collecting calibration activations in model.layers.0
25-01-23 10:00:44 | I | collecting calibration activations in model.layers.0
25-01-23 10:00:44 | I | collecting calibration activations in model.layers.0
25-01-23 10:00:44 | I | collecting calibration activations in model.layers.0
25-01-23 10:00:44 | I | collecting calibration activations in model.layers.0
25-01-23 10:00:44 | I | collecting calibration activations in model.layers.0
25-01-23 10:00:44 | I | collecting calibration activations in model.layers.0
25-01-23 10:00:44 | I | collecting calibration activations in model.layers.0
25-01-23 10:00:44 | I | collecting calibration activations in model.layers.0
25-01-23 10:00:44 | I | collecting calibration activations in model.layers.0
25-01-23 10:00:44 | I | collecting calibration activations in model.layers.0
25-01-23 10:00:44 | I | collecting calibration activations in model.layers.0
25-01-23 10:00:44 | I | collecting calibration activations in model.layers.0
25-01-23 10:00:44 | I | collecting calibration activations in model.layers.0
25-01-23 10:00:44 | I | collecting calibration activations in model.layers.0
25-01-23 10:00:44 | I | collecting calibration activations in model.layers.0
25-01-23 10:00:44 | I | collecting calibration activations in model.layers.0
25-01-23 10:00:44 | I | collecting calibration activations in model.layers.0
25-01-23 10:00:45 | I | collecting calibration activations in model.layers.0
25-01-23 10:00:45 | I | collecting calibration activations in model.layers.0
25-01-23 10:00:45 | I | collecting calibration activations in model.layers.0
25-01-23 10:00:45 | I | collecting calibration activations in model.layers.0
25-01-23 10:00:45 | I | collecting calibration activations in model.layers.0
25-01-23 10:00:45 | I | collecting calibration activations in model.layers.0
25-01-23 10:00:45 | I | collecting calibration activations in model.layers.0
25-01-23 10:00:45 | I | collecting calibration activations in model.layers.0
25-01-23 10:00:45 | I | collecting calibration activations in model.layers.0
25-01-23 10:00:45 | I | collecting calibration activations in model.layers.0
25-01-23 10:00:45 | I | collecting calibration activations in model.layers.0
25-01-23 10:00:45 | I | collecting calibration activations in model.layers.0
25-01-23 10:00:45 | I | collecting calibration activations in model.layers.0
25-01-23 10:00:45 | I | collecting calibration activations in model.layers.0
25-01-23 10:00:45 | I | collecting calibration activations in model.layers.0
25-01-23 10:00:45 | I | collecting calibration activations in model.layers.0
25-01-23 10:00:45 | I | collecting calibration activations in model.layers.0
25-01-23 10:00:45 | I | collecting calibration activations in model.layers.0
25-01-23 10:00:45 | I | collecting calibration activations in model.layers.0
25-01-23 10:00:46 | I | collecting calibration activations in model.layers.0
25-01-23 10:00:46 | I | collecting calibration activations in model.layers.0
25-01-23 10:00:46 | I | collecting calibration activations in model.layers.0
25-01-23 10:00:46 | I | collecting calibration activations in model.layers.0
25-01-23 10:00:46 | I | collecting calibration activations in model.layers.0
25-01-23 10:00:46 | I | collecting calibration activations in model.layers.0
25-01-23 10:00:46 | I | collecting calibration activations in model.layers.0
25-01-23 10:00:48 | I |       - range scale = [    1.0000]
25-01-23 10:00:48 | I |         sum  error  = [    0.1562]
25-01-23 10:00:48 | I |         best error  = [    0.1562]
25-01-23 10:00:48 | I |     + error = [0.1562]
25-01-23 10:00:49 | I |       - range scale = [    1.0000]
25-01-23 10:00:49 | I |         sum  error  = [    2.6738]
25-01-23 10:00:49 | I |         best error  = [    2.6738]
25-01-23 10:00:49 | I |     + error = [2.6738]
25-01-23 10:00:51 | I |       - range scale = [    1.0000]
25-01-23 10:00:51 | I |         sum  error  = [    0.1865]
25-01-23 10:00:51 | I |         best error  = [    0.1865]
25-01-23 10:00:51 | I |     + error = [0.1865]
25-01-23 10:00:52 | I |       - range scale = [    1.0000]
25-01-23 10:00:52 | I |         sum  error  = [    3.6993]
25-01-23 10:00:52 | I |         best error  = [    3.6993]
25-01-23 10:00:52 | I |     + error = [3.6993]
25-01-23 10:00:53 | I |       - range scale = [    1.0000]
25-01-23 10:00:53 | I |         sum  error  = [    0.6442]
25-01-23 10:00:53 | I |         best error  = [    0.6442]
25-01-23 10:00:53 | I |     + error = [0.6442]
25-01-23 10:00:54 | I |       - range scale = [    1.0000]
25-01-23 10:00:54 | I |         sum  error  = [    5.9245]
25-01-23 10:00:54 | I |         best error  = [    5.9245]
25-01-23 10:00:54 | I |     + error = [5.9245]
25-01-23 10:00:55 | I |       - range scale = [    1.0000]
25-01-23 10:00:55 | I |         sum  error  = [    0.1459]
25-01-23 10:00:55 | I |         best error  = [    0.1459]
25-01-23 10:00:55 | I |     + error = [0.1459]
25-01-23 10:00:56 | I |       - range scale = [    1.0000]
25-01-23 10:00:56 | I |         sum  error  = [    1.2002]
25-01-23 10:00:56 | I |         best error  = [    1.2002]
25-01-23 10:00:56 | I |     + error = [1.2002]
25-01-23 10:00:57 | I |       - range scale = [    1.0000]
25-01-23 10:00:57 | I |         sum  error  = [    1.0935]
25-01-23 10:00:57 | I |         best error  = [    1.0935]
25-01-23 10:00:57 | I |     + error = [1.0935]
25-01-23 10:00:58 | I |       - range scale = [    1.0000]
25-01-23 10:00:58 | I |         sum  error  = [   11.6511]
25-01-23 10:00:58 | I |         best error  = [   11.6511]
25-01-23 10:00:58 | I |     + error = [11.6511]
25-01-23 10:00:59 | I |       - range scale = [    1.0000]
25-01-23 10:00:59 | I |         sum  error  = [    1.1586]
25-01-23 10:00:59 | I |         best error  = [    1.1586]
25-01-23 10:00:59 | I |     + error = [1.1586]
25-01-23 10:01:00 | I |       - range scale = [    1.0000]
25-01-23 10:01:00 | I |         sum  error  = [   12.0345]
25-01-23 10:01:00 | I |         best error  = [   12.0345]
25-01-23 10:01:00 | I |     + error = [12.0345]
25-01-23 10:01:01 | I |       - range scale = [    1.0000]
25-01-23 10:01:01 | I |         sum  error  = [    0.1842]
25-01-23 10:01:01 | I |         best error  = [    0.1842]
25-01-23 10:01:01 | I |     + error = [0.1842]
25-01-23 10:01:02 | I |       - range scale = [    1.0000]
25-01-23 10:01:02 | I |         sum  error  = [    1.8164]
25-01-23 10:01:02 | I |         best error  = [    1.8164]
25-01-23 10:01:02 | I |     + error = [1.8164]
25-01-23 10:01:26 | I | after training, quantize activations for layer model.layers.0
25-01-23 10:01:27 | I | collecting calibration activations in model.layers.0
25-01-23 10:01:27 | I | collecting calibration activations in model.layers.0
25-01-23 10:01:28 | I | collecting calibration activations in model.layers.0
25-01-23 10:01:28 | I | collecting calibration activations in model.layers.0
25-01-23 10:01:28 | I | collecting calibration activations in model.layers.0
25-01-23 10:01:28 | I | collecting calibration activations in model.layers.0
25-01-23 10:01:28 | I | collecting calibration activations in model.layers.0
25-01-23 10:01:28 | I | collecting calibration activations in model.layers.0
25-01-23 10:01:28 | I | collecting calibration activations in model.layers.0
25-01-23 10:01:28 | I | collecting calibration activations in model.layers.0
25-01-23 10:01:28 | I | collecting calibration activations in model.layers.0
25-01-23 10:01:28 | I | collecting calibration activations in model.layers.0
25-01-23 10:01:28 | I | collecting calibration activations in model.layers.0
25-01-23 10:01:28 | I | collecting calibration activations in model.layers.0
25-01-23 10:01:28 | I | collecting calibration activations in model.layers.0
25-01-23 10:01:28 | I | collecting calibration activations in model.layers.0
25-01-23 10:01:28 | I | collecting calibration activations in model.layers.0
25-01-23 10:01:28 | I | collecting calibration activations in model.layers.0
25-01-23 10:01:29 | I | collecting calibration activations in model.layers.0
25-01-23 10:01:29 | I | collecting calibration activations in model.layers.0
25-01-23 10:01:29 | I | collecting calibration activations in model.layers.0
25-01-23 10:01:29 | I | collecting calibration activations in model.layers.0
25-01-23 10:01:29 | I | collecting calibration activations in model.layers.0
25-01-23 10:01:29 | I | collecting calibration activations in model.layers.0
25-01-23 10:01:29 | I | collecting calibration activations in model.layers.0
25-01-23 10:01:29 | I | collecting calibration activations in model.layers.0
25-01-23 10:01:29 | I | collecting calibration activations in model.layers.0
25-01-23 10:01:29 | I | collecting calibration activations in model.layers.0
25-01-23 10:01:29 | I | collecting calibration activations in model.layers.0
25-01-23 10:01:29 | I | collecting calibration activations in model.layers.0
25-01-23 10:01:29 | I | collecting calibration activations in model.layers.0
25-01-23 10:01:29 | I | collecting calibration activations in model.layers.0
25-01-23 10:01:29 | I | collecting calibration activations in model.layers.0
25-01-23 10:01:30 | I | collecting calibration activations in model.layers.0
25-01-23 10:01:30 | I | collecting calibration activations in model.layers.0
25-01-23 10:01:30 | I | collecting calibration activations in model.layers.0
25-01-23 10:01:30 | I | collecting calibration activations in model.layers.0
25-01-23 10:01:30 | I | collecting calibration activations in model.layers.0
25-01-23 10:01:30 | I | collecting calibration activations in model.layers.0
25-01-23 10:01:30 | I | collecting calibration activations in model.layers.0
25-01-23 10:01:30 | I | collecting calibration activations in model.layers.0
25-01-23 10:01:30 | I | collecting calibration activations in model.layers.0
25-01-23 10:01:30 | I | collecting calibration activations in model.layers.0
25-01-23 10:01:30 | I | collecting calibration activations in model.layers.0
25-01-23 10:01:30 | I | collecting calibration activations in model.layers.0
25-01-23 10:01:30 | I | collecting calibration activations in model.layers.0
25-01-23 10:01:30 | I | collecting calibration activations in model.layers.0
25-01-23 10:01:30 | I | collecting calibration activations in model.layers.0
25-01-23 10:01:31 | I | collecting calibration activations in model.layers.0
25-01-23 10:01:31 | I | collecting calibration activations in model.layers.0
25-01-23 10:01:31 | I | collecting calibration activations in model.layers.0
25-01-23 10:01:31 | I | collecting calibration activations in model.layers.0
25-01-23 10:01:31 | I | collecting calibration activations in model.layers.0
25-01-23 10:01:31 | I | collecting calibration activations in model.layers.0
25-01-23 10:01:31 | I | collecting calibration activations in model.layers.0
25-01-23 10:01:31 | I | collecting calibration activations in model.layers.0
25-01-23 10:01:31 | I | collecting calibration activations in model.layers.0
25-01-23 10:01:31 | I | collecting calibration activations in model.layers.0
25-01-23 10:01:31 | I | collecting calibration activations in model.layers.0
25-01-23 10:01:31 | I | collecting calibration activations in model.layers.0
25-01-23 10:01:31 | I | collecting calibration activations in model.layers.0
25-01-23 10:01:31 | I | collecting calibration activations in model.layers.0
25-01-23 10:01:31 | I | collecting calibration activations in model.layers.0
25-01-23 10:01:31 | I | collecting calibration activations in model.layers.0
25-01-23 10:01:32 | I | collecting calibration activations in model.layers.0
25-01-23 10:01:32 | I | collecting calibration activations in model.layers.0
25-01-23 10:01:32 | I | collecting calibration activations in model.layers.0
25-01-23 10:01:32 | I | collecting calibration activations in model.layers.0
25-01-23 10:01:32 | I | collecting calibration activations in model.layers.0
25-01-23 10:01:32 | I | collecting calibration activations in model.layers.0
25-01-23 10:01:32 | I | collecting calibration activations in model.layers.0
25-01-23 10:01:32 | I | collecting calibration activations in model.layers.0
25-01-23 10:01:32 | I | collecting calibration activations in model.layers.0
25-01-23 10:01:32 | I | collecting calibration activations in model.layers.0
25-01-23 10:01:32 | I | collecting calibration activations in model.layers.0
25-01-23 10:01:32 | I | collecting calibration activations in model.layers.0
25-01-23 10:01:32 | I | collecting calibration activations in model.layers.0
25-01-23 10:01:32 | I | collecting calibration activations in model.layers.0
25-01-23 10:01:32 | I | collecting calibration activations in model.layers.0
25-01-23 10:01:32 | I | collecting calibration activations in model.layers.0
25-01-23 10:01:33 | I | collecting calibration activations in model.layers.0
25-01-23 10:01:33 | I | collecting calibration activations in model.layers.0
25-01-23 10:01:33 | I | collecting calibration activations in model.layers.0
25-01-23 10:01:33 | I | collecting calibration activations in model.layers.0
25-01-23 10:01:33 | I | collecting calibration activations in model.layers.0
25-01-23 10:01:33 | I | collecting calibration activations in model.layers.0
25-01-23 10:01:33 | I | collecting calibration activations in model.layers.0
25-01-23 10:01:33 | I | collecting calibration activations in model.layers.0
25-01-23 10:01:33 | I | collecting calibration activations in model.layers.0
25-01-23 10:01:33 | I | collecting calibration activations in model.layers.0
25-01-23 10:01:33 | I | collecting calibration activations in model.layers.0
25-01-23 10:01:33 | I | collecting calibration activations in model.layers.0
25-01-23 10:01:33 | I | collecting calibration activations in model.layers.0
25-01-23 10:01:33 | I | collecting calibration activations in model.layers.0
25-01-23 10:01:33 | I | collecting calibration activations in model.layers.0
25-01-23 10:01:34 | I | collecting calibration activations in model.layers.0
25-01-23 10:01:34 | I | collecting calibration activations in model.layers.0
25-01-23 10:01:34 | I | collecting calibration activations in model.layers.0
25-01-23 10:01:34 | I | collecting calibration activations in model.layers.0
25-01-23 10:01:34 | I | collecting calibration activations in model.layers.0
25-01-23 10:01:34 | I | collecting calibration activations in model.layers.0
25-01-23 10:01:34 | I | collecting calibration activations in model.layers.0
25-01-23 10:01:34 | I | collecting calibration activations in model.layers.0
25-01-23 10:01:34 | I | collecting calibration activations in model.layers.0
25-01-23 10:01:34 | I | collecting calibration activations in model.layers.0
25-01-23 10:01:34 | I | collecting calibration activations in model.layers.0
25-01-23 10:01:34 | I | collecting calibration activations in model.layers.0
25-01-23 10:01:34 | I | collecting calibration activations in model.layers.0
25-01-23 10:01:34 | I | collecting calibration activations in model.layers.0
25-01-23 10:01:34 | I | collecting calibration activations in model.layers.0
25-01-23 10:01:34 | I | collecting calibration activations in model.layers.0
25-01-23 10:01:35 | I | collecting calibration activations in model.layers.0
25-01-23 10:01:35 | I | collecting calibration activations in model.layers.0
25-01-23 10:01:35 | I | collecting calibration activations in model.layers.0
25-01-23 10:01:35 | I | collecting calibration activations in model.layers.0
25-01-23 10:01:35 | I | collecting calibration activations in model.layers.0
25-01-23 10:01:35 | I | collecting calibration activations in model.layers.0
25-01-23 10:01:35 | I | collecting calibration activations in model.layers.0
25-01-23 10:01:35 | I | collecting calibration activations in model.layers.0
25-01-23 10:01:35 | I | collecting calibration activations in model.layers.0
25-01-23 10:01:35 | I | collecting calibration activations in model.layers.0
25-01-23 10:01:35 | I | collecting calibration activations in model.layers.0
25-01-23 10:01:35 | I | collecting calibration activations in model.layers.0
25-01-23 10:01:35 | I | collecting calibration activations in model.layers.0
25-01-23 10:01:35 | I | collecting calibration activations in model.layers.0
25-01-23 10:01:35 | I | collecting calibration activations in model.layers.0
25-01-23 10:01:36 | I | collecting calibration activations in model.layers.0
25-01-23 10:01:36 | I | collecting calibration activations in model.layers.0
25-01-23 10:01:38 | I | error: [Errno 2] No such file or directory: '/data/gyy/lmquant-main/lmquant/data/data_without_preprocess_llama_7b_bs16/json/None_input_args_layer_0.json' when deleting /data/gyy/lmquant-main/lmquant/data/data_without_preprocess_llama_7b_bs16/json/None_input_args_layer_0.json
25-01-23 10:01:38 | I | error: [Errno 2] No such file or directory: '/data/gyy/lmquant-main/lmquant/data/data_without_preprocess_llama_7b_bs16/shard/None_input_args_layer_0' when deleting /data/gyy/lmquant-main/lmquant/data/data_without_preprocess_llama_7b_bs16/shard/None_input_args_layer_0
25-01-23 10:01:38 | I | error: [Errno 2] No such file or directory: '/data/gyy/lmquant-main/lmquant/data/data_without_preprocess_llama_7b_bs16/shard/None_input_kwargs_layer_0.pt' when deleting /data/gyy/lmquant-main/lmquant/data/data_without_preprocess_llama_7b_bs16/shard/None_input_kwargs_layer_0.pt
25-01-23 10:01:38 | I | error: [Errno 2] No such file or directory: '/data/gyy/lmquant-main/lmquant/data/data_without_preprocess_llama_7b_bs16/json/None_teacher_output_layer_0.json' when deleting /data/gyy/lmquant-main/lmquant/data/data_without_preprocess_llama_7b_bs16/json/None_teacher_output_layer_0.json
25-01-23 10:01:38 | I | error: [Errno 2] No such file or directory: '/data/gyy/lmquant-main/lmquant/data/data_without_preprocess_llama_7b_bs16/shard/None_teacher_output_layer_0' when deleting /data/gyy/lmquant-main/lmquant/data/data_without_preprocess_llama_7b_bs16/shard/None_teacher_output_layer_0
25-01-23 10:01:38 | I | LlamaModelDecoderLayer(
  (decoder): LlamaDecoderLayerInFairseq(
    (model): LlamaDecoderLayer(
      (self_attn): LlamaSdpaAttention(
        (q_proj): Linear(in_features=4096, out_features=4096, bias=False)
        (k_proj): Linear(in_features=4096, out_features=4096, bias=False)
        (v_proj): Linear(in_features=4096, out_features=4096, bias=False)
        (o_proj): Linear(in_features=4096, out_features=4096, bias=False)
        (rotary_emb): LlamaRotaryEmbedding()
        (q_rotary_emb): RotaryEmbedding()
        (k_rotary_emb): RotaryEmbedding()
      )
      (mlp): LlamaMLP(
        (gate_proj): Linear(in_features=4096, out_features=11008, bias=False)
        (up_proj): Linear(in_features=4096, out_features=11008, bias=False)
        (down_proj): Linear(in_features=11008, out_features=4096, bias=False)
        (act_fn): SiLU()
      )
      (input_layernorm): LlamaRMSNorm((4096,), eps=1e-05)
      (post_attention_layernorm): LlamaRMSNorm((4096,), eps=1e-05)
    )
  )
  (model): LlamaDecoderLayerInFairseq(
    (model): LlamaDecoderLayer(
      (self_attn): LlamaSdpaAttention(
        (q_proj): Linear(in_features=4096, out_features=4096, bias=False)
        (k_proj): Linear(in_features=4096, out_features=4096, bias=False)
        (v_proj): Linear(in_features=4096, out_features=4096, bias=False)
        (o_proj): Linear(in_features=4096, out_features=4096, bias=False)
        (rotary_emb): LlamaRotaryEmbedding()
        (q_rotary_emb): RotaryEmbedding()
        (k_rotary_emb): RotaryEmbedding()
      )
      (mlp): LlamaMLP(
        (gate_proj): Linear(in_features=4096, out_features=11008, bias=False)
        (up_proj): Linear(in_features=4096, out_features=11008, bias=False)
        (down_proj): Linear(in_features=11008, out_features=4096, bias=False)
        (act_fn): SiLU()
      )
      (input_layernorm): LlamaRMSNorm((4096,), eps=1e-05)
      (post_attention_layernorm): LlamaRMSNorm((4096,), eps=1e-05)
    )
  )
)
25-01-23 10:01:38 | I | task: KDTask
25-01-23 10:01:38 | I | model: LlamaModelDecoderLayer
25-01-23 10:01:38 | I | criterion: MSECriterion
25-01-23 10:01:38 | I | num. non-expert model params: 202,383,360 (num. trained: 202,383,360)
25-01-23 10:01:38 | I | num. expert model params: 0 (num. trained: 0)
25-01-23 10:01:38 | I | nvidia-smi stats: {'gpu_0_mem_used_gb': 2.080078125, 'gpu_1_mem_used_gb': 0.001953125, 'gpu_2_mem_used_gb': 0.001953125, 'gpu_3_mem_used_gb': 0.001953125, 'gpu_4_mem_used_gb': 9.587890625, 'gpu_5_mem_used_gb': 7.173828125, 'gpu_6_mem_used_gb': 7.173828125, 'gpu_7_mem_used_gb': 6.14453125}
25-01-23 10:01:38 | I | detected shared parameter: decoder.model.self_attn.q_proj.weight <- model.model.self_attn.q_proj.weight
25-01-23 10:01:38 | I | detected shared parameter: decoder.model.self_attn.q_proj.bias <- decoder.model.self_attn.k_proj.bias
25-01-23 10:01:38 | I | detected shared parameter: decoder.model.self_attn.q_proj.bias <- decoder.model.self_attn.v_proj.bias
25-01-23 10:01:38 | I | detected shared parameter: decoder.model.self_attn.q_proj.bias <- decoder.model.self_attn.o_proj.bias
25-01-23 10:01:38 | I | detected shared parameter: decoder.model.self_attn.q_proj.bias <- decoder.model.mlp.gate_proj.bias
25-01-23 10:01:38 | I | detected shared parameter: decoder.model.self_attn.q_proj.bias <- decoder.model.mlp.up_proj.bias
25-01-23 10:01:38 | I | detected shared parameter: decoder.model.self_attn.q_proj.bias <- decoder.model.mlp.down_proj.bias
25-01-23 10:01:38 | I | detected shared parameter: decoder.model.self_attn.q_proj.bias <- model.model.self_attn.q_proj.bias
25-01-23 10:01:38 | I | detected shared parameter: decoder.model.self_attn.q_proj.bias <- model.model.self_attn.k_proj.bias
25-01-23 10:01:38 | I | detected shared parameter: decoder.model.self_attn.q_proj.bias <- model.model.self_attn.v_proj.bias
25-01-23 10:01:38 | I | detected shared parameter: decoder.model.self_attn.q_proj.bias <- model.model.self_attn.o_proj.bias
25-01-23 10:01:38 | I | detected shared parameter: decoder.model.self_attn.q_proj.bias <- model.model.mlp.gate_proj.bias
25-01-23 10:01:38 | I | detected shared parameter: decoder.model.self_attn.q_proj.bias <- model.model.mlp.up_proj.bias
25-01-23 10:01:38 | I | detected shared parameter: decoder.model.self_attn.q_proj.bias <- model.model.mlp.down_proj.bias
25-01-23 10:01:38 | I | detected shared parameter: decoder.model.self_attn.k_proj.weight <- model.model.self_attn.k_proj.weight
25-01-23 10:01:38 | I | detected shared parameter: decoder.model.self_attn.v_proj.weight <- model.model.self_attn.v_proj.weight
25-01-23 10:01:38 | I | detected shared parameter: decoder.model.self_attn.o_proj.weight <- model.model.self_attn.o_proj.weight
25-01-23 10:01:38 | I | detected shared parameter: decoder.model.mlp.gate_proj.weight <- model.model.mlp.gate_proj.weight
25-01-23 10:01:38 | I | detected shared parameter: decoder.model.mlp.up_proj.weight <- model.model.mlp.up_proj.weight
25-01-23 10:01:38 | I | detected shared parameter: decoder.model.mlp.down_proj.weight <- model.model.mlp.down_proj.weight
25-01-23 10:01:38 | I | detected shared parameter: decoder.model.input_layernorm.weight <- model.model.input_layernorm.weight
25-01-23 10:01:38 | I | detected shared parameter: decoder.model.post_attention_layernorm.weight <- model.model.post_attention_layernorm.weight
25-01-23 10:01:38 | I | nvidia-smi stats: {'gpu_0_mem_used_gb': 2.080078125, 'gpu_1_mem_used_gb': 0.001953125, 'gpu_2_mem_used_gb': 0.001953125, 'gpu_3_mem_used_gb': 0.001953125, 'gpu_4_mem_used_gb': 9.587890625, 'gpu_5_mem_used_gb': 7.173828125, 'gpu_6_mem_used_gb': 7.173828125, 'gpu_7_mem_used_gb': 6.14453125}
25-01-23 10:01:38 | I | ***********************CUDA enviroments for all 1 workers***********************
25-01-23 10:01:38 | I | rank   0: capabilities =  8.6  ; total memory = 23.691 GB ; name = NVIDIA GeForce RTX 3090                 
25-01-23 10:01:38 | I | ***********************CUDA enviroments for all 1 workers***********************
25-01-23 10:01:38 | I | training on 1 devices (GPUs/TPUs)
25-01-23 10:01:38 | I | max tokens per GPU = None and batch size per GPU = 16
25-01-23 10:01:38 | I | nvidia-smi stats: {'gpu_0_mem_used_gb': 2.080078125, 'gpu_1_mem_used_gb': 0.001953125, 'gpu_2_mem_used_gb': 0.001953125, 'gpu_3_mem_used_gb': 0.001953125, 'gpu_4_mem_used_gb': 9.587890625, 'gpu_5_mem_used_gb': 7.173828125, 'gpu_6_mem_used_gb': 7.173828125, 'gpu_7_mem_used_gb': 6.14453125}
25-01-23 10:01:38 | I | loading train data for epoch 1
25-01-23 10:01:40 | I | *** 
 now in /data/gyy/lmquant-main/lmquant/data/data_without_preprocess_llama_1.1b/shard/val/0.jsonl 
 ***
25-01-23 10:01:40 | I | *** 
 total_rows = 700 
 ***
25-01-23 10:01:40 | I | current row num = 0
25-01-23 10:01:40 | I | in forward_and_gen_teacher_outputs, Start iterating over samples
25-01-23 10:01:40 | I | current row num = 16
25-01-23 10:01:41 | I | current row num = 32
25-01-23 10:01:43 | I | current row num = 48
25-01-23 10:01:45 | I | current row num = 64
25-01-23 10:01:47 | I | current row num = 80
25-01-23 10:01:49 | I | current row num = 96
25-01-23 10:01:51 | I | current row num = 112
25-01-23 10:01:53 | I | current row num = 128
25-01-23 10:01:55 | I | current row num = 144
25-01-23 10:01:57 | I | current row num = 160
25-01-23 10:01:59 | I | current row num = 176
25-01-23 10:02:01 | I | current row num = 192
25-01-23 10:02:03 | I | current row num = 208
25-01-23 10:02:05 | I | current row num = 224
25-01-23 10:02:07 | I | current row num = 240
25-01-23 10:02:09 | I | current row num = 256
25-01-23 10:02:11 | I | current row num = 272
25-01-23 10:02:13 | I | current row num = 288
25-01-23 10:02:15 | I | current row num = 304
25-01-23 10:02:17 | I | current row num = 320
25-01-23 10:02:19 | I | current row num = 336
25-01-23 10:02:21 | I | current row num = 352
25-01-23 10:02:23 | I | current row num = 368
25-01-23 10:02:25 | I | current row num = 384
25-01-23 10:02:27 | I | current row num = 400
25-01-23 10:02:29 | I | current row num = 416
25-01-23 10:02:31 | I | current row num = 432
25-01-23 10:02:33 | I | current row num = 448
25-01-23 10:02:35 | I | current row num = 464
25-01-23 10:02:37 | I | current row num = 480
25-01-23 10:02:39 | I | current row num = 496
25-01-23 10:02:41 | I | current row num = 512
25-01-23 10:02:43 | I | current row num = 528
25-01-23 10:02:45 | I | current row num = 544
25-01-23 10:02:47 | I | current row num = 560
25-01-23 10:02:49 | I | current row num = 576
25-01-23 10:02:51 | I | current row num = 592
25-01-23 10:02:53 | I | current row num = 608
25-01-23 10:02:55 | I | current row num = 624
25-01-23 10:02:57 | I | current row num = 640
25-01-23 10:02:59 | I | current row num = 656
25-01-23 10:03:01 | I | current row num = 672
25-01-23 10:03:04 | I | *** 
 now in /data/gyy/lmquant-main/lmquant/data/data_without_preprocess_llama_1.1b/shard/val/1.jsonl 
 ***
25-01-23 10:03:04 | I | *** 
 total_rows = 241 
 ***
25-01-23 10:03:04 | I | current row num = 0
25-01-23 10:03:06 | I | current row num = 16
25-01-23 10:03:08 | I | current row num = 32
25-01-23 10:03:10 | I | current row num = 48
25-01-23 10:03:12 | I | current row num = 64
25-01-23 10:03:14 | I | current row num = 80
25-01-23 10:03:16 | I | current row num = 96
25-01-23 10:03:18 | I | current row num = 112
25-01-23 10:03:20 | I | current row num = 128
25-01-23 10:03:22 | I | current row num = 144
25-01-23 10:03:24 | I | current row num = 160
25-01-23 10:03:26 | I | current row num = 176
25-01-23 10:03:28 | I | current row num = 192
25-01-23 10:03:30 | I | current row num = 208
25-01-23 10:03:32 | I | current row num = 224
25-01-23 10:03:35 | I | *** 
 now in /data/gyy/lmquant-main/lmquant/data/data_without_preprocess_llama_1.1b/shard/val/2.jsonl 
 ***
25-01-23 10:03:35 | I | *** 
 total_rows = 509 
 ***
25-01-23 10:03:35 | I | current row num = 0
25-01-23 10:03:37 | I | current row num = 16
25-01-23 10:03:39 | I | current row num = 32
25-01-23 10:03:41 | I | current row num = 48
25-01-23 10:03:43 | I | current row num = 64
25-01-23 10:03:45 | I | current row num = 80
25-01-23 10:03:47 | I | current row num = 96
25-01-23 10:03:49 | I | current row num = 112
25-01-23 10:03:51 | I | current row num = 128
25-01-23 10:03:53 | I | current row num = 144
25-01-23 10:03:55 | I | current row num = 160
25-01-23 10:03:56 | I | current row num = 176
25-01-23 10:03:58 | I | current row num = 192
25-01-23 10:04:00 | I | current row num = 208
25-01-23 10:04:02 | I | current row num = 224
25-01-23 10:04:04 | I | current row num = 240
25-01-23 10:04:06 | I | current row num = 256
25-01-23 10:04:08 | I | current row num = 272
25-01-23 10:04:10 | I | current row num = 288
25-01-23 10:04:12 | I | current row num = 304
25-01-23 10:04:14 | I | current row num = 320
25-01-23 10:04:16 | I | current row num = 336
25-01-23 10:04:18 | I | current row num = 352
25-01-23 10:04:20 | I | current row num = 368
25-01-23 10:04:22 | I | current row num = 384
25-01-23 10:04:24 | I | current row num = 400
25-01-23 10:04:26 | I | current row num = 416
25-01-23 10:04:28 | I | current row num = 432
25-01-23 10:04:30 | I | current row num = 448
25-01-23 10:04:32 | I | current row num = 464
25-01-23 10:04:34 | I | current row num = 480
25-01-23 10:04:36 | I | current row num = 496
25-01-23 10:04:39 | I | *** 
 now in /data/gyy/lmquant-main/lmquant/data/data_without_preprocess_llama_1.1b/shard/val/3.jsonl 
 ***
25-01-23 10:04:39 | I | *** 
 total_rows = 274 
 ***
25-01-23 10:04:39 | I | current row num = 0
25-01-23 10:04:41 | I | current row num = 16
25-01-23 10:04:43 | I | current row num = 32
25-01-23 10:04:45 | I | current row num = 48
25-01-23 10:04:47 | I | current row num = 64
25-01-23 10:04:49 | I | current row num = 80
25-01-23 10:04:51 | I | current row num = 96
25-01-23 10:04:53 | I | current row num = 112
25-01-23 10:04:55 | I | current row num = 128
25-01-23 10:04:57 | I | current row num = 144
25-01-23 10:04:59 | I | current row num = 160
25-01-23 10:05:01 | I | current row num = 176
25-01-23 10:05:03 | I | current row num = 192
25-01-23 10:05:05 | I | current row num = 208
25-01-23 10:05:07 | I | current row num = 224
25-01-23 10:05:09 | I | current row num = 240
25-01-23 10:05:11 | I | current row num = 256
25-01-23 10:05:13 | I | *** 
 now in /data/gyy/lmquant-main/lmquant/data/data_without_preprocess_llama_1.1b/shard/val/4.jsonl 
 ***
25-01-23 10:05:13 | I | *** 
 total_rows = 191 
 ***
25-01-23 10:05:13 | I | current row num = 0
25-01-23 10:05:15 | I | current row num = 16
25-01-23 10:05:17 | I | current row num = 32
25-01-23 10:05:19 | I | current row num = 48
25-01-23 10:05:21 | I | current row num = 64
25-01-23 10:05:23 | I | current row num = 80
25-01-23 10:05:25 | I | current row num = 96
25-01-23 10:05:27 | I | current row num = 112
25-01-23 10:05:29 | I | current row num = 128
25-01-23 10:05:31 | I | current row num = 144
25-01-23 10:05:33 | I | current row num = 160
25-01-23 10:05:35 | I | current row num = 176
25-01-23 10:05:38 | I | *** 
 now in /data/gyy/lmquant-main/lmquant/data/data_without_preprocess_llama_1.1b/shard/val/5.jsonl 
 ***
25-01-23 10:05:38 | I | *** 
 total_rows = 211 
 ***
25-01-23 10:05:38 | I | current row num = 0
25-01-23 10:05:40 | I | current row num = 16
25-01-23 10:05:42 | I | current row num = 32
25-01-23 10:05:44 | I | current row num = 48
25-01-23 10:05:46 | I | current row num = 64
25-01-23 10:05:48 | I | current row num = 80
25-01-23 10:05:50 | I | current row num = 96
25-01-23 10:05:52 | I | current row num = 112
25-01-23 10:05:54 | I | current row num = 128
25-01-23 10:05:56 | I | current row num = 144
25-01-23 10:05:58 | I | current row num = 160
25-01-23 10:06:00 | I | current row num = 176
25-01-23 10:06:02 | I | current row num = 192
25-01-23 10:06:04 | I | *** 
 now in /data/gyy/lmquant-main/lmquant/data/data_without_preprocess_llama_1.1b/shard/val/6.jsonl 
 ***
25-01-23 10:06:04 | I | *** 
 total_rows = 241 
 ***
25-01-23 10:06:04 | I | current row num = 0
25-01-23 10:06:06 | I | current row num = 16
25-01-23 10:06:08 | I | current row num = 32
25-01-23 10:06:10 | I | current row num = 48
25-01-23 10:06:12 | I | current row num = 64
25-01-23 10:06:14 | I | current row num = 80
25-01-23 10:06:16 | I | current row num = 96
25-01-23 10:06:18 | I | current row num = 112
25-01-23 10:06:20 | I | current row num = 128
25-01-23 10:06:22 | I | current row num = 144
25-01-23 10:06:24 | I | current row num = 160
25-01-23 10:06:26 | I | current row num = 176
25-01-23 10:06:28 | I | current row num = 192
25-01-23 10:06:30 | I | current row num = 208
25-01-23 10:06:32 | I | current row num = 224
25-01-23 10:06:35 | I | *** 
 now in /data/gyy/lmquant-main/lmquant/data/data_without_preprocess_llama_1.1b/shard/val/7.jsonl 
 ***
25-01-23 10:06:35 | I | *** 
 total_rows = 279 
 ***
25-01-23 10:06:35 | I | current row num = 0
25-01-23 10:06:37 | I | current row num = 16
25-01-23 10:06:39 | I | current row num = 32
25-01-23 10:06:41 | I | current row num = 48
25-01-23 10:06:43 | I | current row num = 64
25-01-23 10:06:45 | I | current row num = 80
25-01-23 10:06:47 | I | current row num = 96
25-01-23 10:06:49 | I | current row num = 112
25-01-23 10:06:51 | I | current row num = 128
25-01-23 10:06:53 | I | current row num = 144
25-01-23 10:06:54 | I | current row num = 160
25-01-23 10:06:56 | I | current row num = 176
25-01-23 10:06:58 | I | current row num = 192
25-01-23 10:07:00 | I | current row num = 208
25-01-23 10:07:02 | I | current row num = 224
25-01-23 10:07:04 | I | current row num = 240
25-01-23 10:07:06 | I | current row num = 256
25-01-23 10:07:08 | I | current row num = 272
25-01-23 10:07:11 | I | *** 
 now in /data/gyy/lmquant-main/lmquant/data/data_without_preprocess_llama_1.1b/shard/val/8.jsonl 
 ***
25-01-23 10:07:11 | I | *** 
 total_rows = 186 
 ***
25-01-23 10:07:11 | I | current row num = 0
25-01-23 10:07:13 | I | current row num = 16
25-01-23 10:07:15 | I | current row num = 32
25-01-23 10:07:17 | I | current row num = 48
25-01-23 10:07:19 | I | current row num = 64
25-01-23 10:07:21 | I | current row num = 80
25-01-23 10:07:23 | I | current row num = 96
25-01-23 10:07:25 | I | current row num = 112
25-01-23 10:07:27 | I | current row num = 128
25-01-23 10:07:29 | I | current row num = 144
25-01-23 10:07:31 | I | current row num = 160
25-01-23 10:07:33 | I | current row num = 176
25-01-23 10:07:36 | I | *** 
 now in /data/gyy/lmquant-main/lmquant/data/data_without_preprocess_llama_1.1b/shard/val/9.jsonl 
 ***
25-01-23 10:07:36 | I | *** 
 total_rows = 770 
 ***
25-01-23 10:07:36 | I | current row num = 0
25-01-23 10:07:38 | I | current row num = 16
25-01-23 10:07:40 | I | current row num = 32
25-01-23 10:07:42 | I | current row num = 48
25-01-23 10:07:44 | I | current row num = 64
25-01-23 10:07:46 | I | current row num = 80
25-01-23 10:07:48 | I | current row num = 96
25-01-23 10:07:50 | I | current row num = 112
25-01-23 10:07:52 | I | current row num = 128
25-01-23 10:07:54 | I | current row num = 144
25-01-23 10:07:56 | I | current row num = 160
25-01-23 10:07:58 | I | current row num = 176
25-01-23 10:08:00 | I | current row num = 192
25-01-23 10:08:02 | I | current row num = 208
25-01-23 10:08:04 | I | current row num = 224
25-01-23 10:08:06 | I | current row num = 240
25-01-23 10:08:08 | I | current row num = 256
25-01-23 10:08:10 | I | current row num = 272
25-01-23 10:08:12 | I | current row num = 288
25-01-23 10:08:14 | I | current row num = 304
25-01-23 10:08:16 | I | current row num = 320
25-01-23 10:08:18 | I | current row num = 336
25-01-23 10:08:20 | I | current row num = 352
25-01-23 10:08:22 | I | current row num = 368
25-01-23 10:08:24 | I | current row num = 384
25-01-23 10:08:26 | I | current row num = 400
25-01-23 10:08:28 | I | current row num = 416
25-01-23 10:08:30 | I | current row num = 432
25-01-23 10:08:32 | I | current row num = 448
25-01-23 10:08:34 | I | current row num = 464
25-01-23 10:08:36 | I | current row num = 480
25-01-23 10:08:38 | I | current row num = 496
25-01-23 10:08:40 | I | current row num = 512
25-01-23 10:08:42 | I | current row num = 528
25-01-23 10:08:44 | I | current row num = 544
25-01-23 10:08:46 | I | current row num = 560
25-01-23 10:08:48 | I | current row num = 576
25-01-23 10:08:50 | I | current row num = 592
25-01-23 10:08:52 | I | current row num = 608
25-01-23 10:08:54 | I | current row num = 624
25-01-23 10:08:56 | I | current row num = 640
25-01-23 10:08:58 | I | current row num = 656
25-01-23 10:09:00 | I | current row num = 672
25-01-23 10:09:02 | I | current row num = 688
25-01-23 10:09:04 | I | current row num = 704
25-01-23 10:09:06 | I | current row num = 720
25-01-23 10:09:08 | I | current row num = 736
25-01-23 10:09:10 | I | current row num = 752
25-01-23 10:09:13 | I | *** 
 now in /data/gyy/lmquant-main/lmquant/data/data_without_preprocess_llama_1.1b/shard/val/10.jsonl 
 ***
25-01-23 10:09:13 | I | *** 
 total_rows = 271 
 ***
25-01-23 10:09:13 | I | current row num = 0
25-01-23 10:09:15 | I | current row num = 16
25-01-23 10:09:17 | I | current row num = 32
25-01-23 10:09:19 | I | current row num = 48
25-01-23 10:09:21 | I | current row num = 64
25-01-23 10:09:23 | I | current row num = 80
25-01-23 10:09:25 | I | current row num = 96
25-01-23 10:09:27 | I | current row num = 112
25-01-23 10:09:29 | I | current row num = 128
25-01-23 10:09:31 | I | current row num = 144
25-01-23 10:09:33 | I | current row num = 160
25-01-23 10:09:35 | I | current row num = 176
25-01-23 10:09:37 | I | current row num = 192
25-01-23 10:09:39 | I | current row num = 208
25-01-23 10:09:41 | I | current row num = 224
25-01-23 10:09:43 | I | current row num = 240
25-01-23 10:09:45 | I | current row num = 256
25-01-23 10:09:47 | I | *** 
 now in /data/gyy/lmquant-main/lmquant/data/data_without_preprocess_llama_1.1b/shard/val/11.jsonl 
 ***
25-01-23 10:09:47 | I | *** 
 total_rows = 273 
 ***
25-01-23 10:09:47 | I | current row num = 0
25-01-23 10:09:49 | I | current row num = 16
25-01-23 10:09:51 | I | current row num = 32
25-01-23 10:09:53 | I | current row num = 48
25-01-23 10:09:55 | I | current row num = 64
25-01-23 10:09:57 | I | current row num = 80
25-01-23 10:09:59 | I | current row num = 96
25-01-23 10:10:01 | I | current row num = 112
25-01-23 10:10:03 | I | current row num = 128
25-01-23 10:10:05 | I | current row num = 144
25-01-23 10:10:07 | I | current row num = 160
25-01-23 10:10:09 | I | current row num = 176
25-01-23 10:10:11 | I | current row num = 192
25-01-23 10:10:13 | I | current row num = 208
25-01-23 10:10:15 | I | current row num = 224
25-01-23 10:10:17 | I | current row num = 240
25-01-23 10:10:19 | I | current row num = 256
25-01-23 10:10:22 | I | *** 
 now in /data/gyy/lmquant-main/lmquant/data/data_without_preprocess_llama_1.1b/shard/val/12.jsonl 
 ***
25-01-23 10:10:22 | I | *** 
 total_rows = 301 
 ***
25-01-23 10:10:22 | I | current row num = 0
25-01-23 10:10:24 | I | current row num = 16
25-01-23 10:10:26 | I | current row num = 32
25-01-23 10:10:28 | I | current row num = 48
25-01-23 10:10:30 | I | current row num = 64
25-01-23 10:10:32 | I | current row num = 80
25-01-23 10:10:34 | I | current row num = 96
25-01-23 10:10:36 | I | current row num = 112
25-01-23 10:10:38 | I | current row num = 128
25-01-23 10:10:40 | I | current row num = 144
25-01-23 10:10:42 | I | current row num = 160
25-01-23 10:10:44 | I | current row num = 176
25-01-23 10:10:46 | I | current row num = 192
25-01-23 10:10:48 | I | current row num = 208
25-01-23 10:10:50 | I | current row num = 224
25-01-23 10:10:52 | I | current row num = 240
25-01-23 10:10:54 | I | current row num = 256
25-01-23 10:10:56 | I | current row num = 272
25-01-23 10:10:58 | I | *** 
 now in /data/gyy/lmquant-main/lmquant/data/data_without_preprocess_llama_1.1b/shard/val/13.jsonl 
 ***
25-01-23 10:10:58 | I | *** 
 total_rows = 239 
 ***
25-01-23 10:10:58 | I | current row num = 0
25-01-23 10:11:00 | I | current row num = 16
25-01-23 10:11:02 | I | current row num = 32
25-01-23 10:11:04 | I | current row num = 48
25-01-23 10:11:06 | I | current row num = 64
25-01-23 10:11:08 | I | current row num = 80
25-01-23 10:11:10 | I | current row num = 96
25-01-23 10:11:12 | I | current row num = 112
25-01-23 10:11:14 | I | current row num = 128
25-01-23 10:11:16 | I | current row num = 144
25-01-23 10:11:18 | I | current row num = 160
25-01-23 10:11:20 | I | current row num = 176
25-01-23 10:11:22 | I | current row num = 192
25-01-23 10:11:24 | I | current row num = 208
25-01-23 10:11:26 | I | current row num = 224
25-01-23 10:11:29 | I | *** 
 now in /data/gyy/lmquant-main/lmquant/data/data_without_preprocess_llama_1.1b/shard/val/14.jsonl 
 ***
25-01-23 10:11:29 | I | *** 
 total_rows = 262 
 ***
25-01-23 10:11:29 | I | current row num = 0
25-01-23 10:11:31 | I | current row num = 16
25-01-23 10:11:33 | I | current row num = 32
25-01-23 10:11:35 | I | current row num = 48
25-01-23 10:11:37 | I | current row num = 64
25-01-23 10:11:39 | I | current row num = 80
25-01-23 10:11:41 | I | current row num = 96
25-01-23 10:11:43 | I | current row num = 112
25-01-23 10:11:45 | I | current row num = 128
25-01-23 10:11:47 | I | current row num = 144
25-01-23 10:11:49 | I | current row num = 160
25-01-23 10:11:51 | I | current row num = 176
25-01-23 10:11:53 | I | current row num = 192
25-01-23 10:11:55 | I | current row num = 208
25-01-23 10:11:57 | I | current row num = 224
25-01-23 10:11:59 | I | current row num = 240
25-01-23 10:12:01 | I | current row num = 256
25-01-23 10:12:03 | I | *** 
 now in /data/gyy/lmquant-main/lmquant/data/data_without_preprocess_llama_1.1b/shard/val/15.jsonl 
 ***
25-01-23 10:12:03 | I | *** 
 total_rows = 253 
 ***
25-01-23 10:12:03 | I | current row num = 0
25-01-23 10:12:05 | I | current row num = 16
25-01-23 10:12:07 | I | current row num = 32
25-01-23 10:12:09 | I | current row num = 48
25-01-23 10:12:11 | I | current row num = 64
25-01-23 10:12:13 | I | current row num = 80
25-01-23 10:12:15 | I | current row num = 96
25-01-23 10:12:17 | I | current row num = 112
25-01-23 10:12:19 | I | current row num = 128
25-01-23 10:12:21 | I | current row num = 144
25-01-23 10:12:23 | I | current row num = 160
25-01-23 10:12:25 | I | current row num = 176
25-01-23 10:12:27 | I | current row num = 192
25-01-23 10:12:29 | I | current row num = 208
25-01-23 10:12:31 | I | current row num = 224
25-01-23 10:12:33 | I | current row num = 240
25-01-23 10:12:36 | I | *** 
 now in /data/gyy/lmquant-main/lmquant/data/data_without_preprocess_llama_1.1b/shard/val/16.jsonl 
 ***
25-01-23 10:12:36 | I | *** 
 total_rows = 523 
 ***
25-01-23 10:12:36 | I | current row num = 0
25-01-23 10:12:38 | I | current row num = 16
25-01-23 10:12:40 | I | current row num = 32
25-01-23 10:12:42 | I | current row num = 48
25-01-23 10:12:44 | I | current row num = 64
25-01-23 10:12:46 | I | current row num = 80
25-01-23 10:12:49 | I | current row num = 96
25-01-23 10:12:51 | I | current row num = 112
25-01-23 10:12:53 | I | current row num = 128
25-01-23 10:12:55 | I | current row num = 144
25-01-23 10:12:57 | I | current row num = 160
25-01-23 10:12:59 | I | current row num = 176
25-01-23 10:13:01 | I | current row num = 192
25-01-23 10:13:03 | I | current row num = 208
25-01-23 10:13:05 | I | current row num = 224
25-01-23 10:13:07 | I | current row num = 240
25-01-23 10:13:09 | I | current row num = 256
25-01-23 10:13:11 | I | current row num = 272
25-01-23 10:13:13 | I | current row num = 288
25-01-23 10:13:15 | I | current row num = 304
25-01-23 10:13:17 | I | current row num = 320
25-01-23 10:13:19 | I | current row num = 336
25-01-23 10:13:20 | I | current row num = 352
25-01-23 10:13:22 | I | current row num = 368
25-01-23 10:13:24 | I | current row num = 384
25-01-23 10:13:26 | I | current row num = 400
25-01-23 10:13:28 | I | current row num = 416
25-01-23 10:13:30 | I | current row num = 432
25-01-23 10:13:32 | I | current row num = 448
25-01-23 10:13:34 | I | current row num = 464
25-01-23 10:13:36 | I | current row num = 480
25-01-23 10:13:38 | I | current row num = 496
25-01-23 10:13:41 | I | *** 
 now in /data/gyy/lmquant-main/lmquant/data/data_without_preprocess_llama_1.1b/shard/val/17.jsonl 
 ***
25-01-23 10:13:41 | I | *** 
 total_rows = 251 
 ***
25-01-23 10:13:41 | I | current row num = 0
25-01-23 10:13:43 | I | current row num = 16
25-01-23 10:13:45 | I | current row num = 32
25-01-23 10:13:47 | I | current row num = 48
25-01-23 10:13:49 | I | current row num = 64
25-01-23 10:13:51 | I | current row num = 80
25-01-23 10:13:53 | I | current row num = 96
25-01-23 10:13:55 | I | current row num = 112
25-01-23 10:13:57 | I | current row num = 128
25-01-23 10:13:59 | I | current row num = 144
25-01-23 10:14:01 | I | current row num = 160
25-01-23 10:14:03 | I | current row num = 176
25-01-23 10:14:05 | I | current row num = 192
25-01-23 10:14:07 | I | current row num = 208
25-01-23 10:14:09 | I | current row num = 224
25-01-23 10:14:11 | I | current row num = 240
25-01-23 10:14:14 | I | *** 
 now in /data/gyy/lmquant-main/lmquant/data/data_without_preprocess_llama_1.1b/shard/val/18.jsonl 
 ***
25-01-23 10:14:14 | I | *** 
 total_rows = 274 
 ***
25-01-23 10:14:14 | I | current row num = 0
25-01-23 10:14:16 | I | current row num = 16
25-01-23 10:14:18 | I | current row num = 32
25-01-23 10:14:20 | I | current row num = 48
25-01-23 10:14:22 | I | current row num = 64
25-01-23 10:14:24 | I | current row num = 80
25-01-23 10:14:26 | I | current row num = 96
25-01-23 10:14:28 | I | current row num = 112
25-01-23 10:14:30 | I | current row num = 128
25-01-23 10:14:32 | I | current row num = 144
25-01-23 10:14:34 | I | current row num = 160
25-01-23 10:14:36 | I | current row num = 176
25-01-23 10:14:38 | I | current row num = 192
25-01-23 10:14:40 | I | current row num = 208
25-01-23 10:14:42 | I | current row num = 224
25-01-23 10:14:44 | I | current row num = 240
25-01-23 10:14:46 | I | current row num = 256
25-01-23 10:14:48 | I | *** 
 now in /data/gyy/lmquant-main/lmquant/data/data_without_preprocess_llama_1.1b/shard/val/19.jsonl 
 ***
25-01-23 10:14:48 | I | *** 
 total_rows = 315 
 ***
25-01-23 10:14:48 | I | current row num = 0
25-01-23 10:14:50 | I | current row num = 16
25-01-23 10:14:52 | I | current row num = 32
25-01-23 10:14:54 | I | current row num = 48
25-01-23 10:14:56 | I | current row num = 64
25-01-23 10:14:58 | I | current row num = 80
25-01-23 10:15:00 | I | current row num = 96
25-01-23 10:15:02 | I | current row num = 112
25-01-23 10:15:04 | I | current row num = 128
25-01-23 10:15:06 | I | current row num = 144
25-01-23 10:15:08 | I | current row num = 160
25-01-23 10:15:10 | I | current row num = 176
25-01-23 10:15:12 | I | current row num = 192
25-01-23 10:15:14 | I | current row num = 208
25-01-23 10:15:16 | I | current row num = 224
25-01-23 10:15:18 | I | current row num = 240
25-01-23 10:15:20 | I | current row num = 256
25-01-23 10:15:22 | I | current row num = 272
25-01-23 10:15:24 | I | current row num = 288
25-01-23 10:15:26 | I | current row num = 304
25-01-23 10:15:28 | I | current row num = 320
25-01-23 10:15:30 | I | loading train data for epoch 1
25-01-23 10:15:31 | I | *** 
 now in /data/gyy/lmquant-main/lmquant/data/data_without_preprocess_llama_1.1b/shard/val/0.jsonl 
 ***
25-01-23 10:15:31 | I | *** 
 total_rows = 700 
 ***
25-01-23 10:15:31 | I | current row num = 0
25-01-23 10:15:31 | I | in forward_and_gen_args_and_kwargs, Start iterating over samples
25-01-23 10:15:31 | I | current row num = 16
25-01-23 10:15:33 | I | current row num = 32
25-01-23 10:15:35 | I | current row num = 48
25-01-23 10:15:37 | I | current row num = 64
25-01-23 10:15:39 | I | current row num = 80
25-01-23 10:15:41 | I | current row num = 96
25-01-23 10:15:43 | I | current row num = 112
25-01-23 10:15:45 | I | current row num = 128
25-01-23 10:15:47 | I | current row num = 144
25-01-23 10:15:49 | I | current row num = 160
25-01-23 10:15:51 | I | current row num = 176
25-01-23 10:15:53 | I | current row num = 192
25-01-23 10:15:55 | I | current row num = 208
25-01-23 10:15:58 | I | current row num = 224
25-01-23 10:16:00 | I | current row num = 240
25-01-23 10:16:02 | I | current row num = 256
25-01-23 10:16:04 | I | current row num = 272
25-01-23 10:16:06 | I | current row num = 288
25-01-23 10:16:08 | I | current row num = 304
25-01-23 10:16:10 | I | current row num = 320
25-01-23 10:16:12 | I | current row num = 336
25-01-23 10:16:14 | I | current row num = 352
25-01-23 10:16:16 | I | current row num = 368
25-01-23 10:16:18 | I | current row num = 384
25-01-23 10:16:20 | I | current row num = 400
25-01-23 10:16:22 | I | current row num = 416
25-01-23 10:16:24 | I | current row num = 432
25-01-23 10:16:26 | I | current row num = 448
25-01-23 10:16:28 | I | current row num = 464
25-01-23 10:16:30 | I | current row num = 480
25-01-23 10:16:32 | I | current row num = 496
25-01-23 10:16:34 | I | current row num = 512
25-01-23 10:16:36 | I | current row num = 528
25-01-23 10:16:38 | I | current row num = 544
25-01-23 10:16:40 | I | current row num = 560
25-01-23 10:16:42 | I | current row num = 576
25-01-23 10:16:44 | I | current row num = 592
25-01-23 10:16:46 | I | current row num = 608
25-01-23 10:16:49 | I | current row num = 624
25-01-23 10:16:51 | I | current row num = 640
25-01-23 10:16:53 | I | current row num = 656
25-01-23 10:16:55 | I | current row num = 672
25-01-23 10:16:57 | I | *** 
 now in /data/gyy/lmquant-main/lmquant/data/data_without_preprocess_llama_1.1b/shard/val/1.jsonl 
 ***
25-01-23 10:16:57 | I | *** 
 total_rows = 241 
 ***
25-01-23 10:16:57 | I | current row num = 0
25-01-23 10:16:59 | I | current row num = 16
25-01-23 10:17:02 | I | current row num = 32
25-01-23 10:17:04 | I | current row num = 48
25-01-23 10:17:06 | I | current row num = 64
25-01-23 10:17:08 | I | current row num = 80
25-01-23 10:17:10 | I | current row num = 96
25-01-23 10:17:12 | I | current row num = 112
25-01-23 10:17:14 | I | current row num = 128
25-01-23 10:17:16 | I | current row num = 144
25-01-23 10:17:18 | I | current row num = 160
25-01-23 10:17:20 | I | current row num = 176
25-01-23 10:17:22 | I | current row num = 192
25-01-23 10:17:24 | I | current row num = 208
25-01-23 10:17:26 | I | current row num = 224
25-01-23 10:17:29 | I | *** 
 now in /data/gyy/lmquant-main/lmquant/data/data_without_preprocess_llama_1.1b/shard/val/2.jsonl 
 ***
25-01-23 10:17:29 | I | *** 
 total_rows = 509 
 ***
25-01-23 10:17:29 | I | current row num = 0
25-01-23 10:17:31 | I | current row num = 16
25-01-23 10:17:33 | I | current row num = 32
25-01-23 10:17:35 | I | current row num = 48
25-01-23 10:17:37 | I | current row num = 64
25-01-23 10:17:39 | I | current row num = 80
25-01-23 10:17:41 | I | current row num = 96
25-01-23 10:17:43 | I | current row num = 112
25-01-23 10:17:45 | I | current row num = 128
25-01-23 10:17:47 | I | current row num = 144
25-01-23 10:17:49 | I | current row num = 160
25-01-23 10:17:51 | I | current row num = 176
25-01-23 10:17:53 | I | current row num = 192
25-01-23 10:17:55 | I | current row num = 208
25-01-23 10:17:58 | I | current row num = 224
25-01-23 10:18:00 | I | current row num = 240
25-01-23 10:18:02 | I | current row num = 256
25-01-23 10:18:04 | I | current row num = 272
25-01-23 10:18:06 | I | current row num = 288
25-01-23 10:18:08 | I | current row num = 304
25-01-23 10:18:10 | I | current row num = 320
25-01-23 10:18:12 | I | current row num = 336
25-01-23 10:18:14 | I | current row num = 352
25-01-23 10:18:16 | I | current row num = 368
25-01-23 10:18:18 | I | current row num = 384
25-01-23 10:18:20 | I | current row num = 400
25-01-23 10:18:22 | I | current row num = 416
25-01-23 10:18:24 | I | current row num = 432
25-01-23 10:18:26 | I | current row num = 448
25-01-23 10:18:28 | I | current row num = 464
25-01-23 10:18:30 | I | current row num = 480
25-01-23 10:18:32 | I | current row num = 496
25-01-23 10:18:35 | I | *** 
 now in /data/gyy/lmquant-main/lmquant/data/data_without_preprocess_llama_1.1b/shard/val/3.jsonl 
 ***
25-01-23 10:18:35 | I | *** 
 total_rows = 274 
 ***
25-01-23 10:18:35 | I | current row num = 0
25-01-23 10:18:37 | I | current row num = 16
25-01-23 10:18:39 | I | current row num = 32
25-01-23 10:18:41 | I | current row num = 48
25-01-23 10:18:43 | I | current row num = 64
25-01-23 10:18:45 | I | current row num = 80
25-01-23 10:18:47 | I | current row num = 96
25-01-23 10:18:49 | I | current row num = 112
25-01-23 10:18:51 | I | current row num = 128
25-01-23 10:18:53 | I | current row num = 144
25-01-23 10:18:55 | I | current row num = 160
25-01-23 10:18:57 | I | current row num = 176
25-01-23 10:18:59 | I | current row num = 192
25-01-23 10:19:01 | I | current row num = 208
25-01-23 10:19:03 | I | current row num = 224
25-01-23 10:19:05 | I | current row num = 240
25-01-23 10:19:07 | I | current row num = 256
25-01-23 10:19:10 | I | *** 
 now in /data/gyy/lmquant-main/lmquant/data/data_without_preprocess_llama_1.1b/shard/val/4.jsonl 
 ***
25-01-23 10:19:10 | I | *** 
 total_rows = 191 
 ***
25-01-23 10:19:10 | I | current row num = 0
25-01-23 10:19:12 | I | current row num = 16
25-01-23 10:19:14 | I | current row num = 32
25-01-23 10:19:16 | I | current row num = 48
25-01-23 10:19:18 | I | current row num = 64
25-01-23 10:19:20 | I | current row num = 80
25-01-23 10:19:22 | I | current row num = 96
25-01-23 10:19:24 | I | current row num = 112
25-01-23 10:19:26 | I | current row num = 128
25-01-23 10:19:28 | I | current row num = 144
25-01-23 10:19:30 | I | current row num = 160
25-01-23 10:19:32 | I | current row num = 176
25-01-23 10:19:35 | I | *** 
 now in /data/gyy/lmquant-main/lmquant/data/data_without_preprocess_llama_1.1b/shard/val/5.jsonl 
 ***
25-01-23 10:19:35 | I | *** 
 total_rows = 211 
 ***
25-01-23 10:19:35 | I | current row num = 0
25-01-23 10:19:37 | I | current row num = 16
25-01-23 10:19:39 | I | current row num = 32
25-01-23 10:19:41 | I | current row num = 48
25-01-23 10:19:43 | I | current row num = 64
25-01-23 10:19:45 | I | current row num = 80
25-01-23 10:19:47 | I | current row num = 96
25-01-23 10:19:49 | I | current row num = 112
25-01-23 10:19:51 | I | current row num = 128
25-01-23 10:19:53 | I | current row num = 144
25-01-23 10:19:55 | I | current row num = 160
25-01-23 10:19:57 | I | current row num = 176
25-01-23 10:19:59 | I | current row num = 192
25-01-23 10:20:02 | I | *** 
 now in /data/gyy/lmquant-main/lmquant/data/data_without_preprocess_llama_1.1b/shard/val/6.jsonl 
 ***
25-01-23 10:20:02 | I | *** 
 total_rows = 241 
 ***
25-01-23 10:20:02 | I | current row num = 0
25-01-23 10:20:04 | I | current row num = 16
25-01-23 10:20:06 | I | current row num = 32
25-01-23 10:20:08 | I | current row num = 48
25-01-23 10:20:10 | I | current row num = 64
25-01-23 10:20:12 | I | current row num = 80
25-01-23 10:20:14 | I | current row num = 96
25-01-23 10:20:16 | I | current row num = 112
25-01-23 10:20:18 | I | current row num = 128
25-01-23 10:20:20 | I | current row num = 144
25-01-23 10:20:22 | I | current row num = 160
25-01-23 10:20:24 | I | current row num = 176
25-01-23 10:20:26 | I | current row num = 192
25-01-23 10:20:28 | I | current row num = 208
25-01-23 10:20:30 | I | current row num = 224
25-01-23 10:20:33 | I | *** 
 now in /data/gyy/lmquant-main/lmquant/data/data_without_preprocess_llama_1.1b/shard/val/7.jsonl 
 ***
25-01-23 10:20:33 | I | *** 
 total_rows = 279 
 ***
25-01-23 10:20:33 | I | current row num = 0
25-01-23 10:20:35 | I | current row num = 16
25-01-23 10:20:37 | I | current row num = 32
25-01-23 10:20:39 | I | current row num = 48
25-01-23 10:20:41 | I | current row num = 64
25-01-23 10:20:43 | I | current row num = 80
25-01-23 10:20:45 | I | current row num = 96
25-01-23 10:20:47 | I | current row num = 112
25-01-23 10:20:49 | I | current row num = 128
25-01-23 10:20:51 | I | current row num = 144
25-01-23 10:20:53 | I | current row num = 160
25-01-23 10:20:55 | I | current row num = 176
25-01-23 10:20:57 | I | current row num = 192
25-01-23 10:20:59 | I | current row num = 208
25-01-23 10:21:01 | I | current row num = 224
25-01-23 10:21:03 | I | current row num = 240
25-01-23 10:21:05 | I | current row num = 256
25-01-23 10:21:07 | I | current row num = 272
25-01-23 10:21:10 | I | *** 
 now in /data/gyy/lmquant-main/lmquant/data/data_without_preprocess_llama_1.1b/shard/val/8.jsonl 
 ***
25-01-23 10:21:10 | I | *** 
 total_rows = 186 
 ***
25-01-23 10:21:10 | I | current row num = 0
25-01-23 10:21:12 | I | current row num = 16
25-01-23 10:21:14 | I | current row num = 32
25-01-23 10:21:16 | I | current row num = 48
25-01-23 10:21:18 | I | current row num = 64
25-01-23 10:21:20 | I | current row num = 80
25-01-23 10:21:22 | I | current row num = 96
25-01-23 10:21:24 | I | current row num = 112
25-01-23 10:21:26 | I | current row num = 128
25-01-23 10:21:28 | I | current row num = 144
25-01-23 10:21:30 | I | current row num = 160
25-01-23 10:21:32 | I | current row num = 176
25-01-23 10:21:36 | I | *** 
 now in /data/gyy/lmquant-main/lmquant/data/data_without_preprocess_llama_1.1b/shard/val/9.jsonl 
 ***
25-01-23 10:21:36 | I | *** 
 total_rows = 770 
 ***
25-01-23 10:21:36 | I | current row num = 0
25-01-23 10:21:38 | I | current row num = 16
25-01-23 10:21:40 | I | current row num = 32
25-01-23 10:21:42 | I | current row num = 48
25-01-23 10:21:44 | I | current row num = 64
25-01-23 10:21:46 | I | current row num = 80
25-01-23 10:21:48 | I | current row num = 96
25-01-23 10:21:50 | I | current row num = 112
25-01-23 10:21:52 | I | current row num = 128
25-01-23 10:21:54 | I | current row num = 144
25-01-23 10:21:56 | I | current row num = 160
25-01-23 10:21:58 | I | current row num = 176
25-01-23 10:22:00 | I | current row num = 192
25-01-23 10:22:02 | I | current row num = 208
25-01-23 10:22:04 | I | current row num = 224
25-01-23 10:22:06 | I | current row num = 240
25-01-23 10:22:08 | I | current row num = 256
25-01-23 10:22:10 | I | current row num = 272
25-01-23 10:22:12 | I | current row num = 288
25-01-23 10:22:14 | I | current row num = 304
25-01-23 10:22:16 | I | current row num = 320
25-01-23 10:22:18 | I | current row num = 336
25-01-23 10:22:20 | I | current row num = 352
25-01-23 10:22:23 | I | current row num = 368
25-01-23 10:22:25 | I | current row num = 384
25-01-23 10:22:27 | I | current row num = 400
25-01-23 10:22:29 | I | current row num = 416
25-01-23 10:22:31 | I | current row num = 432
25-01-23 10:22:33 | I | current row num = 448
25-01-23 10:22:35 | I | current row num = 464
25-01-23 10:22:37 | I | current row num = 480
25-01-23 10:22:39 | I | current row num = 496
25-01-23 10:22:41 | I | current row num = 512
25-01-23 10:22:43 | I | current row num = 528
25-01-23 10:22:45 | I | current row num = 544
25-01-23 10:22:47 | I | current row num = 560
25-01-23 10:22:49 | I | current row num = 576
25-01-23 10:22:51 | I | current row num = 592
25-01-23 10:22:53 | I | current row num = 608
25-01-23 10:22:55 | I | current row num = 624
25-01-23 10:22:57 | I | current row num = 640
25-01-23 10:22:59 | I | current row num = 656
25-01-23 10:23:02 | I | current row num = 672
25-01-23 10:23:04 | I | current row num = 688
25-01-23 10:23:06 | I | current row num = 704
25-01-23 10:23:08 | I | current row num = 720
25-01-23 10:23:10 | I | current row num = 736
25-01-23 10:23:12 | I | current row num = 752
25-01-23 10:23:14 | I | *** 
 now in /data/gyy/lmquant-main/lmquant/data/data_without_preprocess_llama_1.1b/shard/val/10.jsonl 
 ***
25-01-23 10:23:14 | I | *** 
 total_rows = 271 
 ***
25-01-23 10:23:14 | I | current row num = 0
25-01-23 10:23:16 | I | current row num = 16
25-01-23 10:23:18 | I | current row num = 32
25-01-23 10:23:20 | I | current row num = 48
25-01-23 10:23:22 | I | current row num = 64
25-01-23 10:23:24 | I | current row num = 80
25-01-23 10:23:26 | I | current row num = 96
25-01-23 10:23:29 | I | current row num = 112
25-01-23 10:23:31 | I | current row num = 128
25-01-23 10:23:33 | I | current row num = 144
25-01-23 10:23:35 | I | current row num = 160
25-01-23 10:23:37 | I | current row num = 176
25-01-23 10:23:39 | I | current row num = 192
25-01-23 10:23:41 | I | current row num = 208
25-01-23 10:23:43 | I | current row num = 224
25-01-23 10:23:45 | I | current row num = 240
25-01-23 10:23:47 | I | current row num = 256
25-01-23 10:23:50 | I | *** 
 now in /data/gyy/lmquant-main/lmquant/data/data_without_preprocess_llama_1.1b/shard/val/11.jsonl 
 ***
25-01-23 10:23:50 | I | *** 
 total_rows = 273 
 ***
25-01-23 10:23:50 | I | current row num = 0
25-01-23 10:23:52 | I | current row num = 16
25-01-23 10:23:54 | I | current row num = 32
25-01-23 10:23:56 | I | current row num = 48
25-01-23 10:23:58 | I | current row num = 64
25-01-23 10:24:00 | I | current row num = 80
25-01-23 10:24:02 | I | current row num = 96
25-01-23 10:24:04 | I | current row num = 112
25-01-23 10:24:06 | I | current row num = 128
25-01-23 10:24:08 | I | current row num = 144
25-01-23 10:24:10 | I | current row num = 160
25-01-23 10:24:12 | I | current row num = 176
25-01-23 10:24:14 | I | current row num = 192
25-01-23 10:24:16 | I | current row num = 208
25-01-23 10:24:18 | I | current row num = 224
25-01-23 10:24:20 | I | current row num = 240
25-01-23 10:24:22 | I | current row num = 256
25-01-23 10:24:25 | I | *** 
 now in /data/gyy/lmquant-main/lmquant/data/data_without_preprocess_llama_1.1b/shard/val/12.jsonl 
 ***
25-01-23 10:24:25 | I | *** 
 total_rows = 301 
 ***
25-01-23 10:24:25 | I | current row num = 0
25-01-23 10:24:27 | I | current row num = 16
25-01-23 10:24:29 | I | current row num = 32
25-01-23 10:24:31 | I | current row num = 48
25-01-23 10:24:33 | I | current row num = 64
25-01-23 10:24:35 | I | current row num = 80
25-01-23 10:24:37 | I | current row num = 96
25-01-23 10:24:39 | I | current row num = 112
25-01-23 10:24:41 | I | current row num = 128
25-01-23 10:24:43 | I | current row num = 144
25-01-23 10:24:45 | I | current row num = 160
25-01-23 10:24:47 | I | current row num = 176
25-01-23 10:24:50 | I | current row num = 192
25-01-23 10:24:52 | I | current row num = 208
25-01-23 10:24:54 | I | current row num = 224
25-01-23 10:24:56 | I | current row num = 240
25-01-23 10:24:58 | I | current row num = 256
25-01-23 10:25:02 | I | current row num = 272
25-01-23 10:25:04 | I | *** 
 now in /data/gyy/lmquant-main/lmquant/data/data_without_preprocess_llama_1.1b/shard/val/13.jsonl 
 ***
25-01-23 10:25:04 | I | *** 
 total_rows = 239 
 ***
25-01-23 10:25:04 | I | current row num = 0
25-01-23 10:25:06 | I | current row num = 16
25-01-23 10:25:08 | I | current row num = 32
25-01-23 10:25:10 | I | current row num = 48
25-01-23 10:25:12 | I | current row num = 64
25-01-23 10:25:14 | I | current row num = 80
25-01-23 10:25:16 | I | current row num = 96
25-01-23 10:25:18 | I | current row num = 112
25-01-23 10:25:20 | I | current row num = 128
25-01-23 10:25:23 | I | current row num = 144
25-01-23 10:25:25 | I | current row num = 160
25-01-23 10:25:27 | I | current row num = 176
25-01-23 10:25:29 | I | current row num = 192
25-01-23 10:25:31 | I | current row num = 208
25-01-23 10:25:33 | I | current row num = 224
25-01-23 10:25:35 | I | *** 
 now in /data/gyy/lmquant-main/lmquant/data/data_without_preprocess_llama_1.1b/shard/val/14.jsonl 
 ***
25-01-23 10:25:35 | I | *** 
 total_rows = 262 
 ***
25-01-23 10:25:35 | I | current row num = 0
25-01-23 10:25:37 | I | current row num = 16
25-01-23 10:25:39 | I | current row num = 32
25-01-23 10:25:41 | I | current row num = 48
25-01-23 10:25:43 | I | current row num = 64
25-01-23 10:25:45 | I | current row num = 80
25-01-23 10:25:47 | I | current row num = 96
25-01-23 10:25:49 | I | current row num = 112
25-01-23 10:25:51 | I | current row num = 128
25-01-23 10:25:54 | I | current row num = 144
25-01-23 10:25:56 | I | current row num = 160
25-01-23 10:25:58 | I | current row num = 176
25-01-23 10:26:00 | I | current row num = 192
25-01-23 10:26:02 | I | current row num = 208
25-01-23 10:26:04 | I | current row num = 224
25-01-23 10:26:06 | I | current row num = 240
25-01-23 10:26:08 | I | current row num = 256
25-01-23 10:26:10 | I | *** 
 now in /data/gyy/lmquant-main/lmquant/data/data_without_preprocess_llama_1.1b/shard/val/15.jsonl 
 ***
25-01-23 10:26:10 | I | *** 
 total_rows = 253 
 ***
25-01-23 10:26:10 | I | current row num = 0
25-01-23 10:26:12 | I | current row num = 16
25-01-23 10:26:14 | I | current row num = 32
25-01-23 10:26:16 | I | current row num = 48
25-01-23 10:26:18 | I | current row num = 64
25-01-23 10:26:20 | I | current row num = 80
25-01-23 10:26:23 | I | current row num = 96
25-01-23 10:26:25 | I | current row num = 112
25-01-23 10:26:27 | I | current row num = 128
25-01-23 10:26:29 | I | current row num = 144
25-01-23 10:26:31 | I | current row num = 160
25-01-23 10:26:33 | I | current row num = 176
25-01-23 10:26:35 | I | current row num = 192
25-01-23 10:26:37 | I | current row num = 208
25-01-23 10:26:39 | I | current row num = 224
25-01-23 10:26:41 | I | current row num = 240
25-01-23 10:26:44 | I | *** 
 now in /data/gyy/lmquant-main/lmquant/data/data_without_preprocess_llama_1.1b/shard/val/16.jsonl 
 ***
25-01-23 10:26:44 | I | *** 
 total_rows = 523 
 ***
25-01-23 10:26:44 | I | current row num = 0
25-01-23 10:26:46 | I | current row num = 16
25-01-23 10:26:48 | I | current row num = 32
25-01-23 10:26:50 | I | current row num = 48
25-01-23 10:26:52 | I | current row num = 64
25-01-23 10:26:54 | I | current row num = 80
25-01-23 10:26:56 | I | current row num = 96
25-01-23 10:26:58 | I | current row num = 112
25-01-23 10:27:00 | I | current row num = 128
25-01-23 10:27:02 | I | current row num = 144
25-01-23 10:27:04 | I | current row num = 160
25-01-23 10:27:06 | I | current row num = 176
25-01-23 10:27:08 | I | current row num = 192
25-01-23 10:27:10 | I | current row num = 208
25-01-23 10:27:13 | I | current row num = 224
25-01-23 10:27:15 | I | current row num = 240
25-01-23 10:27:17 | I | current row num = 256
25-01-23 10:27:19 | I | current row num = 272
25-01-23 10:27:22 | I | current row num = 288
25-01-23 10:27:24 | I | current row num = 304
25-01-23 10:27:26 | I | current row num = 320
25-01-23 10:27:28 | I | current row num = 336
25-01-23 10:27:30 | I | current row num = 352
25-01-23 10:27:32 | I | current row num = 368
25-01-23 10:27:34 | I | current row num = 384
25-01-23 10:27:36 | I | current row num = 400
25-01-23 10:27:38 | I | current row num = 416
25-01-23 10:27:40 | I | current row num = 432
25-01-23 10:27:42 | I | current row num = 448
25-01-23 10:27:44 | I | current row num = 464
25-01-23 10:27:46 | I | current row num = 480
25-01-23 10:27:48 | I | current row num = 496
25-01-23 10:27:51 | I | *** 
 now in /data/gyy/lmquant-main/lmquant/data/data_without_preprocess_llama_1.1b/shard/val/17.jsonl 
 ***
25-01-23 10:27:51 | I | *** 
 total_rows = 251 
 ***
25-01-23 10:27:51 | I | current row num = 0
25-01-23 10:27:53 | I | current row num = 16
25-01-23 10:27:55 | I | current row num = 32
25-01-23 10:27:57 | I | current row num = 48
25-01-23 10:27:59 | I | current row num = 64
25-01-23 10:28:01 | I | current row num = 80
25-01-23 10:28:03 | I | current row num = 96
25-01-23 10:28:05 | I | current row num = 112
25-01-23 10:28:07 | I | current row num = 128
25-01-23 10:28:09 | I | current row num = 144
25-01-23 10:28:11 | I | current row num = 160
25-01-23 10:28:13 | I | current row num = 176
25-01-23 10:28:15 | I | current row num = 192
25-01-23 10:28:17 | I | current row num = 208
25-01-23 10:28:19 | I | current row num = 224
25-01-23 10:28:21 | I | current row num = 240
25-01-23 10:28:24 | I | *** 
 now in /data/gyy/lmquant-main/lmquant/data/data_without_preprocess_llama_1.1b/shard/val/18.jsonl 
 ***
25-01-23 10:28:24 | I | *** 
 total_rows = 274 
 ***
25-01-23 10:28:24 | I | current row num = 0
25-01-23 10:28:26 | I | current row num = 16
25-01-23 10:28:28 | I | current row num = 32
25-01-23 10:28:30 | I | current row num = 48
25-01-23 10:28:32 | I | current row num = 64
25-01-23 10:28:34 | I | current row num = 80
25-01-23 10:28:36 | I | current row num = 96
25-01-23 10:28:38 | I | current row num = 112
25-01-23 10:28:40 | I | current row num = 128
25-01-23 10:28:42 | I | current row num = 144
25-01-23 10:28:44 | I | current row num = 160
25-01-23 10:28:46 | I | current row num = 176
25-01-23 10:28:48 | I | current row num = 192
25-01-23 10:28:50 | I | current row num = 208
25-01-23 10:28:52 | I | current row num = 224
25-01-23 10:28:54 | I | current row num = 240
25-01-23 10:28:56 | I | current row num = 256
25-01-23 10:28:59 | I | *** 
 now in /data/gyy/lmquant-main/lmquant/data/data_without_preprocess_llama_1.1b/shard/val/19.jsonl 
 ***
25-01-23 10:28:59 | I | *** 
 total_rows = 315 
 ***
25-01-23 10:28:59 | I | current row num = 0
25-01-23 10:29:01 | I | current row num = 16
25-01-23 10:29:03 | I | current row num = 32
25-01-23 10:29:05 | I | current row num = 48
25-01-23 10:29:07 | I | current row num = 64
25-01-23 10:29:09 | I | current row num = 80
25-01-23 10:29:11 | I | current row num = 96
25-01-23 10:29:13 | I | current row num = 112
25-01-23 10:29:15 | I | current row num = 128
25-01-23 10:29:17 | I | current row num = 144
25-01-23 10:29:19 | I | current row num = 160
25-01-23 10:29:21 | I | current row num = 176
25-01-23 10:29:23 | I | current row num = 192
25-01-23 10:29:25 | I | current row num = 208
25-01-23 10:29:27 | I | current row num = 224
25-01-23 10:29:29 | I | current row num = 240
25-01-23 10:29:32 | I | current row num = 256
25-01-23 10:29:34 | I | current row num = 272
25-01-23 10:29:36 | I | current row num = 288
25-01-23 10:29:38 | I | current row num = 304
25-01-23 10:29:40 | I | current row num = 320
25-01-23 10:29:41 | I | No existing checkpoint found checkpoints/checkpoint_last.pt
25-01-23 10:29:41 | I | loading train data for epoch 1
25-01-23 10:29:41 | I | loading valid data for epoch 1
25-01-23 10:29:41 | I | begin training epoch 1
25-01-23 10:29:41 | I | Start iterating over samples
25-01-23 10:29:41 | I | TRAIN CURRENT LAYER_IDX = 1
25-01-23 10:29:41 | I | in layer model.layers.1
25-01-23 10:29:41 | I | quantizing weights for layer model.layers.1
25-01-23 10:29:41 | I | collecting calibration activations in model.layers.1
25-01-23 10:29:41 | I | collecting calibration activations in model.layers.1
25-01-23 10:29:42 | I | - Quantizing decoder layer model.layers.1
25-01-23 10:29:42 | I |   - Calibrating model.layers.1.self_attn.q_proj.weight
25-01-23 10:29:42 | I |       - range scale = [    1.0000]
25-01-23 10:29:42 | I |         sum  error  = [    0.1688]
25-01-23 10:29:42 | I |         best error  = [    0.1688]
25-01-23 10:29:42 | I |     + error = [0.1688]
25-01-23 10:29:43 | I |       - range scale = [    1.0000]
25-01-23 10:29:43 | I |         sum  error  = [    1.6036]
25-01-23 10:29:43 | I |         best error  = [    1.6036]
25-01-23 10:29:43 | I |     + error = [1.6036]
25-01-23 10:29:43 | I |   - Calibrating model.layers.1.self_attn.k_proj.weight
25-01-23 10:29:44 | I |       - range scale = [    1.0000]
25-01-23 10:29:44 | I |         sum  error  = [    0.1679]
25-01-23 10:29:44 | I |         best error  = [    0.1679]
25-01-23 10:29:44 | I |     + error = [0.1679]
25-01-23 10:29:45 | I |       - range scale = [    1.0000]
25-01-23 10:29:45 | I |         sum  error  = [    1.8014]
25-01-23 10:29:45 | I |         best error  = [    1.8014]
25-01-23 10:29:45 | I |     + error = [1.8014]
25-01-23 10:29:45 | I |   - Calibrating model.layers.1.self_attn.v_proj.weight
25-01-23 10:29:46 | I |       - range scale = [    1.0000]
25-01-23 10:29:46 | I |         sum  error  = [    1.3615]
25-01-23 10:29:46 | I |         best error  = [    1.3615]
25-01-23 10:29:46 | I |     + error = [1.3615]
25-01-23 10:29:47 | I |       - range scale = [    1.0000]
25-01-23 10:29:47 | I |         sum  error  = [    9.2344]
25-01-23 10:29:47 | I |         best error  = [    9.2344]
25-01-23 10:29:47 | I |     + error = [9.2344]
25-01-23 10:29:47 | I |   - Calibrating model.layers.1.self_attn.o_proj.weight
25-01-23 10:29:48 | I |       - range scale = [    1.0000]
25-01-23 10:29:48 | I |         sum  error  = [    0.2274]
25-01-23 10:29:48 | I |         best error  = [    0.2274]
25-01-23 10:29:48 | I |     + error = [0.2274]
25-01-23 10:29:48 | I |       - range scale = [    1.0000]
25-01-23 10:29:48 | I |         sum  error  = [    2.0190]
25-01-23 10:29:48 | I |         best error  = [    2.0190]
25-01-23 10:29:48 | I |     + error = [2.0190]
25-01-23 10:29:49 | I |   - Calibrating model.layers.1.mlp.up_proj.weight
25-01-23 10:29:49 | I |       - range scale = [    1.0000]
25-01-23 10:29:49 | I |         sum  error  = [    1.7770]
25-01-23 10:29:49 | I |         best error  = [    1.7770]
25-01-23 10:29:49 | I |     + error = [1.7770]
25-01-23 10:29:50 | I |       - range scale = [    1.0000]
25-01-23 10:29:50 | I |         sum  error  = [   18.9031]
25-01-23 10:29:50 | I |         best error  = [   18.9031]
25-01-23 10:29:50 | I |     + error = [18.9031]
25-01-23 10:29:51 | I |   - Calibrating model.layers.1.mlp.gate_proj.weight
25-01-23 10:29:52 | I |       - range scale = [    1.0000]
25-01-23 10:29:52 | I |         sum  error  = [    1.9353]
25-01-23 10:29:52 | I |         best error  = [    1.9353]
25-01-23 10:29:52 | I |     + error = [1.9353]
25-01-23 10:29:53 | I |       - range scale = [    1.0000]
25-01-23 10:29:53 | I |         sum  error  = [   20.1615]
25-01-23 10:29:53 | I |         best error  = [   20.1615]
25-01-23 10:29:53 | I |     + error = [20.1615]
25-01-23 10:29:53 | I |   - Calibrating model.layers.1.mlp.down_proj.weight
25-01-23 10:29:54 | I |       - range scale = [    1.0000]
25-01-23 10:29:54 | I |         sum  error  = [   49.1798]
25-01-23 10:29:54 | I |         best error  = [   49.1798]
25-01-23 10:29:54 | I |     + error = [49.1798]
25-01-23 10:29:55 | I |       - range scale = [    1.0000]
25-01-23 10:29:55 | I |         sum  error  = [  240.7833]
25-01-23 10:29:55 | I |         best error  = [  240.7833]
25-01-23 10:29:55 | I |     + error = [240.7833]
25-01-23 10:29:55 | I |   - Quantizing model.layers.1.self_attn.q_proj.weight
25-01-23 10:29:58 | I |   - Quantizing model.layers.1.self_attn.k_proj.weight
25-01-23 10:30:00 | I |   - Quantizing model.layers.1.self_attn.v_proj.weight
25-01-23 10:30:02 | I |   - Quantizing model.layers.1.self_attn.o_proj.weight
25-01-23 10:30:05 | I |   - Quantizing model.layers.1.mlp.up_proj.weight
25-01-23 10:30:07 | I |   - Quantizing model.layers.1.mlp.gate_proj.weight
25-01-23 10:30:09 | I |   - Quantizing model.layers.1.mlp.down_proj.weight
25-01-23 10:30:15 | I | quantizing activations for layer model.layers.1
25-01-23 10:30:16 | I | collecting calibration activations in model.layers.1
25-01-23 10:30:16 | I | collecting calibration activations in model.layers.1
25-01-23 10:30:17 | I | forward this layer
25-01-23 10:30:17 | I | input_file: /data/gyy/lmquant-main/lmquant/data/data_without_preprocess_llama_7b_bs16/shard/val_input_args_layer_1/00.pt
25-01-23 10:30:17 | I | output_file: /data/gyy/lmquant-main/lmquant/data/data_without_preprocess_llama_7b_bs16/shard/val_teacher_output_layer_1/00.pt
25-01-23 10:30:18 | I | In layer model.layers.1, gradient of model.layers.1.self_attn.q_proj has nan: True
25-01-23 10:30:18 | I | In layer model.layers.1, gradient of model.layers.1.self_attn.q_proj has inf: False
25-01-23 10:30:18 | I | nan: first position tensor([0, 0], device='cuda:0')
25-01-23 10:30:18 | I | In layer model.layers.1, gradient of model.layers.1.self_attn.k_proj has nan: True
25-01-23 10:30:18 | I | In layer model.layers.1, gradient of model.layers.1.self_attn.k_proj has inf: False
25-01-23 10:30:18 | I | nan: first position tensor([0, 0], device='cuda:0')
25-01-23 10:30:18 | I | In layer model.layers.1, gradient of model.layers.1.self_attn.v_proj has nan: True
25-01-23 10:30:18 | I | In layer model.layers.1, gradient of model.layers.1.self_attn.v_proj has inf: False
25-01-23 10:30:18 | I | nan: first position tensor([0, 0], device='cuda:0')
25-01-23 10:30:18 | I | In layer model.layers.1, gradient of model.layers.1.self_attn.o_proj has nan: True
25-01-23 10:30:18 | I | In layer model.layers.1, gradient of model.layers.1.self_attn.o_proj has inf: False
25-01-23 10:30:18 | I | nan: first position tensor([0, 0], device='cuda:0')
25-01-23 10:30:18 | I | In layer model.layers.1, gradient of model.layers.1.mlp.up_proj has nan: True
25-01-23 10:30:18 | I | In layer model.layers.1, gradient of model.layers.1.mlp.up_proj has inf: False
25-01-23 10:30:18 | I | nan: first position tensor([0, 0], device='cuda:0')
25-01-23 10:30:18 | I | In layer model.layers.1, gradient of model.layers.1.mlp.gate_proj has nan: True
25-01-23 10:30:18 | I | In layer model.layers.1, gradient of model.layers.1.mlp.gate_proj has inf: False
25-01-23 10:30:18 | I | nan: first position tensor([0, 0], device='cuda:0')
25-01-23 10:30:18 | I | In layer model.layers.1, gradient of model.layers.1.mlp.down_proj has nan: True
25-01-23 10:30:18 | I | In layer model.layers.1, gradient of model.layers.1.mlp.down_proj has inf: True
25-01-23 10:30:18 | I | inf: first position tensor([   0, 7890], device='cuda:0')
25-01-23 10:30:18 | I | nan: first position tensor([1415,    0], device='cuda:0')
25-01-23 10:30:18 | I | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 64.0
25-01-23 10:30:18 | I | TRAIN CURRENT LAYER_IDX = 1
25-01-23 10:30:18 | I | in layer model.layers.1
25-01-23 10:30:18 | I | quantizing weights for layer model.layers.1
25-01-23 10:30:19 | I | collecting calibration activations in model.layers.1
25-01-23 10:30:19 | I | collecting calibration activations in model.layers.1
25-01-23 10:30:19 | I | - Quantizing decoder layer model.layers.1
25-01-23 10:30:19 | I |   - Calibrating model.layers.1.self_attn.q_proj.weight
25-01-23 10:30:20 | I |       - range scale = [    1.0000]
25-01-23 10:30:20 | I |         sum  error  = [    0.1721]
25-01-23 10:30:20 | I |         best error  = [    0.1721]
25-01-23 10:30:20 | I |     + error = [0.1721]
25-01-23 10:30:20 | I |       - range scale = [    1.0000]
25-01-23 10:30:20 | I |         sum  error  = [    1.6561]
25-01-23 10:30:20 | I |         best error  = [    1.6561]
25-01-23 10:30:20 | I |     + error = [1.6561]
25-01-23 10:30:21 | I |   - Calibrating model.layers.1.self_attn.k_proj.weight
25-01-23 10:30:21 | I |       - range scale = [    1.0000]
25-01-23 10:30:21 | I |         sum  error  = [    0.1764]
25-01-23 10:30:21 | I |         best error  = [    0.1764]
25-01-23 10:30:21 | I |     + error = [0.1764]
25-01-23 10:30:22 | I |       - range scale = [    1.0000]
25-01-23 10:30:22 | I |         sum  error  = [    1.8668]
25-01-23 10:30:22 | I |         best error  = [    1.8668]
25-01-23 10:30:22 | I |     + error = [1.8668]
25-01-23 10:30:22 | I |   - Calibrating model.layers.1.self_attn.v_proj.weight
25-01-23 10:30:23 | I |       - range scale = [    1.0000]
25-01-23 10:30:23 | I |         sum  error  = [    1.3575]
25-01-23 10:30:23 | I |         best error  = [    1.3575]
25-01-23 10:30:23 | I |     + error = [1.3575]
25-01-23 10:30:24 | I |       - range scale = [    1.0000]
25-01-23 10:30:24 | I |         sum  error  = [    9.4468]
25-01-23 10:30:24 | I |         best error  = [    9.4468]
25-01-23 10:30:24 | I |     + error = [9.4468]
25-01-23 10:30:24 | I |   - Calibrating model.layers.1.self_attn.o_proj.weight
25-01-23 10:30:25 | I |       - range scale = [    1.0000]
25-01-23 10:30:25 | I |         sum  error  = [    0.2336]
25-01-23 10:30:25 | I |         best error  = [    0.2336]
25-01-23 10:30:25 | I |     + error = [0.2336]
25-01-23 10:30:25 | I |       - range scale = [    1.0000]
25-01-23 10:30:25 | I |         sum  error  = [    2.1082]
25-01-23 10:30:25 | I |         best error  = [    2.1082]
25-01-23 10:30:26 | I |     + error = [2.1082]
25-01-23 10:30:26 | I |   - Calibrating model.layers.1.mlp.up_proj.weight
25-01-23 10:30:26 | I |       - range scale = [    1.0000]
25-01-23 10:30:26 | I |         sum  error  = [    1.8321]
25-01-23 10:30:26 | I |         best error  = [    1.8321]
25-01-23 10:30:26 | I |     + error = [1.8321]
25-01-23 10:30:28 | I |       - range scale = [    1.0000]
25-01-23 10:30:28 | I |         sum  error  = [   19.4782]
25-01-23 10:30:28 | I |         best error  = [   19.4782]
25-01-23 10:30:28 | I |     + error = [19.4782]
25-01-23 10:30:28 | I |   - Calibrating model.layers.1.mlp.gate_proj.weight
25-01-23 10:30:29 | I |       - range scale = [    1.0000]
25-01-23 10:30:29 | I |         sum  error  = [    1.9945]
25-01-23 10:30:29 | I |         best error  = [    1.9945]
25-01-23 10:30:29 | I |     + error = [1.9945]
25-01-23 10:30:30 | I |       - range scale = [    1.0000]
25-01-23 10:30:30 | I |         sum  error  = [   20.7766]
25-01-23 10:30:30 | I |         best error  = [   20.7766]
25-01-23 10:30:30 | I |     + error = [20.7766]
25-01-23 10:30:30 | I |   - Calibrating model.layers.1.mlp.down_proj.weight
25-01-23 10:30:31 | I |       - range scale = [    1.0000]
25-01-23 10:30:31 | I |         sum  error  = [   47.8901]
25-01-23 10:30:31 | I |         best error  = [   47.8901]
25-01-23 10:30:31 | I |     + error = [47.8901]
25-01-23 10:30:32 | I |       - range scale = [    1.0000]
25-01-23 10:30:32 | I |         sum  error  = [  235.8210]
25-01-23 10:30:32 | I |         best error  = [  235.8210]
25-01-23 10:30:32 | I |     + error = [235.8210]
25-01-23 10:30:32 | I |   - Quantizing model.layers.1.self_attn.q_proj.weight
25-01-23 10:30:34 | I |   - Quantizing model.layers.1.self_attn.k_proj.weight
25-01-23 10:30:37 | I |   - Quantizing model.layers.1.self_attn.v_proj.weight
25-01-23 10:30:39 | I |   - Quantizing model.layers.1.self_attn.o_proj.weight
25-01-23 10:30:41 | I |   - Quantizing model.layers.1.mlp.up_proj.weight
25-01-23 10:30:44 | I |   - Quantizing model.layers.1.mlp.gate_proj.weight
25-01-23 10:30:46 | I |   - Quantizing model.layers.1.mlp.down_proj.weight
25-01-23 10:30:52 | I | quantizing activations for layer model.layers.1
25-01-23 10:30:52 | I | collecting calibration activations in model.layers.1
25-01-23 10:30:52 | I | collecting calibration activations in model.layers.1
25-01-23 10:30:54 | I | forward this layer
25-01-23 10:30:54 | I | input_file: /data/gyy/lmquant-main/lmquant/data/data_without_preprocess_llama_7b_bs16/shard/val_input_args_layer_1/01.pt
25-01-23 10:30:54 | I | output_file: /data/gyy/lmquant-main/lmquant/data/data_without_preprocess_llama_7b_bs16/shard/val_teacher_output_layer_1/01.pt
25-01-23 10:30:55 | I | In layer model.layers.1, gradient of model.layers.1.self_attn.q_proj has nan: True
25-01-23 10:30:55 | I | In layer model.layers.1, gradient of model.layers.1.self_attn.q_proj has inf: False
25-01-23 10:30:55 | I | nan: first position tensor([0, 0], device='cuda:0')
25-01-23 10:30:55 | I | In layer model.layers.1, gradient of model.layers.1.self_attn.k_proj has nan: True
25-01-23 10:30:55 | I | In layer model.layers.1, gradient of model.layers.1.self_attn.k_proj has inf: False
25-01-23 10:30:55 | I | nan: first position tensor([0, 0], device='cuda:0')
25-01-23 10:30:55 | I | In layer model.layers.1, gradient of model.layers.1.self_attn.v_proj has nan: True
25-01-23 10:30:55 | I | In layer model.layers.1, gradient of model.layers.1.self_attn.v_proj has inf: False
25-01-23 10:30:55 | I | nan: first position tensor([0, 0], device='cuda:0')
25-01-23 10:30:55 | I | In layer model.layers.1, gradient of model.layers.1.self_attn.o_proj has nan: True
25-01-23 10:30:55 | I | In layer model.layers.1, gradient of model.layers.1.self_attn.o_proj has inf: False
25-01-23 10:30:55 | I | nan: first position tensor([0, 0], device='cuda:0')
25-01-23 10:30:55 | I | In layer model.layers.1, gradient of model.layers.1.mlp.up_proj has nan: True
25-01-23 10:30:55 | I | In layer model.layers.1, gradient of model.layers.1.mlp.up_proj has inf: False
25-01-23 10:30:55 | I | nan: first position tensor([0, 0], device='cuda:0')
25-01-23 10:30:55 | I | In layer model.layers.1, gradient of model.layers.1.mlp.gate_proj has nan: True
25-01-23 10:30:55 | I | In layer model.layers.1, gradient of model.layers.1.mlp.gate_proj has inf: False
25-01-23 10:30:55 | I | nan: first position tensor([0, 0], device='cuda:0')
25-01-23 10:30:55 | I | In layer model.layers.1, gradient of model.layers.1.mlp.down_proj has nan: True
25-01-23 10:30:55 | I | In layer model.layers.1, gradient of model.layers.1.mlp.down_proj has inf: True
25-01-23 10:30:55 | I | inf: first position tensor([   0, 7890], device='cuda:0')
25-01-23 10:30:55 | I | nan: first position tensor([1415,    0], device='cuda:0')
25-01-23 10:30:55 | I | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 32.0
25-01-23 10:30:55 | I | TRAIN CURRENT LAYER_IDX = 1
25-01-23 10:30:55 | I | in layer model.layers.1
25-01-23 10:30:55 | I | quantizing weights for layer model.layers.1
25-01-23 10:30:55 | I | collecting calibration activations in model.layers.1
25-01-23 10:30:55 | I | collecting calibration activations in model.layers.1
25-01-23 10:30:56 | I | - Quantizing decoder layer model.layers.1
25-01-23 10:30:56 | I |   - Calibrating model.layers.1.self_attn.q_proj.weight
25-01-23 10:30:56 | I |       - range scale = [    1.0000]
25-01-23 10:30:56 | I |         sum  error  = [    0.1716]
25-01-23 10:30:56 | I |         best error  = [    0.1716]
25-01-23 10:30:56 | I |     + error = [0.1716]
25-01-23 10:30:57 | I |       - range scale = [    1.0000]
25-01-23 10:30:57 | I |         sum  error  = [    1.6414]
25-01-23 10:30:57 | I |         best error  = [    1.6414]
25-01-23 10:30:57 | I |     + error = [1.6414]
25-01-23 10:30:57 | I |   - Calibrating model.layers.1.self_attn.k_proj.weight
25-01-23 10:30:58 | I |       - range scale = [    1.0000]
25-01-23 10:30:58 | I |         sum  error  = [    0.1771]
25-01-23 10:30:58 | I |         best error  = [    0.1771]
25-01-23 10:30:58 | I |     + error = [0.1771]
25-01-23 10:30:59 | I |       - range scale = [    1.0000]
25-01-23 10:30:59 | I |         sum  error  = [    1.8848]
25-01-23 10:30:59 | I |         best error  = [    1.8848]
25-01-23 10:30:59 | I |     + error = [1.8848]
25-01-23 10:30:59 | I |   - Calibrating model.layers.1.self_attn.v_proj.weight
25-01-23 10:31:00 | I |       - range scale = [    1.0000]
25-01-23 10:31:00 | I |         sum  error  = [    1.3512]
25-01-23 10:31:00 | I |         best error  = [    1.3512]
25-01-23 10:31:00 | I |     + error = [1.3512]
25-01-23 10:31:01 | I |       - range scale = [    1.0000]
25-01-23 10:31:01 | I |         sum  error  = [    9.5438]
25-01-23 10:31:01 | I |         best error  = [    9.5438]
25-01-23 10:31:01 | I |     + error = [9.5438]
25-01-23 10:31:01 | I |   - Calibrating model.layers.1.self_attn.o_proj.weight
25-01-23 10:31:02 | I |       - range scale = [    1.0000]
25-01-23 10:31:02 | I |         sum  error  = [    0.2339]
25-01-23 10:31:02 | I |         best error  = [    0.2339]
25-01-23 10:31:02 | I |     + error = [0.2339]
25-01-23 10:31:02 | I |       - range scale = [    1.0000]
25-01-23 10:31:02 | I |         sum  error  = [    2.0995]
25-01-23 10:31:02 | I |         best error  = [    2.0995]
25-01-23 10:31:02 | I |     + error = [2.0995]
25-01-23 10:31:03 | I |   - Calibrating model.layers.1.mlp.up_proj.weight
25-01-23 10:31:03 | I |       - range scale = [    1.0000]
25-01-23 10:31:03 | I |         sum  error  = [    1.8424]
25-01-23 10:31:03 | I |         best error  = [    1.8424]
25-01-23 10:31:03 | I |     + error = [1.8424]
25-01-23 10:31:04 | I |       - range scale = [    1.0000]
25-01-23 10:31:04 | I |         sum  error  = [   19.5724]
25-01-23 10:31:04 | I |         best error  = [   19.5724]
25-01-23 10:31:04 | I |     + error = [19.5724]
25-01-23 10:31:05 | I |   - Calibrating model.layers.1.mlp.gate_proj.weight
25-01-23 10:31:06 | I |       - range scale = [    1.0000]
25-01-23 10:31:06 | I |         sum  error  = [    2.0065]
25-01-23 10:31:06 | I |         best error  = [    2.0065]
25-01-23 10:31:06 | I |     + error = [2.0065]
25-01-23 10:31:07 | I |       - range scale = [    1.0000]
25-01-23 10:31:07 | I |         sum  error  = [   20.8769]
25-01-23 10:31:07 | I |         best error  = [   20.8769]
25-01-23 10:31:07 | I |     + error = [20.8769]
25-01-23 10:31:07 | I |   - Calibrating model.layers.1.mlp.down_proj.weight
25-01-23 10:31:08 | I |       - range scale = [    1.0000]
25-01-23 10:31:08 | I |         sum  error  = [   45.4052]
25-01-23 10:31:08 | I |         best error  = [   45.4052]
25-01-23 10:31:08 | I |     + error = [45.4052]
25-01-23 10:31:09 | I |       - range scale = [    1.0000]
25-01-23 10:31:09 | I |         sum  error  = [  223.2004]
25-01-23 10:31:09 | I |         best error  = [  223.2004]
25-01-23 10:31:09 | I |     + error = [223.2004]
25-01-23 10:31:09 | I |   - Quantizing model.layers.1.self_attn.q_proj.weight
25-01-23 10:31:12 | I |   - Quantizing model.layers.1.self_attn.k_proj.weight
25-01-23 10:31:15 | I |   - Quantizing model.layers.1.self_attn.v_proj.weight
25-01-23 10:31:18 | I |   - Quantizing model.layers.1.self_attn.o_proj.weight
25-01-23 10:31:21 | I |   - Quantizing model.layers.1.mlp.up_proj.weight
25-01-23 10:31:24 | I |   - Quantizing model.layers.1.mlp.gate_proj.weight
25-01-23 10:31:27 | I |   - Quantizing model.layers.1.mlp.down_proj.weight
25-01-23 10:31:34 | I | quantizing activations for layer model.layers.1
25-01-23 10:31:34 | I | collecting calibration activations in model.layers.1
25-01-23 10:31:35 | I | collecting calibration activations in model.layers.1
25-01-23 10:31:36 | I | forward this layer
25-01-23 10:31:36 | I | input_file: /data/gyy/lmquant-main/lmquant/data/data_without_preprocess_llama_7b_bs16/shard/val_input_args_layer_1/02.pt
25-01-23 10:31:36 | I | output_file: /data/gyy/lmquant-main/lmquant/data/data_without_preprocess_llama_7b_bs16/shard/val_teacher_output_layer_1/02.pt
25-01-23 10:31:37 | I | In layer model.layers.1, gradient of model.layers.1.self_attn.q_proj has nan: True
25-01-23 10:31:37 | I | In layer model.layers.1, gradient of model.layers.1.self_attn.q_proj has inf: False
25-01-23 10:31:37 | I | nan: first position tensor([0, 0], device='cuda:0')
25-01-23 10:31:37 | I | In layer model.layers.1, gradient of model.layers.1.self_attn.k_proj has nan: True
25-01-23 10:31:37 | I | In layer model.layers.1, gradient of model.layers.1.self_attn.k_proj has inf: False
25-01-23 10:31:37 | I | nan: first position tensor([0, 0], device='cuda:0')
25-01-23 10:31:37 | I | In layer model.layers.1, gradient of model.layers.1.self_attn.v_proj has nan: True
25-01-23 10:31:37 | I | In layer model.layers.1, gradient of model.layers.1.self_attn.v_proj has inf: False
25-01-23 10:31:37 | I | nan: first position tensor([0, 0], device='cuda:0')
25-01-23 10:31:37 | I | In layer model.layers.1, gradient of model.layers.1.self_attn.o_proj has nan: True
25-01-23 10:31:37 | I | In layer model.layers.1, gradient of model.layers.1.self_attn.o_proj has inf: False
25-01-23 10:31:37 | I | nan: first position tensor([0, 0], device='cuda:0')
25-01-23 10:31:37 | I | In layer model.layers.1, gradient of model.layers.1.mlp.up_proj has nan: True
25-01-23 10:31:37 | I | In layer model.layers.1, gradient of model.layers.1.mlp.up_proj has inf: False
25-01-23 10:31:37 | I | nan: first position tensor([0, 0], device='cuda:0')
25-01-23 10:31:37 | I | In layer model.layers.1, gradient of model.layers.1.mlp.gate_proj has nan: True
25-01-23 10:31:37 | I | In layer model.layers.1, gradient of model.layers.1.mlp.gate_proj has inf: False
25-01-23 10:31:37 | I | nan: first position tensor([0, 0], device='cuda:0')
25-01-23 10:31:37 | I | In layer model.layers.1, gradient of model.layers.1.mlp.down_proj has nan: False
25-01-23 10:31:37 | I | In layer model.layers.1, gradient of model.layers.1.mlp.down_proj has inf: True
25-01-23 10:31:37 | I | inf: first position tensor([   0, 7890], device='cuda:0')
25-01-23 10:31:37 | I | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
25-01-23 10:31:37 | I | TRAIN CURRENT LAYER_IDX = 1
25-01-23 10:31:37 | I | in layer model.layers.1
25-01-23 10:31:37 | I | quantizing weights for layer model.layers.1
25-01-23 10:31:37 | I | collecting calibration activations in model.layers.1
25-01-23 10:31:37 | I | collecting calibration activations in model.layers.1
25-01-23 10:31:38 | I | - Quantizing decoder layer model.layers.1
25-01-23 10:31:38 | I |   - Calibrating model.layers.1.self_attn.q_proj.weight
25-01-23 10:31:38 | I |       - range scale = [    1.0000]
25-01-23 10:31:38 | I |         sum  error  = [    0.1695]
25-01-23 10:31:38 | I |         best error  = [    0.1695]
25-01-23 10:31:38 | I |     + error = [0.1695]
25-01-23 10:31:39 | I |       - range scale = [    1.0000]
25-01-23 10:31:39 | I |         sum  error  = [    1.5959]
25-01-23 10:31:39 | I |         best error  = [    1.5959]
25-01-23 10:31:39 | I |     + error = [1.5959]
25-01-23 10:31:39 | I |   - Calibrating model.layers.1.self_attn.k_proj.weight
25-01-23 10:31:40 | I |       - range scale = [    1.0000]
25-01-23 10:31:40 | I |         sum  error  = [    0.1674]
25-01-23 10:31:40 | I |         best error  = [    0.1674]
25-01-23 10:31:40 | I |     + error = [0.1674]
25-01-23 10:31:41 | I |       - range scale = [    1.0000]
25-01-23 10:31:41 | I |         sum  error  = [    1.7743]
25-01-23 10:31:41 | I |         best error  = [    1.7743]
25-01-23 10:31:41 | I |     + error = [1.7743]
25-01-23 10:31:41 | I |   - Calibrating model.layers.1.self_attn.v_proj.weight
25-01-23 10:31:42 | I |       - range scale = [    1.0000]
25-01-23 10:31:42 | I |         sum  error  = [    1.3627]
25-01-23 10:31:42 | I |         best error  = [    1.3627]
25-01-23 10:31:42 | I |     + error = [1.3627]
25-01-23 10:31:42 | I |       - range scale = [    1.0000]
25-01-23 10:31:42 | I |         sum  error  = [    9.2399]
25-01-23 10:31:42 | I |         best error  = [    9.2399]
25-01-23 10:31:42 | I |     + error = [9.2399]
25-01-23 10:31:43 | I |   - Calibrating model.layers.1.self_attn.o_proj.weight
25-01-23 10:31:43 | I |       - range scale = [    1.0000]
25-01-23 10:31:43 | I |         sum  error  = [    0.2269]
25-01-23 10:31:43 | I |         best error  = [    0.2269]
25-01-23 10:31:43 | I |     + error = [0.2269]
25-01-23 10:31:44 | I |       - range scale = [    1.0000]
25-01-23 10:31:44 | I |         sum  error  = [    2.0136]
25-01-23 10:31:44 | I |         best error  = [    2.0136]
25-01-23 10:31:44 | I |     + error = [2.0136]
25-01-23 10:31:44 | I |   - Calibrating model.layers.1.mlp.up_proj.weight
25-01-23 10:31:45 | I |       - range scale = [    1.0000]
25-01-23 10:31:45 | I |         sum  error  = [    1.7844]
25-01-23 10:31:45 | I |         best error  = [    1.7844]
25-01-23 10:31:45 | I |     + error = [1.7844]
25-01-23 10:31:46 | I |       - range scale = [    1.0000]
25-01-23 10:31:46 | I |         sum  error  = [   18.9802]
25-01-23 10:31:46 | I |         best error  = [   18.9802]
25-01-23 10:31:46 | I |     + error = [18.9802]
25-01-23 10:31:46 | I |   - Calibrating model.layers.1.mlp.gate_proj.weight
25-01-23 10:31:47 | I |       - range scale = [    1.0000]
25-01-23 10:31:47 | I |         sum  error  = [    1.9446]
25-01-23 10:31:47 | I |         best error  = [    1.9446]
25-01-23 10:31:47 | I |     + error = [1.9446]
25-01-23 10:31:48 | I |       - range scale = [    1.0000]
25-01-23 10:31:48 | I |         sum  error  = [   20.2474]
25-01-23 10:31:48 | I |         best error  = [   20.2474]
25-01-23 10:31:48 | I |     + error = [20.2474]
25-01-23 10:31:49 | I |   - Calibrating model.layers.1.mlp.down_proj.weight
25-01-23 10:31:49 | I |       - range scale = [    1.0000]
25-01-23 10:31:49 | I |         sum  error  = [   50.3519]
25-01-23 10:31:49 | I |         best error  = [   50.3519]
25-01-23 10:31:49 | I |     + error = [50.3519]
25-01-23 10:31:50 | I |       - range scale = [    1.0000]
25-01-23 10:31:50 | I |         sum  error  = [  246.6967]
25-01-23 10:31:50 | I |         best error  = [  246.6967]
25-01-23 10:31:50 | I |     + error = [246.6967]
25-01-23 10:31:51 | I |   - Quantizing model.layers.1.self_attn.q_proj.weight
25-01-23 10:31:53 | I |   - Quantizing model.layers.1.self_attn.k_proj.weight
25-01-23 10:31:55 | I |   - Quantizing model.layers.1.self_attn.v_proj.weight
25-01-23 10:31:57 | I |   - Quantizing model.layers.1.self_attn.o_proj.weight
25-01-23 10:32:00 | I |   - Quantizing model.layers.1.mlp.up_proj.weight
25-01-23 10:32:02 | I |   - Quantizing model.layers.1.mlp.gate_proj.weight
25-01-23 10:32:04 | I |   - Quantizing model.layers.1.mlp.down_proj.weight
25-01-23 10:32:10 | I | quantizing activations for layer model.layers.1
25-01-23 10:32:11 | I | collecting calibration activations in model.layers.1
25-01-23 10:32:11 | I | collecting calibration activations in model.layers.1
25-01-23 10:32:13 | I | forward this layer
25-01-23 10:32:13 | I | input_file: /data/gyy/lmquant-main/lmquant/data/data_without_preprocess_llama_7b_bs16/shard/val_input_args_layer_1/03.pt
25-01-23 10:32:13 | I | output_file: /data/gyy/lmquant-main/lmquant/data/data_without_preprocess_llama_7b_bs16/shard/val_teacher_output_layer_1/03.pt
25-01-23 10:32:13 | I | In layer model.layers.1, gradient of model.layers.1.self_attn.q_proj has nan: True
25-01-23 10:32:13 | I | In layer model.layers.1, gradient of model.layers.1.self_attn.q_proj has inf: False
25-01-23 10:32:13 | I | nan: first position tensor([0, 0], device='cuda:0')
25-01-23 10:32:13 | I | In layer model.layers.1, gradient of model.layers.1.self_attn.k_proj has nan: True
25-01-23 10:32:13 | I | In layer model.layers.1, gradient of model.layers.1.self_attn.k_proj has inf: False
25-01-23 10:32:13 | I | nan: first position tensor([0, 0], device='cuda:0')
25-01-23 10:32:13 | I | In layer model.layers.1, gradient of model.layers.1.self_attn.v_proj has nan: True
25-01-23 10:32:13 | I | In layer model.layers.1, gradient of model.layers.1.self_attn.v_proj has inf: False
25-01-23 10:32:13 | I | nan: first position tensor([0, 0], device='cuda:0')
25-01-23 10:32:13 | I | In layer model.layers.1, gradient of model.layers.1.self_attn.o_proj has nan: True
25-01-23 10:32:13 | I | In layer model.layers.1, gradient of model.layers.1.self_attn.o_proj has inf: False
25-01-23 10:32:13 | I | nan: first position tensor([0, 0], device='cuda:0')
25-01-23 10:32:13 | I | In layer model.layers.1, gradient of model.layers.1.mlp.up_proj has nan: True
25-01-23 10:32:13 | I | In layer model.layers.1, gradient of model.layers.1.mlp.up_proj has inf: True
25-01-23 10:32:13 | I | inf: first position tensor([1192,    1], device='cuda:0')
25-01-23 10:32:13 | I | nan: first position tensor([1192,    0], device='cuda:0')
25-01-23 10:32:13 | I | In layer model.layers.1, gradient of model.layers.1.mlp.gate_proj has nan: True
25-01-23 10:32:13 | I | In layer model.layers.1, gradient of model.layers.1.mlp.gate_proj has inf: True
25-01-23 10:32:13 | I | inf: first position tensor([1192,    7], device='cuda:0')
25-01-23 10:32:13 | I | nan: first position tensor([7890,    0], device='cuda:0')
25-01-23 10:32:13 | I | In layer model.layers.1, gradient of model.layers.1.mlp.down_proj has nan: False
25-01-23 10:32:13 | I | In layer model.layers.1, gradient of model.layers.1.mlp.down_proj has inf: True
25-01-23 10:32:13 | I | inf: first position tensor([   0, 7890], device='cuda:0')
25-01-23 10:32:13 | I | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
25-01-23 10:32:13 | I | TRAIN CURRENT LAYER_IDX = 1
25-01-23 10:32:13 | I | in layer model.layers.1
25-01-23 10:32:13 | I | quantizing weights for layer model.layers.1
25-01-23 10:32:14 | I | collecting calibration activations in model.layers.1
25-01-23 10:32:14 | I | collecting calibration activations in model.layers.1
25-01-23 10:32:14 | I | - Quantizing decoder layer model.layers.1
25-01-23 10:32:14 | I |   - Calibrating model.layers.1.self_attn.q_proj.weight
25-01-23 10:32:15 | I |       - range scale = [    1.0000]
25-01-23 10:32:15 | I |         sum  error  = [    0.1700]
25-01-23 10:32:15 | I |         best error  = [    0.1700]
25-01-23 10:32:15 | I |     + error = [0.1700]
25-01-23 10:32:16 | I |       - range scale = [    1.0000]
25-01-23 10:32:16 | I |         sum  error  = [    1.6143]
25-01-23 10:32:16 | I |         best error  = [    1.6143]
25-01-23 10:32:16 | I |     + error = [1.6143]
25-01-23 10:32:16 | I |   - Calibrating model.layers.1.self_attn.k_proj.weight
25-01-23 10:32:17 | I |       - range scale = [    1.0000]
25-01-23 10:32:17 | I |         sum  error  = [    0.1749]
25-01-23 10:32:17 | I |         best error  = [    0.1749]
25-01-23 10:32:17 | I |     + error = [0.1749]
25-01-23 10:32:17 | I |       - range scale = [    1.0000]
25-01-23 10:32:17 | I |         sum  error  = [    1.8414]
25-01-23 10:32:17 | I |         best error  = [    1.8414]
25-01-23 10:32:17 | I |     + error = [1.8414]
25-01-23 10:32:17 | I |   - Calibrating model.layers.1.self_attn.v_proj.weight
25-01-23 10:32:18 | I |       - range scale = [    1.0000]
25-01-23 10:32:18 | I |         sum  error  = [    1.3590]
25-01-23 10:32:18 | I |         best error  = [    1.3590]
25-01-23 10:32:18 | I |     + error = [1.3590]
25-01-23 10:32:19 | I |       - range scale = [    1.0000]
25-01-23 10:32:19 | I |         sum  error  = [    9.5618]
25-01-23 10:32:19 | I |         best error  = [    9.5618]
25-01-23 10:32:19 | I |     + error = [9.5618]
25-01-23 10:32:19 | I |   - Calibrating model.layers.1.self_attn.o_proj.weight
25-01-23 10:32:20 | I |       - range scale = [    1.0000]
25-01-23 10:32:20 | I |         sum  error  = [    0.2318]
25-01-23 10:32:20 | I |         best error  = [    0.2318]
25-01-23 10:32:20 | I |     + error = [0.2318]
25-01-23 10:32:21 | I |       - range scale = [    1.0000]
25-01-23 10:32:21 | I |         sum  error  = [    2.0705]
25-01-23 10:32:21 | I |         best error  = [    2.0705]
25-01-23 10:32:21 | I |     + error = [2.0705]
25-01-23 10:32:21 | I |   - Calibrating model.layers.1.mlp.up_proj.weight
25-01-23 10:32:22 | I |       - range scale = [    1.0000]
25-01-23 10:32:22 | I |         sum  error  = [    1.8242]
25-01-23 10:32:22 | I |         best error  = [    1.8242]
25-01-23 10:32:22 | I |     + error = [1.8242]
25-01-23 10:32:23 | I |       - range scale = [    1.0000]
25-01-23 10:32:23 | I |         sum  error  = [   19.3925]
25-01-23 10:32:23 | I |         best error  = [   19.3925]
25-01-23 10:32:23 | I |     + error = [19.3925]
25-01-23 10:32:23 | I |   - Calibrating model.layers.1.mlp.gate_proj.weight
25-01-23 10:32:24 | I |       - range scale = [    1.0000]
25-01-23 10:32:24 | I |         sum  error  = [    1.9848]
25-01-23 10:32:24 | I |         best error  = [    1.9848]
25-01-23 10:32:24 | I |     + error = [1.9848]
25-01-23 10:32:25 | I |       - range scale = [    1.0000]
25-01-23 10:32:25 | I |         sum  error  = [   20.6819]
25-01-23 10:32:25 | I |         best error  = [   20.6819]
25-01-23 10:32:25 | I |     + error = [20.6819]
25-01-23 10:32:26 | I |   - Calibrating model.layers.1.mlp.down_proj.weight
25-01-23 10:32:26 | I |       - range scale = [    1.0000]
25-01-23 10:32:26 | I |         sum  error  = [   41.8460]
25-01-23 10:32:26 | I |         best error  = [   41.8460]
25-01-23 10:32:26 | I |     + error = [41.8460]
25-01-23 10:32:28 | I |       - range scale = [    1.0000]
25-01-23 10:32:28 | I |         sum  error  = [  206.3153]
25-01-23 10:32:28 | I |         best error  = [  206.3153]
25-01-23 10:32:28 | I |     + error = [206.3153]
25-01-23 10:32:28 | I |   - Quantizing model.layers.1.self_attn.q_proj.weight
25-01-23 10:32:30 | I |   - Quantizing model.layers.1.self_attn.k_proj.weight
25-01-23 10:32:32 | I |   - Quantizing model.layers.1.self_attn.v_proj.weight
25-01-23 10:32:35 | I |   - Quantizing model.layers.1.self_attn.o_proj.weight
25-01-23 10:32:37 | I |   - Quantizing model.layers.1.mlp.up_proj.weight
25-01-23 10:32:39 | I |   - Quantizing model.layers.1.mlp.gate_proj.weight
25-01-23 10:32:42 | I |   - Quantizing model.layers.1.mlp.down_proj.weight
25-01-23 10:32:47 | I | quantizing activations for layer model.layers.1
25-01-23 10:32:48 | I | collecting calibration activations in model.layers.1
25-01-23 10:32:48 | I | collecting calibration activations in model.layers.1
25-01-23 10:32:50 | I | forward this layer
25-01-23 10:32:50 | I | input_file: /data/gyy/lmquant-main/lmquant/data/data_without_preprocess_llama_7b_bs16/shard/val_input_args_layer_1/04.pt
25-01-23 10:32:50 | I | output_file: /data/gyy/lmquant-main/lmquant/data/data_without_preprocess_llama_7b_bs16/shard/val_teacher_output_layer_1/04.pt
25-01-23 10:32:50 | I | In layer model.layers.1, gradient of model.layers.1.self_attn.q_proj has nan: True
25-01-23 10:32:50 | I | In layer model.layers.1, gradient of model.layers.1.self_attn.q_proj has inf: False
25-01-23 10:32:50 | I | nan: first position tensor([0, 0], device='cuda:0')
25-01-23 10:32:50 | I | In layer model.layers.1, gradient of model.layers.1.self_attn.k_proj has nan: True
25-01-23 10:32:50 | I | In layer model.layers.1, gradient of model.layers.1.self_attn.k_proj has inf: False
25-01-23 10:32:50 | I | nan: first position tensor([0, 0], device='cuda:0')
25-01-23 10:32:50 | I | In layer model.layers.1, gradient of model.layers.1.self_attn.v_proj has nan: True
25-01-23 10:32:50 | I | In layer model.layers.1, gradient of model.layers.1.self_attn.v_proj has inf: False
25-01-23 10:32:50 | I | nan: first position tensor([0, 0], device='cuda:0')
25-01-23 10:32:50 | I | In layer model.layers.1, gradient of model.layers.1.self_attn.o_proj has nan: True
25-01-23 10:32:50 | I | In layer model.layers.1, gradient of model.layers.1.self_attn.o_proj has inf: False
25-01-23 10:32:50 | I | nan: first position tensor([0, 0], device='cuda:0')
25-01-23 10:32:50 | I | In layer model.layers.1, gradient of model.layers.1.mlp.up_proj has nan: True
25-01-23 10:32:50 | I | In layer model.layers.1, gradient of model.layers.1.mlp.up_proj has inf: True
25-01-23 10:32:50 | I | inf: first position tensor([1192,    0], device='cuda:0')
25-01-23 10:32:50 | I | nan: first position tensor([1192,    5], device='cuda:0')
25-01-23 10:32:50 | I | In layer model.layers.1, gradient of model.layers.1.mlp.gate_proj has nan: True
25-01-23 10:32:50 | I | In layer model.layers.1, gradient of model.layers.1.mlp.gate_proj has inf: True
25-01-23 10:32:50 | I | inf: first position tensor([1192,    7], device='cuda:0')
25-01-23 10:32:50 | I | nan: first position tensor([7890,    0], device='cuda:0')
25-01-23 10:32:50 | I | In layer model.layers.1, gradient of model.layers.1.mlp.down_proj has nan: False
25-01-23 10:32:50 | I | In layer model.layers.1, gradient of model.layers.1.mlp.down_proj has inf: True
25-01-23 10:32:50 | I | inf: first position tensor([   0, 7890], device='cuda:0')
25-01-23 10:32:50 | I | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 4.0
25-01-23 10:32:50 | I | TRAIN CURRENT LAYER_IDX = 1
25-01-23 10:32:50 | I | in layer model.layers.1
25-01-23 10:32:50 | I | quantizing weights for layer model.layers.1
25-01-23 10:32:51 | I | collecting calibration activations in model.layers.1
25-01-23 10:32:51 | I | collecting calibration activations in model.layers.1
25-01-23 10:32:51 | I | - Quantizing decoder layer model.layers.1
25-01-23 10:32:51 | I |   - Calibrating model.layers.1.self_attn.q_proj.weight
25-01-23 10:32:52 | I |       - range scale = [    1.0000]
25-01-23 10:32:52 | I |         sum  error  = [    0.1676]
25-01-23 10:32:52 | I |         best error  = [    0.1676]
25-01-23 10:32:52 | I |     + error = [0.1676]
25-01-23 10:32:53 | I |       - range scale = [    1.0000]
25-01-23 10:32:53 | I |         sum  error  = [    1.6106]
25-01-23 10:32:53 | I |         best error  = [    1.6106]
25-01-23 10:32:53 | I |     + error = [1.6106]
25-01-23 10:32:53 | I |   - Calibrating model.layers.1.self_attn.k_proj.weight
25-01-23 10:32:54 | I |       - range scale = [    1.0000]
25-01-23 10:32:54 | I |         sum  error  = [    0.1690]
25-01-23 10:32:54 | I |         best error  = [    0.1690]
25-01-23 10:32:54 | I |     + error = [0.1690]
25-01-23 10:32:54 | I |       - range scale = [    1.0000]
25-01-23 10:32:54 | I |         sum  error  = [    1.8088]
25-01-23 10:32:54 | I |         best error  = [    1.8088]
25-01-23 10:32:54 | I |     + error = [1.8088]
25-01-23 10:32:54 | I |   - Calibrating model.layers.1.self_attn.v_proj.weight
25-01-23 10:32:55 | I |       - range scale = [    1.0000]
25-01-23 10:32:55 | I |         sum  error  = [    1.3559]
25-01-23 10:32:55 | I |         best error  = [    1.3559]
25-01-23 10:32:55 | I |     + error = [1.3559]
25-01-23 10:32:56 | I |       - range scale = [    1.0000]
25-01-23 10:32:56 | I |         sum  error  = [    9.3320]
25-01-23 10:32:56 | I |         best error  = [    9.3320]
25-01-23 10:32:56 | I |     + error = [9.3320]
25-01-23 10:32:56 | I |   - Calibrating model.layers.1.self_attn.o_proj.weight
25-01-23 10:32:57 | I |       - range scale = [    1.0000]
25-01-23 10:32:57 | I |         sum  error  = [    0.2313]
25-01-23 10:32:57 | I |         best error  = [    0.2313]
25-01-23 10:32:57 | I |     + error = [0.2313]
25-01-23 10:32:58 | I |       - range scale = [    1.0000]
25-01-23 10:32:58 | I |         sum  error  = [    2.0508]
25-01-23 10:32:58 | I |         best error  = [    2.0508]
25-01-23 10:32:58 | I |     + error = [2.0508]
25-01-23 10:32:58 | I |   - Calibrating model.layers.1.mlp.up_proj.weight
25-01-23 10:32:59 | I |       - range scale = [    1.0000]
25-01-23 10:32:59 | I |         sum  error  = [    1.8120]
25-01-23 10:32:59 | I |         best error  = [    1.8120]
25-01-23 10:32:59 | I |     + error = [1.8120]
25-01-23 10:33:00 | I |       - range scale = [    1.0000]
25-01-23 10:33:00 | I |         sum  error  = [   19.2625]
25-01-23 10:33:00 | I |         best error  = [   19.2625]
25-01-23 10:33:00 | I |     + error = [19.2625]
25-01-23 10:33:00 | I |   - Calibrating model.layers.1.mlp.gate_proj.weight
25-01-23 10:33:01 | I |       - range scale = [    1.0000]
25-01-23 10:33:01 | I |         sum  error  = [    1.9714]
25-01-23 10:33:01 | I |         best error  = [    1.9714]
25-01-23 10:33:01 | I |     + error = [1.9714]
25-01-23 10:33:02 | I |       - range scale = [    1.0000]
25-01-23 10:33:02 | I |         sum  error  = [   20.5453]
25-01-23 10:33:02 | I |         best error  = [   20.5453]
25-01-23 10:33:02 | I |     + error = [20.5453]
25-01-23 10:33:02 | I |   - Calibrating model.layers.1.mlp.down_proj.weight
25-01-23 10:33:03 | I |       - range scale = [    1.0000]
25-01-23 10:33:03 | I |         sum  error  = [   44.1822]
25-01-23 10:33:03 | I |         best error  = [   44.1822]
25-01-23 10:33:03 | I |     + error = [44.1822]
25-01-23 10:33:04 | I |       - range scale = [    1.0000]
25-01-23 10:33:04 | I |         sum  error  = [  216.9527]
25-01-23 10:33:04 | I |         best error  = [  216.9527]
25-01-23 10:33:04 | I |     + error = [216.9527]
25-01-23 10:33:05 | I |   - Quantizing model.layers.1.self_attn.q_proj.weight
25-01-23 10:33:07 | I |   - Quantizing model.layers.1.self_attn.k_proj.weight
25-01-23 10:33:09 | I |   - Quantizing model.layers.1.self_attn.v_proj.weight
25-01-23 10:33:12 | I |   - Quantizing model.layers.1.self_attn.o_proj.weight
25-01-23 10:33:14 | I |   - Quantizing model.layers.1.mlp.up_proj.weight
25-01-23 10:33:16 | I |   - Quantizing model.layers.1.mlp.gate_proj.weight
25-01-23 10:33:18 | I |   - Quantizing model.layers.1.mlp.down_proj.weight
25-01-23 10:33:24 | I | quantizing activations for layer model.layers.1
25-01-23 10:33:25 | I | collecting calibration activations in model.layers.1
25-01-23 10:33:25 | I | collecting calibration activations in model.layers.1
25-01-23 10:33:27 | I | forward this layer
25-01-23 10:33:27 | I | input_file: /data/gyy/lmquant-main/lmquant/data/data_without_preprocess_llama_7b_bs16/shard/val_input_args_layer_1/05.pt
25-01-23 10:33:27 | I | output_file: /data/gyy/lmquant-main/lmquant/data/data_without_preprocess_llama_7b_bs16/shard/val_teacher_output_layer_1/05.pt
25-01-23 10:33:27 | I | In layer model.layers.1, gradient of model.layers.1.self_attn.q_proj has nan: True
25-01-23 10:33:27 | I | In layer model.layers.1, gradient of model.layers.1.self_attn.q_proj has inf: False
25-01-23 10:33:27 | I | nan: first position tensor([0, 0], device='cuda:0')
25-01-23 10:33:27 | I | In layer model.layers.1, gradient of model.layers.1.self_attn.k_proj has nan: True
25-01-23 10:33:27 | I | In layer model.layers.1, gradient of model.layers.1.self_attn.k_proj has inf: False
25-01-23 10:33:27 | I | nan: first position tensor([0, 0], device='cuda:0')
25-01-23 10:33:27 | I | In layer model.layers.1, gradient of model.layers.1.self_attn.v_proj has nan: True
25-01-23 10:33:27 | I | In layer model.layers.1, gradient of model.layers.1.self_attn.v_proj has inf: False
25-01-23 10:33:27 | I | nan: first position tensor([0, 0], device='cuda:0')
25-01-23 10:33:27 | I | In layer model.layers.1, gradient of model.layers.1.self_attn.o_proj has nan: True
25-01-23 10:33:27 | I | In layer model.layers.1, gradient of model.layers.1.self_attn.o_proj has inf: False
25-01-23 10:33:27 | I | nan: first position tensor([0, 0], device='cuda:0')
25-01-23 10:33:27 | I | In layer model.layers.1, gradient of model.layers.1.mlp.up_proj has nan: True
25-01-23 10:33:27 | I | In layer model.layers.1, gradient of model.layers.1.mlp.up_proj has inf: True
25-01-23 10:33:27 | I | inf: first position tensor([1192,    0], device='cuda:0')
25-01-23 10:33:27 | I | nan: first position tensor([1192,    6], device='cuda:0')
25-01-23 10:33:27 | I | In layer model.layers.1, gradient of model.layers.1.mlp.gate_proj has nan: True
25-01-23 10:33:27 | I | In layer model.layers.1, gradient of model.layers.1.mlp.gate_proj has inf: True
25-01-23 10:33:27 | I | inf: first position tensor([1192,   88], device='cuda:0')
25-01-23 10:33:27 | I | nan: first position tensor([7890,    0], device='cuda:0')
25-01-23 10:33:27 | I | In layer model.layers.1, gradient of model.layers.1.mlp.down_proj has nan: False
25-01-23 10:33:27 | I | In layer model.layers.1, gradient of model.layers.1.mlp.down_proj has inf: True
25-01-23 10:33:27 | I | inf: first position tensor([   0, 7890], device='cuda:0')
25-01-23 10:33:27 | I | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 2.0
25-01-23 10:33:27 | I | TRAIN CURRENT LAYER_IDX = 1
25-01-23 10:33:27 | I | in layer model.layers.1
25-01-23 10:33:27 | I | quantizing weights for layer model.layers.1
25-01-23 10:33:28 | I | collecting calibration activations in model.layers.1
25-01-23 10:33:28 | I | collecting calibration activations in model.layers.1
25-01-23 10:33:28 | I | - Quantizing decoder layer model.layers.1
25-01-23 10:33:28 | I |   - Calibrating model.layers.1.self_attn.q_proj.weight
25-01-23 10:33:29 | I |       - range scale = [    1.0000]
25-01-23 10:33:29 | I |         sum  error  = [    0.1654]
25-01-23 10:33:29 | I |         best error  = [    0.1654]
25-01-23 10:33:29 | I |     + error = [0.1654]
25-01-23 10:33:30 | I |       - range scale = [    1.0000]
25-01-23 10:33:30 | I |         sum  error  = [    1.5900]
25-01-23 10:33:30 | I |         best error  = [    1.5900]
25-01-23 10:33:30 | I |     + error = [1.5900]
25-01-23 10:33:30 | I |   - Calibrating model.layers.1.self_attn.k_proj.weight
25-01-23 10:33:30 | I |       - range scale = [    1.0000]
25-01-23 10:33:30 | I |         sum  error  = [    0.1737]
25-01-23 10:33:30 | I |         best error  = [    0.1737]
25-01-23 10:33:30 | I |     + error = [0.1737]
25-01-23 10:33:31 | I |       - range scale = [    1.0000]
25-01-23 10:33:31 | I |         sum  error  = [    1.8053]
25-01-23 10:33:31 | I |         best error  = [    1.8053]
25-01-23 10:33:31 | I |     + error = [1.8053]
25-01-23 10:33:31 | I |   - Calibrating model.layers.1.self_attn.v_proj.weight
25-01-23 10:33:32 | I |       - range scale = [    1.0000]
25-01-23 10:33:32 | I |         sum  error  = [    1.3564]
25-01-23 10:33:32 | I |         best error  = [    1.3564]
25-01-23 10:33:32 | I |     + error = [1.3564]
25-01-23 10:33:33 | I |       - range scale = [    1.0000]
25-01-23 10:33:33 | I |         sum  error  = [    9.3944]
25-01-23 10:33:33 | I |         best error  = [    9.3944]
25-01-23 10:33:33 | I |     + error = [9.3944]
25-01-23 10:33:33 | I |   - Calibrating model.layers.1.self_attn.o_proj.weight
25-01-23 10:33:34 | I |       - range scale = [    1.0000]
25-01-23 10:33:34 | I |         sum  error  = [    0.2299]
25-01-23 10:33:34 | I |         best error  = [    0.2299]
25-01-23 10:33:34 | I |     + error = [0.2299]
25-01-23 10:33:35 | I |       - range scale = [    1.0000]
25-01-23 10:33:35 | I |         sum  error  = [    2.0609]
25-01-23 10:33:35 | I |         best error  = [    2.0609]
25-01-23 10:33:35 | I |     + error = [2.0609]
25-01-23 10:33:35 | I |   - Calibrating model.layers.1.mlp.up_proj.weight
25-01-23 10:33:36 | I |       - range scale = [    1.0000]
25-01-23 10:33:36 | I |         sum  error  = [    1.8195]
25-01-23 10:33:36 | I |         best error  = [    1.8195]
25-01-23 10:33:36 | I |     + error = [1.8195]
25-01-23 10:33:37 | I |       - range scale = [    1.0000]
25-01-23 10:33:37 | I |         sum  error  = [   19.3612]
25-01-23 10:33:37 | I |         best error  = [   19.3612]
25-01-23 10:33:37 | I |     + error = [19.3612]
25-01-23 10:33:37 | I |   - Calibrating model.layers.1.mlp.gate_proj.weight
25-01-23 10:33:38 | I |       - range scale = [    1.0000]
25-01-23 10:33:38 | I |         sum  error  = [    1.9825]
25-01-23 10:33:38 | I |         best error  = [    1.9825]
25-01-23 10:33:38 | I |     + error = [1.9825]
25-01-23 10:33:39 | I |       - range scale = [    1.0000]
25-01-23 10:33:39 | I |         sum  error  = [   20.6499]
25-01-23 10:33:39 | I |         best error  = [   20.6499]
25-01-23 10:33:39 | I |     + error = [20.6499]
25-01-23 10:33:39 | I |   - Calibrating model.layers.1.mlp.down_proj.weight
25-01-23 10:33:40 | I |       - range scale = [    1.0000]
25-01-23 10:33:40 | I |         sum  error  = [   47.5143]
25-01-23 10:33:40 | I |         best error  = [   47.5143]
25-01-23 10:33:40 | I |     + error = [47.5143]
25-01-23 10:33:41 | I |       - range scale = [    1.0000]
25-01-23 10:33:41 | I |         sum  error  = [  233.3189]
25-01-23 10:33:41 | I |         best error  = [  233.3189]
25-01-23 10:33:41 | I |     + error = [233.3189]
25-01-23 10:33:42 | I |   - Quantizing model.layers.1.self_attn.q_proj.weight
25-01-23 10:33:44 | I |   - Quantizing model.layers.1.self_attn.k_proj.weight
25-01-23 10:33:46 | I |   - Quantizing model.layers.1.self_attn.v_proj.weight
25-01-23 10:33:48 | I |   - Quantizing model.layers.1.self_attn.o_proj.weight
25-01-23 10:33:51 | I |   - Quantizing model.layers.1.mlp.up_proj.weight
25-01-23 10:33:53 | I |   - Quantizing model.layers.1.mlp.gate_proj.weight
25-01-23 10:33:55 | I |   - Quantizing model.layers.1.mlp.down_proj.weight
25-01-23 10:34:01 | I | quantizing activations for layer model.layers.1
25-01-23 10:34:02 | I | collecting calibration activations in model.layers.1
25-01-23 10:34:02 | I | collecting calibration activations in model.layers.1
25-01-23 10:34:03 | I | forward this layer
25-01-23 10:34:03 | I | input_file: /data/gyy/lmquant-main/lmquant/data/data_without_preprocess_llama_7b_bs16/shard/val_input_args_layer_1/06.pt
25-01-23 10:34:03 | I | output_file: /data/gyy/lmquant-main/lmquant/data/data_without_preprocess_llama_7b_bs16/shard/val_teacher_output_layer_1/06.pt
25-01-23 10:34:04 | I | In layer model.layers.1, gradient of model.layers.1.self_attn.q_proj has nan: True
25-01-23 10:34:04 | I | In layer model.layers.1, gradient of model.layers.1.self_attn.q_proj has inf: False
25-01-23 10:34:04 | I | nan: first position tensor([0, 0], device='cuda:0')
25-01-23 10:34:04 | I | In layer model.layers.1, gradient of model.layers.1.self_attn.k_proj has nan: True
25-01-23 10:34:04 | I | In layer model.layers.1, gradient of model.layers.1.self_attn.k_proj has inf: False
25-01-23 10:34:04 | I | nan: first position tensor([0, 0], device='cuda:0')
25-01-23 10:34:04 | I | In layer model.layers.1, gradient of model.layers.1.self_attn.v_proj has nan: True
25-01-23 10:34:04 | I | In layer model.layers.1, gradient of model.layers.1.self_attn.v_proj has inf: False
25-01-23 10:34:04 | I | nan: first position tensor([0, 0], device='cuda:0')
25-01-23 10:34:04 | I | In layer model.layers.1, gradient of model.layers.1.self_attn.o_proj has nan: True
25-01-23 10:34:04 | I | In layer model.layers.1, gradient of model.layers.1.self_attn.o_proj has inf: False
25-01-23 10:34:04 | I | nan: first position tensor([0, 0], device='cuda:0')
25-01-23 10:34:04 | I | In layer model.layers.1, gradient of model.layers.1.mlp.up_proj has nan: True
25-01-23 10:34:04 | I | In layer model.layers.1, gradient of model.layers.1.mlp.up_proj has inf: True
25-01-23 10:34:04 | I | inf: first position tensor([1192,   88], device='cuda:0')
25-01-23 10:34:04 | I | nan: first position tensor([7890,    0], device='cuda:0')
25-01-23 10:34:04 | I | In layer model.layers.1, gradient of model.layers.1.mlp.gate_proj has nan: True
25-01-23 10:34:04 | I | In layer model.layers.1, gradient of model.layers.1.mlp.gate_proj has inf: True
25-01-23 10:34:04 | I | inf: first position tensor([7890,    0], device='cuda:0')
25-01-23 10:34:04 | I | nan: first position tensor([7890,   78], device='cuda:0')
25-01-23 10:34:04 | I | In layer model.layers.1, gradient of model.layers.1.mlp.down_proj has nan: False
25-01-23 10:34:04 | I | In layer model.layers.1, gradient of model.layers.1.mlp.down_proj has inf: True
25-01-23 10:34:04 | I | inf: first position tensor([  14, 7890], device='cuda:0')
25-01-23 10:34:04 | I | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 1.0
25-01-23 10:34:04 | I | TRAIN CURRENT LAYER_IDX = 1
25-01-23 10:34:04 | I | in layer model.layers.1
25-01-23 10:34:04 | I | quantizing weights for layer model.layers.1
25-01-23 10:34:04 | I | collecting calibration activations in model.layers.1
25-01-23 10:34:05 | I | collecting calibration activations in model.layers.1
25-01-23 10:34:05 | I | - Quantizing decoder layer model.layers.1
25-01-23 10:34:05 | I |   - Calibrating model.layers.1.self_attn.q_proj.weight
25-01-23 10:34:05 | I |       - range scale = [    1.0000]
25-01-23 10:34:05 | I |         sum  error  = [    0.1728]
25-01-23 10:34:05 | I |         best error  = [    0.1728]
25-01-23 10:34:05 | I |     + error = [0.1728]
25-01-23 10:34:06 | I |       - range scale = [    1.0000]
25-01-23 10:34:06 | I |         sum  error  = [    1.6635]
25-01-23 10:34:06 | I |         best error  = [    1.6635]
25-01-23 10:34:06 | I |     + error = [1.6635]
25-01-23 10:34:06 | I |   - Calibrating model.layers.1.self_attn.k_proj.weight
25-01-23 10:34:07 | I |       - range scale = [    1.0000]
25-01-23 10:34:07 | I |         sum  error  = [    0.1830]
25-01-23 10:34:07 | I |         best error  = [    0.1830]
25-01-23 10:34:07 | I |     + error = [0.1830]
25-01-23 10:34:08 | I |       - range scale = [    1.0000]
25-01-23 10:34:08 | I |         sum  error  = [    1.8327]
25-01-23 10:34:08 | I |         best error  = [    1.8327]
25-01-23 10:34:08 | I |     + error = [1.8327]
25-01-23 10:34:08 | I |   - Calibrating model.layers.1.self_attn.v_proj.weight
25-01-23 10:34:09 | I |       - range scale = [    1.0000]
25-01-23 10:34:09 | I |         sum  error  = [    1.3506]
25-01-23 10:34:09 | I |         best error  = [    1.3506]
25-01-23 10:34:09 | I |     + error = [1.3506]
25-01-23 10:34:10 | I |       - range scale = [    1.0000]
25-01-23 10:34:10 | I |         sum  error  = [    9.4242]
25-01-23 10:34:10 | I |         best error  = [    9.4242]
25-01-23 10:34:10 | I |     + error = [9.4242]
25-01-23 10:34:10 | I |   - Calibrating model.layers.1.self_attn.o_proj.weight
25-01-23 10:34:11 | I |       - range scale = [    1.0000]
25-01-23 10:34:11 | I |         sum  error  = [    0.2266]
25-01-23 10:34:11 | I |         best error  = [    0.2266]
25-01-23 10:34:11 | I |     + error = [0.2266]
25-01-23 10:34:11 | I |       - range scale = [    1.0000]
25-01-23 10:34:11 | I |         sum  error  = [    2.0168]
25-01-23 10:34:11 | I |         best error  = [    2.0168]
25-01-23 10:34:11 | I |     + error = [2.0168]
25-01-23 10:34:12 | I |   - Calibrating model.layers.1.mlp.up_proj.weight
25-01-23 10:34:12 | I |       - range scale = [    1.0000]
25-01-23 10:34:12 | I |         sum  error  = [    1.8204]
25-01-23 10:34:12 | I |         best error  = [    1.8204]
25-01-23 10:34:12 | I |     + error = [1.8204]
25-01-23 10:34:13 | I |       - range scale = [    1.0000]
25-01-23 10:34:13 | I |         sum  error  = [   19.3756]
25-01-23 10:34:13 | I |         best error  = [   19.3756]
25-01-23 10:34:13 | I |     + error = [19.3756]
25-01-23 10:34:14 | I |   - Calibrating model.layers.1.mlp.gate_proj.weight
25-01-23 10:34:14 | I |       - range scale = [    1.0000]
25-01-23 10:34:14 | I |         sum  error  = [    1.9848]
25-01-23 10:34:14 | I |         best error  = [    1.9848]
25-01-23 10:34:14 | I |     + error = [1.9848]
25-01-23 10:34:16 | I |       - range scale = [    1.0000]
25-01-23 10:34:16 | I |         sum  error  = [   20.6665]
25-01-23 10:34:16 | I |         best error  = [   20.6665]
25-01-23 10:34:16 | I |     + error = [20.6665]
25-01-23 10:34:16 | I |   - Calibrating model.layers.1.mlp.down_proj.weight
25-01-23 10:34:17 | I |       - range scale = [    1.0000]
25-01-23 10:34:17 | I |         sum  error  = [   49.8792]
25-01-23 10:34:17 | I |         best error  = [   49.8792]
25-01-23 10:34:17 | I |     + error = [49.8792]
25-01-23 10:34:18 | I |       - range scale = [    1.0000]
25-01-23 10:34:18 | I |         sum  error  = [  244.1320]
25-01-23 10:34:18 | I |         best error  = [  244.1320]
25-01-23 10:34:18 | I |     + error = [244.1320]
25-01-23 10:34:18 | I |   - Quantizing model.layers.1.self_attn.q_proj.weight
25-01-23 10:34:20 | I |   - Quantizing model.layers.1.self_attn.k_proj.weight
25-01-23 10:34:23 | I |   - Quantizing model.layers.1.self_attn.v_proj.weight
25-01-23 10:34:25 | I |   - Quantizing model.layers.1.self_attn.o_proj.weight
25-01-23 10:34:27 | I |   - Quantizing model.layers.1.mlp.up_proj.weight
25-01-23 10:34:29 | I |   - Quantizing model.layers.1.mlp.gate_proj.weight
25-01-23 10:34:32 | I |   - Quantizing model.layers.1.mlp.down_proj.weight
25-01-23 10:34:38 | I | quantizing activations for layer model.layers.1
25-01-23 10:34:38 | I | collecting calibration activations in model.layers.1
25-01-23 10:34:38 | I | collecting calibration activations in model.layers.1
25-01-23 10:34:40 | I | forward this layer
25-01-23 10:34:40 | I | input_file: /data/gyy/lmquant-main/lmquant/data/data_without_preprocess_llama_7b_bs16/shard/val_input_args_layer_1/07.pt
25-01-23 10:34:40 | I | output_file: /data/gyy/lmquant-main/lmquant/data/data_without_preprocess_llama_7b_bs16/shard/val_teacher_output_layer_1/07.pt
25-01-23 10:34:41 | I | In layer model.layers.1, gradient of model.layers.1.self_attn.q_proj has nan: True
25-01-23 10:34:41 | I | In layer model.layers.1, gradient of model.layers.1.self_attn.q_proj has inf: False
25-01-23 10:34:41 | I | nan: first position tensor([0, 0], device='cuda:0')
25-01-23 10:34:41 | I | In layer model.layers.1, gradient of model.layers.1.self_attn.k_proj has nan: True
25-01-23 10:34:41 | I | In layer model.layers.1, gradient of model.layers.1.self_attn.k_proj has inf: False
25-01-23 10:34:41 | I | nan: first position tensor([0, 0], device='cuda:0')
25-01-23 10:34:41 | I | In layer model.layers.1, gradient of model.layers.1.self_attn.v_proj has nan: True
25-01-23 10:34:41 | I | In layer model.layers.1, gradient of model.layers.1.self_attn.v_proj has inf: False
25-01-23 10:34:41 | I | nan: first position tensor([0, 0], device='cuda:0')
25-01-23 10:34:41 | I | In layer model.layers.1, gradient of model.layers.1.self_attn.o_proj has nan: True
25-01-23 10:34:41 | I | In layer model.layers.1, gradient of model.layers.1.self_attn.o_proj has inf: False
25-01-23 10:34:41 | I | nan: first position tensor([0, 0], device='cuda:0')
25-01-23 10:34:41 | I | In layer model.layers.1, gradient of model.layers.1.mlp.up_proj has nan: False
25-01-23 10:34:41 | I | In layer model.layers.1, gradient of model.layers.1.mlp.up_proj has inf: True
25-01-23 10:34:41 | I | inf: first position tensor([7890,    7], device='cuda:0')
25-01-23 10:34:41 | I | In layer model.layers.1, gradient of model.layers.1.mlp.gate_proj has nan: False
25-01-23 10:34:41 | I | In layer model.layers.1, gradient of model.layers.1.mlp.gate_proj has inf: True
25-01-23 10:34:41 | I | inf: first position tensor([7890,    7], device='cuda:0')
25-01-23 10:34:41 | I | In layer model.layers.1, gradient of model.layers.1.mlp.down_proj has nan: False
25-01-23 10:34:41 | I | In layer model.layers.1, gradient of model.layers.1.mlp.down_proj has inf: True
25-01-23 10:34:41 | I | inf: first position tensor([  78, 7890], device='cuda:0')
25-01-23 10:34:41 | I | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 0.5
25-01-23 10:34:41 | I | TRAIN CURRENT LAYER_IDX = 1
25-01-23 10:34:41 | I | in layer model.layers.1
25-01-23 10:34:41 | I | quantizing weights for layer model.layers.1
25-01-23 10:34:41 | I | collecting calibration activations in model.layers.1
25-01-23 10:34:41 | I | collecting calibration activations in model.layers.1
25-01-23 10:34:42 | I | - Quantizing decoder layer model.layers.1
25-01-23 10:34:42 | I |   - Calibrating model.layers.1.self_attn.q_proj.weight
25-01-23 10:34:42 | I |       - range scale = [    1.0000]
25-01-23 10:34:42 | I |         sum  error  = [    0.1697]
25-01-23 10:34:42 | I |         best error  = [    0.1697]
25-01-23 10:34:42 | I |     + error = [0.1697]
25-01-23 10:34:43 | I |       - range scale = [    1.0000]
25-01-23 10:34:43 | I |         sum  error  = [    1.6245]
25-01-23 10:34:43 | I |         best error  = [    1.6245]
25-01-23 10:34:43 | I |     + error = [1.6245]
25-01-23 10:34:43 | I |   - Calibrating model.layers.1.self_attn.k_proj.weight
25-01-23 10:34:44 | I |       - range scale = [    1.0000]
25-01-23 10:34:44 | I |         sum  error  = [    0.1734]
25-01-23 10:34:44 | I |         best error  = [    0.1734]
25-01-23 10:34:44 | I |     + error = [0.1734]
25-01-23 10:34:45 | I |       - range scale = [    1.0000]
25-01-23 10:34:45 | I |         sum  error  = [    1.8224]
25-01-23 10:34:45 | I |         best error  = [    1.8224]
25-01-23 10:34:45 | I |     + error = [1.8224]
25-01-23 10:34:45 | I |   - Calibrating model.layers.1.self_attn.v_proj.weight
25-01-23 10:34:46 | I |       - range scale = [    1.0000]
25-01-23 10:34:46 | I |         sum  error  = [    1.3488]
25-01-23 10:34:46 | I |         best error  = [    1.3488]
25-01-23 10:34:46 | I |     + error = [1.3488]
25-01-23 10:34:47 | I |       - range scale = [    1.0000]
25-01-23 10:34:47 | I |         sum  error  = [    9.4491]
25-01-23 10:34:47 | I |         best error  = [    9.4491]
25-01-23 10:34:47 | I |     + error = [9.4491]
25-01-23 10:34:47 | I |   - Calibrating model.layers.1.self_attn.o_proj.weight
25-01-23 10:34:48 | I |       - range scale = [    1.0000]
25-01-23 10:34:48 | I |         sum  error  = [    0.2293]
25-01-23 10:34:48 | I |         best error  = [    0.2293]
25-01-23 10:34:48 | I |     + error = [0.2293]
25-01-23 10:34:48 | I |       - range scale = [    1.0000]
25-01-23 10:34:48 | I |         sum  error  = [    2.0612]
25-01-23 10:34:48 | I |         best error  = [    2.0612]
25-01-23 10:34:48 | I |     + error = [2.0612]
25-01-23 10:34:49 | I |   - Calibrating model.layers.1.mlp.up_proj.weight
25-01-23 10:34:49 | I |       - range scale = [    1.0000]
25-01-23 10:34:49 | I |         sum  error  = [    1.8367]
25-01-23 10:34:49 | I |         best error  = [    1.8367]
25-01-23 10:34:49 | I |     + error = [1.8367]
25-01-23 10:34:50 | I |       - range scale = [    1.0000]
25-01-23 10:34:50 | I |         sum  error  = [   19.5241]
25-01-23 10:34:50 | I |         best error  = [   19.5241]
25-01-23 10:34:50 | I |     + error = [19.5241]
25-01-23 10:34:51 | I |   - Calibrating model.layers.1.mlp.gate_proj.weight
25-01-23 10:34:52 | I |       - range scale = [    1.0000]
25-01-23 10:34:52 | I |         sum  error  = [    1.9980]
25-01-23 10:34:52 | I |         best error  = [    1.9980]
25-01-23 10:34:52 | I |     + error = [1.9980]
25-01-23 10:34:53 | I |       - range scale = [    1.0000]
25-01-23 10:34:53 | I |         sum  error  = [   20.8301]
25-01-23 10:34:53 | I |         best error  = [   20.8301]
25-01-23 10:34:53 | I |     + error = [20.8301]
25-01-23 10:34:53 | I |   - Calibrating model.layers.1.mlp.down_proj.weight
25-01-23 10:34:54 | I |       - range scale = [    1.0000]
25-01-23 10:34:54 | I |         sum  error  = [   51.7480]
25-01-23 10:34:54 | I |         best error  = [   51.7480]
25-01-23 10:34:54 | I |     + error = [51.7480]
25-01-23 10:34:55 | I |       - range scale = [    1.0000]
25-01-23 10:34:55 | I |         sum  error  = [  253.2485]
25-01-23 10:34:55 | I |         best error  = [  253.2485]
25-01-23 10:34:55 | I |     + error = [253.2485]
25-01-23 10:34:55 | I |   - Quantizing model.layers.1.self_attn.q_proj.weight
25-01-23 10:34:57 | I |   - Quantizing model.layers.1.self_attn.k_proj.weight
25-01-23 10:35:00 | I |   - Quantizing model.layers.1.self_attn.v_proj.weight
25-01-23 10:35:02 | I |   - Quantizing model.layers.1.self_attn.o_proj.weight
25-01-23 10:35:04 | I |   - Quantizing model.layers.1.mlp.up_proj.weight
25-01-23 10:35:07 | I |   - Quantizing model.layers.1.mlp.gate_proj.weight
25-01-23 10:35:09 | I |   - Quantizing model.layers.1.mlp.down_proj.weight
25-01-23 10:35:15 | I | quantizing activations for layer model.layers.1
25-01-23 10:35:15 | I | collecting calibration activations in model.layers.1
25-01-23 10:35:15 | I | collecting calibration activations in model.layers.1
25-01-23 10:35:17 | I | forward this layer
25-01-23 10:35:17 | I | input_file: /data/gyy/lmquant-main/lmquant/data/data_without_preprocess_llama_7b_bs16/shard/val_input_args_layer_1/08.pt
25-01-23 10:35:17 | I | output_file: /data/gyy/lmquant-main/lmquant/data/data_without_preprocess_llama_7b_bs16/shard/val_teacher_output_layer_1/08.pt
25-01-23 10:35:17 | I | In layer model.layers.1, gradient of model.layers.1.self_attn.q_proj has nan: True
25-01-23 10:35:17 | I | In layer model.layers.1, gradient of model.layers.1.self_attn.q_proj has inf: False
25-01-23 10:35:17 | I | nan: first position tensor([0, 0], device='cuda:0')
25-01-23 10:35:17 | I | In layer model.layers.1, gradient of model.layers.1.self_attn.k_proj has nan: True
25-01-23 10:35:17 | I | In layer model.layers.1, gradient of model.layers.1.self_attn.k_proj has inf: False
25-01-23 10:35:17 | I | nan: first position tensor([0, 0], device='cuda:0')
25-01-23 10:35:17 | I | In layer model.layers.1, gradient of model.layers.1.self_attn.v_proj has nan: True
25-01-23 10:35:17 | I | In layer model.layers.1, gradient of model.layers.1.self_attn.v_proj has inf: False
25-01-23 10:35:17 | I | nan: first position tensor([0, 0], device='cuda:0')
25-01-23 10:35:17 | I | In layer model.layers.1, gradient of model.layers.1.self_attn.o_proj has nan: True
25-01-23 10:35:17 | I | In layer model.layers.1, gradient of model.layers.1.self_attn.o_proj has inf: False
25-01-23 10:35:17 | I | nan: first position tensor([0, 0], device='cuda:0')
25-01-23 10:35:17 | I | In layer model.layers.1, gradient of model.layers.1.mlp.up_proj has nan: False
25-01-23 10:35:17 | I | In layer model.layers.1, gradient of model.layers.1.mlp.up_proj has inf: True
25-01-23 10:35:17 | I | inf: first position tensor([7890,   88], device='cuda:0')
25-01-23 10:35:17 | I | In layer model.layers.1, gradient of model.layers.1.mlp.gate_proj has nan: False
25-01-23 10:35:17 | I | In layer model.layers.1, gradient of model.layers.1.mlp.gate_proj has inf: True
25-01-23 10:35:17 | I | inf: first position tensor([7890,   88], device='cuda:0')
25-01-23 10:35:17 | I | In layer model.layers.1, gradient of model.layers.1.mlp.down_proj has nan: False
25-01-23 10:35:17 | I | In layer model.layers.1, gradient of model.layers.1.mlp.down_proj has inf: True
25-01-23 10:35:17 | I | inf: first position tensor([ 490, 7890], device='cuda:0')
25-01-23 10:35:17 | I | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 0.25
25-01-23 10:35:18 | I | TRAIN CURRENT LAYER_IDX = 1
25-01-23 10:35:18 | I | in layer model.layers.1
25-01-23 10:35:18 | I | quantizing weights for layer model.layers.1
25-01-23 10:35:18 | I | collecting calibration activations in model.layers.1
25-01-23 10:35:18 | I | collecting calibration activations in model.layers.1
25-01-23 10:35:18 | I | - Quantizing decoder layer model.layers.1
25-01-23 10:35:18 | I |   - Calibrating model.layers.1.self_attn.q_proj.weight
25-01-23 10:35:19 | I |       - range scale = [    1.0000]
25-01-23 10:35:19 | I |         sum  error  = [    0.1682]
25-01-23 10:35:19 | I |         best error  = [    0.1682]
25-01-23 10:35:19 | I |     + error = [0.1682]
25-01-23 10:35:20 | I |       - range scale = [    1.0000]
25-01-23 10:35:20 | I |         sum  error  = [    1.6222]
25-01-23 10:35:20 | I |         best error  = [    1.6222]
25-01-23 10:35:20 | I |     + error = [1.6222]
25-01-23 10:35:20 | I |   - Calibrating model.layers.1.self_attn.k_proj.weight
25-01-23 10:35:21 | I |       - range scale = [    1.0000]
25-01-23 10:35:21 | I |         sum  error  = [    0.1759]
25-01-23 10:35:21 | I |         best error  = [    0.1759]
25-01-23 10:35:21 | I |     + error = [0.1759]
25-01-23 10:35:22 | I |       - range scale = [    1.0000]
25-01-23 10:35:22 | I |         sum  error  = [    1.8148]
25-01-23 10:35:22 | I |         best error  = [    1.8148]
25-01-23 10:35:22 | I |     + error = [1.8148]
25-01-23 10:35:22 | I |   - Calibrating model.layers.1.self_attn.v_proj.weight
25-01-23 10:35:23 | I |       - range scale = [    1.0000]
25-01-23 10:35:23 | I |         sum  error  = [    1.3630]
25-01-23 10:35:23 | I |         best error  = [    1.3630]
25-01-23 10:35:23 | I |     + error = [1.3630]
25-01-23 10:35:24 | I |       - range scale = [    1.0000]
25-01-23 10:35:24 | I |         sum  error  = [    9.3841]
25-01-23 10:35:24 | I |         best error  = [    9.3841]
25-01-23 10:35:24 | I |     + error = [9.3841]
25-01-23 10:35:25 | I |   - Calibrating model.layers.1.self_attn.o_proj.weight
25-01-23 10:35:26 | I |       - range scale = [    1.0000]
25-01-23 10:35:26 | I |         sum  error  = [    0.2249]
25-01-23 10:35:26 | I |         best error  = [    0.2249]
25-01-23 10:35:26 | I |     + error = [0.2249]
25-01-23 10:35:27 | I |       - range scale = [    1.0000]
25-01-23 10:35:27 | I |         sum  error  = [    2.0005]
25-01-23 10:35:27 | I |         best error  = [    2.0005]
25-01-23 10:35:27 | I |     + error = [2.0005]
25-01-23 10:35:27 | I |   - Calibrating model.layers.1.mlp.up_proj.weight
25-01-23 10:35:28 | I |       - range scale = [    1.0000]
25-01-23 10:35:28 | I |         sum  error  = [    1.7966]
25-01-23 10:35:28 | I |         best error  = [    1.7966]
25-01-23 10:35:28 | I |     + error = [1.7966]
25-01-23 10:35:29 | I |       - range scale = [    1.0000]
25-01-23 10:35:29 | I |         sum  error  = [   19.1142]
25-01-23 10:35:29 | I |         best error  = [   19.1142]
25-01-23 10:35:29 | I |     + error = [19.1142]
25-01-23 10:35:29 | I |   - Calibrating model.layers.1.mlp.gate_proj.weight
25-01-23 10:35:30 | I |       - range scale = [    1.0000]
25-01-23 10:35:30 | I |         sum  error  = [    1.9571]
25-01-23 10:35:30 | I |         best error  = [    1.9571]
25-01-23 10:35:30 | I |     + error = [1.9571]
25-01-23 10:35:31 | I |       - range scale = [    1.0000]
25-01-23 10:35:31 | I |         sum  error  = [   20.3932]
25-01-23 10:35:31 | I |         best error  = [   20.3932]
25-01-23 10:35:31 | I |     + error = [20.3932]
25-01-23 10:35:31 | I |   - Calibrating model.layers.1.mlp.down_proj.weight
25-01-23 10:35:32 | I |       - range scale = [    1.0000]
25-01-23 10:35:32 | I |         sum  error  = [   48.2804]
25-01-23 10:35:32 | I |         best error  = [   48.2804]
25-01-23 10:35:32 | I |     + error = [48.2804]
25-01-23 10:35:34 | I |       - range scale = [    1.0000]
25-01-23 10:35:34 | I |         sum  error  = [  236.7991]
25-01-23 10:35:34 | I |         best error  = [  236.7991]
25-01-23 10:35:34 | I |     + error = [236.7991]
25-01-23 10:35:34 | I |   - Quantizing model.layers.1.self_attn.q_proj.weight
25-01-23 10:35:36 | I |   - Quantizing model.layers.1.self_attn.k_proj.weight
25-01-23 10:35:38 | I |   - Quantizing model.layers.1.self_attn.v_proj.weight
25-01-23 10:35:41 | I |   - Quantizing model.layers.1.self_attn.o_proj.weight
25-01-23 10:35:43 | I |   - Quantizing model.layers.1.mlp.up_proj.weight
25-01-23 10:35:45 | I |   - Quantizing model.layers.1.mlp.gate_proj.weight
25-01-23 10:35:47 | I |   - Quantizing model.layers.1.mlp.down_proj.weight
25-01-23 10:35:53 | I | quantizing activations for layer model.layers.1
25-01-23 10:35:54 | I | collecting calibration activations in model.layers.1
25-01-23 10:35:54 | I | collecting calibration activations in model.layers.1
25-01-23 10:35:56 | I | forward this layer
25-01-23 10:35:56 | I | input_file: /data/gyy/lmquant-main/lmquant/data/data_without_preprocess_llama_7b_bs16/shard/val_input_args_layer_1/09.pt
25-01-23 10:35:56 | I | output_file: /data/gyy/lmquant-main/lmquant/data/data_without_preprocess_llama_7b_bs16/shard/val_teacher_output_layer_1/09.pt
25-01-23 10:35:56 | I | In layer model.layers.1, gradient of model.layers.1.self_attn.q_proj has nan: True
25-01-23 10:35:56 | I | In layer model.layers.1, gradient of model.layers.1.self_attn.q_proj has inf: False
25-01-23 10:35:56 | I | nan: first position tensor([0, 0], device='cuda:0')
25-01-23 10:35:56 | I | In layer model.layers.1, gradient of model.layers.1.self_attn.k_proj has nan: True
25-01-23 10:35:56 | I | In layer model.layers.1, gradient of model.layers.1.self_attn.k_proj has inf: False
25-01-23 10:35:56 | I | nan: first position tensor([0, 0], device='cuda:0')
25-01-23 10:35:56 | I | In layer model.layers.1, gradient of model.layers.1.self_attn.v_proj has nan: True
25-01-23 10:35:56 | I | In layer model.layers.1, gradient of model.layers.1.self_attn.v_proj has inf: False
25-01-23 10:35:56 | I | nan: first position tensor([0, 0], device='cuda:0')
25-01-23 10:35:56 | I | In layer model.layers.1, gradient of model.layers.1.self_attn.o_proj has nan: True
25-01-23 10:35:56 | I | In layer model.layers.1, gradient of model.layers.1.self_attn.o_proj has inf: True
25-01-23 10:35:56 | I | inf: first position tensor([  11, 3257], device='cuda:0')
25-01-23 10:35:56 | I | nan: first position tensor([310,   4], device='cuda:0')
25-01-23 10:35:56 | I | In layer model.layers.1, gradient of model.layers.1.mlp.up_proj has nan: False
25-01-23 10:35:56 | I | In layer model.layers.1, gradient of model.layers.1.mlp.up_proj has inf: True
25-01-23 10:35:56 | I | inf: first position tensor([7890, 3124], device='cuda:0')
25-01-23 10:35:56 | I | In layer model.layers.1, gradient of model.layers.1.mlp.gate_proj has nan: False
25-01-23 10:35:56 | I | In layer model.layers.1, gradient of model.layers.1.mlp.gate_proj has inf: False
25-01-23 10:35:56 | I | In layer model.layers.1, gradient of model.layers.1.mlp.down_proj has nan: False
25-01-23 10:35:56 | I | In layer model.layers.1, gradient of model.layers.1.mlp.down_proj has inf: True
25-01-23 10:35:56 | I | inf: first position tensor([1415, 7890], device='cuda:0')
25-01-23 10:35:56 | I | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 0.125
25-01-23 10:35:56 | I | TRAIN CURRENT LAYER_IDX = 1
25-01-23 10:35:56 | I | in layer model.layers.1
25-01-23 10:35:56 | I | quantizing weights for layer model.layers.1
25-01-23 10:35:57 | I | collecting calibration activations in model.layers.1
25-01-23 10:35:57 | I | collecting calibration activations in model.layers.1
25-01-23 10:35:57 | I | - Quantizing decoder layer model.layers.1
25-01-23 10:35:57 | I |   - Calibrating model.layers.1.self_attn.q_proj.weight
25-01-23 10:35:58 | I |       - range scale = [    1.0000]
25-01-23 10:35:58 | I |         sum  error  = [    0.1680]
25-01-23 10:35:58 | I |         best error  = [    0.1680]
25-01-23 10:35:58 | I |     + error = [0.1680]
25-01-23 10:35:59 | I |       - range scale = [    1.0000]
25-01-23 10:35:59 | I |         sum  error  = [    1.6148]
25-01-23 10:35:59 | I |         best error  = [    1.6148]
25-01-23 10:35:59 | I |     + error = [1.6148]
25-01-23 10:35:59 | I |   - Calibrating model.layers.1.self_attn.k_proj.weight
25-01-23 10:36:00 | I |       - range scale = [    1.0000]
25-01-23 10:36:00 | I |         sum  error  = [    0.1728]
25-01-23 10:36:00 | I |         best error  = [    0.1728]
25-01-23 10:36:00 | I |     + error = [0.1728]
25-01-23 10:36:00 | I |       - range scale = [    1.0000]
25-01-23 10:36:00 | I |         sum  error  = [    1.8149]
25-01-23 10:36:00 | I |         best error  = [    1.8149]
25-01-23 10:36:00 | I |     + error = [1.8149]
25-01-23 10:36:00 | I |   - Calibrating model.layers.1.self_attn.v_proj.weight
25-01-23 10:36:01 | I |       - range scale = [    1.0000]
25-01-23 10:36:01 | I |         sum  error  = [    1.3524]
25-01-23 10:36:01 | I |         best error  = [    1.3524]
25-01-23 10:36:01 | I |     + error = [1.3524]
25-01-23 10:36:02 | I |       - range scale = [    1.0000]
25-01-23 10:36:02 | I |         sum  error  = [    9.4340]
25-01-23 10:36:02 | I |         best error  = [    9.4340]
25-01-23 10:36:02 | I |     + error = [9.4340]
25-01-23 10:36:02 | I |   - Calibrating model.layers.1.self_attn.o_proj.weight
25-01-23 10:36:03 | I |       - range scale = [    1.0000]
25-01-23 10:36:03 | I |         sum  error  = [    0.2260]
25-01-23 10:36:03 | I |         best error  = [    0.2260]
25-01-23 10:36:03 | I |     + error = [0.2260]
25-01-23 10:36:04 | I |       - range scale = [    1.0000]
25-01-23 10:36:04 | I |         sum  error  = [    2.0352]
25-01-23 10:36:04 | I |         best error  = [    2.0352]
25-01-23 10:36:04 | I |     + error = [2.0352]
25-01-23 10:36:04 | I |   - Calibrating model.layers.1.mlp.up_proj.weight
25-01-23 10:36:05 | I |       - range scale = [    1.0000]
25-01-23 10:36:05 | I |         sum  error  = [    1.8149]
25-01-23 10:36:05 | I |         best error  = [    1.8149]
25-01-23 10:36:05 | I |     + error = [1.8149]
25-01-23 10:36:06 | I |       - range scale = [    1.0000]
25-01-23 10:36:06 | I |         sum  error  = [   19.2826]
25-01-23 10:36:06 | I |         best error  = [   19.2826]
25-01-23 10:36:06 | I |     + error = [19.2826]
25-01-23 10:36:06 | I |   - Calibrating model.layers.1.mlp.gate_proj.weight
25-01-23 10:36:07 | I |       - range scale = [    1.0000]
25-01-23 10:36:07 | I |         sum  error  = [    1.9751]
25-01-23 10:36:07 | I |         best error  = [    1.9751]
25-01-23 10:36:07 | I |     + error = [1.9751]
25-01-23 10:36:08 | I |       - range scale = [    1.0000]
25-01-23 10:36:08 | I |         sum  error  = [   20.5685]
25-01-23 10:36:08 | I |         best error  = [   20.5685]
25-01-23 10:36:08 | I |     + error = [20.5685]
25-01-23 10:36:08 | I |   - Calibrating model.layers.1.mlp.down_proj.weight
25-01-23 10:36:09 | I |       - range scale = [    1.0000]
25-01-23 10:36:09 | I |         sum  error  = [   42.6610]
25-01-23 10:36:09 | I |         best error  = [   42.6610]
25-01-23 10:36:09 | I |     + error = [42.6610]
25-01-23 10:36:10 | I |       - range scale = [    1.0000]
25-01-23 10:36:10 | I |         sum  error  = [  210.0678]
25-01-23 10:36:10 | I |         best error  = [  210.0678]
25-01-23 10:36:10 | I |     + error = [210.0678]
25-01-23 10:36:11 | I |   - Quantizing model.layers.1.self_attn.q_proj.weight
25-01-23 10:36:13 | I |   - Quantizing model.layers.1.self_attn.k_proj.weight
25-01-23 10:36:15 | I |   - Quantizing model.layers.1.self_attn.v_proj.weight
25-01-23 10:36:17 | I |   - Quantizing model.layers.1.self_attn.o_proj.weight
25-01-23 10:36:20 | I |   - Quantizing model.layers.1.mlp.up_proj.weight
25-01-23 10:36:22 | I |   - Quantizing model.layers.1.mlp.gate_proj.weight
25-01-23 10:36:24 | I |   - Quantizing model.layers.1.mlp.down_proj.weight
25-01-23 10:36:30 | I | quantizing activations for layer model.layers.1
25-01-23 10:36:31 | I | collecting calibration activations in model.layers.1
25-01-23 10:36:31 | I | collecting calibration activations in model.layers.1
25-01-23 10:36:32 | I | forward this layer
25-01-23 10:36:32 | I | input_file: /data/gyy/lmquant-main/lmquant/data/data_without_preprocess_llama_7b_bs16/shard/val_input_args_layer_1/10.pt
25-01-23 10:36:32 | I | output_file: /data/gyy/lmquant-main/lmquant/data/data_without_preprocess_llama_7b_bs16/shard/val_teacher_output_layer_1/10.pt
25-01-23 10:36:33 | I | In layer model.layers.1, gradient of model.layers.1.self_attn.q_proj has nan: True
25-01-23 10:36:33 | I | In layer model.layers.1, gradient of model.layers.1.self_attn.q_proj has inf: False
25-01-23 10:36:33 | I | nan: first position tensor([0, 0], device='cuda:0')
25-01-23 10:36:33 | I | In layer model.layers.1, gradient of model.layers.1.self_attn.k_proj has nan: True
25-01-23 10:36:33 | I | In layer model.layers.1, gradient of model.layers.1.self_attn.k_proj has inf: False
25-01-23 10:36:33 | I | nan: first position tensor([0, 0], device='cuda:0')
25-01-23 10:36:33 | I | In layer model.layers.1, gradient of model.layers.1.self_attn.v_proj has nan: True
25-01-23 10:36:33 | I | In layer model.layers.1, gradient of model.layers.1.self_attn.v_proj has inf: False
25-01-23 10:36:33 | I | nan: first position tensor([0, 0], device='cuda:0')
25-01-23 10:36:33 | I | In layer model.layers.1, gradient of model.layers.1.self_attn.o_proj has nan: False
25-01-23 10:36:33 | I | In layer model.layers.1, gradient of model.layers.1.self_attn.o_proj has inf: True
25-01-23 10:36:33 | I | inf: first position tensor([  11, 3257], device='cuda:0')
25-01-23 10:36:33 | I | In layer model.layers.1, gradient of model.layers.1.mlp.up_proj has nan: False
25-01-23 10:36:33 | I | In layer model.layers.1, gradient of model.layers.1.mlp.up_proj has inf: False
25-01-23 10:36:33 | I | In layer model.layers.1, gradient of model.layers.1.mlp.gate_proj has nan: False
25-01-23 10:36:33 | I | In layer model.layers.1, gradient of model.layers.1.mlp.gate_proj has inf: False
25-01-23 10:36:33 | I | In layer model.layers.1, gradient of model.layers.1.mlp.down_proj has nan: False
25-01-23 10:36:33 | I | In layer model.layers.1, gradient of model.layers.1.mlp.down_proj has inf: True
25-01-23 10:36:33 | I | inf: first position tensor([1415, 7890], device='cuda:0')
25-01-23 10:36:33 | I | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 0.0625
25-01-23 10:36:33 | I | TRAIN CURRENT LAYER_IDX = 1
25-01-23 10:36:33 | I | in layer model.layers.1
25-01-23 10:36:33 | I | quantizing weights for layer model.layers.1
25-01-23 10:36:33 | I | collecting calibration activations in model.layers.1
25-01-23 10:36:33 | I | collecting calibration activations in model.layers.1
25-01-23 10:36:34 | I | - Quantizing decoder layer model.layers.1
25-01-23 10:36:34 | I |   - Calibrating model.layers.1.self_attn.q_proj.weight
25-01-23 10:36:34 | I |       - range scale = [    1.0000]
25-01-23 10:36:34 | I |         sum  error  = [    0.1643]
25-01-23 10:36:34 | I |         best error  = [    0.1643]
25-01-23 10:36:34 | I |     + error = [0.1643]
25-01-23 10:36:35 | I |       - range scale = [    1.0000]
25-01-23 10:36:35 | I |         sum  error  = [    1.5704]
25-01-23 10:36:35 | I |         best error  = [    1.5704]
25-01-23 10:36:35 | I |     + error = [1.5704]
25-01-23 10:36:35 | I |   - Calibrating model.layers.1.self_attn.k_proj.weight
25-01-23 10:36:36 | I |       - range scale = [    1.0000]
25-01-23 10:36:36 | I |         sum  error  = [    0.1676]
25-01-23 10:36:36 | I |         best error  = [    0.1676]
25-01-23 10:36:36 | I |     + error = [0.1676]
25-01-23 10:36:37 | I |       - range scale = [    1.0000]
25-01-23 10:36:37 | I |         sum  error  = [    1.7957]
25-01-23 10:36:37 | I |         best error  = [    1.7957]
25-01-23 10:36:37 | I |     + error = [1.7957]
25-01-23 10:36:37 | I |   - Calibrating model.layers.1.self_attn.v_proj.weight
25-01-23 10:36:38 | I |       - range scale = [    1.0000]
25-01-23 10:36:38 | I |         sum  error  = [    1.3566]
25-01-23 10:36:38 | I |         best error  = [    1.3566]
25-01-23 10:36:38 | I |     + error = [1.3566]
25-01-23 10:36:38 | I |       - range scale = [    1.0000]
25-01-23 10:36:38 | I |         sum  error  = [    9.3079]
25-01-23 10:36:38 | I |         best error  = [    9.3079]
25-01-23 10:36:38 | I |     + error = [9.3079]
25-01-23 10:36:39 | I |   - Calibrating model.layers.1.self_attn.o_proj.weight
25-01-23 10:36:39 | I |       - range scale = [    1.0000]
25-01-23 10:36:39 | I |         sum  error  = [    0.2318]
25-01-23 10:36:39 | I |         best error  = [    0.2318]
25-01-23 10:36:39 | I |     + error = [0.2318]
25-01-23 10:36:40 | I |       - range scale = [    1.0000]
25-01-23 10:36:40 | I |         sum  error  = [    2.0564]
25-01-23 10:36:40 | I |         best error  = [    2.0564]
25-01-23 10:36:40 | I |     + error = [2.0564]
25-01-23 10:36:40 | I |   - Calibrating model.layers.1.mlp.up_proj.weight
25-01-23 10:36:41 | I |       - range scale = [    1.0000]
25-01-23 10:36:41 | I |         sum  error  = [    1.8211]
25-01-23 10:36:41 | I |         best error  = [    1.8211]
25-01-23 10:36:41 | I |     + error = [1.8211]
25-01-23 10:36:42 | I |       - range scale = [    1.0000]
25-01-23 10:36:42 | I |         sum  error  = [   19.3727]
25-01-23 10:36:42 | I |         best error  = [   19.3727]
25-01-23 10:36:42 | I |     + error = [19.3727]
25-01-23 10:36:43 | I |   - Calibrating model.layers.1.mlp.gate_proj.weight
25-01-23 10:36:43 | I |       - range scale = [    1.0000]
25-01-23 10:36:43 | I |         sum  error  = [    1.9871]
25-01-23 10:36:43 | I |         best error  = [    1.9871]
25-01-23 10:36:43 | I |     + error = [1.9871]
25-01-23 10:36:44 | I |       - range scale = [    1.0000]
25-01-23 10:36:44 | I |         sum  error  = [   20.6633]
25-01-23 10:36:44 | I |         best error  = [   20.6633]
25-01-23 10:36:44 | I |     + error = [20.6633]
25-01-23 10:36:45 | I |   - Calibrating model.layers.1.mlp.down_proj.weight
25-01-23 10:36:45 | I |       - range scale = [    1.0000]
25-01-23 10:36:45 | I |         sum  error  = [   43.5877]
25-01-23 10:36:45 | I |         best error  = [   43.5877]
25-01-23 10:36:45 | I |     + error = [43.5877]
25-01-23 10:36:47 | I |       - range scale = [    1.0000]
25-01-23 10:36:47 | I |         sum  error  = [  214.7035]
25-01-23 10:36:47 | I |         best error  = [  214.7035]
25-01-23 10:36:47 | I |     + error = [214.7035]
25-01-23 10:36:47 | I |   - Quantizing model.layers.1.self_attn.q_proj.weight
25-01-23 10:36:49 | I |   - Quantizing model.layers.1.self_attn.k_proj.weight
25-01-23 10:36:52 | I |   - Quantizing model.layers.1.self_attn.v_proj.weight
25-01-23 10:36:54 | I |   - Quantizing model.layers.1.self_attn.o_proj.weight
25-01-23 10:36:56 | I |   - Quantizing model.layers.1.mlp.up_proj.weight
25-01-23 10:36:58 | I |   - Quantizing model.layers.1.mlp.gate_proj.weight
25-01-23 10:37:01 | I |   - Quantizing model.layers.1.mlp.down_proj.weight
25-01-23 10:37:07 | I | quantizing activations for layer model.layers.1
25-01-23 10:37:07 | I | collecting calibration activations in model.layers.1
25-01-23 10:37:07 | I | collecting calibration activations in model.layers.1
25-01-23 10:37:09 | I | forward this layer
25-01-23 10:37:09 | I | input_file: /data/gyy/lmquant-main/lmquant/data/data_without_preprocess_llama_7b_bs16/shard/val_input_args_layer_1/11.pt
25-01-23 10:37:09 | I | output_file: /data/gyy/lmquant-main/lmquant/data/data_without_preprocess_llama_7b_bs16/shard/val_teacher_output_layer_1/11.pt
25-01-23 10:37:10 | I | In layer model.layers.1, gradient of model.layers.1.self_attn.q_proj has nan: True
25-01-23 10:37:10 | I | In layer model.layers.1, gradient of model.layers.1.self_attn.q_proj has inf: False
25-01-23 10:37:10 | I | nan: first position tensor([0, 0], device='cuda:0')
25-01-23 10:37:10 | I | In layer model.layers.1, gradient of model.layers.1.self_attn.k_proj has nan: True
25-01-23 10:37:10 | I | In layer model.layers.1, gradient of model.layers.1.self_attn.k_proj has inf: False
25-01-23 10:37:10 | I | nan: first position tensor([0, 0], device='cuda:0')
25-01-23 10:37:10 | I | In layer model.layers.1, gradient of model.layers.1.self_attn.v_proj has nan: True
25-01-23 10:37:10 | I | In layer model.layers.1, gradient of model.layers.1.self_attn.v_proj has inf: False
25-01-23 10:37:10 | I | nan: first position tensor([0, 0], device='cuda:0')
25-01-23 10:37:10 | I | In layer model.layers.1, gradient of model.layers.1.self_attn.o_proj has nan: False
25-01-23 10:37:10 | I | In layer model.layers.1, gradient of model.layers.1.self_attn.o_proj has inf: True
25-01-23 10:37:10 | I | inf: first position tensor([ 310, 3257], device='cuda:0')
25-01-23 10:37:10 | I | In layer model.layers.1, gradient of model.layers.1.mlp.up_proj has nan: False
25-01-23 10:37:10 | I | In layer model.layers.1, gradient of model.layers.1.mlp.up_proj has inf: False
25-01-23 10:37:10 | I | In layer model.layers.1, gradient of model.layers.1.mlp.gate_proj has nan: False
25-01-23 10:37:10 | I | In layer model.layers.1, gradient of model.layers.1.mlp.gate_proj has inf: False
25-01-23 10:37:10 | I | In layer model.layers.1, gradient of model.layers.1.mlp.down_proj has nan: False
25-01-23 10:37:10 | I | In layer model.layers.1, gradient of model.layers.1.mlp.down_proj has inf: True
25-01-23 10:37:10 | I | inf: first position tensor([1415, 7890], device='cuda:0')
25-01-23 10:37:10 | I | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 0.03125
25-01-23 10:37:10 | I | TRAIN CURRENT LAYER_IDX = 1
25-01-23 10:37:10 | I | in layer model.layers.1
25-01-23 10:37:10 | I | quantizing weights for layer model.layers.1
25-01-23 10:37:10 | I | collecting calibration activations in model.layers.1
25-01-23 10:37:10 | I | collecting calibration activations in model.layers.1
25-01-23 10:37:10 | I | - Quantizing decoder layer model.layers.1
25-01-23 10:37:10 | I |   - Calibrating model.layers.1.self_attn.q_proj.weight
25-01-23 10:37:11 | I |       - range scale = [    1.0000]
25-01-23 10:37:11 | I |         sum  error  = [    0.1649]
25-01-23 10:37:11 | I |         best error  = [    0.1649]
25-01-23 10:37:11 | I |     + error = [0.1649]
25-01-23 10:37:12 | I |       - range scale = [    1.0000]
25-01-23 10:37:12 | I |         sum  error  = [    1.5809]
25-01-23 10:37:12 | I |         best error  = [    1.5809]
25-01-23 10:37:12 | I |     + error = [1.5809]
25-01-23 10:37:12 | I |   - Calibrating model.layers.1.self_attn.k_proj.weight
25-01-23 10:37:13 | I |       - range scale = [    1.0000]
25-01-23 10:37:13 | I |         sum  error  = [    0.1702]
25-01-23 10:37:13 | I |         best error  = [    0.1702]
25-01-23 10:37:13 | I |     + error = [0.1702]
25-01-23 10:37:13 | I |       - range scale = [    1.0000]
25-01-23 10:37:13 | I |         sum  error  = [    1.7868]
25-01-23 10:37:13 | I |         best error  = [    1.7868]
25-01-23 10:37:13 | I |     + error = [1.7868]
25-01-23 10:37:14 | I |   - Calibrating model.layers.1.self_attn.v_proj.weight
25-01-23 10:37:14 | I |       - range scale = [    1.0000]
25-01-23 10:37:14 | I |         sum  error  = [    1.3568]
25-01-23 10:37:14 | I |         best error  = [    1.3568]
25-01-23 10:37:14 | I |     + error = [1.3568]
25-01-23 10:37:15 | I |       - range scale = [    1.0000]
25-01-23 10:37:15 | I |         sum  error  = [    9.4362]
25-01-23 10:37:15 | I |         best error  = [    9.4362]
25-01-23 10:37:15 | I |     + error = [9.4362]
25-01-23 10:37:15 | I |   - Calibrating model.layers.1.self_attn.o_proj.weight
25-01-23 10:37:16 | I |       - range scale = [    1.0000]
25-01-23 10:37:16 | I |         sum  error  = [    0.2311]
25-01-23 10:37:16 | I |         best error  = [    0.2311]
25-01-23 10:37:16 | I |     + error = [0.2311]
25-01-23 10:37:17 | I |       - range scale = [    1.0000]
25-01-23 10:37:17 | I |         sum  error  = [    2.0608]
25-01-23 10:37:17 | I |         best error  = [    2.0608]
25-01-23 10:37:17 | I |     + error = [2.0608]
25-01-23 10:37:17 | I |   - Calibrating model.layers.1.mlp.up_proj.weight
25-01-23 10:37:18 | I |       - range scale = [    1.0000]
25-01-23 10:37:18 | I |         sum  error  = [    1.8192]
25-01-23 10:37:18 | I |         best error  = [    1.8192]
25-01-23 10:37:18 | I |     + error = [1.8192]
25-01-23 10:37:19 | I |       - range scale = [    1.0000]
25-01-23 10:37:19 | I |         sum  error  = [   19.3245]
25-01-23 10:37:19 | I |         best error  = [   19.3245]
25-01-23 10:37:19 | I |     + error = [19.3245]
25-01-23 10:37:19 | I |   - Calibrating model.layers.1.mlp.gate_proj.weight
25-01-23 10:37:20 | I |       - range scale = [    1.0000]
25-01-23 10:37:20 | I |         sum  error  = [    1.9778]
25-01-23 10:37:20 | I |         best error  = [    1.9778]
25-01-23 10:37:20 | I |     + error = [1.9778]
25-01-23 10:37:21 | I |       - range scale = [    1.0000]
25-01-23 10:37:21 | I |         sum  error  = [   20.6188]
25-01-23 10:37:21 | I |         best error  = [   20.6188]
25-01-23 10:37:21 | I |     + error = [20.6188]
25-01-23 10:37:21 | I |   - Calibrating model.layers.1.mlp.down_proj.weight
25-01-23 10:37:22 | I |       - range scale = [    1.0000]
25-01-23 10:37:22 | I |         sum  error  = [   43.8910]
25-01-23 10:37:22 | I |         best error  = [   43.8910]
25-01-23 10:37:22 | I |     + error = [43.8910]
25-01-23 10:37:23 | I |       - range scale = [    1.0000]
25-01-23 10:37:23 | I |         sum  error  = [  216.0753]
25-01-23 10:37:23 | I |         best error  = [  216.0753]
25-01-23 10:37:23 | I |     + error = [216.0753]
25-01-23 10:37:24 | I |   - Quantizing model.layers.1.self_attn.q_proj.weight
25-01-23 10:37:26 | I |   - Quantizing model.layers.1.self_attn.k_proj.weight
25-01-23 10:37:28 | I |   - Quantizing model.layers.1.self_attn.v_proj.weight
25-01-23 10:37:30 | I |   - Quantizing model.layers.1.self_attn.o_proj.weight
25-01-23 10:37:33 | I |   - Quantizing model.layers.1.mlp.up_proj.weight
25-01-23 10:37:35 | I |   - Quantizing model.layers.1.mlp.gate_proj.weight
25-01-23 10:37:37 | I |   - Quantizing model.layers.1.mlp.down_proj.weight
25-01-23 10:37:43 | I | quantizing activations for layer model.layers.1
25-01-23 10:37:43 | I | collecting calibration activations in model.layers.1
25-01-23 10:37:44 | I | collecting calibration activations in model.layers.1
25-01-23 10:37:45 | I | forward this layer
25-01-23 10:37:45 | I | input_file: /data/gyy/lmquant-main/lmquant/data/data_without_preprocess_llama_7b_bs16/shard/val_input_args_layer_1/12.pt
25-01-23 10:37:45 | I | output_file: /data/gyy/lmquant-main/lmquant/data/data_without_preprocess_llama_7b_bs16/shard/val_teacher_output_layer_1/12.pt
25-01-23 10:37:46 | I | In layer model.layers.1, gradient of model.layers.1.self_attn.q_proj has nan: False
25-01-23 10:37:46 | I | In layer model.layers.1, gradient of model.layers.1.self_attn.q_proj has inf: False
25-01-23 10:37:46 | I | In layer model.layers.1, gradient of model.layers.1.self_attn.k_proj has nan: False
25-01-23 10:37:46 | I | In layer model.layers.1, gradient of model.layers.1.self_attn.k_proj has inf: False
25-01-23 10:37:46 | I | In layer model.layers.1, gradient of model.layers.1.self_attn.v_proj has nan: False
25-01-23 10:37:46 | I | In layer model.layers.1, gradient of model.layers.1.self_attn.v_proj has inf: True
25-01-23 10:37:46 | I | inf: first position tensor([  55, 1512], device='cuda:0')
25-01-23 10:37:46 | I | In layer model.layers.1, gradient of model.layers.1.self_attn.o_proj has nan: False
25-01-23 10:37:46 | I | In layer model.layers.1, gradient of model.layers.1.self_attn.o_proj has inf: True
25-01-23 10:37:46 | I | inf: first position tensor([ 310, 3257], device='cuda:0')
25-01-23 10:37:46 | I | In layer model.layers.1, gradient of model.layers.1.mlp.up_proj has nan: False
25-01-23 10:37:46 | I | In layer model.layers.1, gradient of model.layers.1.mlp.up_proj has inf: False
25-01-23 10:37:46 | I | In layer model.layers.1, gradient of model.layers.1.mlp.gate_proj has nan: False
25-01-23 10:37:46 | I | In layer model.layers.1, gradient of model.layers.1.mlp.gate_proj has inf: False
25-01-23 10:37:46 | I | In layer model.layers.1, gradient of model.layers.1.mlp.down_proj has nan: False
25-01-23 10:37:46 | I | In layer model.layers.1, gradient of model.layers.1.mlp.down_proj has inf: True
25-01-23 10:37:46 | I | inf: first position tensor([1415, 7890], device='cuda:0')
25-01-23 10:37:46 | I | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 0.015625
25-01-23 10:37:46 | I | TRAIN CURRENT LAYER_IDX = 1
25-01-23 10:37:46 | I | in layer model.layers.1
25-01-23 10:37:46 | I | quantizing weights for layer model.layers.1
25-01-23 10:37:46 | I | collecting calibration activations in model.layers.1
25-01-23 10:37:46 | I | collecting calibration activations in model.layers.1
25-01-23 10:37:47 | I | - Quantizing decoder layer model.layers.1
25-01-23 10:37:47 | I |   - Calibrating model.layers.1.self_attn.q_proj.weight
25-01-23 10:37:47 | I |       - range scale = [    1.0000]
25-01-23 10:37:47 | I |         sum  error  = [    0.1701]
25-01-23 10:37:47 | I |         best error  = [    0.1701]
25-01-23 10:37:47 | I |     + error = [0.1701]
25-01-23 10:37:48 | I |       - range scale = [    1.0000]
25-01-23 10:37:48 | I |         sum  error  = [    1.6448]
25-01-23 10:37:48 | I |         best error  = [    1.6448]
25-01-23 10:37:48 | I |     + error = [1.6448]
25-01-23 10:37:48 | I |   - Calibrating model.layers.1.self_attn.k_proj.weight
25-01-23 10:37:49 | I |       - range scale = [    1.0000]
25-01-23 10:37:49 | I |         sum  error  = [    0.1760]
25-01-23 10:37:49 | I |         best error  = [    0.1760]
25-01-23 10:37:49 | I |     + error = [0.1760]
25-01-23 10:37:50 | I |       - range scale = [    1.0000]
25-01-23 10:37:50 | I |         sum  error  = [    1.8383]
25-01-23 10:37:50 | I |         best error  = [    1.8383]
25-01-23 10:37:50 | I |     + error = [1.8383]
25-01-23 10:37:50 | I |   - Calibrating model.layers.1.self_attn.v_proj.weight
25-01-23 10:37:51 | I |       - range scale = [    1.0000]
25-01-23 10:37:51 | I |         sum  error  = [    1.3568]
25-01-23 10:37:51 | I |         best error  = [    1.3568]
25-01-23 10:37:51 | I |     + error = [1.3568]
25-01-23 10:37:51 | I |       - range scale = [    1.0000]
25-01-23 10:37:51 | I |         sum  error  = [    9.3668]
25-01-23 10:37:51 | I |         best error  = [    9.3668]
25-01-23 10:37:51 | I |     + error = [9.3668]
25-01-23 10:37:52 | I |   - Calibrating model.layers.1.self_attn.o_proj.weight
25-01-23 10:37:52 | I |       - range scale = [    1.0000]
25-01-23 10:37:52 | I |         sum  error  = [    0.2305]
25-01-23 10:37:52 | I |         best error  = [    0.2305]
25-01-23 10:37:52 | I |     + error = [0.2305]
25-01-23 10:37:53 | I |       - range scale = [    1.0000]
25-01-23 10:37:53 | I |         sum  error  = [    2.0679]
25-01-23 10:37:53 | I |         best error  = [    2.0679]
25-01-23 10:37:53 | I |     + error = [2.0679]
25-01-23 10:37:53 | I |   - Calibrating model.layers.1.mlp.up_proj.weight
25-01-23 10:37:54 | I |       - range scale = [    1.0000]
25-01-23 10:37:54 | I |         sum  error  = [    1.8135]
25-01-23 10:37:54 | I |         best error  = [    1.8135]
25-01-23 10:37:54 | I |     + error = [1.8135]
25-01-23 10:37:55 | I |       - range scale = [    1.0000]
25-01-23 10:37:55 | I |         sum  error  = [   19.2776]
25-01-23 10:37:55 | I |         best error  = [   19.2776]
25-01-23 10:37:55 | I |     + error = [19.2776]
25-01-23 10:37:56 | I |   - Calibrating model.layers.1.mlp.gate_proj.weight
25-01-23 10:37:56 | I |       - range scale = [    1.0000]
25-01-23 10:37:56 | I |         sum  error  = [    1.9773]
25-01-23 10:37:56 | I |         best error  = [    1.9773]
25-01-23 10:37:56 | I |     + error = [1.9773]
25-01-23 10:37:57 | I |       - range scale = [    1.0000]
25-01-23 10:37:57 | I |         sum  error  = [   20.5611]
25-01-23 10:37:57 | I |         best error  = [   20.5611]
25-01-23 10:37:57 | I |     + error = [20.5611]
25-01-23 10:37:58 | I |   - Calibrating model.layers.1.mlp.down_proj.weight
25-01-23 10:37:58 | I |       - range scale = [    1.0000]
25-01-23 10:37:58 | I |         sum  error  = [   46.2180]
25-01-23 10:37:58 | I |         best error  = [   46.2180]
25-01-23 10:37:58 | I |     + error = [46.2180]
25-01-23 10:37:59 | I |       - range scale = [    1.0000]
25-01-23 10:37:59 | I |         sum  error  = [  226.8618]
25-01-23 10:37:59 | I |         best error  = [  226.8618]
25-01-23 10:37:59 | I |     + error = [226.8618]
25-01-23 10:38:00 | I |   - Quantizing model.layers.1.self_attn.q_proj.weight
25-01-23 10:38:02 | I |   - Quantizing model.layers.1.self_attn.k_proj.weight
25-01-23 10:38:04 | I |   - Quantizing model.layers.1.self_attn.v_proj.weight
25-01-23 10:38:07 | I |   - Quantizing model.layers.1.self_attn.o_proj.weight
25-01-23 10:38:09 | I |   - Quantizing model.layers.1.mlp.up_proj.weight
25-01-23 10:38:11 | I |   - Quantizing model.layers.1.mlp.gate_proj.weight
25-01-23 10:38:13 | I |   - Quantizing model.layers.1.mlp.down_proj.weight
25-01-23 10:38:19 | I | quantizing activations for layer model.layers.1
25-01-23 10:38:20 | I | collecting calibration activations in model.layers.1
25-01-23 10:38:20 | I | collecting calibration activations in model.layers.1
25-01-23 10:38:22 | I | forward this layer
25-01-23 10:38:22 | I | input_file: /data/gyy/lmquant-main/lmquant/data/data_without_preprocess_llama_7b_bs16/shard/val_input_args_layer_1/13.pt
25-01-23 10:38:22 | I | output_file: /data/gyy/lmquant-main/lmquant/data/data_without_preprocess_llama_7b_bs16/shard/val_teacher_output_layer_1/13.pt
25-01-23 10:38:22 | I | In layer model.layers.1, gradient of model.layers.1.self_attn.q_proj has nan: False
25-01-23 10:38:22 | I | In layer model.layers.1, gradient of model.layers.1.self_attn.q_proj has inf: False
25-01-23 10:38:22 | I | In layer model.layers.1, gradient of model.layers.1.self_attn.k_proj has nan: False
25-01-23 10:38:22 | I | In layer model.layers.1, gradient of model.layers.1.self_attn.k_proj has inf: False
25-01-23 10:38:22 | I | In layer model.layers.1, gradient of model.layers.1.self_attn.v_proj has nan: False
25-01-23 10:38:22 | I | In layer model.layers.1, gradient of model.layers.1.self_attn.v_proj has inf: True
25-01-23 10:38:22 | I | inf: first position tensor([ 133, 1512], device='cuda:0')
25-01-23 10:38:22 | I | In layer model.layers.1, gradient of model.layers.1.self_attn.o_proj has nan: False
25-01-23 10:38:22 | I | In layer model.layers.1, gradient of model.layers.1.self_attn.o_proj has inf: False
25-01-23 10:38:22 | I | In layer model.layers.1, gradient of model.layers.1.mlp.up_proj has nan: False
25-01-23 10:38:22 | I | In layer model.layers.1, gradient of model.layers.1.mlp.up_proj has inf: False
25-01-23 10:38:22 | I | In layer model.layers.1, gradient of model.layers.1.mlp.gate_proj has nan: False
25-01-23 10:38:22 | I | In layer model.layers.1, gradient of model.layers.1.mlp.gate_proj has inf: False
25-01-23 10:38:22 | I | In layer model.layers.1, gradient of model.layers.1.mlp.down_proj has nan: False
25-01-23 10:38:22 | I | In layer model.layers.1, gradient of model.layers.1.mlp.down_proj has inf: True
25-01-23 10:38:22 | I | inf: first position tensor([1415, 7890], device='cuda:0')
25-01-23 10:38:22 | I | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 0.0078125
25-01-23 10:38:22 | I | TRAIN CURRENT LAYER_IDX = 1
25-01-23 10:38:22 | I | in layer model.layers.1
25-01-23 10:38:22 | I | quantizing weights for layer model.layers.1
25-01-23 10:38:23 | I | collecting calibration activations in model.layers.1
25-01-23 10:38:23 | I | collecting calibration activations in model.layers.1
25-01-23 10:38:23 | I | - Quantizing decoder layer model.layers.1
25-01-23 10:38:23 | I |   - Calibrating model.layers.1.self_attn.q_proj.weight
25-01-23 10:38:24 | I |       - range scale = [    1.0000]
25-01-23 10:38:24 | I |         sum  error  = [    0.1711]
25-01-23 10:38:24 | I |         best error  = [    0.1711]
25-01-23 10:38:24 | I |     + error = [0.1711]
25-01-23 10:38:25 | I |       - range scale = [    1.0000]
25-01-23 10:38:25 | I |         sum  error  = [    1.5969]
25-01-23 10:38:25 | I |         best error  = [    1.5969]
25-01-23 10:38:25 | I |     + error = [1.5969]
25-01-23 10:38:25 | I |   - Calibrating model.layers.1.self_attn.k_proj.weight
25-01-23 10:38:26 | I |       - range scale = [    1.0000]
25-01-23 10:38:26 | I |         sum  error  = [    0.1679]
25-01-23 10:38:26 | I |         best error  = [    0.1679]
25-01-23 10:38:26 | I |     + error = [0.1679]
25-01-23 10:38:26 | I |       - range scale = [    1.0000]
25-01-23 10:38:26 | I |         sum  error  = [    1.7829]
25-01-23 10:38:26 | I |         best error  = [    1.7829]
25-01-23 10:38:26 | I |     + error = [1.7829]
25-01-23 10:38:26 | I |   - Calibrating model.layers.1.self_attn.v_proj.weight
25-01-23 10:38:27 | I |       - range scale = [    1.0000]
25-01-23 10:38:27 | I |         sum  error  = [    1.3526]
25-01-23 10:38:27 | I |         best error  = [    1.3526]
25-01-23 10:38:27 | I |     + error = [1.3526]
25-01-23 10:38:28 | I |       - range scale = [    1.0000]
25-01-23 10:38:28 | I |         sum  error  = [    9.1790]
25-01-23 10:38:28 | I |         best error  = [    9.1790]
25-01-23 10:38:28 | I |     + error = [9.1790]
25-01-23 10:38:28 | I |   - Calibrating model.layers.1.self_attn.o_proj.weight
25-01-23 10:38:29 | I |       - range scale = [    1.0000]
25-01-23 10:38:29 | I |         sum  error  = [    0.2322]
25-01-23 10:38:29 | I |         best error  = [    0.2322]
25-01-23 10:38:29 | I |     + error = [0.2322]
25-01-23 10:38:30 | I |       - range scale = [    1.0000]
25-01-23 10:38:30 | I |         sum  error  = [    2.0798]
25-01-23 10:38:30 | I |         best error  = [    2.0798]
25-01-23 10:38:30 | I |     + error = [2.0798]
25-01-23 10:38:30 | I |   - Calibrating model.layers.1.mlp.up_proj.weight
25-01-23 10:38:31 | I |       - range scale = [    1.0000]
25-01-23 10:38:31 | I |         sum  error  = [    1.8081]
25-01-23 10:38:31 | I |         best error  = [    1.8081]
25-01-23 10:38:31 | I |     + error = [1.8081]
25-01-23 10:38:32 | I |       - range scale = [    1.0000]
25-01-23 10:38:32 | I |         sum  error  = [   19.2224]
25-01-23 10:38:32 | I |         best error  = [   19.2224]
25-01-23 10:38:32 | I |     + error = [19.2224]
25-01-23 10:38:32 | I |   - Calibrating model.layers.1.mlp.gate_proj.weight
25-01-23 10:38:33 | I |       - range scale = [    1.0000]
25-01-23 10:38:33 | I |         sum  error  = [    1.9708]
25-01-23 10:38:33 | I |         best error  = [    1.9708]
25-01-23 10:38:33 | I |     + error = [1.9708]
25-01-23 10:38:34 | I |       - range scale = [    1.0000]
25-01-23 10:38:34 | I |         sum  error  = [   20.4977]
25-01-23 10:38:34 | I |         best error  = [   20.4977]
25-01-23 10:38:34 | I |     + error = [20.4977]
25-01-23 10:38:34 | I |   - Calibrating model.layers.1.mlp.down_proj.weight
25-01-23 10:38:35 | I |       - range scale = [    1.0000]
25-01-23 10:38:35 | I |         sum  error  = [   52.6618]
25-01-23 10:38:35 | I |         best error  = [   52.6618]
25-01-23 10:38:35 | I |     + error = [52.6618]
25-01-23 10:38:36 | I |       - range scale = [    1.0000]
25-01-23 10:38:36 | I |         sum  error  = [  257.9705]
25-01-23 10:38:36 | I |         best error  = [  257.9705]
25-01-23 10:38:36 | I |     + error = [257.9705]
25-01-23 10:38:36 | I |   - Quantizing model.layers.1.self_attn.q_proj.weight
25-01-23 10:38:39 | I |   - Quantizing model.layers.1.self_attn.k_proj.weight
25-01-23 10:38:41 | I |   - Quantizing model.layers.1.self_attn.v_proj.weight
25-01-23 10:38:43 | I |   - Quantizing model.layers.1.self_attn.o_proj.weight
25-01-23 10:38:46 | I |   - Quantizing model.layers.1.mlp.up_proj.weight
25-01-23 10:38:48 | I |   - Quantizing model.layers.1.mlp.gate_proj.weight
25-01-23 10:38:50 | I |   - Quantizing model.layers.1.mlp.down_proj.weight
25-01-23 10:38:56 | I | quantizing activations for layer model.layers.1
25-01-23 10:38:57 | I | collecting calibration activations in model.layers.1
25-01-23 10:38:57 | I | collecting calibration activations in model.layers.1
25-01-23 10:38:58 | I | forward this layer
25-01-23 10:38:58 | I | input_file: /data/gyy/lmquant-main/lmquant/data/data_without_preprocess_llama_7b_bs16/shard/val_input_args_layer_1/14.pt
25-01-23 10:38:58 | I | output_file: /data/gyy/lmquant-main/lmquant/data/data_without_preprocess_llama_7b_bs16/shard/val_teacher_output_layer_1/14.pt
25-01-23 10:38:59 | I | In layer model.layers.1, gradient of model.layers.1.self_attn.q_proj has nan: False
25-01-23 10:38:59 | I | In layer model.layers.1, gradient of model.layers.1.self_attn.q_proj has inf: False
25-01-23 10:38:59 | I | In layer model.layers.1, gradient of model.layers.1.self_attn.k_proj has nan: False
25-01-23 10:38:59 | I | In layer model.layers.1, gradient of model.layers.1.self_attn.k_proj has inf: False
25-01-23 10:38:59 | I | In layer model.layers.1, gradient of model.layers.1.self_attn.v_proj has nan: False
25-01-23 10:38:59 | I | In layer model.layers.1, gradient of model.layers.1.self_attn.v_proj has inf: True
25-01-23 10:38:59 | I | inf: first position tensor([1062, 1512], device='cuda:0')
25-01-23 10:38:59 | I | In layer model.layers.1, gradient of model.layers.1.self_attn.o_proj has nan: False
25-01-23 10:38:59 | I | In layer model.layers.1, gradient of model.layers.1.self_attn.o_proj has inf: False
25-01-23 10:38:59 | I | In layer model.layers.1, gradient of model.layers.1.mlp.up_proj has nan: False
25-01-23 10:38:59 | I | In layer model.layers.1, gradient of model.layers.1.mlp.up_proj has inf: False
25-01-23 10:38:59 | I | In layer model.layers.1, gradient of model.layers.1.mlp.gate_proj has nan: False
25-01-23 10:38:59 | I | In layer model.layers.1, gradient of model.layers.1.mlp.gate_proj has inf: False
25-01-23 10:38:59 | I | In layer model.layers.1, gradient of model.layers.1.mlp.down_proj has nan: False
25-01-23 10:38:59 | I | In layer model.layers.1, gradient of model.layers.1.mlp.down_proj has inf: False
25-01-23 10:38:59 | I | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 0.00390625
25-01-23 10:38:59 | I | TRAIN CURRENT LAYER_IDX = 1
25-01-23 10:38:59 | I | in layer model.layers.1
25-01-23 10:38:59 | I | quantizing weights for layer model.layers.1
25-01-23 10:39:00 | I | collecting calibration activations in model.layers.1
25-01-23 10:39:00 | I | collecting calibration activations in model.layers.1
25-01-23 10:39:00 | I | - Quantizing decoder layer model.layers.1
25-01-23 10:39:00 | I |   - Calibrating model.layers.1.self_attn.q_proj.weight
25-01-23 10:39:01 | I |       - range scale = [    1.0000]
25-01-23 10:39:01 | I |         sum  error  = [    0.1765]
25-01-23 10:39:01 | I |         best error  = [    0.1765]
25-01-23 10:39:01 | I |     + error = [0.1765]
25-01-23 10:39:01 | I |       - range scale = [    1.0000]
25-01-23 10:39:01 | I |         sum  error  = [    1.6661]
25-01-23 10:39:01 | I |         best error  = [    1.6661]
25-01-23 10:39:01 | I |     + error = [1.6661]
25-01-23 10:39:02 | I |   - Calibrating model.layers.1.self_attn.k_proj.weight
25-01-23 10:39:02 | I |       - range scale = [    1.0000]
25-01-23 10:39:02 | I |         sum  error  = [    0.1796]
25-01-23 10:39:02 | I |         best error  = [    0.1796]
25-01-23 10:39:02 | I |     + error = [0.1796]
25-01-23 10:39:03 | I |       - range scale = [    1.0000]
25-01-23 10:39:03 | I |         sum  error  = [    1.9082]
25-01-23 10:39:03 | I |         best error  = [    1.9082]
25-01-23 10:39:03 | I |     + error = [1.9082]
25-01-23 10:39:03 | I |   - Calibrating model.layers.1.self_attn.v_proj.weight
25-01-23 10:39:04 | I |       - range scale = [    1.0000]
25-01-23 10:39:04 | I |         sum  error  = [    1.3504]
25-01-23 10:39:04 | I |         best error  = [    1.3504]
25-01-23 10:39:04 | I |     + error = [1.3504]
25-01-23 10:39:05 | I |       - range scale = [    1.0000]
25-01-23 10:39:05 | I |         sum  error  = [    9.5516]
25-01-23 10:39:05 | I |         best error  = [    9.5516]
25-01-23 10:39:05 | I |     + error = [9.5516]
25-01-23 10:39:05 | I |   - Calibrating model.layers.1.self_attn.o_proj.weight
25-01-23 10:39:06 | I |       - range scale = [    1.0000]
25-01-23 10:39:06 | I |         sum  error  = [    0.2397]
25-01-23 10:39:06 | I |         best error  = [    0.2397]
25-01-23 10:39:06 | I |     + error = [0.2397]
25-01-23 10:39:06 | I |       - range scale = [    1.0000]
25-01-23 10:39:06 | I |         sum  error  = [    2.1327]
25-01-23 10:39:06 | I |         best error  = [    2.1327]
25-01-23 10:39:06 | I |     + error = [2.1327]
25-01-23 10:39:07 | I |   - Calibrating model.layers.1.mlp.up_proj.weight
25-01-23 10:39:07 | I |       - range scale = [    1.0000]
25-01-23 10:39:07 | I |         sum  error  = [    1.8596]
25-01-23 10:39:07 | I |         best error  = [    1.8596]
25-01-23 10:39:07 | I |     + error = [1.8596]
25-01-23 10:39:08 | I |       - range scale = [    1.0000]
25-01-23 10:39:08 | I |         sum  error  = [   19.7679]
25-01-23 10:39:08 | I |         best error  = [   19.7679]
25-01-23 10:39:08 | I |     + error = [19.7679]
25-01-23 10:39:09 | I |   - Calibrating model.layers.1.mlp.gate_proj.weight
25-01-23 10:39:09 | I |       - range scale = [    1.0000]
25-01-23 10:39:09 | I |         sum  error  = [    2.0265]
25-01-23 10:39:09 | I |         best error  = [    2.0265]
25-01-23 10:39:09 | I |     + error = [2.0265]
25-01-23 10:39:11 | I |       - range scale = [    1.0000]
25-01-23 10:39:11 | I |         sum  error  = [   21.0824]
25-01-23 10:39:11 | I |         best error  = [   21.0824]
25-01-23 10:39:11 | I |     + error = [21.0824]
25-01-23 10:39:11 | I |   - Calibrating model.layers.1.mlp.down_proj.weight
25-01-23 10:39:11 | I |       - range scale = [    1.0000]
25-01-23 10:39:11 | I |         sum  error  = [   52.4565]
25-01-23 10:39:11 | I |         best error  = [   52.4565]
25-01-23 10:39:11 | I |     + error = [52.4565]
25-01-23 10:39:13 | I |       - range scale = [    1.0000]
25-01-23 10:39:13 | I |         sum  error  = [  257.1226]
25-01-23 10:39:13 | I |         best error  = [  257.1226]
25-01-23 10:39:13 | I |     + error = [257.1226]
25-01-23 10:39:13 | I |   - Quantizing model.layers.1.self_attn.q_proj.weight
25-01-23 10:39:16 | I |   - Quantizing model.layers.1.self_attn.k_proj.weight
25-01-23 10:39:18 | I |   - Quantizing model.layers.1.self_attn.v_proj.weight
25-01-23 10:39:21 | I |   - Quantizing model.layers.1.self_attn.o_proj.weight
25-01-23 10:39:24 | I |   - Quantizing model.layers.1.mlp.up_proj.weight
25-01-23 10:39:27 | I |   - Quantizing model.layers.1.mlp.gate_proj.weight
25-01-23 10:39:30 | I |   - Quantizing model.layers.1.mlp.down_proj.weight
25-01-23 10:39:37 | I | quantizing activations for layer model.layers.1
25-01-23 10:39:38 | I | collecting calibration activations in model.layers.1
25-01-23 10:39:38 | I | collecting calibration activations in model.layers.1
25-01-23 10:39:40 | I | forward this layer
25-01-23 10:39:40 | I | input_file: /data/gyy/lmquant-main/lmquant/data/data_without_preprocess_llama_7b_bs16/shard/val_input_args_layer_1/15.pt
25-01-23 10:39:40 | I | output_file: /data/gyy/lmquant-main/lmquant/data/data_without_preprocess_llama_7b_bs16/shard/val_teacher_output_layer_1/15.pt
25-01-23 10:39:40 | I | In layer model.layers.1, gradient of model.layers.1.self_attn.q_proj has nan: False
25-01-23 10:39:40 | I | In layer model.layers.1, gradient of model.layers.1.self_attn.q_proj has inf: False
25-01-23 10:39:40 | I | In layer model.layers.1, gradient of model.layers.1.self_attn.k_proj has nan: False
25-01-23 10:39:40 | I | In layer model.layers.1, gradient of model.layers.1.self_attn.k_proj has inf: False
25-01-23 10:39:40 | I | In layer model.layers.1, gradient of model.layers.1.self_attn.v_proj has nan: False
25-01-23 10:39:40 | I | In layer model.layers.1, gradient of model.layers.1.self_attn.v_proj has inf: True
25-01-23 10:39:40 | I | inf: first position tensor([1062, 1512], device='cuda:0')
25-01-23 10:39:40 | I | In layer model.layers.1, gradient of model.layers.1.self_attn.o_proj has nan: False
25-01-23 10:39:40 | I | In layer model.layers.1, gradient of model.layers.1.self_attn.o_proj has inf: False
25-01-23 10:39:40 | I | In layer model.layers.1, gradient of model.layers.1.mlp.up_proj has nan: False
25-01-23 10:39:40 | I | In layer model.layers.1, gradient of model.layers.1.mlp.up_proj has inf: False
25-01-23 10:39:40 | I | In layer model.layers.1, gradient of model.layers.1.mlp.gate_proj has nan: False
25-01-23 10:39:40 | I | In layer model.layers.1, gradient of model.layers.1.mlp.gate_proj has inf: False
25-01-23 10:39:40 | I | In layer model.layers.1, gradient of model.layers.1.mlp.down_proj has nan: False
25-01-23 10:39:40 | I | In layer model.layers.1, gradient of model.layers.1.mlp.down_proj has inf: False
25-01-23 10:39:40 | I | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 0.001953125
25-01-23 10:39:40 | I | TRAIN CURRENT LAYER_IDX = 1
25-01-23 10:39:40 | I | in layer model.layers.1
25-01-23 10:39:40 | I | quantizing weights for layer model.layers.1
25-01-23 10:39:41 | I | collecting calibration activations in model.layers.1
25-01-23 10:39:41 | I | collecting calibration activations in model.layers.1
25-01-23 10:39:41 | I | - Quantizing decoder layer model.layers.1
25-01-23 10:39:41 | I |   - Calibrating model.layers.1.self_attn.q_proj.weight
25-01-23 10:39:42 | I |       - range scale = [    1.0000]
25-01-23 10:39:42 | I |         sum  error  = [    0.1806]
25-01-23 10:39:42 | I |         best error  = [    0.1806]
25-01-23 10:39:42 | I |     + error = [0.1806]
25-01-23 10:39:42 | I |       - range scale = [    1.0000]
25-01-23 10:39:42 | I |         sum  error  = [    1.7131]
25-01-23 10:39:42 | I |         best error  = [    1.7131]
25-01-23 10:39:42 | I |     + error = [1.7131]
25-01-23 10:39:43 | I |   - Calibrating model.layers.1.self_attn.k_proj.weight
25-01-23 10:39:43 | I |       - range scale = [    1.0000]
25-01-23 10:39:43 | I |         sum  error  = [    0.1838]
25-01-23 10:39:43 | I |         best error  = [    0.1838]
25-01-23 10:39:43 | I |     + error = [0.1838]
25-01-23 10:39:44 | I |       - range scale = [    1.0000]
25-01-23 10:39:44 | I |         sum  error  = [    1.8841]
25-01-23 10:39:44 | I |         best error  = [    1.8841]
25-01-23 10:39:44 | I |     + error = [1.8841]
25-01-23 10:39:44 | I |   - Calibrating model.layers.1.self_attn.v_proj.weight
25-01-23 10:39:45 | I |       - range scale = [    1.0000]
25-01-23 10:39:45 | I |         sum  error  = [    1.3500]
25-01-23 10:39:45 | I |         best error  = [    1.3500]
25-01-23 10:39:45 | I |     + error = [1.3500]
25-01-23 10:39:46 | I |       - range scale = [    1.0000]
25-01-23 10:39:46 | I |         sum  error  = [    9.5936]
25-01-23 10:39:46 | I |         best error  = [    9.5936]
25-01-23 10:39:46 | I |     + error = [9.5936]
25-01-23 10:39:46 | I |   - Calibrating model.layers.1.self_attn.o_proj.weight
25-01-23 10:39:47 | I |       - range scale = [    1.0000]
25-01-23 10:39:47 | I |         sum  error  = [    0.2562]
25-01-23 10:39:47 | I |         best error  = [    0.2562]
25-01-23 10:39:47 | I |     + error = [0.2562]
25-01-23 10:39:47 | I |       - range scale = [    1.0000]
25-01-23 10:39:47 | I |         sum  error  = [    2.2612]
25-01-23 10:39:47 | I |         best error  = [    2.2612]
25-01-23 10:39:47 | I |     + error = [2.2612]
25-01-23 10:39:48 | I |   - Calibrating model.layers.1.mlp.up_proj.weight
25-01-23 10:39:48 | I |       - range scale = [    1.0000]
25-01-23 10:39:48 | I |         sum  error  = [    1.9516]
25-01-23 10:39:48 | I |         best error  = [    1.9516]
25-01-23 10:39:48 | I |     + error = [1.9516]
25-01-23 10:39:49 | I |       - range scale = [    1.0000]
25-01-23 10:39:49 | I |         sum  error  = [   20.7393]
25-01-23 10:39:49 | I |         best error  = [   20.7393]
25-01-23 10:39:49 | I |     + error = [20.7393]
25-01-23 10:39:50 | I |   - Calibrating model.layers.1.mlp.gate_proj.weight
25-01-23 10:39:50 | I |       - range scale = [    1.0000]
25-01-23 10:39:50 | I |         sum  error  = [    2.1302]
25-01-23 10:39:50 | I |         best error  = [    2.1302]
25-01-23 10:39:50 | I |     + error = [2.1302]
25-01-23 10:39:51 | I |       - range scale = [    1.0000]
25-01-23 10:39:51 | I |         sum  error  = [   22.1112]
25-01-23 10:39:51 | I |         best error  = [   22.1112]
25-01-23 10:39:51 | I |     + error = [22.1112]
25-01-23 10:39:52 | I |   - Calibrating model.layers.1.mlp.down_proj.weight
25-01-23 10:39:52 | I |       - range scale = [    1.0000]
25-01-23 10:39:52 | I |         sum  error  = [   47.3261]
25-01-23 10:39:52 | I |         best error  = [   47.3261]
25-01-23 10:39:52 | I |     + error = [47.3261]
25-01-23 10:39:54 | I |       - range scale = [    1.0000]
25-01-23 10:39:54 | I |         sum  error  = [  232.2162]
25-01-23 10:39:54 | I |         best error  = [  232.2162]
25-01-23 10:39:54 | I |     + error = [232.2162]
25-01-23 10:39:54 | I |   - Quantizing model.layers.1.self_attn.q_proj.weight
25-01-23 10:39:57 | I |   - Quantizing model.layers.1.self_attn.k_proj.weight
25-01-23 10:40:00 | I |   - Quantizing model.layers.1.self_attn.v_proj.weight
25-01-23 10:40:02 | I |   - Quantizing model.layers.1.self_attn.o_proj.weight
25-01-23 10:40:05 | I |   - Quantizing model.layers.1.mlp.up_proj.weight
25-01-23 10:40:08 | I |   - Quantizing model.layers.1.mlp.gate_proj.weight
25-01-23 10:40:11 | I |   - Quantizing model.layers.1.mlp.down_proj.weight
25-01-23 10:40:18 | I | quantizing activations for layer model.layers.1
25-01-23 10:40:19 | I | collecting calibration activations in model.layers.1
25-01-23 10:40:19 | I | collecting calibration activations in model.layers.1
25-01-23 10:40:21 | I | forward this layer
25-01-23 10:40:21 | I | input_file: /data/gyy/lmquant-main/lmquant/data/data_without_preprocess_llama_7b_bs16/shard/val_input_args_layer_1/16.pt
25-01-23 10:40:21 | I | output_file: /data/gyy/lmquant-main/lmquant/data/data_without_preprocess_llama_7b_bs16/shard/val_teacher_output_layer_1/16.pt
25-01-23 10:40:21 | I | In layer model.layers.1, gradient of model.layers.1.self_attn.q_proj has nan: False
25-01-23 10:40:21 | I | In layer model.layers.1, gradient of model.layers.1.self_attn.q_proj has inf: False
25-01-23 10:40:21 | I | In layer model.layers.1, gradient of model.layers.1.self_attn.k_proj has nan: False
25-01-23 10:40:21 | I | In layer model.layers.1, gradient of model.layers.1.self_attn.k_proj has inf: False
25-01-23 10:40:21 | I | In layer model.layers.1, gradient of model.layers.1.self_attn.v_proj has nan: False
25-01-23 10:40:21 | I | In layer model.layers.1, gradient of model.layers.1.self_attn.v_proj has inf: True
25-01-23 10:40:21 | I | inf: first position tensor([1062, 1512], device='cuda:0')
25-01-23 10:40:21 | I | In layer model.layers.1, gradient of model.layers.1.self_attn.o_proj has nan: False
25-01-23 10:40:21 | I | In layer model.layers.1, gradient of model.layers.1.self_attn.o_proj has inf: False
25-01-23 10:40:21 | I | In layer model.layers.1, gradient of model.layers.1.mlp.up_proj has nan: False
25-01-23 10:40:21 | I | In layer model.layers.1, gradient of model.layers.1.mlp.up_proj has inf: False
25-01-23 10:40:21 | I | In layer model.layers.1, gradient of model.layers.1.mlp.gate_proj has nan: False
25-01-23 10:40:21 | I | In layer model.layers.1, gradient of model.layers.1.mlp.gate_proj has inf: False
25-01-23 10:40:21 | I | In layer model.layers.1, gradient of model.layers.1.mlp.down_proj has nan: False
25-01-23 10:40:21 | I | In layer model.layers.1, gradient of model.layers.1.mlp.down_proj has inf: False
25-01-23 10:40:21 | I | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 0.0009765625
25-01-23 10:40:21 | I | TRAIN CURRENT LAYER_IDX = 1
25-01-23 10:40:21 | I | in layer model.layers.1
25-01-23 10:40:21 | I | quantizing weights for layer model.layers.1
25-01-23 10:40:22 | I | collecting calibration activations in model.layers.1
25-01-23 10:40:22 | I | collecting calibration activations in model.layers.1
25-01-23 10:40:22 | I | - Quantizing decoder layer model.layers.1
25-01-23 10:40:22 | I |   - Calibrating model.layers.1.self_attn.q_proj.weight
25-01-23 10:40:23 | I |       - range scale = [    1.0000]
25-01-23 10:40:23 | I |         sum  error  = [    0.1728]
25-01-23 10:40:23 | I |         best error  = [    0.1728]
25-01-23 10:40:23 | I |     + error = [0.1728]
25-01-23 10:40:23 | I |       - range scale = [    1.0000]
25-01-23 10:40:23 | I |         sum  error  = [    1.6819]
25-01-23 10:40:23 | I |         best error  = [    1.6819]
25-01-23 10:40:23 | I |     + error = [1.6819]
25-01-23 10:40:24 | I |   - Calibrating model.layers.1.self_attn.k_proj.weight
25-01-23 10:40:24 | I |       - range scale = [    1.0000]
25-01-23 10:40:24 | I |         sum  error  = [    0.1735]
25-01-23 10:40:24 | I |         best error  = [    0.1735]
25-01-23 10:40:24 | I |     + error = [0.1735]
25-01-23 10:40:25 | I |       - range scale = [    1.0000]
25-01-23 10:40:25 | I |         sum  error  = [    1.8827]
25-01-23 10:40:25 | I |         best error  = [    1.8827]
25-01-23 10:40:25 | I |     + error = [1.8827]
25-01-23 10:40:25 | I |   - Calibrating model.layers.1.self_attn.v_proj.weight
25-01-23 10:40:26 | I |       - range scale = [    1.0000]
25-01-23 10:40:26 | I |         sum  error  = [    1.3577]
25-01-23 10:40:26 | I |         best error  = [    1.3577]
25-01-23 10:40:26 | I |     + error = [1.3577]
25-01-23 10:40:27 | I |       - range scale = [    1.0000]
25-01-23 10:40:27 | I |         sum  error  = [    9.4232]
25-01-23 10:40:27 | I |         best error  = [    9.4232]
25-01-23 10:40:27 | I |     + error = [9.4232]
25-01-23 10:40:27 | I |   - Calibrating model.layers.1.self_attn.o_proj.weight
25-01-23 10:40:28 | I |       - range scale = [    1.0000]
25-01-23 10:40:28 | I |         sum  error  = [    0.2339]
25-01-23 10:40:28 | I |         best error  = [    0.2339]
25-01-23 10:40:28 | I |     + error = [0.2339]
25-01-23 10:40:28 | I |       - range scale = [    1.0000]
25-01-23 10:40:28 | I |         sum  error  = [    2.0782]
25-01-23 10:40:28 | I |         best error  = [    2.0782]
25-01-23 10:40:28 | I |     + error = [2.0782]
25-01-23 10:40:29 | I |   - Calibrating model.layers.1.mlp.up_proj.weight
25-01-23 10:40:29 | I |       - range scale = [    1.0000]
25-01-23 10:40:29 | I |         sum  error  = [    1.8169]
25-01-23 10:40:29 | I |         best error  = [    1.8169]
25-01-23 10:40:29 | I |     + error = [1.8169]
25-01-23 10:40:31 | I |       - range scale = [    1.0000]
25-01-23 10:40:31 | I |         sum  error  = [   19.3439]
25-01-23 10:40:31 | I |         best error  = [   19.3439]
25-01-23 10:40:31 | I |     + error = [19.3439]
25-01-23 10:40:31 | I |   - Calibrating model.layers.1.mlp.gate_proj.weight
25-01-23 10:40:31 | I |       - range scale = [    1.0000]
25-01-23 10:40:31 | I |         sum  error  = [    1.9804]
25-01-23 10:40:31 | I |         best error  = [    1.9804]
25-01-23 10:40:31 | I |     + error = [1.9804]
25-01-23 10:40:33 | I |       - range scale = [    1.0000]
25-01-23 10:40:33 | I |         sum  error  = [   20.6237]
25-01-23 10:40:33 | I |         best error  = [   20.6237]
25-01-23 10:40:33 | I |     + error = [20.6237]
25-01-23 10:40:33 | I |   - Calibrating model.layers.1.mlp.down_proj.weight
25-01-23 10:40:33 | I |       - range scale = [    1.0000]
25-01-23 10:40:33 | I |         sum  error  = [   52.1569]
25-01-23 10:40:33 | I |         best error  = [   52.1569]
25-01-23 10:40:33 | I |     + error = [52.1569]
25-01-23 10:40:35 | I |       - range scale = [    1.0000]
25-01-23 10:40:35 | I |         sum  error  = [  255.7056]
25-01-23 10:40:35 | I |         best error  = [  255.7056]
25-01-23 10:40:35 | I |     + error = [255.7056]
25-01-23 10:40:35 | I |   - Quantizing model.layers.1.self_attn.q_proj.weight
25-01-23 10:40:37 | I |   - Quantizing model.layers.1.self_attn.k_proj.weight
25-01-23 10:40:39 | I |   - Quantizing model.layers.1.self_attn.v_proj.weight
25-01-23 10:40:42 | I |   - Quantizing model.layers.1.self_attn.o_proj.weight
25-01-23 10:40:44 | I |   - Quantizing model.layers.1.mlp.up_proj.weight
25-01-23 10:40:46 | I |   - Quantizing model.layers.1.mlp.gate_proj.weight
25-01-23 10:40:48 | I |   - Quantizing model.layers.1.mlp.down_proj.weight
25-01-23 10:40:54 | I | quantizing activations for layer model.layers.1
25-01-23 10:40:55 | I | collecting calibration activations in model.layers.1
25-01-23 10:40:55 | I | collecting calibration activations in model.layers.1
25-01-23 10:40:57 | I | forward this layer
25-01-23 10:40:57 | I | input_file: /data/gyy/lmquant-main/lmquant/data/data_without_preprocess_llama_7b_bs16/shard/val_input_args_layer_1/17.pt
25-01-23 10:40:57 | I | output_file: /data/gyy/lmquant-main/lmquant/data/data_without_preprocess_llama_7b_bs16/shard/val_teacher_output_layer_1/17.pt
25-01-23 10:40:57 | I | In layer model.layers.1, gradient of model.layers.1.self_attn.q_proj has nan: False
25-01-23 10:40:57 | I | In layer model.layers.1, gradient of model.layers.1.self_attn.q_proj has inf: False
25-01-23 10:40:57 | I | In layer model.layers.1, gradient of model.layers.1.self_attn.k_proj has nan: False
25-01-23 10:40:57 | I | In layer model.layers.1, gradient of model.layers.1.self_attn.k_proj has inf: False
25-01-23 10:40:57 | I | In layer model.layers.1, gradient of model.layers.1.self_attn.v_proj has nan: False
25-01-23 10:40:57 | I | In layer model.layers.1, gradient of model.layers.1.self_attn.v_proj has inf: True
25-01-23 10:40:57 | I | inf: first position tensor([1444, 1512], device='cuda:0')
25-01-23 10:40:57 | I | In layer model.layers.1, gradient of model.layers.1.self_attn.o_proj has nan: False
25-01-23 10:40:57 | I | In layer model.layers.1, gradient of model.layers.1.self_attn.o_proj has inf: False
25-01-23 10:40:57 | I | In layer model.layers.1, gradient of model.layers.1.mlp.up_proj has nan: False
25-01-23 10:40:57 | I | In layer model.layers.1, gradient of model.layers.1.mlp.up_proj has inf: False
25-01-23 10:40:57 | I | In layer model.layers.1, gradient of model.layers.1.mlp.gate_proj has nan: False
25-01-23 10:40:57 | I | In layer model.layers.1, gradient of model.layers.1.mlp.gate_proj has inf: False
25-01-23 10:40:57 | I | In layer model.layers.1, gradient of model.layers.1.mlp.down_proj has nan: False
25-01-23 10:40:57 | I | In layer model.layers.1, gradient of model.layers.1.mlp.down_proj has inf: False
25-01-23 10:40:57 | I | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 0.00048828125
25-01-23 10:40:57 | I | TRAIN CURRENT LAYER_IDX = 1
25-01-23 10:40:57 | I | in layer model.layers.1
25-01-23 10:40:57 | I | quantizing weights for layer model.layers.1
25-01-23 10:40:58 | I | collecting calibration activations in model.layers.1
25-01-23 10:40:58 | I | collecting calibration activations in model.layers.1
25-01-23 10:40:58 | I | - Quantizing decoder layer model.layers.1
25-01-23 10:40:58 | I |   - Calibrating model.layers.1.self_attn.q_proj.weight
25-01-23 10:40:59 | I |       - range scale = [    1.0000]
25-01-23 10:40:59 | I |         sum  error  = [    0.1744]
25-01-23 10:40:59 | I |         best error  = [    0.1744]
25-01-23 10:40:59 | I |     + error = [0.1744]
25-01-23 10:41:00 | I |       - range scale = [    1.0000]
25-01-23 10:41:00 | I |         sum  error  = [    1.7351]
25-01-23 10:41:00 | I |         best error  = [    1.7351]
25-01-23 10:41:00 | I |     + error = [1.7351]
25-01-23 10:41:00 | I |   - Calibrating model.layers.1.self_attn.k_proj.weight
25-01-23 10:41:01 | I |       - range scale = [    1.0000]
25-01-23 10:41:01 | I |         sum  error  = [    0.1884]
25-01-23 10:41:01 | I |         best error  = [    0.1884]
25-01-23 10:41:01 | I |     + error = [0.1884]
25-01-23 10:41:01 | I |       - range scale = [    1.0000]
25-01-23 10:41:01 | I |         sum  error  = [    1.9611]
25-01-23 10:41:01 | I |         best error  = [    1.9611]
25-01-23 10:41:01 | I |     + error = [1.9611]
25-01-23 10:41:02 | I |   - Calibrating model.layers.1.self_attn.v_proj.weight
25-01-23 10:41:02 | I |       - range scale = [    1.0000]
25-01-23 10:41:02 | I |         sum  error  = [    1.3544]
25-01-23 10:41:02 | I |         best error  = [    1.3544]
25-01-23 10:41:02 | I |     + error = [1.3544]
25-01-23 10:41:03 | I |       - range scale = [    1.0000]
25-01-23 10:41:03 | I |         sum  error  = [    9.4891]
25-01-23 10:41:03 | I |         best error  = [    9.4891]
25-01-23 10:41:03 | I |     + error = [9.4891]
25-01-23 10:41:03 | I |   - Calibrating model.layers.1.self_attn.o_proj.weight
25-01-23 10:41:04 | I |       - range scale = [    1.0000]
25-01-23 10:41:04 | I |         sum  error  = [    0.2325]
25-01-23 10:41:04 | I |         best error  = [    0.2325]
25-01-23 10:41:04 | I |     + error = [0.2325]
25-01-23 10:41:05 | I |       - range scale = [    1.0000]
25-01-23 10:41:05 | I |         sum  error  = [    2.0483]
25-01-23 10:41:05 | I |         best error  = [    2.0483]
25-01-23 10:41:05 | I |     + error = [2.0483]
25-01-23 10:41:05 | I |   - Calibrating model.layers.1.mlp.up_proj.weight
25-01-23 10:41:06 | I |       - range scale = [    1.0000]
25-01-23 10:41:06 | I |         sum  error  = [    1.8555]
25-01-23 10:41:06 | I |         best error  = [    1.8555]
25-01-23 10:41:06 | I |     + error = [1.8555]
25-01-23 10:41:07 | I |       - range scale = [    1.0000]
25-01-23 10:41:07 | I |         sum  error  = [   19.7482]
25-01-23 10:41:07 | I |         best error  = [   19.7482]
25-01-23 10:41:07 | I |     + error = [19.7482]
25-01-23 10:41:07 | I |   - Calibrating model.layers.1.mlp.gate_proj.weight
25-01-23 10:41:08 | I |       - range scale = [    1.0000]
25-01-23 10:41:08 | I |         sum  error  = [    2.0216]
25-01-23 10:41:08 | I |         best error  = [    2.0216]
25-01-23 10:41:08 | I |     + error = [2.0216]
25-01-23 10:41:09 | I |       - range scale = [    1.0000]
25-01-23 10:41:09 | I |         sum  error  = [   21.0585]
25-01-23 10:41:09 | I |         best error  = [   21.0585]
25-01-23 10:41:09 | I |     + error = [21.0585]
25-01-23 10:41:09 | I |   - Calibrating model.layers.1.mlp.down_proj.weight
25-01-23 10:41:10 | I |       - range scale = [    1.0000]
25-01-23 10:41:10 | I |         sum  error  = [   49.0031]
25-01-23 10:41:10 | I |         best error  = [   49.0031]
25-01-23 10:41:10 | I |     + error = [49.0031]
25-01-23 10:41:11 | I |       - range scale = [    1.0000]
25-01-23 10:41:11 | I |         sum  error  = [  240.6154]
25-01-23 10:41:11 | I |         best error  = [  240.6154]
25-01-23 10:41:11 | I |     + error = [240.6154]
25-01-23 10:41:11 | I |   - Quantizing model.layers.1.self_attn.q_proj.weight
25-01-23 10:41:13 | I |   - Quantizing model.layers.1.self_attn.k_proj.weight
25-01-23 10:41:16 | I |   - Quantizing model.layers.1.self_attn.v_proj.weight
25-01-23 10:41:18 | I |   - Quantizing model.layers.1.self_attn.o_proj.weight
25-01-23 10:41:20 | I |   - Quantizing model.layers.1.mlp.up_proj.weight
25-01-23 10:41:23 | I |   - Quantizing model.layers.1.mlp.gate_proj.weight
25-01-23 10:41:25 | I |   - Quantizing model.layers.1.mlp.down_proj.weight
25-01-23 10:41:31 | I | quantizing activations for layer model.layers.1
25-01-23 10:41:31 | I | collecting calibration activations in model.layers.1
25-01-23 10:41:31 | I | collecting calibration activations in model.layers.1
25-01-23 10:41:33 | I | forward this layer
25-01-23 10:41:33 | I | input_file: /data/gyy/lmquant-main/lmquant/data/data_without_preprocess_llama_7b_bs16/shard/val_input_args_layer_1/18.pt
25-01-23 10:41:33 | I | output_file: /data/gyy/lmquant-main/lmquant/data/data_without_preprocess_llama_7b_bs16/shard/val_teacher_output_layer_1/18.pt
25-01-23 10:41:33 | I | In layer model.layers.1, gradient of model.layers.1.self_attn.q_proj has nan: False
25-01-23 10:41:33 | I | In layer model.layers.1, gradient of model.layers.1.self_attn.q_proj has inf: False
25-01-23 10:41:33 | I | In layer model.layers.1, gradient of model.layers.1.self_attn.k_proj has nan: False
25-01-23 10:41:33 | I | In layer model.layers.1, gradient of model.layers.1.self_attn.k_proj has inf: False
25-01-23 10:41:33 | I | In layer model.layers.1, gradient of model.layers.1.self_attn.v_proj has nan: False
25-01-23 10:41:33 | I | In layer model.layers.1, gradient of model.layers.1.self_attn.v_proj has inf: False
25-01-23 10:41:33 | I | In layer model.layers.1, gradient of model.layers.1.self_attn.o_proj has nan: False
25-01-23 10:41:33 | I | In layer model.layers.1, gradient of model.layers.1.self_attn.o_proj has inf: False
25-01-23 10:41:33 | I | In layer model.layers.1, gradient of model.layers.1.mlp.up_proj has nan: False
25-01-23 10:41:33 | I | In layer model.layers.1, gradient of model.layers.1.mlp.up_proj has inf: False
25-01-23 10:41:33 | I | In layer model.layers.1, gradient of model.layers.1.mlp.gate_proj has nan: False
25-01-23 10:41:33 | I | In layer model.layers.1, gradient of model.layers.1.mlp.gate_proj has inf: False
25-01-23 10:41:33 | I | In layer model.layers.1, gradient of model.layers.1.mlp.down_proj has nan: False
25-01-23 10:41:33 | I | In layer model.layers.1, gradient of model.layers.1.mlp.down_proj has inf: False
25-01-23 10:41:33 | I | [0] done with optimizer step
25-01-23 10:41:33 | I | epoch 001:     19 / 819200000 loss=0.0182754, loss_per_token=74.8562, loss_sum=613222, wps=2.7, ups=0, wpb=8192, bsz=16, num_updates=1, lr=0.0001, gnorm=10171.2, clip=100, loss_scale=0.0005, train_wall=711, cuda_gb_allocated=17.2, cuda_gb_reserved=20.8, cuda_gb_free=6.5, wall=2395
25-01-23 10:41:34 | I | TRAIN CURRENT LAYER_IDX = 1
25-01-23 10:41:34 | I | in layer model.layers.1
25-01-23 10:41:34 | I | quantizing weights for layer model.layers.1
25-01-23 10:41:34 | I | collecting calibration activations in model.layers.1
25-01-23 10:41:34 | I | collecting calibration activations in model.layers.1
25-01-23 10:41:34 | I | - Quantizing decoder layer model.layers.1
25-01-23 10:41:34 | I |   - Calibrating model.layers.1.self_attn.q_proj.weight
25-01-23 10:41:35 | I |       - range scale = [    1.0000]
25-01-23 10:41:35 | I |         sum  error  = [    0.1765]
25-01-23 10:41:35 | I |         best error  = [    0.1765]
25-01-23 10:41:35 | I |     + error = [0.1765]
25-01-23 10:41:36 | I |       - range scale = [    1.0000]
25-01-23 10:41:36 | I |         sum  error  = [    1.6438]
25-01-23 10:41:36 | I |         best error  = [    1.6438]
25-01-23 10:41:36 | I |     + error = [1.6438]
25-01-23 10:41:36 | I |   - Calibrating model.layers.1.self_attn.k_proj.weight
25-01-23 10:41:37 | I |       - range scale = [    1.0000]
25-01-23 10:41:37 | I |         sum  error  = [    0.1768]
25-01-23 10:41:37 | I |         best error  = [    0.1768]
25-01-23 10:41:37 | I |     + error = [0.1768]
25-01-23 10:41:37 | I |       - range scale = [    1.0000]
25-01-23 10:41:37 | I |         sum  error  = [    1.8818]
25-01-23 10:41:37 | I |         best error  = [    1.8818]
25-01-23 10:41:37 | I |     + error = [1.8818]
25-01-23 10:41:37 | I |   - Calibrating model.layers.1.self_attn.v_proj.weight
25-01-23 10:41:38 | I |       - range scale = [    1.0000]
25-01-23 10:41:38 | I |         sum  error  = [    1.3532]
25-01-23 10:41:38 | I |         best error  = [    1.3532]
25-01-23 10:41:38 | I |     + error = [1.3532]
25-01-23 10:41:39 | I |       - range scale = [    1.0000]
25-01-23 10:41:39 | I |         sum  error  = [    9.2457]
25-01-23 10:41:39 | I |         best error  = [    9.2457]
25-01-23 10:41:39 | I |     + error = [9.2457]
25-01-23 10:41:39 | I |   - Calibrating model.layers.1.self_attn.o_proj.weight
25-01-23 10:41:40 | I |       - range scale = [    1.0000]
25-01-23 10:41:40 | I |         sum  error  = [    0.2331]
25-01-23 10:41:40 | I |         best error  = [    0.2331]
25-01-23 10:41:40 | I |     + error = [0.2331]
25-01-23 10:41:41 | I |       - range scale = [    1.0000]
25-01-23 10:41:41 | I |         sum  error  = [    2.0767]
25-01-23 10:41:41 | I |         best error  = [    2.0767]
25-01-23 10:41:41 | I |     + error = [2.0767]
25-01-23 10:41:41 | I |   - Calibrating model.layers.1.mlp.up_proj.weight
25-01-23 10:41:42 | I |       - range scale = [    1.0000]
25-01-23 10:41:42 | I |         sum  error  = [    1.8109]
25-01-23 10:41:42 | I |         best error  = [    1.8109]
25-01-23 10:41:42 | I |     + error = [1.8109]
25-01-23 10:41:43 | I |       - range scale = [    1.0000]
25-01-23 10:41:43 | I |         sum  error  = [   19.2806]
25-01-23 10:41:43 | I |         best error  = [   19.2806]
25-01-23 10:41:43 | I |     + error = [19.2806]
25-01-23 10:41:43 | I |   - Calibrating model.layers.1.mlp.gate_proj.weight
25-01-23 10:41:44 | I |       - range scale = [    1.0000]
25-01-23 10:41:44 | I |         sum  error  = [    1.9652]
25-01-23 10:41:44 | I |         best error  = [    1.9652]
25-01-23 10:41:44 | I |     + error = [1.9652]
25-01-23 10:41:45 | I |       - range scale = [    1.0000]
25-01-23 10:41:45 | I |         sum  error  = [   20.5498]
25-01-23 10:41:45 | I |         best error  = [   20.5498]
25-01-23 10:41:45 | I |     + error = [20.5498]
25-01-23 10:41:45 | I |   - Calibrating model.layers.1.mlp.down_proj.weight
25-01-23 10:41:46 | I |       - range scale = [    1.0000]
25-01-23 10:41:46 | I |         sum  error  = [   49.0700]
25-01-23 10:41:46 | I |         best error  = [   49.0700]
25-01-23 10:41:46 | I |     + error = [49.0700]
25-01-23 10:41:47 | I |       - range scale = [    1.0000]
25-01-23 10:41:47 | I |         sum  error  = [  240.5681]
25-01-23 10:41:47 | I |         best error  = [  240.5681]
25-01-23 10:41:47 | I |     + error = [240.5681]
25-01-23 10:41:47 | I |   - Quantizing model.layers.1.self_attn.q_proj.weight
25-01-23 10:41:50 | I |   - Quantizing model.layers.1.self_attn.k_proj.weight
25-01-23 10:41:52 | I |   - Quantizing model.layers.1.self_attn.v_proj.weight
25-01-23 10:41:54 | I |   - Quantizing model.layers.1.self_attn.o_proj.weight
25-01-23 10:41:56 | I |   - Quantizing model.layers.1.mlp.up_proj.weight
25-01-23 10:41:59 | I |   - Quantizing model.layers.1.mlp.gate_proj.weight
25-01-23 10:42:01 | I |   - Quantizing model.layers.1.mlp.down_proj.weight
25-01-23 10:42:09 | I | quantizing activations for layer model.layers.1
25-01-23 10:42:09 | I | collecting calibration activations in model.layers.1
25-01-23 10:42:09 | I | collecting calibration activations in model.layers.1
25-01-23 10:42:11 | I | forward this layer
25-01-23 10:42:11 | I | input_file: /data/gyy/lmquant-main/lmquant/data/data_without_preprocess_llama_7b_bs16/shard/val_input_args_layer_1/19.pt
25-01-23 10:42:11 | I | output_file: /data/gyy/lmquant-main/lmquant/data/data_without_preprocess_llama_7b_bs16/shard/val_teacher_output_layer_1/19.pt
25-01-23 10:42:12 | I | In layer model.layers.1, gradient of model.layers.1.self_attn.q_proj has nan: False
25-01-23 10:42:12 | I | In layer model.layers.1, gradient of model.layers.1.self_attn.q_proj has inf: False
25-01-23 10:42:12 | I | In layer model.layers.1, gradient of model.layers.1.self_attn.k_proj has nan: False
25-01-23 10:42:12 | I | In layer model.layers.1, gradient of model.layers.1.self_attn.k_proj has inf: False
25-01-23 10:42:12 | I | In layer model.layers.1, gradient of model.layers.1.self_attn.v_proj has nan: False
25-01-23 10:42:12 | I | In layer model.layers.1, gradient of model.layers.1.self_attn.v_proj has inf: False
25-01-23 10:42:12 | I | In layer model.layers.1, gradient of model.layers.1.self_attn.o_proj has nan: False
25-01-23 10:42:12 | I | In layer model.layers.1, gradient of model.layers.1.self_attn.o_proj has inf: False
25-01-23 10:42:12 | I | In layer model.layers.1, gradient of model.layers.1.mlp.up_proj has nan: False
25-01-23 10:42:12 | I | In layer model.layers.1, gradient of model.layers.1.mlp.up_proj has inf: False
25-01-23 10:42:12 | I | In layer model.layers.1, gradient of model.layers.1.mlp.gate_proj has nan: False
25-01-23 10:42:12 | I | In layer model.layers.1, gradient of model.layers.1.mlp.gate_proj has inf: False
25-01-23 10:42:12 | I | In layer model.layers.1, gradient of model.layers.1.mlp.down_proj has nan: False
25-01-23 10:42:12 | I | In layer model.layers.1, gradient of model.layers.1.mlp.down_proj has inf: False
25-01-23 10:42:12 | I | [1] done with optimizer step
25-01-23 10:42:12 | I | epoch 001:     20 / 819200000 loss=0.0232119, loss_per_token=95.076, loss_sum=778863, wps=214.8, ups=0.03, wpb=8192, bsz=16, num_updates=2, lr=0.0002, gnorm=15538.3, clip=100, loss_scale=0.0005, train_wall=38, cuda_gb_allocated=16.3, cuda_gb_reserved=17.5, cuda_gb_free=7.4, wall=2433
25-01-23 10:42:12 | I | TRAIN CURRENT LAYER_IDX = 1
25-01-23 10:42:12 | I | in layer model.layers.1
25-01-23 10:42:12 | I | quantizing weights for layer model.layers.1
25-01-23 10:42:12 | I | collecting calibration activations in model.layers.1
25-01-23 10:42:12 | I | collecting calibration activations in model.layers.1
25-01-23 10:42:12 | I | - Quantizing decoder layer model.layers.1
25-01-23 10:42:12 | I |   - Calibrating model.layers.1.self_attn.q_proj.weight
25-01-23 10:42:13 | I |       - range scale = [    1.0000]
25-01-23 10:42:13 | I |         sum  error  = [    0.1700]
25-01-23 10:42:13 | I |         best error  = [    0.1700]
25-01-23 10:42:13 | I |     + error = [0.1700]
25-01-23 10:42:14 | I |       - range scale = [    1.0000]
25-01-23 10:42:14 | I |         sum  error  = [    1.6751]
25-01-23 10:42:14 | I |         best error  = [    1.6751]
25-01-23 10:42:14 | I |     + error = [1.6751]
25-01-23 10:42:14 | I |   - Calibrating model.layers.1.self_attn.k_proj.weight
25-01-23 10:42:15 | I |       - range scale = [    1.0000]
25-01-23 10:42:15 | I |         sum  error  = [    0.1698]
25-01-23 10:42:15 | I |         best error  = [    0.1698]
25-01-23 10:42:15 | I |     + error = [0.1698]
25-01-23 10:42:15 | I |       - range scale = [    1.0000]
25-01-23 10:42:15 | I |         sum  error  = [    1.8775]
25-01-23 10:42:15 | I |         best error  = [    1.8775]
25-01-23 10:42:15 | I |     + error = [1.8775]
25-01-23 10:42:16 | I |   - Calibrating model.layers.1.self_attn.v_proj.weight
25-01-23 10:42:16 | I |       - range scale = [    1.0000]
25-01-23 10:42:16 | I |         sum  error  = [    1.3567]
25-01-23 10:42:16 | I |         best error  = [    1.3567]
25-01-23 10:42:16 | I |     + error = [1.3567]
25-01-23 10:42:17 | I |       - range scale = [    1.0000]
25-01-23 10:42:17 | I |         sum  error  = [    9.3189]
25-01-23 10:42:17 | I |         best error  = [    9.3189]
25-01-23 10:42:17 | I |     + error = [9.3189]
25-01-23 10:42:17 | I |   - Calibrating model.layers.1.self_attn.o_proj.weight
25-01-23 10:42:18 | I |       - range scale = [    1.0000]
25-01-23 10:42:18 | I |         sum  error  = [    0.2445]
25-01-23 10:42:18 | I |         best error  = [    0.2445]
25-01-23 10:42:18 | I |     + error = [0.2445]
25-01-23 10:42:19 | I |       - range scale = [    1.0000]
25-01-23 10:42:19 | I |         sum  error  = [    2.1435]
25-01-23 10:42:19 | I |         best error  = [    2.1435]
25-01-23 10:42:19 | I |     + error = [2.1435]
25-01-23 10:42:19 | I |   - Calibrating model.layers.1.mlp.up_proj.weight
25-01-23 10:42:20 | I |       - range scale = [    1.0000]
25-01-23 10:42:20 | I |         sum  error  = [    1.8051]
25-01-23 10:42:20 | I |         best error  = [    1.8051]
25-01-23 10:42:20 | I |     + error = [1.8051]
25-01-23 10:42:21 | I |       - range scale = [    1.0000]
25-01-23 10:42:21 | I |         sum  error  = [   19.1865]
25-01-23 10:42:21 | I |         best error  = [   19.1865]
25-01-23 10:42:21 | I |     + error = [19.1865]
25-01-23 10:42:21 | I |   - Calibrating model.layers.1.mlp.gate_proj.weight
25-01-23 10:42:22 | I |       - range scale = [    1.0000]
25-01-23 10:42:22 | I |         sum  error  = [    1.9623]
25-01-23 10:42:22 | I |         best error  = [    1.9623]
25-01-23 10:42:22 | I |     + error = [1.9623]
25-01-23 10:42:23 | I |       - range scale = [    1.0000]
25-01-23 10:42:23 | I |         sum  error  = [   20.4515]
25-01-23 10:42:23 | I |         best error  = [   20.4515]
25-01-23 10:42:23 | I |     + error = [20.4515]
25-01-23 10:42:23 | I |   - Calibrating model.layers.1.mlp.down_proj.weight
25-01-23 10:42:24 | I |       - range scale = [    1.0000]
25-01-23 10:42:24 | I |         sum  error  = [   58.0221]
25-01-23 10:42:24 | I |         best error  = [   58.0221]
25-01-23 10:42:24 | I |     + error = [58.0221]
25-01-23 10:42:25 | I |       - range scale = [    1.0000]
25-01-23 10:42:25 | I |         sum  error  = [  305.7037]
25-01-23 10:42:25 | I |         best error  = [  305.7037]
25-01-23 10:42:25 | I |     + error = [305.7037]
25-01-23 10:42:25 | I |   - Quantizing model.layers.1.self_attn.q_proj.weight
25-01-23 10:42:28 | I |   - Quantizing model.layers.1.self_attn.k_proj.weight
25-01-23 10:42:30 | I |   - Quantizing model.layers.1.self_attn.v_proj.weight
25-01-23 10:42:32 | I |   - Quantizing model.layers.1.self_attn.o_proj.weight
25-01-23 10:42:35 | I |   - Quantizing model.layers.1.mlp.up_proj.weight
25-01-23 10:42:37 | I |   - Quantizing model.layers.1.mlp.gate_proj.weight
25-01-23 10:42:39 | I |   - Quantizing model.layers.1.mlp.down_proj.weight
25-01-23 10:42:45 | I | quantizing activations for layer model.layers.1
25-01-23 10:42:46 | I | collecting calibration activations in model.layers.1
25-01-23 10:42:46 | I | collecting calibration activations in model.layers.1
25-01-23 10:42:48 | I | forward this layer
25-01-23 10:42:48 | I | input_file: /data/gyy/lmquant-main/lmquant/data/data_without_preprocess_llama_7b_bs16/shard/val_input_args_layer_1/20.pt
25-01-23 10:42:48 | I | output_file: /data/gyy/lmquant-main/lmquant/data/data_without_preprocess_llama_7b_bs16/shard/val_teacher_output_layer_1/20.pt
25-01-23 10:42:48 | I | In layer model.layers.1, gradient of model.layers.1.self_attn.q_proj has nan: False
25-01-23 10:42:48 | I | In layer model.layers.1, gradient of model.layers.1.self_attn.q_proj has inf: False
25-01-23 10:42:48 | I | In layer model.layers.1, gradient of model.layers.1.self_attn.k_proj has nan: False
25-01-23 10:42:48 | I | In layer model.layers.1, gradient of model.layers.1.self_attn.k_proj has inf: False
25-01-23 10:42:48 | I | In layer model.layers.1, gradient of model.layers.1.self_attn.v_proj has nan: False
25-01-23 10:42:48 | I | In layer model.layers.1, gradient of model.layers.1.self_attn.v_proj has inf: True
25-01-23 10:42:48 | I | inf: first position tensor([1444, 1512], device='cuda:0')
25-01-23 10:42:48 | I | In layer model.layers.1, gradient of model.layers.1.self_attn.o_proj has nan: False
25-01-23 10:42:48 | I | In layer model.layers.1, gradient of model.layers.1.self_attn.o_proj has inf: False
25-01-23 10:42:48 | I | In layer model.layers.1, gradient of model.layers.1.mlp.up_proj has nan: False
25-01-23 10:42:48 | I | In layer model.layers.1, gradient of model.layers.1.mlp.up_proj has inf: False
25-01-23 10:42:48 | I | In layer model.layers.1, gradient of model.layers.1.mlp.gate_proj has nan: False
25-01-23 10:42:48 | I | In layer model.layers.1, gradient of model.layers.1.mlp.gate_proj has inf: False
25-01-23 10:42:48 | I | In layer model.layers.1, gradient of model.layers.1.mlp.down_proj has nan: False
25-01-23 10:42:48 | I | In layer model.layers.1, gradient of model.layers.1.mlp.down_proj has inf: False
25-01-23 10:42:48 | I | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 0.000244140625
25-01-23 10:42:48 | I | TRAIN CURRENT LAYER_IDX = 1
25-01-23 10:42:48 | I | in layer model.layers.1
25-01-23 10:42:48 | I | quantizing weights for layer model.layers.1
25-01-23 10:42:49 | I | collecting calibration activations in model.layers.1
25-01-23 10:42:49 | I | collecting calibration activations in model.layers.1
25-01-23 10:42:49 | I | - Quantizing decoder layer model.layers.1
25-01-23 10:42:49 | I |   - Calibrating model.layers.1.self_attn.q_proj.weight
25-01-23 10:42:50 | I |       - range scale = [    1.0000]
25-01-23 10:42:50 | I |         sum  error  = [    0.1683]
25-01-23 10:42:50 | I |         best error  = [    0.1683]
25-01-23 10:42:50 | I |     + error = [0.1683]
25-01-23 10:42:51 | I |       - range scale = [    1.0000]
25-01-23 10:42:51 | I |         sum  error  = [    1.6957]
25-01-23 10:42:51 | I |         best error  = [    1.6957]
25-01-23 10:42:51 | I |     + error = [1.6957]
25-01-23 10:42:51 | I |   - Calibrating model.layers.1.self_attn.k_proj.weight
25-01-23 10:42:52 | I |       - range scale = [    1.0000]
25-01-23 10:42:52 | I |         sum  error  = [    0.1709]
25-01-23 10:42:52 | I |         best error  = [    0.1709]
25-01-23 10:42:52 | I |     + error = [0.1709]
25-01-23 10:42:52 | I |       - range scale = [    1.0000]
25-01-23 10:42:52 | I |         sum  error  = [    1.9187]
25-01-23 10:42:52 | I |         best error  = [    1.9187]
25-01-23 10:42:52 | I |     + error = [1.9187]
25-01-23 10:42:53 | I |   - Calibrating model.layers.1.self_attn.v_proj.weight
25-01-23 10:42:53 | I |       - range scale = [    1.0000]
25-01-23 10:42:53 | I |         sum  error  = [    1.3557]
25-01-23 10:42:53 | I |         best error  = [    1.3557]
25-01-23 10:42:53 | I |     + error = [1.3557]
25-01-23 10:42:54 | I |       - range scale = [    1.0000]
25-01-23 10:42:54 | I |         sum  error  = [    9.4932]
25-01-23 10:42:54 | I |         best error  = [    9.4932]
25-01-23 10:42:54 | I |     + error = [9.4932]
25-01-23 10:42:54 | I |   - Calibrating model.layers.1.self_attn.o_proj.weight
25-01-23 10:42:55 | I |       - range scale = [    1.0000]
25-01-23 10:42:55 | I |         sum  error  = [    0.2331]
25-01-23 10:42:55 | I |         best error  = [    0.2331]
25-01-23 10:42:55 | I |     + error = [0.2331]
25-01-23 10:42:56 | I |       - range scale = [    1.0000]
25-01-23 10:42:56 | I |         sum  error  = [    2.0672]
25-01-23 10:42:56 | I |         best error  = [    2.0672]
25-01-23 10:42:56 | I |     + error = [2.0672]
25-01-23 10:42:56 | I |   - Calibrating model.layers.1.mlp.up_proj.weight
25-01-23 10:42:57 | I |       - range scale = [    1.0000]
25-01-23 10:42:57 | I |         sum  error  = [    1.8273]
25-01-23 10:42:57 | I |         best error  = [    1.8273]
25-01-23 10:42:57 | I |     + error = [1.8273]
25-01-23 10:42:58 | I |       - range scale = [    1.0000]
25-01-23 10:42:58 | I |         sum  error  = [   19.4334]
25-01-23 10:42:58 | I |         best error  = [   19.4334]
25-01-23 10:42:58 | I |     + error = [19.4334]
25-01-23 10:42:58 | I |   - Calibrating model.layers.1.mlp.gate_proj.weight
25-01-23 10:42:59 | I |       - range scale = [    1.0000]
25-01-23 10:42:59 | I |         sum  error  = [    1.9873]
25-01-23 10:42:59 | I |         best error  = [    1.9873]
25-01-23 10:42:59 | I |     + error = [1.9873]
25-01-23 10:43:00 | I |       - range scale = [    1.0000]
25-01-23 10:43:00 | I |         sum  error  = [   20.7245]
25-01-23 10:43:00 | I |         best error  = [   20.7245]
25-01-23 10:43:00 | I |     + error = [20.7245]
25-01-23 10:43:01 | I |   - Calibrating model.layers.1.mlp.down_proj.weight
25-01-23 10:43:01 | I |       - range scale = [    1.0000]
25-01-23 10:43:01 | I |         sum  error  = [   59.6651]
25-01-23 10:43:01 | I |         best error  = [   59.6651]
25-01-23 10:43:01 | I |     + error = [59.6651]
25-01-23 10:43:03 | I |       - range scale = [    1.0000]
25-01-23 10:43:03 | I |         sum  error  = [  314.1035]
25-01-23 10:43:03 | I |         best error  = [  314.1035]
25-01-23 10:43:03 | I |     + error = [314.1035]
25-01-23 10:43:03 | I |   - Quantizing model.layers.1.self_attn.q_proj.weight
25-01-23 10:43:05 | I |   - Quantizing model.layers.1.self_attn.k_proj.weight
25-01-23 10:43:08 | I |   - Quantizing model.layers.1.self_attn.v_proj.weight
25-01-23 10:43:10 | I |   - Quantizing model.layers.1.self_attn.o_proj.weight
25-01-23 10:43:12 | I |   - Quantizing model.layers.1.mlp.up_proj.weight
25-01-23 10:43:14 | I |   - Quantizing model.layers.1.mlp.gate_proj.weight
25-01-23 10:43:17 | I |   - Quantizing model.layers.1.mlp.down_proj.weight
25-01-23 10:43:22 | I | quantizing activations for layer model.layers.1
25-01-23 10:43:23 | I | collecting calibration activations in model.layers.1
25-01-23 10:43:23 | I | collecting calibration activations in model.layers.1
25-01-23 10:43:25 | I | forward this layer
25-01-23 10:43:25 | I | input_file: /data/gyy/lmquant-main/lmquant/data/data_without_preprocess_llama_7b_bs16/shard/val_input_args_layer_1/21.pt
25-01-23 10:43:25 | I | output_file: /data/gyy/lmquant-main/lmquant/data/data_without_preprocess_llama_7b_bs16/shard/val_teacher_output_layer_1/21.pt
25-01-23 10:43:25 | I | In layer model.layers.1, gradient of model.layers.1.self_attn.q_proj has nan: False
25-01-23 10:43:25 | I | In layer model.layers.1, gradient of model.layers.1.self_attn.q_proj has inf: False
25-01-23 10:43:25 | I | In layer model.layers.1, gradient of model.layers.1.self_attn.k_proj has nan: False
25-01-23 10:43:25 | I | In layer model.layers.1, gradient of model.layers.1.self_attn.k_proj has inf: False
25-01-23 10:43:25 | I | In layer model.layers.1, gradient of model.layers.1.self_attn.v_proj has nan: False
25-01-23 10:43:25 | I | In layer model.layers.1, gradient of model.layers.1.self_attn.v_proj has inf: False
25-01-23 10:43:25 | I | In layer model.layers.1, gradient of model.layers.1.self_attn.o_proj has nan: False
25-01-23 10:43:25 | I | In layer model.layers.1, gradient of model.layers.1.self_attn.o_proj has inf: False
25-01-23 10:43:25 | I | In layer model.layers.1, gradient of model.layers.1.mlp.up_proj has nan: False
25-01-23 10:43:25 | I | In layer model.layers.1, gradient of model.layers.1.mlp.up_proj has inf: False
25-01-23 10:43:25 | I | In layer model.layers.1, gradient of model.layers.1.mlp.gate_proj has nan: False
25-01-23 10:43:25 | I | In layer model.layers.1, gradient of model.layers.1.mlp.gate_proj has inf: False
25-01-23 10:43:25 | I | In layer model.layers.1, gradient of model.layers.1.mlp.down_proj has nan: False
25-01-23 10:43:25 | I | In layer model.layers.1, gradient of model.layers.1.mlp.down_proj has inf: False
25-01-23 10:43:25 | I | [2] done with optimizer step
25-01-23 10:43:25 | I | epoch 001:     22 / 819200000 loss=0.204537, loss_per_token=837.783, loss_sum=6.86312e+06, wps=111.1, ups=0.01, wpb=8192, bsz=16, num_updates=3, lr=0.0003, gnorm=47533.1, clip=100, loss_scale=0.0002, train_wall=74, cuda_gb_allocated=16.3, cuda_gb_reserved=17.5, cuda_gb_free=7.4, wall=2507
25-01-23 10:43:25 | I | TRAIN CURRENT LAYER_IDX = 1
25-01-23 10:43:25 | I | in layer model.layers.1
25-01-23 10:43:25 | I | quantizing weights for layer model.layers.1
25-01-23 10:43:26 | I | collecting calibration activations in model.layers.1
25-01-23 10:43:26 | I | collecting calibration activations in model.layers.1
25-01-23 10:43:26 | I | - Quantizing decoder layer model.layers.1
25-01-23 10:43:26 | I |   - Calibrating model.layers.1.self_attn.q_proj.weight
25-01-23 10:43:27 | I |       - range scale = [    1.0000]
25-01-23 10:43:27 | I |         sum  error  = [    0.1713]
25-01-23 10:43:27 | I |         best error  = [    0.1713]
25-01-23 10:43:27 | I |     + error = [0.1713]
25-01-23 10:43:28 | I |       - range scale = [    1.0000]
25-01-23 10:43:28 | I |         sum  error  = [    1.6649]
25-01-23 10:43:28 | I |         best error  = [    1.6649]
25-01-23 10:43:28 | I |     + error = [1.6649]
25-01-23 10:43:28 | I |   - Calibrating model.layers.1.self_attn.k_proj.weight
25-01-23 10:43:29 | I |       - range scale = [    1.0000]
25-01-23 10:43:29 | I |         sum  error  = [    0.1672]
25-01-23 10:43:29 | I |         best error  = [    0.1672]
25-01-23 10:43:29 | I |     + error = [0.1672]
25-01-23 10:43:29 | I |       - range scale = [    1.0000]
25-01-23 10:43:29 | I |         sum  error  = [    1.9798]
25-01-23 10:43:29 | I |         best error  = [    1.9798]
25-01-23 10:43:29 | I |     + error = [1.9798]
25-01-23 10:43:30 | I |   - Calibrating model.layers.1.self_attn.v_proj.weight
25-01-23 10:43:30 | I |       - range scale = [    1.0000]
25-01-23 10:43:30 | I |         sum  error  = [    1.3820]
25-01-23 10:43:30 | I |         best error  = [    1.3820]
25-01-23 10:43:30 | I |     + error = [1.3820]
25-01-23 10:43:31 | I |       - range scale = [    1.0000]
25-01-23 10:43:31 | I |         sum  error  = [    9.7144]
25-01-23 10:43:31 | I |         best error  = [    9.7144]
25-01-23 10:43:31 | I |     + error = [9.7144]
25-01-23 10:43:31 | I |   - Calibrating model.layers.1.self_attn.o_proj.weight
25-01-23 10:43:32 | I |       - range scale = [    1.0000]
25-01-23 10:43:32 | I |         sum  error  = [    0.2404]
25-01-23 10:43:32 | I |         best error  = [    0.2404]
25-01-23 10:43:32 | I |     + error = [0.2404]
25-01-23 10:43:33 | I |       - range scale = [    1.0000]
25-01-23 10:43:33 | I |         sum  error  = [    2.1136]
25-01-23 10:43:33 | I |         best error  = [    2.1136]
25-01-23 10:43:33 | I |     + error = [2.1136]
25-01-23 10:43:33 | I |   - Calibrating model.layers.1.mlp.up_proj.weight
25-01-23 10:43:34 | I |       - range scale = [    1.0000]
25-01-23 10:43:34 | I |         sum  error  = [    1.7472]
25-01-23 10:43:34 | I |         best error  = [    1.7472]
25-01-23 10:43:34 | I |     + error = [1.7472]
25-01-23 10:43:35 | I |       - range scale = [    1.0000]
25-01-23 10:43:35 | I |         sum  error  = [   18.5752]
25-01-23 10:43:35 | I |         best error  = [   18.5752]
25-01-23 10:43:35 | I |     + error = [18.5752]
25-01-23 10:43:35 | I |   - Calibrating model.layers.1.mlp.gate_proj.weight
25-01-23 10:43:36 | I |       - range scale = [    1.0000]
25-01-23 10:43:36 | I |         sum  error  = [    1.9028]
25-01-23 10:43:36 | I |         best error  = [    1.9028]
25-01-23 10:43:36 | I |     + error = [1.9028]
25-01-23 10:43:37 | I |       - range scale = [    1.0000]
25-01-23 10:43:37 | I |         sum  error  = [   19.8088]
25-01-23 10:43:37 | I |         best error  = [   19.8088]
25-01-23 10:43:37 | I |     + error = [19.8088]
25-01-23 10:43:37 | I |   - Calibrating model.layers.1.mlp.down_proj.weight
25-01-23 10:43:38 | I |       - range scale = [    1.0000]
25-01-23 10:43:38 | I |         sum  error  = [   50.8542]
25-01-23 10:43:38 | I |         best error  = [   50.8542]
25-01-23 10:43:38 | I |     + error = [50.8542]
25-01-23 10:43:39 | I |       - range scale = [    1.0000]
25-01-23 10:43:39 | I |         sum  error  = [  289.4513]
25-01-23 10:43:39 | I |         best error  = [  289.4513]
25-01-23 10:43:39 | I |     + error = [289.4513]
25-01-23 10:43:39 | I |   - Quantizing model.layers.1.self_attn.q_proj.weight
25-01-23 10:43:42 | I |   - Quantizing model.layers.1.self_attn.k_proj.weight
25-01-23 10:43:45 | I |   - Quantizing model.layers.1.self_attn.v_proj.weight
25-01-23 10:43:48 | I |   - Quantizing model.layers.1.self_attn.o_proj.weight
25-01-23 10:43:51 | I |   - Quantizing model.layers.1.mlp.up_proj.weight
25-01-23 10:43:54 | I |   - Quantizing model.layers.1.mlp.gate_proj.weight
25-01-23 10:43:57 | I |   - Quantizing model.layers.1.mlp.down_proj.weight
25-01-23 10:44:04 | I | quantizing activations for layer model.layers.1
25-01-23 10:44:04 | I | collecting calibration activations in model.layers.1
25-01-23 10:44:05 | I | collecting calibration activations in model.layers.1
25-01-23 10:44:07 | I | forward this layer
25-01-23 10:44:07 | I | input_file: /data/gyy/lmquant-main/lmquant/data/data_without_preprocess_llama_7b_bs16/shard/val_input_args_layer_1/22.pt
25-01-23 10:44:07 | I | output_file: /data/gyy/lmquant-main/lmquant/data/data_without_preprocess_llama_7b_bs16/shard/val_teacher_output_layer_1/22.pt
25-01-23 10:44:07 | I | In layer model.layers.1, gradient of model.layers.1.self_attn.q_proj has nan: False
25-01-23 10:44:07 | I | In layer model.layers.1, gradient of model.layers.1.self_attn.q_proj has inf: False
25-01-23 10:44:07 | I | In layer model.layers.1, gradient of model.layers.1.self_attn.k_proj has nan: False
25-01-23 10:44:07 | I | In layer model.layers.1, gradient of model.layers.1.self_attn.k_proj has inf: False
25-01-23 10:44:07 | I | In layer model.layers.1, gradient of model.layers.1.self_attn.v_proj has nan: False
25-01-23 10:44:07 | I | In layer model.layers.1, gradient of model.layers.1.self_attn.v_proj has inf: False
25-01-23 10:44:07 | I | In layer model.layers.1, gradient of model.layers.1.self_attn.o_proj has nan: False
25-01-23 10:44:07 | I | In layer model.layers.1, gradient of model.layers.1.self_attn.o_proj has inf: False
25-01-23 10:44:07 | I | In layer model.layers.1, gradient of model.layers.1.mlp.up_proj has nan: False
25-01-23 10:44:07 | I | In layer model.layers.1, gradient of model.layers.1.mlp.up_proj has inf: False
25-01-23 10:44:07 | I | In layer model.layers.1, gradient of model.layers.1.mlp.gate_proj has nan: False
25-01-23 10:44:07 | I | In layer model.layers.1, gradient of model.layers.1.mlp.gate_proj has inf: False
25-01-23 10:44:07 | I | In layer model.layers.1, gradient of model.layers.1.mlp.down_proj has nan: False
25-01-23 10:44:07 | I | In layer model.layers.1, gradient of model.layers.1.mlp.down_proj has inf: False
25-01-23 10:44:07 | I | [3] done with optimizer step
25-01-23 10:44:07 | I | epoch 001:     23 / 819200000 loss=0.126067, loss_per_token=516.369, loss_sum=4.2301e+06, wps=196.7, ups=0.02, wpb=8192, bsz=16, num_updates=4, lr=0.0004, gnorm=35214.3, clip=100, loss_scale=0.0002, train_wall=42, cuda_gb_allocated=16.3, cuda_gb_reserved=17.5, cuda_gb_free=7.4, wall=2549
25-01-23 10:44:07 | I | TRAIN CURRENT LAYER_IDX = 1
25-01-23 10:44:07 | I | in layer model.layers.1
25-01-23 10:44:07 | I | quantizing weights for layer model.layers.1
25-01-23 10:44:08 | I | collecting calibration activations in model.layers.1
25-01-23 10:44:08 | I | collecting calibration activations in model.layers.1
25-01-23 10:44:08 | I | - Quantizing decoder layer model.layers.1
25-01-23 10:44:08 | I |   - Calibrating model.layers.1.self_attn.q_proj.weight
25-01-23 10:44:09 | I |       - range scale = [    1.0000]
25-01-23 10:44:09 | I |         sum  error  = [    0.1589]
25-01-23 10:44:09 | I |         best error  = [    0.1589]
25-01-23 10:44:09 | I |     + error = [0.1589]
25-01-23 10:44:09 | I |       - range scale = [    1.0000]
25-01-23 10:44:09 | I |         sum  error  = [    1.5823]
25-01-23 10:44:09 | I |         best error  = [    1.5823]
25-01-23 10:44:09 | I |     + error = [1.5823]
25-01-23 10:44:10 | I |   - Calibrating model.layers.1.self_attn.k_proj.weight
25-01-23 10:44:10 | I |       - range scale = [    1.0000]
25-01-23 10:44:10 | I |         sum  error  = [    0.1481]
25-01-23 10:44:10 | I |         best error  = [    0.1481]
25-01-23 10:44:10 | I |     + error = [0.1481]
25-01-23 10:44:11 | I |       - range scale = [    1.0000]
25-01-23 10:44:11 | I |         sum  error  = [    1.8972]
25-01-23 10:44:11 | I |         best error  = [    1.8972]
25-01-23 10:44:11 | I |     + error = [1.8972]
25-01-23 10:44:11 | I |   - Calibrating model.layers.1.self_attn.v_proj.weight
25-01-23 10:44:12 | I |       - range scale = [    1.0000]
25-01-23 10:44:12 | I |         sum  error  = [    1.3797]
25-01-23 10:44:12 | I |         best error  = [    1.3797]
25-01-23 10:44:12 | I |     + error = [1.3797]
25-01-23 10:44:13 | I |       - range scale = [    1.0000]
25-01-23 10:44:13 | I |         sum  error  = [    9.8919]
25-01-23 10:44:13 | I |         best error  = [    9.8919]
25-01-23 10:44:13 | I |     + error = [9.8919]
25-01-23 10:44:13 | I |   - Calibrating model.layers.1.self_attn.o_proj.weight
25-01-23 10:44:14 | I |       - range scale = [    1.0000]
25-01-23 10:44:14 | I |         sum  error  = [    0.2601]
25-01-23 10:44:14 | I |         best error  = [    0.2601]
25-01-23 10:44:14 | I |     + error = [0.2601]
25-01-23 10:44:14 | I |       - range scale = [    1.0000]
25-01-23 10:44:14 | I |         sum  error  = [    2.2216]
25-01-23 10:44:14 | I |         best error  = [    2.2216]
25-01-23 10:44:14 | I |     + error = [2.2216]
25-01-23 10:44:15 | I |   - Calibrating model.layers.1.mlp.up_proj.weight
25-01-23 10:44:15 | I |       - range scale = [    1.0000]
25-01-23 10:44:15 | I |         sum  error  = [    1.6613]
25-01-23 10:44:15 | I |         best error  = [    1.6613]
25-01-23 10:44:15 | I |     + error = [1.6613]
25-01-23 10:44:16 | I |       - range scale = [    1.0000]
25-01-23 10:44:16 | I |         sum  error  = [   17.6352]
25-01-23 10:44:16 | I |         best error  = [   17.6352]
25-01-23 10:44:16 | I |     + error = [17.6352]
25-01-23 10:44:17 | I |   - Calibrating model.layers.1.mlp.gate_proj.weight
25-01-23 10:44:17 | I |       - range scale = [    1.0000]
25-01-23 10:44:17 | I |         sum  error  = [    1.8104]
25-01-23 10:44:17 | I |         best error  = [    1.8104]
25-01-23 10:44:17 | I |     + error = [1.8104]
25-01-23 10:44:19 | I |       - range scale = [    1.0000]
25-01-23 10:44:19 | I |         sum  error  = [   18.8258]
25-01-23 10:44:19 | I |         best error  = [   18.8258]
25-01-23 10:44:19 | I |     + error = [18.8258]
25-01-23 10:44:19 | I |   - Calibrating model.layers.1.mlp.down_proj.weight
25-01-23 10:44:20 | I |       - range scale = [    1.0000]
25-01-23 10:44:20 | I |         sum  error  = [   37.2329]
25-01-23 10:44:20 | I |         best error  = [   37.2329]
25-01-23 10:44:20 | I |     + error = [37.2329]
25-01-23 10:44:21 | I |       - range scale = [    1.0000]
25-01-23 10:44:21 | I |         sum  error  = [  221.8375]
25-01-23 10:44:21 | I |         best error  = [  221.8375]
25-01-23 10:44:21 | I |     + error = [221.8375]
25-01-23 10:44:21 | I |   - Quantizing model.layers.1.self_attn.q_proj.weight
25-01-23 10:44:24 | I |   - Quantizing model.layers.1.self_attn.k_proj.weight
25-01-23 10:44:27 | I |   - Quantizing model.layers.1.self_attn.v_proj.weight
25-01-23 10:44:30 | I |   - Quantizing model.layers.1.self_attn.o_proj.weight
25-01-23 10:44:33 | I |   - Quantizing model.layers.1.mlp.up_proj.weight
25-01-23 10:44:35 | I |   - Quantizing model.layers.1.mlp.gate_proj.weight
25-01-23 10:44:38 | I |   - Quantizing model.layers.1.mlp.down_proj.weight
25-01-23 10:44:46 | I | quantizing activations for layer model.layers.1
25-01-23 10:44:46 | I | collecting calibration activations in model.layers.1
25-01-23 10:44:47 | I | collecting calibration activations in model.layers.1
25-01-23 10:44:48 | I | forward this layer
25-01-23 10:44:48 | I | input_file: /data/gyy/lmquant-main/lmquant/data/data_without_preprocess_llama_7b_bs16/shard/val_input_args_layer_1/23.pt
25-01-23 10:44:48 | I | output_file: /data/gyy/lmquant-main/lmquant/data/data_without_preprocess_llama_7b_bs16/shard/val_teacher_output_layer_1/23.pt
25-01-23 10:44:49 | I | In layer model.layers.1, gradient of model.layers.1.self_attn.q_proj has nan: False
25-01-23 10:44:49 | I | In layer model.layers.1, gradient of model.layers.1.self_attn.q_proj has inf: False
25-01-23 10:44:49 | I | In layer model.layers.1, gradient of model.layers.1.self_attn.k_proj has nan: False
25-01-23 10:44:49 | I | In layer model.layers.1, gradient of model.layers.1.self_attn.k_proj has inf: False
25-01-23 10:44:49 | I | In layer model.layers.1, gradient of model.layers.1.self_attn.v_proj has nan: False
25-01-23 10:44:49 | I | In layer model.layers.1, gradient of model.layers.1.self_attn.v_proj has inf: False
25-01-23 10:44:49 | I | In layer model.layers.1, gradient of model.layers.1.self_attn.o_proj has nan: False
25-01-23 10:44:49 | I | In layer model.layers.1, gradient of model.layers.1.self_attn.o_proj has inf: False
25-01-23 10:44:49 | I | In layer model.layers.1, gradient of model.layers.1.mlp.up_proj has nan: False
25-01-23 10:44:49 | I | In layer model.layers.1, gradient of model.layers.1.mlp.up_proj has inf: False
25-01-23 10:44:49 | I | In layer model.layers.1, gradient of model.layers.1.mlp.gate_proj has nan: False
25-01-23 10:44:49 | I | In layer model.layers.1, gradient of model.layers.1.mlp.gate_proj has inf: False
25-01-23 10:44:49 | I | In layer model.layers.1, gradient of model.layers.1.mlp.down_proj has nan: False
25-01-23 10:44:49 | I | In layer model.layers.1, gradient of model.layers.1.mlp.down_proj has inf: False
25-01-23 10:44:49 | I | [4] done with optimizer step
25-01-23 10:44:49 | I | epoch 001:     24 / 819200000 loss=0.602463, loss_per_token=2467.69, loss_sum=2.02153e+07, wps=195.8, ups=0.02, wpb=8192, bsz=16, num_updates=5, lr=0.0005, gnorm=78829.2, clip=100, loss_scale=0.0002, train_wall=42, cuda_gb_allocated=16.3, cuda_gb_reserved=17.5, cuda_gb_free=7.4, wall=2591
25-01-23 10:44:49 | I | TRAIN CURRENT LAYER_IDX = 1
25-01-23 10:44:49 | I | in layer model.layers.1
25-01-23 10:44:49 | I | quantizing weights for layer model.layers.1
25-01-23 10:44:49 | I | collecting calibration activations in model.layers.1
25-01-23 10:44:49 | I | collecting calibration activations in model.layers.1
25-01-23 10:44:50 | I | - Quantizing decoder layer model.layers.1
25-01-23 10:44:50 | I |   - Calibrating model.layers.1.self_attn.q_proj.weight
25-01-23 10:44:50 | I |       - range scale = [    1.0000]
25-01-23 10:44:50 | I |         sum  error  = [    0.1522]
25-01-23 10:44:50 | I |         best error  = [    0.1522]
25-01-23 10:44:50 | I |     + error = [0.1522]
25-01-23 10:44:51 | I |       - range scale = [    1.0000]
25-01-23 10:44:51 | I |         sum  error  = [    1.6369]
25-01-23 10:44:51 | I |         best error  = [    1.6369]
25-01-23 10:44:51 | I |     + error = [1.6369]
25-01-23 10:44:51 | I |   - Calibrating model.layers.1.self_attn.k_proj.weight
25-01-23 10:44:52 | I |       - range scale = [    1.0000]
25-01-23 10:44:52 | I |         sum  error  = [    0.1994]
25-01-23 10:44:52 | I |         best error  = [    0.1994]
25-01-23 10:44:52 | I |     + error = [0.1994]
25-01-23 10:44:53 | I |       - range scale = [    1.0000]
25-01-23 10:44:53 | I |         sum  error  = [    1.8320]
25-01-23 10:44:53 | I |         best error  = [    1.8320]
25-01-23 10:44:53 | I |     + error = [1.8320]
25-01-23 10:44:53 | I |   - Calibrating model.layers.1.self_attn.v_proj.weight
25-01-23 10:44:54 | I |       - range scale = [    1.0000]
25-01-23 10:44:54 | I |         sum  error  = [    1.3721]
25-01-23 10:44:54 | I |         best error  = [    1.3721]
25-01-23 10:44:54 | I |     + error = [1.3721]
25-01-23 10:44:55 | I |       - range scale = [    1.0000]
25-01-23 10:44:55 | I |         sum  error  = [   10.1320]
25-01-23 10:44:55 | I |         best error  = [   10.1320]
25-01-23 10:44:55 | I |     + error = [10.1320]
25-01-23 10:44:55 | I |   - Calibrating model.layers.1.self_attn.o_proj.weight
25-01-23 10:44:55 | I |       - range scale = [    1.0000]
25-01-23 10:44:55 | I |         sum  error  = [    0.2722]
25-01-23 10:44:55 | I |         best error  = [    0.2722]
25-01-23 10:44:55 | I |     + error = [0.2722]
25-01-23 10:44:56 | I |       - range scale = [    1.0000]
25-01-23 10:44:56 | I |         sum  error  = [    2.3263]
25-01-23 10:44:56 | I |         best error  = [    2.3263]
25-01-23 10:44:56 | I |     + error = [2.3263]
25-01-23 10:44:57 | I |   - Calibrating model.layers.1.mlp.up_proj.weight
25-01-23 10:44:57 | I |       - range scale = [    1.0000]
25-01-23 10:44:57 | I |         sum  error  = [    1.7315]
25-01-23 10:44:57 | I |         best error  = [    1.7315]
25-01-23 10:44:57 | I |     + error = [1.7315]
25-01-23 10:44:58 | I |       - range scale = [    1.0000]
25-01-23 10:44:58 | I |         sum  error  = [   18.4183]
25-01-23 10:44:58 | I |         best error  = [   18.4183]
25-01-23 10:44:58 | I |     + error = [18.4183]
25-01-23 10:44:59 | I |   - Calibrating model.layers.1.mlp.gate_proj.weight
25-01-23 10:44:59 | I |       - range scale = [    1.0000]
25-01-23 10:44:59 | I |         sum  error  = [    1.8840]
25-01-23 10:44:59 | I |         best error  = [    1.8840]
25-01-23 10:44:59 | I |     + error = [1.8840]
25-01-23 10:45:01 | I |       - range scale = [    1.0000]
25-01-23 10:45:01 | I |         sum  error  = [   19.6669]
25-01-23 10:45:01 | I |         best error  = [   19.6669]
25-01-23 10:45:01 | I |     + error = [19.6669]
25-01-23 10:45:01 | I |   - Calibrating model.layers.1.mlp.down_proj.weight
25-01-23 10:45:01 | I |       - range scale = [    1.0000]
25-01-23 10:45:01 | I |         sum  error  = [   42.9024]
25-01-23 10:45:01 | I |         best error  = [   42.9024]
25-01-23 10:45:01 | I |     + error = [42.9024]
25-01-23 10:45:03 | I |       - range scale = [    1.0000]
25-01-23 10:45:03 | I |         sum  error  = [  278.4219]
25-01-23 10:45:03 | I |         best error  = [  278.4219]
25-01-23 10:45:03 | I |     + error = [278.4219]
25-01-23 10:45:03 | I |   - Quantizing model.layers.1.self_attn.q_proj.weight
25-01-23 10:45:06 | I |   - Quantizing model.layers.1.self_attn.k_proj.weight
25-01-23 10:45:09 | I |   - Quantizing model.layers.1.self_attn.v_proj.weight
25-01-23 10:45:12 | I |   - Quantizing model.layers.1.self_attn.o_proj.weight
25-01-23 10:45:14 | I |   - Quantizing model.layers.1.mlp.up_proj.weight
25-01-23 10:45:17 | I |   - Quantizing model.layers.1.mlp.gate_proj.weight
25-01-23 10:45:20 | I |   - Quantizing model.layers.1.mlp.down_proj.weight
25-01-23 10:45:28 | I | quantizing activations for layer model.layers.1
25-01-23 10:45:28 | I | collecting calibration activations in model.layers.1
25-01-23 10:45:28 | I | collecting calibration activations in model.layers.1
25-01-23 10:45:30 | I | forward this layer
25-01-23 10:45:30 | I | input_file: /data/gyy/lmquant-main/lmquant/data/data_without_preprocess_llama_7b_bs16/shard/val_input_args_layer_1/24.pt
25-01-23 10:45:30 | I | output_file: /data/gyy/lmquant-main/lmquant/data/data_without_preprocess_llama_7b_bs16/shard/val_teacher_output_layer_1/24.pt
25-01-23 10:45:31 | I | In layer model.layers.1, gradient of model.layers.1.self_attn.q_proj has nan: False
25-01-23 10:45:31 | I | In layer model.layers.1, gradient of model.layers.1.self_attn.q_proj has inf: False
25-01-23 10:45:31 | I | In layer model.layers.1, gradient of model.layers.1.self_attn.k_proj has nan: False
25-01-23 10:45:31 | I | In layer model.layers.1, gradient of model.layers.1.self_attn.k_proj has inf: False
25-01-23 10:45:31 | I | In layer model.layers.1, gradient of model.layers.1.self_attn.v_proj has nan: False
25-01-23 10:45:31 | I | In layer model.layers.1, gradient of model.layers.1.self_attn.v_proj has inf: False
25-01-23 10:45:31 | I | In layer model.layers.1, gradient of model.layers.1.self_attn.o_proj has nan: False
25-01-23 10:45:31 | I | In layer model.layers.1, gradient of model.layers.1.self_attn.o_proj has inf: False
25-01-23 10:45:31 | I | In layer model.layers.1, gradient of model.layers.1.mlp.up_proj has nan: False
25-01-23 10:45:31 | I | In layer model.layers.1, gradient of model.layers.1.mlp.up_proj has inf: False
25-01-23 10:45:31 | I | In layer model.layers.1, gradient of model.layers.1.mlp.gate_proj has nan: False
25-01-23 10:45:31 | I | In layer model.layers.1, gradient of model.layers.1.mlp.gate_proj has inf: False
25-01-23 10:45:31 | I | In layer model.layers.1, gradient of model.layers.1.mlp.down_proj has nan: False
25-01-23 10:45:31 | I | In layer model.layers.1, gradient of model.layers.1.mlp.down_proj has inf: False
25-01-23 10:45:31 | I | [5] done with optimizer step
25-01-23 10:45:31 | I | epoch 001:     25 / 819200000 loss=0.511797, loss_per_token=2096.32, loss_sum=1.7173e+07, wps=196.4, ups=0.02, wpb=8192, bsz=16, num_updates=6, lr=0.0006, gnorm=34293.2, clip=100, loss_scale=0.0002, train_wall=42, cuda_gb_allocated=16.3, cuda_gb_reserved=17.5, cuda_gb_free=7.4, wall=2632
25-01-23 10:45:31 | I | TRAIN CURRENT LAYER_IDX = 1
25-01-23 10:45:31 | I | in layer model.layers.1
25-01-23 10:45:31 | I | quantizing weights for layer model.layers.1
25-01-23 10:45:31 | I | collecting calibration activations in model.layers.1
25-01-23 10:45:31 | I | collecting calibration activations in model.layers.1
25-01-23 10:45:31 | I | - Quantizing decoder layer model.layers.1
25-01-23 10:45:31 | I |   - Calibrating model.layers.1.self_attn.q_proj.weight
25-01-23 10:45:32 | I |       - range scale = [    1.0000]
25-01-23 10:45:32 | I |         sum  error  = [    0.1714]
25-01-23 10:45:32 | I |         best error  = [    0.1714]
25-01-23 10:45:32 | I |     + error = [0.1714]
25-01-23 10:45:33 | I |       - range scale = [    1.0000]
25-01-23 10:45:33 | I |         sum  error  = [    1.7275]
25-01-23 10:45:33 | I |         best error  = [    1.7275]
25-01-23 10:45:33 | I |     + error = [1.7275]
25-01-23 10:45:33 | I |   - Calibrating model.layers.1.self_attn.k_proj.weight
25-01-23 10:45:34 | I |       - range scale = [    1.0000]
25-01-23 10:45:34 | I |         sum  error  = [    0.1715]
25-01-23 10:45:34 | I |         best error  = [    0.1715]
25-01-23 10:45:34 | I |     + error = [0.1715]
25-01-23 10:45:35 | I |       - range scale = [    1.0000]
25-01-23 10:45:35 | I |         sum  error  = [    1.8699]
25-01-23 10:45:35 | I |         best error  = [    1.8699]
25-01-23 10:45:35 | I |     + error = [1.8699]
25-01-23 10:45:35 | I |   - Calibrating model.layers.1.self_attn.v_proj.weight
25-01-23 10:45:35 | I |       - range scale = [    1.0000]
25-01-23 10:45:35 | I |         sum  error  = [    1.3837]
25-01-23 10:45:35 | I |         best error  = [    1.3837]
25-01-23 10:45:35 | I |     + error = [1.3837]
25-01-23 10:45:36 | I |       - range scale = [    1.0000]
25-01-23 10:45:36 | I |         sum  error  = [   10.0804]
25-01-23 10:45:36 | I |         best error  = [   10.0804]
25-01-23 10:45:36 | I |     + error = [10.0804]
25-01-23 10:45:36 | I |   - Calibrating model.layers.1.self_attn.o_proj.weight
25-01-23 10:45:37 | I |       - range scale = [    1.0000]
25-01-23 10:45:37 | I |         sum  error  = [    0.2759]
25-01-23 10:45:37 | I |         best error  = [    0.2759]
25-01-23 10:45:37 | I |     + error = [0.2759]
25-01-23 10:45:38 | I |       - range scale = [    1.0000]
25-01-23 10:45:38 | I |         sum  error  = [    2.3805]
25-01-23 10:45:38 | I |         best error  = [    2.3805]
25-01-23 10:45:38 | I |     + error = [2.3805]
25-01-23 10:45:38 | I |   - Calibrating model.layers.1.mlp.up_proj.weight
25-01-23 10:45:39 | I |       - range scale = [    1.0000]
25-01-23 10:45:39 | I |         sum  error  = [    1.8101]
25-01-23 10:45:39 | I |         best error  = [    1.8101]
25-01-23 10:45:39 | I |     + error = [1.8101]
25-01-23 10:45:40 | I |       - range scale = [    1.0000]
25-01-23 10:45:40 | I |         sum  error  = [   19.2532]
25-01-23 10:45:40 | I |         best error  = [   19.2532]
25-01-23 10:45:40 | I |     + error = [19.2532]
25-01-23 10:45:40 | I |   - Calibrating model.layers.1.mlp.gate_proj.weight
25-01-23 10:45:41 | I |       - range scale = [    1.0000]
25-01-23 10:45:41 | I |         sum  error  = [    1.9682]
25-01-23 10:45:41 | I |         best error  = [    1.9682]
25-01-23 10:45:41 | I |     + error = [1.9682]
25-01-23 10:45:42 | I |       - range scale = [    1.0000]
25-01-23 10:45:42 | I |         sum  error  = [   20.5459]
25-01-23 10:45:42 | I |         best error  = [   20.5459]
25-01-23 10:45:42 | I |     + error = [20.5459]
25-01-23 10:45:42 | I |   - Calibrating model.layers.1.mlp.down_proj.weight
25-01-23 10:45:43 | I |       - range scale = [    1.0000]
25-01-23 10:45:43 | I |         sum  error  = [   60.6276]
25-01-23 10:45:43 | I |         best error  = [   60.6276]
25-01-23 10:45:43 | I |     + error = [60.6276]
25-01-23 10:45:44 | I |       - range scale = [    1.0000]
25-01-23 10:45:44 | I |         sum  error  = [  428.0169]
25-01-23 10:45:44 | I |         best error  = [  428.0169]
25-01-23 10:45:44 | I |     + error = [428.0169]
25-01-23 10:45:44 | I |   - Quantizing model.layers.1.self_attn.q_proj.weight
25-01-23 10:45:47 | I |   - Quantizing model.layers.1.self_attn.k_proj.weight
25-01-23 10:45:50 | I |   - Quantizing model.layers.1.self_attn.v_proj.weight
25-01-23 10:45:53 | I |   - Quantizing model.layers.1.self_attn.o_proj.weight
25-01-23 10:45:56 | I |   - Quantizing model.layers.1.mlp.up_proj.weight
25-01-23 10:45:59 | I |   - Quantizing model.layers.1.mlp.gate_proj.weight
25-01-23 10:46:02 | I |   - Quantizing model.layers.1.mlp.down_proj.weight
25-01-23 10:46:09 | I | quantizing activations for layer model.layers.1
25-01-23 10:46:10 | I | collecting calibration activations in model.layers.1
25-01-23 10:46:10 | I | collecting calibration activations in model.layers.1
25-01-23 10:46:12 | I | forward this layer
25-01-23 10:46:12 | I | input_file: /data/gyy/lmquant-main/lmquant/data/data_without_preprocess_llama_7b_bs16/shard/val_input_args_layer_1/25.pt
25-01-23 10:46:12 | I | output_file: /data/gyy/lmquant-main/lmquant/data/data_without_preprocess_llama_7b_bs16/shard/val_teacher_output_layer_1/25.pt
25-01-23 10:46:12 | I | In layer model.layers.1, gradient of model.layers.1.self_attn.q_proj has nan: False
25-01-23 10:46:12 | I | In layer model.layers.1, gradient of model.layers.1.self_attn.q_proj has inf: False
25-01-23 10:46:12 | I | In layer model.layers.1, gradient of model.layers.1.self_attn.k_proj has nan: False
25-01-23 10:46:12 | I | In layer model.layers.1, gradient of model.layers.1.self_attn.k_proj has inf: False
25-01-23 10:46:12 | I | In layer model.layers.1, gradient of model.layers.1.self_attn.v_proj has nan: False
25-01-23 10:46:12 | I | In layer model.layers.1, gradient of model.layers.1.self_attn.v_proj has inf: False
25-01-23 10:46:12 | I | In layer model.layers.1, gradient of model.layers.1.self_attn.o_proj has nan: False
25-01-23 10:46:12 | I | In layer model.layers.1, gradient of model.layers.1.self_attn.o_proj has inf: False
25-01-23 10:46:12 | I | In layer model.layers.1, gradient of model.layers.1.mlp.up_proj has nan: False
25-01-23 10:46:12 | I | In layer model.layers.1, gradient of model.layers.1.mlp.up_proj has inf: False
25-01-23 10:46:12 | I | In layer model.layers.1, gradient of model.layers.1.mlp.gate_proj has nan: False
25-01-23 10:46:12 | I | In layer model.layers.1, gradient of model.layers.1.mlp.gate_proj has inf: False
25-01-23 10:46:12 | I | In layer model.layers.1, gradient of model.layers.1.mlp.down_proj has nan: False
25-01-23 10:46:12 | I | In layer model.layers.1, gradient of model.layers.1.mlp.down_proj has inf: False
25-01-23 10:46:12 | I | [6] done with optimizer step
25-01-23 10:46:12 | I | epoch 001:     26 / 819200000 loss=0.549815, loss_per_token=2252.04, loss_sum=1.84487e+07, wps=196.4, ups=0.02, wpb=8192, bsz=16, num_updates=7, lr=0.0007, gnorm=45006.5, clip=100, loss_scale=0.0002, train_wall=42, cuda_gb_allocated=16.3, cuda_gb_reserved=17.5, cuda_gb_free=7.4, wall=2674
25-01-23 10:46:12 | I | TRAIN CURRENT LAYER_IDX = 1
25-01-23 10:46:12 | I | in layer model.layers.1
25-01-23 10:46:12 | I | quantizing weights for layer model.layers.1
25-01-23 10:46:13 | I | collecting calibration activations in model.layers.1
25-01-23 10:46:13 | I | collecting calibration activations in model.layers.1
25-01-23 10:46:13 | I | - Quantizing decoder layer model.layers.1
25-01-23 10:46:13 | I |   - Calibrating model.layers.1.self_attn.q_proj.weight
25-01-23 10:46:14 | I |       - range scale = [    1.0000]
25-01-23 10:46:14 | I |         sum  error  = [    0.1809]
25-01-23 10:46:14 | I |         best error  = [    0.1809]
25-01-23 10:46:14 | I |     + error = [0.1809]
25-01-23 10:46:15 | I |       - range scale = [    1.0000]
25-01-23 10:46:15 | I |         sum  error  = [    1.8196]
25-01-23 10:46:15 | I |         best error  = [    1.8196]
25-01-23 10:46:15 | I |     + error = [1.8196]
25-01-23 10:46:15 | I |   - Calibrating model.layers.1.self_attn.k_proj.weight
25-01-23 10:46:16 | I |       - range scale = [    1.0000]
25-01-23 10:46:16 | I |         sum  error  = [    0.2397]
25-01-23 10:46:16 | I |         best error  = [    0.2397]
25-01-23 10:46:16 | I |     + error = [0.2397]
25-01-23 10:46:16 | I |       - range scale = [    1.0000]
25-01-23 10:46:16 | I |         sum  error  = [    1.9852]
25-01-23 10:46:16 | I |         best error  = [    1.9852]
25-01-23 10:46:16 | I |     + error = [1.9852]
25-01-23 10:46:16 | I |   - Calibrating model.layers.1.self_attn.v_proj.weight
25-01-23 10:46:17 | I |       - range scale = [    1.0000]
25-01-23 10:46:17 | I |         sum  error  = [    1.3699]
25-01-23 10:46:17 | I |         best error  = [    1.3699]
25-01-23 10:46:17 | I |     + error = [1.3699]
25-01-23 10:46:18 | I |       - range scale = [    1.0000]
25-01-23 10:46:18 | I |         sum  error  = [   10.2346]
25-01-23 10:46:18 | I |         best error  = [   10.2346]
25-01-23 10:46:18 | I |     + error = [10.2346]
25-01-23 10:46:18 | I |   - Calibrating model.layers.1.self_attn.o_proj.weight
25-01-23 10:46:19 | I |       - range scale = [    1.0000]
25-01-23 10:46:19 | I |         sum  error  = [    0.2825]
25-01-23 10:46:19 | I |         best error  = [    0.2825]
25-01-23 10:46:19 | I |     + error = [0.2825]
25-01-23 10:46:20 | I |       - range scale = [    1.0000]
25-01-23 10:46:20 | I |         sum  error  = [    2.4308]
25-01-23 10:46:20 | I |         best error  = [    2.4308]
25-01-23 10:46:20 | I |     + error = [2.4308]
25-01-23 10:46:20 | I |   - Calibrating model.layers.1.mlp.up_proj.weight
25-01-23 10:46:21 | I |       - range scale = [    1.0000]
25-01-23 10:46:21 | I |         sum  error  = [    2.0008]
25-01-23 10:46:21 | I |         best error  = [    2.0008]
25-01-23 10:46:21 | I |     + error = [2.0008]
25-01-23 10:46:22 | I |       - range scale = [    1.0000]
25-01-23 10:46:22 | I |         sum  error  = [   21.2775]
25-01-23 10:46:22 | I |         best error  = [   21.2775]
25-01-23 10:46:22 | I |     + error = [21.2775]
25-01-23 10:46:22 | I |   - Calibrating model.layers.1.mlp.gate_proj.weight
25-01-23 10:46:23 | I |       - range scale = [    1.0000]
25-01-23 10:46:23 | I |         sum  error  = [    2.1734]
25-01-23 10:46:23 | I |         best error  = [    2.1734]
25-01-23 10:46:23 | I |     + error = [2.1734]
25-01-23 10:46:24 | I |       - range scale = [    1.0000]
25-01-23 10:46:24 | I |         sum  error  = [   22.7017]
25-01-23 10:46:24 | I |         best error  = [   22.7017]
25-01-23 10:46:24 | I |     + error = [22.7017]
25-01-23 10:46:24 | I |   - Calibrating model.layers.1.mlp.down_proj.weight
25-01-23 10:46:25 | I |       - range scale = [    1.0000]
25-01-23 10:46:25 | I |         sum  error  = [   62.1833]
25-01-23 10:46:25 | I |         best error  = [   62.1833]
25-01-23 10:46:25 | I |     + error = [62.1833]
25-01-23 10:46:26 | I |       - range scale = [    1.0000]
25-01-23 10:46:26 | I |         sum  error  = [  419.6615]
25-01-23 10:46:26 | I |         best error  = [  419.6615]
25-01-23 10:46:26 | I |     + error = [419.6615]
25-01-23 10:46:26 | I |   - Quantizing model.layers.1.self_attn.q_proj.weight
25-01-23 10:46:29 | I |   - Quantizing model.layers.1.self_attn.k_proj.weight
25-01-23 10:46:32 | I |   - Quantizing model.layers.1.self_attn.v_proj.weight
25-01-23 10:46:35 | I |   - Quantizing model.layers.1.self_attn.o_proj.weight
25-01-23 10:46:38 | I |   - Quantizing model.layers.1.mlp.up_proj.weight
25-01-23 10:46:41 | I |   - Quantizing model.layers.1.mlp.gate_proj.weight
25-01-23 10:46:44 | I |   - Quantizing model.layers.1.mlp.down_proj.weight
25-01-23 10:46:52 | I | quantizing activations for layer model.layers.1
25-01-23 10:46:52 | I | collecting calibration activations in model.layers.1
25-01-23 10:46:52 | I | collecting calibration activations in model.layers.1
25-01-23 10:46:54 | I | forward this layer
25-01-23 10:46:54 | I | input_file: /data/gyy/lmquant-main/lmquant/data/data_without_preprocess_llama_7b_bs16/shard/val_input_args_layer_1/26.pt
25-01-23 10:46:54 | I | output_file: /data/gyy/lmquant-main/lmquant/data/data_without_preprocess_llama_7b_bs16/shard/val_teacher_output_layer_1/26.pt
25-01-23 10:46:54 | I | In layer model.layers.1, gradient of model.layers.1.self_attn.q_proj has nan: False
25-01-23 10:46:54 | I | In layer model.layers.1, gradient of model.layers.1.self_attn.q_proj has inf: False
25-01-23 10:46:54 | I | In layer model.layers.1, gradient of model.layers.1.self_attn.k_proj has nan: False
25-01-23 10:46:54 | I | In layer model.layers.1, gradient of model.layers.1.self_attn.k_proj has inf: False
25-01-23 10:46:54 | I | In layer model.layers.1, gradient of model.layers.1.self_attn.v_proj has nan: False
25-01-23 10:46:54 | I | In layer model.layers.1, gradient of model.layers.1.self_attn.v_proj has inf: True
25-01-23 10:46:54 | I | inf: first position tensor([1062, 1512], device='cuda:0')
25-01-23 10:46:54 | I | In layer model.layers.1, gradient of model.layers.1.self_attn.o_proj has nan: False
25-01-23 10:46:54 | I | In layer model.layers.1, gradient of model.layers.1.self_attn.o_proj has inf: False
25-01-23 10:46:54 | I | In layer model.layers.1, gradient of model.layers.1.mlp.up_proj has nan: False
25-01-23 10:46:54 | I | In layer model.layers.1, gradient of model.layers.1.mlp.up_proj has inf: False
25-01-23 10:46:54 | I | In layer model.layers.1, gradient of model.layers.1.mlp.gate_proj has nan: False
25-01-23 10:46:54 | I | In layer model.layers.1, gradient of model.layers.1.mlp.gate_proj has inf: False
25-01-23 10:46:54 | I | In layer model.layers.1, gradient of model.layers.1.mlp.down_proj has nan: False
25-01-23 10:46:54 | I | In layer model.layers.1, gradient of model.layers.1.mlp.down_proj has inf: False
25-01-23 10:46:54 | I | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 0.0001220703125
25-01-23 10:46:55 | I | TRAIN CURRENT LAYER_IDX = 1
25-01-23 10:46:55 | I | in layer model.layers.1
25-01-23 10:46:55 | I | quantizing weights for layer model.layers.1
25-01-23 10:46:55 | I | collecting calibration activations in model.layers.1
25-01-23 10:46:55 | I | collecting calibration activations in model.layers.1
25-01-23 10:46:55 | I | - Quantizing decoder layer model.layers.1
25-01-23 10:46:55 | I |   - Calibrating model.layers.1.self_attn.q_proj.weight
25-01-23 10:46:56 | I |       - range scale = [    1.0000]
25-01-23 10:46:56 | I |         sum  error  = [    0.1713]
25-01-23 10:46:56 | I |         best error  = [    0.1713]
25-01-23 10:46:56 | I |     + error = [0.1713]
25-01-23 10:46:57 | I |       - range scale = [    1.0000]
25-01-23 10:46:57 | I |         sum  error  = [    1.7479]
25-01-23 10:46:57 | I |         best error  = [    1.7479]
25-01-23 10:46:57 | I |     + error = [1.7479]
25-01-23 10:46:57 | I |   - Calibrating model.layers.1.self_attn.k_proj.weight
25-01-23 10:46:58 | I |       - range scale = [    1.0000]
25-01-23 10:46:58 | I |         sum  error  = [    0.2256]
25-01-23 10:46:58 | I |         best error  = [    0.2256]
25-01-23 10:46:58 | I |     + error = [0.2256]
25-01-23 10:46:58 | I |       - range scale = [    1.0000]
25-01-23 10:46:58 | I |         sum  error  = [    1.8825]
25-01-23 10:46:58 | I |         best error  = [    1.8825]
25-01-23 10:46:58 | I |     + error = [1.8825]
25-01-23 10:46:59 | I |   - Calibrating model.layers.1.self_attn.v_proj.weight
25-01-23 10:46:59 | I |       - range scale = [    1.0000]
25-01-23 10:46:59 | I |         sum  error  = [    1.3689]
25-01-23 10:46:59 | I |         best error  = [    1.3689]
25-01-23 10:46:59 | I |     + error = [1.3689]
25-01-23 10:47:00 | I |       - range scale = [    1.0000]
25-01-23 10:47:00 | I |         sum  error  = [   10.1075]
25-01-23 10:47:00 | I |         best error  = [   10.1075]
25-01-23 10:47:00 | I |     + error = [10.1075]
25-01-23 10:47:00 | I |   - Calibrating model.layers.1.self_attn.o_proj.weight
25-01-23 10:47:01 | I |       - range scale = [    1.0000]
25-01-23 10:47:01 | I |         sum  error  = [    0.2808]
25-01-23 10:47:01 | I |         best error  = [    0.2808]
25-01-23 10:47:01 | I |     + error = [0.2808]
25-01-23 10:47:02 | I |       - range scale = [    1.0000]
25-01-23 10:47:02 | I |         sum  error  = [    2.4090]
25-01-23 10:47:02 | I |         best error  = [    2.4090]
25-01-23 10:47:02 | I |     + error = [2.4090]
25-01-23 10:47:02 | I |   - Calibrating model.layers.1.mlp.up_proj.weight
25-01-23 10:47:03 | I |       - range scale = [    1.0000]
25-01-23 10:47:03 | I |         sum  error  = [    1.9610]
25-01-23 10:47:03 | I |         best error  = [    1.9610]
25-01-23 10:47:03 | I |     + error = [1.9610]
25-01-23 10:47:04 | I |       - range scale = [    1.0000]
25-01-23 10:47:04 | I |         sum  error  = [   20.8604]
25-01-23 10:47:04 | I |         best error  = [   20.8604]
25-01-23 10:47:04 | I |     + error = [20.8604]
25-01-23 10:47:04 | I |   - Calibrating model.layers.1.mlp.gate_proj.weight
25-01-23 10:47:05 | I |       - range scale = [    1.0000]
25-01-23 10:47:05 | I |         sum  error  = [    2.1348]
25-01-23 10:47:05 | I |         best error  = [    2.1348]
25-01-23 10:47:05 | I |     + error = [2.1348]
25-01-23 10:47:06 | I |       - range scale = [    1.0000]
25-01-23 10:47:06 | I |         sum  error  = [   22.2487]
25-01-23 10:47:06 | I |         best error  = [   22.2487]
25-01-23 10:47:06 | I |     + error = [22.2487]
25-01-23 10:47:06 | I |   - Calibrating model.layers.1.mlp.down_proj.weight
25-01-23 10:47:07 | I |       - range scale = [    1.0000]
25-01-23 10:47:07 | I |         sum  error  = [   62.6868]
25-01-23 10:47:07 | I |         best error  = [   62.6868]
25-01-23 10:47:07 | I |     + error = [62.6868]
25-01-23 10:47:08 | I |       - range scale = [    1.0000]
25-01-23 10:47:08 | I |         sum  error  = [  423.5063]
25-01-23 10:47:08 | I |         best error  = [  423.5063]
25-01-23 10:47:08 | I |     + error = [423.5063]
25-01-23 10:47:09 | I |   - Quantizing model.layers.1.self_attn.q_proj.weight
25-01-23 10:47:11 | I |   - Quantizing model.layers.1.self_attn.k_proj.weight
25-01-23 10:47:14 | I |   - Quantizing model.layers.1.self_attn.v_proj.weight
25-01-23 10:47:17 | I |   - Quantizing model.layers.1.self_attn.o_proj.weight
25-01-23 10:47:20 | I |   - Quantizing model.layers.1.mlp.up_proj.weight
25-01-23 10:47:23 | I |   - Quantizing model.layers.1.mlp.gate_proj.weight
25-01-23 10:47:26 | I |   - Quantizing model.layers.1.mlp.down_proj.weight
25-01-23 10:47:34 | I | quantizing activations for layer model.layers.1
25-01-23 10:47:34 | I | collecting calibration activations in model.layers.1
25-01-23 10:47:34 | I | collecting calibration activations in model.layers.1
25-01-23 10:47:36 | I | forward this layer
25-01-23 10:47:36 | I | input_file: /data/gyy/lmquant-main/lmquant/data/data_without_preprocess_llama_7b_bs16/shard/val_input_args_layer_1/27.pt
25-01-23 10:47:36 | I | output_file: /data/gyy/lmquant-main/lmquant/data/data_without_preprocess_llama_7b_bs16/shard/val_teacher_output_layer_1/27.pt
25-01-23 10:47:36 | I | In layer model.layers.1, gradient of model.layers.1.self_attn.q_proj has nan: False
25-01-23 10:47:36 | I | In layer model.layers.1, gradient of model.layers.1.self_attn.q_proj has inf: False
25-01-23 10:47:36 | I | In layer model.layers.1, gradient of model.layers.1.self_attn.k_proj has nan: False
25-01-23 10:47:36 | I | In layer model.layers.1, gradient of model.layers.1.self_attn.k_proj has inf: False
25-01-23 10:47:36 | I | In layer model.layers.1, gradient of model.layers.1.self_attn.v_proj has nan: False
25-01-23 10:47:36 | I | In layer model.layers.1, gradient of model.layers.1.self_attn.v_proj has inf: True
25-01-23 10:47:36 | I | inf: first position tensor([1065, 1512], device='cuda:0')
25-01-23 10:47:36 | I | In layer model.layers.1, gradient of model.layers.1.self_attn.o_proj has nan: False
25-01-23 10:47:36 | I | In layer model.layers.1, gradient of model.layers.1.self_attn.o_proj has inf: False
25-01-23 10:47:36 | I | In layer model.layers.1, gradient of model.layers.1.mlp.up_proj has nan: False
25-01-23 10:47:36 | I | In layer model.layers.1, gradient of model.layers.1.mlp.up_proj has inf: False
25-01-23 10:47:36 | I | In layer model.layers.1, gradient of model.layers.1.mlp.gate_proj has nan: False
25-01-23 10:47:36 | I | In layer model.layers.1, gradient of model.layers.1.mlp.gate_proj has inf: False
25-01-23 10:47:36 | I | In layer model.layers.1, gradient of model.layers.1.mlp.down_proj has nan: False
25-01-23 10:47:36 | I | In layer model.layers.1, gradient of model.layers.1.mlp.down_proj has inf: False
25-01-23 10:47:36 | I | in layer model.layers.1
25-01-23 10:47:36 | I | quantizing weights for layer model.layers.1
25-01-23 10:47:37 | I | collecting calibration activations in model.layers.1
25-01-23 10:47:37 | I | collecting calibration activations in model.layers.1
25-01-23 10:47:37 | I | - Quantizing decoder layer model.layers.1
25-01-23 10:47:37 | I |   - Calibrating model.layers.1.self_attn.q_proj.weight
25-01-23 10:47:38 | I |       - range scale = [    1.0000]
25-01-23 10:47:38 | I |         sum  error  = [    0.1713]
25-01-23 10:47:38 | I |         best error  = [    0.1713]
25-01-23 10:47:38 | I |     + error = [0.1713]
25-01-23 10:47:39 | I |       - range scale = [    1.0000]
25-01-23 10:47:39 | I |         sum  error  = [    1.7479]
25-01-23 10:47:39 | I |         best error  = [    1.7479]
25-01-23 10:47:39 | I |     + error = [1.7479]
25-01-23 10:47:39 | I |   - Calibrating model.layers.1.self_attn.k_proj.weight
25-01-23 10:47:40 | I |       - range scale = [    1.0000]
25-01-23 10:47:40 | I |         sum  error  = [    0.2256]
25-01-23 10:47:40 | I |         best error  = [    0.2256]
25-01-23 10:47:40 | I |     + error = [0.2256]
25-01-23 10:47:41 | I |       - range scale = [    1.0000]
25-01-23 10:47:41 | I |         sum  error  = [    1.8825]
25-01-23 10:47:41 | I |         best error  = [    1.8825]
25-01-23 10:47:41 | I |     + error = [1.8825]
25-01-23 10:47:41 | I |   - Calibrating model.layers.1.self_attn.v_proj.weight
25-01-23 10:47:41 | I |       - range scale = [    1.0000]
25-01-23 10:47:41 | I |         sum  error  = [    1.3689]
25-01-23 10:47:41 | I |         best error  = [    1.3689]
25-01-23 10:47:41 | I |     + error = [1.3689]
25-01-23 10:47:42 | I |       - range scale = [    1.0000]
25-01-23 10:47:42 | I |         sum  error  = [   10.1075]
25-01-23 10:47:42 | I |         best error  = [   10.1075]
25-01-23 10:47:42 | I |     + error = [10.1075]
25-01-23 10:47:43 | I |   - Calibrating model.layers.1.self_attn.o_proj.weight
25-01-23 10:47:43 | I |       - range scale = [    1.0000]
25-01-23 10:47:43 | I |         sum  error  = [    0.2808]
25-01-23 10:47:43 | I |         best error  = [    0.2808]
25-01-23 10:47:43 | I |     + error = [0.2808]
25-01-23 10:47:44 | I |       - range scale = [    1.0000]
25-01-23 10:47:44 | I |         sum  error  = [    2.4090]
25-01-23 10:47:44 | I |         best error  = [    2.4090]
25-01-23 10:47:44 | I |     + error = [2.4090]
25-01-23 10:47:44 | I |   - Calibrating model.layers.1.mlp.up_proj.weight
25-01-23 10:47:45 | I |       - range scale = [    1.0000]
25-01-23 10:47:45 | I |         sum  error  = [    1.9610]
25-01-23 10:47:45 | I |         best error  = [    1.9610]
25-01-23 10:47:45 | I |     + error = [1.9610]
25-01-23 10:47:46 | I |       - range scale = [    1.0000]
25-01-23 10:47:46 | I |         sum  error  = [   20.8604]
25-01-23 10:47:46 | I |         best error  = [   20.8604]
25-01-23 10:47:46 | I |     + error = [20.8604]
25-01-23 10:47:46 | I |   - Calibrating model.layers.1.mlp.gate_proj.weight
25-01-23 10:47:47 | I |       - range scale = [    1.0000]
25-01-23 10:47:47 | I |         sum  error  = [    2.1348]
25-01-23 10:47:47 | I |         best error  = [    2.1348]
25-01-23 10:47:47 | I |     + error = [2.1348]
25-01-23 10:47:48 | I |       - range scale = [    1.0000]
25-01-23 10:47:48 | I |         sum  error  = [   22.2487]
25-01-23 10:47:48 | I |         best error  = [   22.2487]
25-01-23 10:47:48 | I |     + error = [22.2487]
25-01-23 10:47:48 | I |   - Calibrating model.layers.1.mlp.down_proj.weight
25-01-23 10:47:49 | I |       - range scale = [    1.0000]
25-01-23 10:47:49 | I |         sum  error  = [   62.6868]
25-01-23 10:47:49 | I |         best error  = [   62.6868]
25-01-23 10:47:49 | I |     + error = [62.6868]
25-01-23 10:47:50 | I |       - range scale = [    1.0000]
25-01-23 10:47:50 | I |         sum  error  = [  423.5063]
25-01-23 10:47:50 | I |         best error  = [  423.5063]
25-01-23 10:47:50 | I |     + error = [423.5063]
25-01-23 10:47:51 | I |   - Quantizing model.layers.1.self_attn.q_proj.weight
25-01-23 10:47:53 | I |   - Quantizing model.layers.1.self_attn.k_proj.weight
25-01-23 10:47:55 | I |   - Quantizing model.layers.1.self_attn.v_proj.weight
25-01-23 10:47:58 | I |   - Quantizing model.layers.1.self_attn.o_proj.weight
25-01-23 10:48:00 | I |   - Quantizing model.layers.1.mlp.up_proj.weight
25-01-23 10:48:02 | I |   - Quantizing model.layers.1.mlp.gate_proj.weight
25-01-23 10:48:05 | I |   - Quantizing model.layers.1.mlp.down_proj.weight
25-01-23 10:48:11 | I | quantizing activations for layer model.layers.1
25-01-23 10:48:11 | I | collecting calibration activations in model.layers.1
25-01-23 10:48:11 | I | collecting calibration activations in model.layers.1
25-01-23 10:48:13 | I | forward this layer
25-01-23 10:48:13 | I | input_file: /data/gyy/lmquant-main/lmquant/data/data_without_preprocess_llama_7b_bs16/shard/val_input_args_layer_1/27.pt
25-01-23 10:48:13 | I | output_file: /data/gyy/lmquant-main/lmquant/data/data_without_preprocess_llama_7b_bs16/shard/val_teacher_output_layer_1/27.pt
25-01-23 10:48:14 | I | In layer model.layers.1, gradient of model.layers.1.self_attn.q_proj has nan: False
25-01-23 10:48:14 | I | In layer model.layers.1, gradient of model.layers.1.self_attn.q_proj has inf: False
25-01-23 10:48:14 | I | In layer model.layers.1, gradient of model.layers.1.self_attn.k_proj has nan: False
25-01-23 10:48:14 | I | In layer model.layers.1, gradient of model.layers.1.self_attn.k_proj has inf: False
25-01-23 10:48:14 | I | In layer model.layers.1, gradient of model.layers.1.self_attn.v_proj has nan: False
25-01-23 10:48:14 | I | In layer model.layers.1, gradient of model.layers.1.self_attn.v_proj has inf: True
25-01-23 10:48:14 | I | inf: first position tensor([1065, 1512], device='cuda:0')
25-01-23 10:48:14 | I | In layer model.layers.1, gradient of model.layers.1.self_attn.o_proj has nan: False
25-01-23 10:48:14 | I | In layer model.layers.1, gradient of model.layers.1.self_attn.o_proj has inf: False
25-01-23 10:48:14 | I | In layer model.layers.1, gradient of model.layers.1.mlp.up_proj has nan: False
25-01-23 10:48:14 | I | In layer model.layers.1, gradient of model.layers.1.mlp.up_proj has inf: False
25-01-23 10:48:14 | I | In layer model.layers.1, gradient of model.layers.1.mlp.gate_proj has nan: False
25-01-23 10:48:14 | I | In layer model.layers.1, gradient of model.layers.1.mlp.gate_proj has inf: False
25-01-23 10:48:14 | I | In layer model.layers.1, gradient of model.layers.1.mlp.down_proj has nan: False
25-01-23 10:48:14 | I | In layer model.layers.1, gradient of model.layers.1.mlp.down_proj has inf: False
25-01-23 10:48:14 | I | Detected nan/inf grad norm, dumping norms...
25-01-23 10:48:14 | I | norms: {'decoder.model.self_attn.q_proj.weight': 16931.265625, 'decoder.model.self_attn.k_proj.weight': 7973.09912109375, 'decoder.model.self_attn.v_proj.weight': inf, 'decoder.model.self_attn.o_proj.weight': 42218.05859375, 'decoder.model.mlp.gate_proj.weight': 2916.322021484375, 'decoder.model.mlp.up_proj.weight': 3716.895751953125, 'decoder.model.mlp.down_proj.weight': 4510.90771484375, 'decoder.model.input_layernorm.weight': 36532.06640625, 'decoder.model.post_attention_layernorm.weight': 16528.345703125}
25-01-23 10:48:14 | I | gradients: {'decoder.model.self_attn.v_proj.weight': tensor([[-11.0469,   1.5918, -20.4688,  ...,  -4.8242,   3.1426,   8.1406],
        [  1.9600,  -0.2095,   3.9141,  ...,   0.8374,  -0.9033,  -1.5537],
        [  4.8164,  -0.7910,   9.4062,  ...,   1.9268,  -1.8818,  -3.5684],
        ...,
        [ -6.6367,   0.9287, -14.1172,  ...,  -0.9048,   3.7266,   3.2637],
        [ 12.9297,  -1.6943,  26.3281,  ...,   3.1133,  -6.5195,  -6.5039],
        [ -3.1016,   0.3772,  -6.2148,  ...,  -0.9727,   1.4980,   1.6377]],
       device='cuda:0', dtype=torch.float16)}
